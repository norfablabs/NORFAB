{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>Through lifting others we rise </p>"},{"location":"#network-automations-fabric-norfab","title":"Network Automations Fabric (NorFab)","text":"<p>NorFab is the platform that unifies, simplifies, and accelerates network automation. Designed for engineers who want to move fast and deliver results, NorFab eliminates the friction of integrating disparate tools and empowers you to automate with confidence.</p>"},{"location":"#why-norfab","title":"Why NorFab?","text":"<p>Network automation is essential, but the landscape is fragmented. Too many tools, too much glue code, and not enough time. NorFab solves this by providing a cohesive automation fabric\u2014integrating best-in-class tools, delivering ready-to-use solutions, and letting you focus on outcomes, not plumbing.</p> <ul> <li>No more glue code: Integrate tools and workflows seamlessly.</li> <li>Accelerate delivery: Use batteries-included, use-case-driven implementations.</li> <li>Future-proof: Adapt and scale as your network evolves.</li> </ul>"},{"location":"#what-is-norfab","title":"What is NorFab?","text":"<p>NorFab is a distributed task automation and orchestration framework built for real-world network operations.</p> <ul> <li>Run Anywhere: Laptop, server, container, or cloud\u2014centralized or distributed.</li> <li>Feature-Rich: Lightweight, powerful, and cost-effective.</li> <li>Empowering: Unlock your potential to automate and manage modern networks.</li> <li>Integrate Everything: Python API, REST API, and CLI for seamless integration.</li> <li>Model-Driven: Pydantic models ensure robust validation and documentation.</li> <li>Automate Anything: From simple tasks to complex workflows\u2014NorFab handles it all.</li> </ul>"},{"location":"#how-norfab-works","title":"How NorFab Works","text":"<p>NorFab\u2019s architecture is designed for flexibility, scalability, and reliability:</p> <ul> <li>Clients: Submit jobs and consume services.</li> <li>Broker: Central hub that routes jobs and manages services.</li> <li>Services: Logical groupings of workers managing specific resources.</li> <li>Workers: Resource proxies that execute tasks anywhere.</li> <li>Resources: Devices, databases, filesystems\u2014anything you need to manage.</li> </ul> <p>Workflow: 1. Clients submit jobs to the broker. 2. Broker distributes jobs to the appropriate service workers. 3. Workers execute tasks and return results.</p> <p>Services are hosted by Workers and accessed by Clients via the Broker.</p> <p></p>"},{"location":"#get-started","title":"Get Started","text":"<ul> <li>Jump into the Getting Started Tutorial</li> <li>Discover Why NorFab is Different</li> <li>Ready to transform your network automation? Contact us for more information or a demo.</li> </ul>"},{"location":"api_reference_clients_nfcli_client/","title":"NFCLI Client API","text":"","tags":["nfcli"]},{"location":"api_reference_clients_nfcli_client/#norfab.clients.picle_shell_client--picle-shell-client","title":"PICLE Shell CLient","text":"<p>Client that implements interactive shell to work with NorFab.</p>","tags":["nfcli"]},{"location":"api_reference_clients_nfcli_client/#norfab.clients.picle_shell_client.FileServiceCommands","title":"<code>FileServiceCommands</code>","text":"<p>               Bases: <code>BaseModel</code></p>","tags":["nfcli"]},{"location":"api_reference_clients_nfcli_client/#norfab.clients.picle_shell_client.FileServiceCommands--sample-usage","title":"Sample Usage","text":"","tags":["nfcli"]},{"location":"api_reference_clients_nfcli_client/#norfab.clients.picle_shell_client.FileServiceCommands--copy","title":"copy","text":"<p>Copy to client's fetched files directory:</p> <p><code>file copy_ url nf://cli/commands.txt</code></p> <p>Copy file to destination relative to current directory</p> <p><code>file copy_ url nf://cli/commands.txt destination commands.txt</code></p>","tags":["nfcli"]},{"location":"api_reference_clients_nfcli_client/#norfab.clients.picle_shell_client.FileServiceCommands--list","title":"list","text":"<p>List files at broker root directory:</p> <p><code>file list file list url nf://</code></p> <p>List files details:</p> <pre><code>file details\nfile details url nf://\n</code></pre>","tags":["nfcli"]},{"location":"api_reference_clients_nfcli_client/#norfab.clients.picle_shell_client.NorFabShell","title":"<code>NorFabShell</code>","text":"<p>               Bases: <code>BaseModel</code></p>","tags":["nfcli"]},{"location":"api_reference_clients_nfcli_client/#norfab.clients.picle_shell_client.NorFabShell.cmd_preloop_override","title":"<code>cmd_preloop_override()</code>  <code>classmethod</code>","text":"<p>This method called before CMD loop starts</p> Source code in <code>norfab\\clients\\picle_shell_client.py</code> <pre><code>@classmethod\ndef cmd_preloop_override(self):\n    \"\"\"This method called before CMD loop starts\"\"\"\n    pass\n</code></pre>","tags":["nfcli"]},{"location":"api_reference_clients_nfcli_client/#norfab.clients.picle_shell_client.mount_shell_plugins","title":"<code>mount_shell_plugins(shell: App, inventory: object) -&gt; None</code>","text":"<p>Mounts shell plugins to the given shell application.</p> <p>This function iterates over the plugins in the inventory and mounts those that have an \"nfcli\" configuration to the shell application.</p> <p>Parameters:</p> Name Type Description Default <code>shell</code> <code>App</code> <p>The shell application to which the plugins will be mounted.</p> required <code>inventory</code> <code>object</code> <p>An object containing the plugins to be mounted.                 It should have an attribute <code>plugins</code> which is a dictionary                 where keys are service names and values are service data dictionaries.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>norfab\\clients\\picle_shell_client.py</code> <pre><code>def mount_shell_plugins(shell: App, inventory: object) -&gt; None:\n    \"\"\"\n    Mounts shell plugins to the given shell application.\n\n    This function iterates over the plugins in the inventory and mounts\n    those that have an \"nfcli\" configuration to the shell application.\n\n    Args:\n        shell (App): The shell application to which the plugins will be mounted.\n        inventory (object): An object containing the plugins to be mounted.\n                            It should have an attribute `plugins` which is a dictionary\n                            where keys are service names and values are service data dictionaries.\n\n    Returns:\n        None\n    \"\"\"\n    for service_name, service_data in inventory.plugins.items():\n        if service_data.get(\"nfcli\"):\n            plugin = inventory.load_plugin(service_name)\n            shell.model_mount(\n                path=plugin[service_name][\"nfcli\"][\"mount_path\"],\n                model=plugin[service_name][\"nfcli\"][\"shell_model\"],\n            )\n\n    # mount MAN commands\n    shell.model_mount(\n        path=[\"man\", \"tasks\"],\n        model=ManTasks,\n        description=\"SHow NorFab services tasks documentation\",\n    )\n</code></pre>","tags":["nfcli"]},{"location":"api_reference_clients_robot_client/","title":"ROBOT Client API","text":"Source code in <code>norfab\\clients\\robot_client.py</code> <pre><code>def __init__(\n    self,\n    inventory=\"./inventory.yaml\",\n    log_level=\"WARNING\",\n):\n    self.ROBOT_LIBRARY_LISTENER = self\n\n    # initiate NorFab\n    self.nf = NorFab(inventory=inventory, log_level=log_level)\n    self.nf.start()\n    self.client = self.nf.make_client()\n</code></pre>","tags":["robot"]},{"location":"api_reference_clients_robot_client/#norfab.clients.robot_client.NorFabRobot.workers","title":"<code>workers(*args, **kwargs)</code>","text":"<p>Collect workers to target</p> Source code in <code>norfab\\clients\\robot_client.py</code> <pre><code>@keyword(\"Workers\")\ndef workers(self, *args, **kwargs):\n    \"\"\"Collect workers to target\"\"\"\n    if args:\n        DATA[\"workers\"] = args\n    else:\n        DATA[\"workers\"] = kwargs.pop(\"workers\", \"all\")\n</code></pre>","tags":["robot"]},{"location":"api_reference_clients_robot_client/#norfab.clients.robot_client.NorFabRobot.hosts","title":"<code>hosts(*args, **kwargs)</code>","text":"<p>Collect hosts to target</p> Source code in <code>norfab\\clients\\robot_client.py</code> <pre><code>@keyword(\"Hosts\")\ndef hosts(self, *args, **kwargs):\n    \"\"\"Collect hosts to target\"\"\"\n    if args:\n        DATA[\"hosts\"] = {\"FB\": \", \".join(args), **kwargs}\n    else:\n        DATA[\"hosts\"] = kwargs\n</code></pre>","tags":["robot"]},{"location":"api_reference_clients_robot_client/#norfab.clients.robot_client.NorFabRobot.nr_test","title":"<code>nr_test(*args, **kwargs)</code>","text":"<p>Run nr.test  task</p> Source code in <code>norfab\\clients\\robot_client.py</code> <pre><code>@keyword(\"nr.test\")\ndef nr_test(self, *args, **kwargs):\n    \"\"\"Run nr.test  task\"\"\"\n    tests_pass = 0\n    tests_fail = 0\n    tests_results = []\n    commands_output = {}\n    if args:\n        kwargs[\"suite\"] = args[0]\n    kwargs = {\n        **DATA.pop(\"hosts\", {\"FB\": \"*\"}),\n        **kwargs,\n        \"remove_tasks\": False,\n        \"add_details\": True,\n        \"return_tests_suite\": True,\n        \"to_dict\": False,\n    }\n    logger.info(f\"Running nr.test with kwargs '{kwargs}', global DATA '{DATA}'\")\n    has_errors = False\n    # run this function\n    ret = self.client.run_job(\n        service=\"nornir\",\n        task=\"test\",\n        workers=DATA.get(\"workers\", \"all\"),\n        kwargs=kwargs,\n    )\n    # iterate over results and log tests and task statuses\n    for worker, worker_results in ret.items():\n        for result in worker_results[\"result\"][\"test_results\"]:\n            host = result[\"host\"]\n            # evaluate and log test result\n            if \"success\" in result:\n                if (\n                    result[\"failed\"]\n                    or result[\"exception\"]\n                    or not result[\"success\"]\n                    or \"traceback\" in str(result[\"result\"]).lower()\n                ):\n                    tests_fail += 1\n                    has_errors = True\n                    logger.error(\n                        (\n                            f'{worker} worker, {host} test \"{result[\"name\"]}\" - '\n                            f'&lt;span style=\"background-color: #CE3E01\"&gt;\"{result[\"exception\"]}\"&lt;/span&gt;'\n                        ),\n                        html=True,\n                    )\n                else:\n                    tests_pass += 1\n                    logger.info(\n                        (\n                            f'{worker} worker, {host} test \"{result[\"name\"]}\" - '\n                            f'&lt;span style=\"background-color: #97BD61\"&gt;success&lt;/span&gt;'\n                        ),\n                        html=True,\n                    )\n                # save test results to log them later\n                tests_results.append({\"worker\": worker, **result})\n            # evaluate and log task result\n            else:\n                # log exception for task\n                if result[\"failed\"] or result[\"exception\"]:\n                    has_errors = True\n                    logger.error(\n                        (\n                            f'{worker} worker, {host} task \"{result[\"name\"]}\" - '\n                            f'&lt;span style=\"background-color: #CE3E01\"&gt;\"{result[\"exception\"].strip()}\"&lt;/span&gt;'\n                        ),\n                        html=True,\n                    )\n                # save device commands output to log it later\n                commands_output.setdefault(host, {})\n                commands_output[host][result[\"name\"]] = result[\"result\"]\n    # clear global state to prep for next test\n    clean_global_data()\n\n    tests_results_html_table = TabulateFormatter(\n        tests_results,\n        tabulate={\"tablefmt\": \"html\"},\n        headers=[\n            \"worker\",\n            \"host\",\n            \"name\",\n            \"result\",\n            \"failed\",\n            \"task\",\n            \"test\",\n            \"criteria\",\n            \"exception\",\n        ],\n    )\n\n    tests_results_csv_table = [\n        f'''\"{i['worker']}\",\"{i['host']}\",\"{i['name']}\",\"{i['result']}\",\"{i['failed']}\",\"{i['task']}\",\"{i['test']}\",\"{i['criteria']}\",\"{i['exception']}\"'''\n        for i in tests_results\n    ]\n    tests_results_csv_table.insert(\n        0,\n        '\"worker\",\"host\",\"name\",\"result\",\"failed\",\"task\",\"test\",\"criteria\",\"exception\"',\n    )\n    tests_results_csv_table = \"\\n\".join(tests_results_csv_table)\n\n    # form nested HTML of commands output\n    devices_output_html = []\n    for host in sorted(commands_output.keys()):\n        commands = commands_output[host]\n        commands_output_html = []\n        for command, result in commands.items():\n            commands_output_html.append(\n                f'&lt;p&gt;&lt;details style=\"margin-left:20px;\"&gt;&lt;summary&gt;{command}&lt;/summary&gt;&lt;p style=\"margin-left:20px;\"&gt;&lt;font face=\"courier new\"&gt;{result}&lt;/font&gt;&lt;/p&gt;&lt;/details&gt;&lt;/p&gt;'\n            )\n        devices_output_html.append(\n            f'&lt;p&gt;&lt;details&gt;&lt;summary&gt;{host} ({len(commands_output_html)} commands)&lt;/summary&gt;&lt;p&gt;{\"\".join(commands_output_html)}&lt;/p&gt;&lt;/details&gt;&lt;/p&gt;'\n        )\n\n    # form nested HTML for devices tes suite\n    devices_test_suite = []\n    for worker, worker_results in ret.items():\n        for host in sorted(worker_results[\"result\"][\"suite\"].keys()):\n            suite_content = worker_results[\"result\"][\"suite\"][host]\n            devices_test_suite.append(\n                f'&lt;p&gt;&lt;details&gt;&lt;summary&gt;{host} ({len(suite_content)} tests)&lt;/summary&gt;&lt;p style=\"margin-left:20px;\"&gt;{yaml.dump(suite_content, default_flow_style=False)}&lt;/p&gt;&lt;/details&gt;&lt;/p&gt;'\n            )\n\n    logger.info(\n        f\"&lt;details&gt;&lt;summary&gt;Workers results&lt;/summary&gt;{pprint.pformat(ret)}&lt;/details&gt;\",\n        html=True,\n    )\n    logger.info(\n        f\"&lt;details&gt;&lt;summary&gt;Test suite results details&lt;/summary&gt;&lt;p&gt;{tests_results_html_table}&lt;/p&gt;&lt;/details&gt;\",\n        html=True,\n    )\n    logger.info(\n        f\"&lt;details&gt;&lt;summary&gt;Test suite results CSV table&lt;/summary&gt;&lt;p&gt;{tests_results_csv_table}&lt;/p&gt;&lt;/details&gt;\",\n        html=True,\n    )\n    logger.info(\n        f\"&lt;details&gt;&lt;summary&gt;Devices tests suites content&lt;/summary&gt;{''.join(devices_test_suite)}&lt;/details&gt;\",\n        html=True,\n    )\n    logger.info(\n        f\"&lt;details&gt;&lt;summary&gt;Collected devices output&lt;/summary&gt;{''.join(devices_output_html)}&lt;/details&gt;\",\n        html=True,\n    )\n    logger.info(\n        (\n            f\"Tests completed - {tests_pass + tests_fail}, \"\n            f'&lt;span style=\"background-color: #97BD61\"&gt;success - {tests_pass}&lt;/span&gt;, '\n            f'&lt;span style=\"background-color: #CE3E01\"&gt;failed - {tests_fail}&lt;/span&gt;'\n        ),\n        html=True,\n    )\n\n    # raise if has errors\n    if has_errors:\n        raise ContinuableFailure(\"Tests failed\")\n    # return test results with no errors in structured format\n    return ret\n</code></pre>","tags":["robot"]},{"location":"api_reference_clients_robot_client/#norfab.clients.robot_client.NorFabRobot.nr_cli","title":"<code>nr_cli(*args, **kwargs)</code>","text":"<p>Run Nornir service cli task</p> Source code in <code>norfab\\clients\\robot_client.py</code> <pre><code>@keyword(\"nr.cli\")\ndef nr_cli(self, *args, **kwargs):\n    \"\"\"Run Nornir service cli task\"\"\"\n    log.info(\n        f\"Running nr.cli with args '{args}', kwargs '{kwargs}', global DATA '{DATA}'\"\n    )\n    has_errors = False\n    if args:\n        kwargs[\"commands\"] = args\n    kwargs = {\n        **DATA.pop(\"hosts\", {\"FB\": \"*\"}),\n        **kwargs,\n        \"add_details\": True,\n        \"to_dict\": False,\n    }\n    # run this function\n    ret = self.client.run_job(\n        service=\"nornir\",\n        task=\"cli\",\n        workers=DATA.get(\"workers\", \"all\"),\n        kwargs=kwargs,\n    )\n    # extract results for the host\n    for worker, worker_results in ret.items():\n        for result in worker_results[\"result\"]:\n            host = result[\"host\"]\n            # evaluate and log results\n            if (\n                result[\"failed\"]\n                or result[\"exception\"]\n                or \"traceback\" in str(result[\"result\"]).lower()\n            ):\n                has_errors = True\n                logger.error(\n                    (\n                        f'&lt;details&gt;&lt;summary&gt;{worker} worker, {host} device, command \"{result[\"name\"]}\" failed - '\n                        f'&lt;span style=\"background-color: #CE3E01\"&gt;\"{result[\"exception\"]}\"&lt;/span&gt;&lt;/summary&gt;'\n                        f'&lt;p style=\"margin-left:20px;\"&gt;&lt;font face=\"courier new\"&gt;{result[\"result\"]}'\n                        f\"&lt;/font&gt;&lt;/p&gt;&lt;/details&gt;\"\n                    ),\n                    html=True,\n                )\n            else:\n                logger.info(\n                    (\n                        f'&lt;details&gt;&lt;summary&gt;{worker} worker, {host} device, command \"{result[\"name\"]}\" - '\n                        f'&lt;span style=\"background-color: #97BD61\"&gt;success&lt;/span&gt;&lt;/summary&gt;'\n                        f'&lt;p style=\"margin-left:20px;\"&gt;&lt;font face=\"courier new\"&gt;{result[\"result\"]}'\n                        f\"&lt;/font&gt;&lt;/p&gt;&lt;/details&gt;\"\n                    ),\n                    html=True,\n                )\n    logger.info(\n        f\"&lt;details&gt;&lt;summary&gt;Workers results&lt;/summary&gt;{pprint.pformat(ret)}&lt;/details&gt;\",\n        html=True,\n    )\n    # clean global state to prep for next test\n    clean_global_data()\n    # raise exception if cli command failed\n    if has_errors:\n        raise ContinuableFailure(ret)\n    # return ret with no errors in structured format\n    return ret\n</code></pre>","tags":["robot"]},{"location":"api_reference_clients_robot_client/#norfab.clients.robot_client.NorFabRobot.nr_cfg","title":"<code>nr_cfg(*args, **kwargs)</code>","text":"<p>Run Nornir service cfg task</p> Source code in <code>norfab\\clients\\robot_client.py</code> <pre><code>@keyword(\"nr.cfg\")\ndef nr_cfg(self, *args, **kwargs):\n    \"\"\"Run Nornir service cfg task\"\"\"\n    log.info(\n        f\"Running nr.cfg with args '{args}', kwargs '{kwargs}', global DATA '{DATA}'\"\n    )\n    if args:\n        kwargs[\"config\"] = args\n    kwargs = {\n        **DATA.pop(\"hosts\", {\"FB\": \"*\"}),\n        **kwargs,\n        \"add_details\": True,\n        \"to_dict\": False,\n    }\n    has_errors = False\n    # run this function\n    ret = self.client.run_job(\n        service=\"nornir\",\n        task=\"cfg\",\n        workers=DATA.get(\"workers\", \"all\"),\n        kwargs=kwargs,\n    )\n    # extract results for the host\n    for worker, worker_results in ret.items():\n        for result in worker_results[\"result\"]:\n            host = result[\"host\"]\n            # evaluate and log results\n            if (\n                result[\"failed\"]\n                or result[\"exception\"]\n                or \"traceback\" in str(result[\"result\"]).lower()\n            ):\n                has_errors = True\n                logger.error(\n                    (\n                        f'&lt;details&gt;&lt;summary&gt;{worker} worker, {host} device, \"{result[\"name\"]}\" failed - '\n                        f'&lt;span style=\"background-color: #CE3E01\"&gt;\"{result[\"exception\"]}\"&lt;/span&gt;&lt;/summary&gt;'\n                        f'&lt;p style=\"margin-left:20px;\"&gt;&lt;font face=\"courier new\"&gt;{result[\"result\"]}'\n                        f\"&lt;/font&gt;&lt;/p&gt;&lt;/details&gt;\"\n                    ),\n                    html=True,\n                )\n            else:\n                logger.info(\n                    (\n                        f'&lt;details&gt;&lt;summary&gt;{worker} worker, {host} device, \"{result[\"name\"]}\" - '\n                        f'&lt;span style=\"background-color: #97BD61\"&gt;success&lt;/span&gt;&lt;/summary&gt;'\n                        f'&lt;p style=\"margin-left:20px;\"&gt;&lt;font face=\"courier new\"&gt;{result[\"result\"]}'\n                        f\"&lt;/font&gt;&lt;/p&gt;&lt;/details&gt;\"\n                    ),\n                    html=True,\n                )\n    logger.info(\n        f\"&lt;details&gt;&lt;summary&gt;Workers results&lt;/summary&gt;{pprint.pformat(ret)}&lt;/details&gt;\",\n        html=True,\n    )\n    # clean global state to prep for next test\n    clean_global_data()\n    # raise exception if cli command failed\n    if has_errors:\n        raise ContinuableFailure(ret)\n    # return ret with no errors in structured format\n    return ret\n</code></pre>","tags":["robot"]},{"location":"api_reference_core_norfab_broker/","title":"Broker","text":""},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPService","title":"<code>NFPService(name: str)</code>","text":"<p>               Bases: <code>object</code></p> <p>A single NFP Service</p> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def __init__(self, name: str):\n    self.name = name  # Service name\n    self.workers = []  # list of known workers\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPWorker","title":"<code>NFPWorker(address: str, socket: Any, socket_lock: Any, multiplier: int, keepalive: int, service: Optional[NFPService] = None)</code>","text":"<p>               Bases: <code>object</code></p> <p>An NFP Worker convenience class.</p> <p>Attributes:</p> Name Type Description <code>service</code> <code>NFPService</code> <p>The service instance.</p> <code>ready</code> <code>bool</code> <p>Indicates if the worker is ready.</p> <code>exit_event</code> <code>Event</code> <p>Event to signal exit.</p> <code>keepalive</code> <code>int</code> <p>Keepalive interval in milliseconds.</p> <code>multiplier</code> <code>int</code> <p>Multiplier value.</p> <p>Methods:</p> Name Description <code>start_keepalives</code> <p>Starts the keepalive process for the worker.</p> <code>is_ready</code> <p>Checks if the worker has signaled W.READY.</p> <code>destroy</code> <p>Cleans up the worker, optionally disconnecting it.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>Address to route to.</p> required <code>socket</code> <code>Any</code> <p>The socket object used for communication.</p> required <code>socket_lock</code> <code>Any</code> <p>The lock object to synchronize socket access.</p> required <code>multiplier</code> <code>int</code> <p>Multiplier value, e.g., 6 times.</p> required <code>keepalive</code> <code>int</code> <p>Keepalive interval in milliseconds, e.g., 5000 ms.</p> required <code>service</code> <code>NFPService</code> <p>The service instance. Defaults to None.</p> <code>None</code> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    socket: Any,\n    socket_lock: Any,\n    multiplier: int,  # e.g. 6 times\n    keepalive: int,  # e.g. 5000 ms\n    service: Optional[NFPService] = None,\n):\n    self.address = address  # Address to route to\n    self.service = service\n    self.ready = False\n    self.socket = socket\n    self.exit_event = threading.Event()\n    self.keepalive = keepalive\n    self.multiplier = multiplier\n    self.socket_lock = socket_lock\n    self.build_message = NFP.MessageBuilder()\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPWorker.is_ready","title":"<code>is_ready() -&gt; bool</code>","text":"<p>Check if the worker is ready.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the worker has signaled readiness (W.READY) and the service is not None, otherwise False.</p> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def is_ready(self) -&gt; bool:\n    \"\"\"\n    Check if the worker is ready.\n\n    Returns:\n        bool: True if the worker has signaled readiness (W.READY) and the service is not None, otherwise False.\n    \"\"\"\n    return self.service is not None and self.ready is True\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPWorker.destroy","title":"<code>destroy(disconnect: bool = False) -&gt; None</code>","text":"<p>Clean up routine for the worker.</p> <p>This method performs the following actions:</p> <ol> <li>Sets the exit event to signal termination.</li> <li>Stops the keepaliver if it exists.</li> <li>Removes the current worker from the service's worker list.</li> <li>Optionally sends a disconnect message to the broker if <code>disconnect</code> is True.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>disconnect</code> <code>bool</code> <p>If True, sends a disconnect message to the broker.</p> <code>False</code> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def destroy(self, disconnect: bool = False) -&gt; None:\n    \"\"\"\n    Clean up routine for the worker.\n\n    This method performs the following actions:\n\n    1. Sets the exit event to signal termination.\n    2. Stops the keepaliver if it exists.\n    3. Removes the current worker from the service's worker list.\n    4. Optionally sends a disconnect message to the broker if `disconnect` is True.\n\n    Args:\n        disconnect (bool): If True, sends a disconnect message to the broker.\n    \"\"\"\n    self.exit_event.set()\n    if hasattr(self, \"keepaliver\"):\n        self.keepaliver.stop()\n    if self.service is not None:\n        self.service.workers.remove(self)\n\n    if disconnect is True and self.service is not None:\n        msg = self.build_message.broker_to_worker_disconnect(\n            worker_address=self.address, service=self.service.name\n        )\n        with self.socket_lock:\n            self.socket.send_multipart(msg)\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker","title":"<code>NFPBroker(endpoint: str, exit_event: Event, inventory: NorFabInventory, log_level: str = None, log_queue: object = None, multiplier: int = 6, keepalive: int = 2500, init_done_event: Event = None)</code>","text":"<p>Attributes:</p> Name Type Description <code>private_keys_dir</code> <code>str</code> <p>Directory for private keys.</p> <code>public_keys_dir</code> <code>str</code> <p>Directory for public keys.</p> <code>broker_private_key_file</code> <code>str</code> <p>File path for broker's private key.</p> <code>broker_public_key_file</code> <code>str</code> <p>File path for broker's public key.</p> <code>keepalive</code> <code>int</code> <p>The keepalive interval.</p> <code>multiplier</code> <code>int</code> <p>The multiplier value.</p> <code>services</code> <code>dict</code> <p>A dictionary to store services.</p> <code>workers</code> <code>dict</code> <p>A dictionary to store workers.</p> <code>exit_event</code> <code>Event</code> <p>The event to signal the broker to exit.</p> <code>inventory</code> <code>NorFabInventory</code> <p>The inventory object.</p> <code>base_dir</code> <code>str</code> <p>The base directory path from the inventory.</p> <code>broker_base_dir</code> <code>str</code> <p>The broker's base directory path.</p> <code>ctx</code> <code>Context</code> <p>The ZeroMQ context.</p> <code>auth</code> <code>ThreadAuthenticator</code> <p>The authenticator for the ZeroMQ context.</p> <code>socket</code> <code>Socket</code> <p>The ZeroMQ socket.</p> <code>poller</code> <code>Poller</code> <p>The ZeroMQ poller.</p> <code>socket_lock</code> <code>Lock</code> <p>The lock to protect the socket object.</p> <p>Methods:</p> Name Description <code>setup_logging</code> <p>str) -&gt; None: Method to apply logging configuration.</p> <code>mediate</code> <p>Main broker work happens here.</p> <code>destroy</code> <p>Disconnect all workers, destroy context.</p> <code>delete_worker</code> <p>Deletes worker from all data structures, and deletes worker.</p> <code>purge_workers</code> <p>Look for &amp; delete expired workers.</p> <code>send_to_worker</code> <p>NFPWorker, command: bytes, sender: bytes, uuid: bytes, data: bytes): Send message to worker. If message is provided, sends that message.</p> <code>send_to_client</code> <p>str, command: str, service: str, message: list): Send message to client.</p> <code>process_worker</code> <p>Process message received from worker.</p> <code>require_worker</code> <p>Finds the worker, creates if necessary.</p> <code>require_service</code> <p>Locates the service (creates if necessary).</p> <code>process_client</code> <p>Process a request coming from a client.</p> <code>filter_workers</code> <p>bytes, service: NFPService) -&gt; list: Helper function to filter workers.</p> <code>dispatch</code> <p>Dispatch requests to waiting workers as possible.</p> <code>mmi_service</code> <p>Handle internal service according to 8/MMI specification.</p> <code>inventory_service</code> <p>Handle inventory service requests.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The endpoint address for the broker to bind to.</p> required <code>exit_event</code> <code>Event</code> <p>An event to signal the broker to exit.</p> required <code>inventory</code> <code>NorFabInventory</code> <p>The inventory object containing configuration and state.</p> required <code>log_level</code> <code>str</code> <p>The logging level. Defaults to None.</p> <code>None</code> <code>log_queue</code> <code>object</code> <p>The logging queue. Defaults to None.</p> <code>None</code> <code>multiplier</code> <code>int</code> <p>A multiplier value for internal use. Defaults to 6.</p> <code>6</code> <code>keepalive</code> <code>int</code> <p>The keepalive interval in milliseconds. Defaults to 2500.</p> <code>2500</code> <code>init_done_event</code> <code>Event</code> <p>An event to signal that initialization is done. Defaults to None.</p> <code>None</code> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def __init__(\n    self,\n    endpoint: str,\n    exit_event: Event,\n    inventory: NorFabInventory,\n    log_level: str = None,\n    log_queue: object = None,\n    multiplier: int = 6,\n    keepalive: int = 2500,\n    init_done_event: Event = None,\n):\n    self.setup_logging(log_queue, log_level)\n    self.keepalive = keepalive\n    self.multiplier = multiplier\n    init_done_event = init_done_event or Event()\n\n    self.services = {}\n    self.workers = {}\n    self.build_message = NFP.MessageBuilder()\n    self.exit_event = exit_event\n    self.inventory = inventory\n    self.zmq_auth = self.inventory.broker.get(\"zmq_auth\", True)\n\n    self.base_dir = self.inventory.base_dir\n    self.broker_base_dir = os.path.join(\n        self.base_dir, \"__norfab__\", \"files\", \"broker\"\n    )\n    os.makedirs(self.base_dir, exist_ok=True)\n    os.makedirs(self.broker_base_dir, exist_ok=True)\n\n    self.ctx = zmq.Context()\n    self.socket = self.ctx.socket(zmq.ROUTER)\n    self.socket.linger = (\n        0  # discard pending messages immediately when the socket is closed\n    )\n    self.socket.setsockopt(\n        zmq.ROUTER_HANDOVER, 1\n    )  # allow worker/client reconnect using same identity\n\n    # generate certificates, create directories and load certs\n    if self.zmq_auth is not False:\n        generate_certificates(\n            self.broker_base_dir, cert_name=\"broker\", inventory=inventory\n        )\n        self.private_keys_dir = os.path.join(self.broker_base_dir, \"private_keys\")\n        self.public_keys_dir = os.path.join(self.broker_base_dir, \"public_keys\")\n        self.broker_private_key_file = os.path.join(\n            self.private_keys_dir, \"broker.key_secret\"\n        )\n        self.broker_public_key_file = os.path.join(\n            self.public_keys_dir, \"broker.key\"\n        )\n        server_public, server_secret = zmq.auth.load_certificate(\n            self.broker_private_key_file\n        )\n\n        # Start an authenticator for this context.\n        self.auth = ThreadAuthenticator(self.ctx)\n        self.auth.start()\n        # self.auth.allow(\"0.0.0.0\")\n        self.auth.allow_any = True\n        # Tell the authenticator how to handle CURVE requests\n        self.auth.configure_curve(location=zmq.auth.CURVE_ALLOW_ANY)\n        self.socket.curve_secretkey = server_secret\n        self.socket.curve_publickey = server_public\n        self.socket.curve_server = True  # must come before bind\n\n    self.poller = zmq.Poller()\n    self.poller.register(self.socket, zmq.POLLIN)\n    self.socket.bind(endpoint)\n    self.socket_lock = (\n        threading.Lock()\n    )  # used for keepalives to protect socket object\n\n    init_done_event.set()  # signal finished initializing broker\n    log.debug(f\"NFPBroker - is ready and listening on {endpoint}\")\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker.setup_logging","title":"<code>setup_logging(log_queue, log_level: str) -&gt; None</code>","text":"<p>Configures logging for the application.</p> <p>This method sets up the logging configuration using a provided log queue and log level. It updates the logging configuration dictionary with the given log queue and log level, and then applies the configuration using <code>logging.config.dictConfig</code>.</p> <p>Parameters:</p> Name Type Description Default <code>log_queue</code> <code>Queue</code> <p>The queue to be used for logging.</p> required <code>log_level</code> <code>str</code> <p>The logging level to be set. If None, the default level is used.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def setup_logging(self, log_queue, log_level: str) -&gt; None:\n    \"\"\"\n    Configures logging for the application.\n\n    This method sets up the logging configuration using a provided log queue and log level.\n    It updates the logging configuration dictionary with the given log queue and log level,\n    and then applies the configuration using `logging.config.dictConfig`.\n\n    Args:\n        log_queue (queue.Queue): The queue to be used for logging.\n        log_level (str): The logging level to be set. If None, the default level is used.\n\n    Returns:\n        None\n    \"\"\"\n    logging_config_producer[\"handlers\"][\"queue\"][\"queue\"] = log_queue\n    if log_level is not None:\n        logging_config_producer[\"root\"][\"level\"] = log_level\n    logging.config.dictConfig(logging_config_producer)\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker.mediate","title":"<code>mediate()</code>","text":"<p>Main broker work happens here.</p> <p>This method continuously polls for incoming messages and processes them based on their headers. It handles messages from clients and workers, and purges inactive workers periodically. The method also checks for an exit event to gracefully shut down the broker.</p> <p>Raises:</p> Type Description <code>KeyboardInterrupt</code> <p>If the process is interrupted by a keyboard signal.</p> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def mediate(self):\n    \"\"\"\n    Main broker work happens here.\n\n    This method continuously polls for incoming messages and processes them\n    based on their headers. It handles messages from clients and workers,\n    and purges inactive workers periodically. The method also checks for an\n    exit event to gracefully shut down the broker.\n\n    Raises:\n        KeyboardInterrupt: If the process is interrupted by a keyboard signal.\n    \"\"\"\n    while True:\n        try:\n            items = self.poller.poll(1000)\n        except KeyboardInterrupt:\n            break  # Interrupted\n\n        if items:\n            with self.socket_lock:\n                msg = self.socket.recv_multipart()\n            log.debug(f\"NFPBroker - received '{msg}'\")\n\n            if len(msg) &lt; 3:\n                log.error(f\"NFPBroker - received malformed message: {msg}\")\n                continue\n\n            sender = msg.pop(0)\n            empty = msg.pop(0)  # noqa\n            header = msg.pop(0)\n\n            if header == NFP.CLIENT:\n                self.process_client(sender, msg)\n            elif header == NFP.WORKER:\n                self.process_worker(sender, msg)\n            else:\n                log.error(\n                    f\"NFPBroker - message from '{sender}' contains unsupported header '{header}'\"\n                )\n\n        self.purge_workers()\n\n        # check if need to stop\n        if self.exit_event.is_set():\n            self.destroy()\n            break\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker.destroy","title":"<code>destroy()</code>","text":"<p>Disconnect all workers and destroy the context.</p> <p>This method performs the following actions:</p> <ol> <li>Logs an interrupt message indicating that the broker is being killed.</li> <li>Iterates through all</li> </ol> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def destroy(self):\n    \"\"\"\n    Disconnect all workers and destroy the context.\n\n    This method performs the following actions:\n\n    1. Logs an interrupt message indicating that the broker is being killed.\n    2. Iterates through all\n    \"\"\"\n    log.info(\"NFPBroker - interrupt received, killing broker\")\n    for name in list(self.workers.keys()):\n        # in case worker self destroyed while we iterating\n        if self.workers.get(name):\n            self.delete_worker(self.workers[name], True)\n    if self.zmq_auth is not False:\n        self.auth.stop()\n    self.ctx.destroy(0)\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker.delete_worker","title":"<code>delete_worker(worker: NFPWorker, disconnect: bool) -&gt; None</code>","text":"<p>Deletes a worker from all data structures and destroys the worker.</p> <p>Parameters:</p> Name Type Description Default <code>worker</code> <code>Worker</code> <p>The worker instance to be deleted.</p> required <code>disconnect</code> <code>bool</code> <p>A flag indicating whether to disconnect the worker before deletion.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def delete_worker(self, worker: NFPWorker, disconnect: bool) -&gt; None:\n    \"\"\"\n    Deletes a worker from all data structures and destroys the worker.\n\n    Args:\n        worker (Worker): The worker instance to be deleted.\n        disconnect (bool): A flag indicating whether to disconnect the worker before deletion.\n\n    Returns:\n        None\n    \"\"\"\n    worker.destroy(disconnect)\n    self.workers.pop(worker.address, None)\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker.purge_workers","title":"<code>purge_workers() -&gt; None</code>","text":"<p>Look for and delete expired workers.</p> <p>This method iterates through the list of workers and checks if each worker's keepalive thread is still alive. If a worker's keepalive thread is not alive, the worker is considered expired and is deleted from the list of workers. Additionally, a log message is generated indicating that the worker's keepalive has expired.</p> Note <p>The method handles the case where a worker might be destroyed while iterating through the list of workers.</p> Logging <p>Logs an info message when a worker's keepalive has expired, including the worker's address.</p> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def purge_workers(self) -&gt; None:\n    \"\"\"\n    Look for and delete expired workers.\n\n    This method iterates through the list of workers and checks if each worker's\n    keepalive thread is still alive. If a worker's keepalive thread is not alive,\n    the worker is considered expired and is deleted from the list of workers.\n    Additionally, a log message is generated indicating that the worker's keepalive\n    has expired.\n\n    Note:\n        The method handles the case where a worker might be destroyed while\n        iterating through the list of workers.\n\n    Logging:\n        Logs an info message when a worker's keepalive has expired, including the\n        worker's address.\n    \"\"\"\n    for name in list(self.workers.keys()):\n        # in case worker self destroyed while we iterating\n        if self.workers.get(name):\n            w = self.workers[name]\n            if not w.keepaliver.is_alive():\n                self.delete_worker(w, False)\n                log.info(\n                    f\"NFPBroker - {w.address.decode(encoding='utf-8')} worker keepalives expired\"\n                )\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker.send_to_worker","title":"<code>send_to_worker(worker: NFPWorker, command: bytes, sender: bytes, uuid: bytes, data: bytes) -&gt; None</code>","text":"<p>Send a message to a worker. If a message is provided, sends that message.</p> <p>Parameters:</p> Name Type Description Default <code>worker</code> <code>NFPWorker</code> <p>The worker to send the message to.</p> required <code>command</code> <code>bytes</code> <p>The command to send (e.g., NFP.POST or NFP.GET).</p> required <code>sender</code> <code>bytes</code> <p>The sender's identifier.</p> required <code>uuid</code> <code>bytes</code> <p>The unique identifier for the message.</p> required <code>data</code> <code>bytes</code> <p>The data to be sent with the message.</p> required Logs <p>Logs an error if the command is invalid. Logs a debug message when sending the message to the worker.</p> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def send_to_worker(\n    self, worker: NFPWorker, command: bytes, sender: bytes, uuid: bytes, data: bytes\n) -&gt; None:\n    \"\"\"\n    Send a message to a worker. If a message is provided, sends that message.\n\n    Args:\n        worker (NFPWorker): The worker to send the message to.\n        command (bytes): The command to send (e.g., NFP.POST or NFP.GET).\n        sender (bytes): The sender's identifier.\n        uuid (bytes): The unique identifier for the message.\n        data (bytes): The data to be sent with the message.\n\n    Logs:\n        Logs an error if the command is invalid.\n        Logs a debug message when sending the message to the worker.\n    \"\"\"\n    # Stack routing and protocol envelopes to start of message\n    if command == NFP.POST:\n        msg = self.build_message.broker_to_worker_post(\n            worker_address=worker.address, sender=sender, uuid=uuid, data=data\n        )\n    elif command == NFP.GET:\n        msg = self.build_message.broker_to_worker_get(\n            worker_address=worker.address, sender=sender, uuid=uuid, data=data\n        )\n    elif command == NFP.PUT:\n        msg = self.build_message.broker_to_worker_put(\n            worker_address=worker.address, sender=sender, uuid=uuid, data=data\n        )\n    else:\n        log.error(f\"NFPBroker - invalid worker command: {command}\")\n        return\n    with self.socket_lock:\n        log.debug(\n            f\"NFPBroker - sending command '{command}' to worker '{worker.address}', job '{uuid}', from client '{sender}'\"\n        )\n        self.socket.send_multipart(msg)\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker.send_to_client","title":"<code>send_to_client(client: str, command: str, service: str, message: list) -&gt; None</code>","text":"<p>Send a message to a specified client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>str</code> <p>The identifier of the client to send the message to.</p> required <code>command</code> <code>str</code> <p>The command type, either 'RESPONSE' or 'EVENT'.</p> required <code>service</code> <code>str</code> <p>The service associated with the message.</p> required <code>message</code> <code>list</code> <p>The message content to be sent.</p> required Source code in <code>norfab\\core\\broker.py</code> <pre><code>def send_to_client(\n    self, client: str, command: str, service: str, message: list\n) -&gt; None:\n    \"\"\"\n    Send a message to a specified client.\n\n    Args:\n        client (str): The identifier of the client to send the message to.\n        command (str): The command type, either 'RESPONSE' or 'EVENT'.\n        service (str): The service associated with the message.\n        message (list): The message content to be sent.\n    \"\"\"\n\n    # Stack routing and protocol envelopes to start of message\n    if command == NFP.RESPONSE:\n        msg = self.build_message.broker_to_client_response(\n            client=client, service=service, message=message\n        )\n    elif command == NFP.EVENT:\n        msg = self.build_message.broker_to_client_event(\n            client=client, service=service, message=message\n        )\n    elif command == NFP.STREAM:\n        msg = self.build_message.broker_to_client_stream(\n            client=client, service=service, message=message\n        )\n    else:\n        log.error(f\"NFPBroker - invalid client command: {command}\")\n        return\n    with self.socket_lock:\n        log.debug(\n            f\"NFPBroker - sending to client '{client}', command '{command}', service '{service}'\"\n        )\n        self.socket.send_multipart(msg)\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker.process_worker","title":"<code>process_worker(sender: str, msg: list)</code>","text":"<p>Process message received from worker.</p> <p>Parameters:</p> Name Type Description Default <code>sender</code> <code>str</code> <p>The identifier of the sender (worker).</p> required <code>msg</code> <code>list</code> <p>The message received from the worker, where the first element is the command.</p> required <p>Commands:</p> <ul> <li>NFP.READY: Marks the worker as ready and assigns a service to it.</li> <li>NFP.RESPONSE: Sends a response to a client.</li> <li>NFP.KEEPALIVE: Processes a keepalive message from the worker.</li> <li>NFP.DISCONNECT: Handles worker disconnection.</li> <li>NFP.EVENT: Sends an event to a client.</li> <li>NFP.STREAM: Sends an stream data to a client.</li> </ul> <p>If the worker is not ready and an invalid command is received, the worker is deleted.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the worker does not have the required attributes for certain commands.</p> <code>IndexError</code> <p>If the message list does not contain the expected number of elements for certain commands.</p> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def process_worker(self, sender: str, msg: list):\n    \"\"\"\n    Process message received from worker.\n\n    Parameters:\n        sender (str): The identifier of the sender (worker).\n        msg (list): The message received from the worker, where the first element is the command.\n\n    Commands:\n\n    - NFP.READY: Marks the worker as ready and assigns a service to it.\n    - NFP.RESPONSE: Sends a response to a client.\n    - NFP.KEEPALIVE: Processes a keepalive message from the worker.\n    - NFP.DISCONNECT: Handles worker disconnection.\n    - NFP.EVENT: Sends an event to a client.\n    - NFP.STREAM: Sends an stream data to a client.\n\n    If the worker is not ready and an invalid command is received, the worker is deleted.\n\n    Raises:\n        AttributeError: If the worker does not have the required attributes for certain commands.\n        IndexError: If the message list does not contain the expected number of elements for certain commands.\n    \"\"\"\n    command = msg.pop(0)\n    worker = self.require_worker(sender)\n\n    log.debug(f\"NFPBroker - processing '{sender}' worker message: '{msg}'\")\n\n    if NFP.READY == command and not worker.is_ready():\n        service = msg.pop(0)\n        worker.service = self.require_service(service)\n        worker.ready = True\n        worker.start_keepalives()\n        worker.service.workers.append(worker)\n    elif NFP.RESPONSE == command and worker.is_ready():\n        client = msg.pop(0)\n        empty = msg.pop(0)\n        self.send_to_client(client, NFP.RESPONSE, worker.service.name, msg)\n    elif NFP.KEEPALIVE == command and hasattr(worker, \"keepaliver\"):\n        worker.keepaliver.received_heartbeat([worker.address] + msg)\n    elif NFP.DISCONNECT == command and worker.is_ready():\n        self.delete_worker(worker, False)\n    elif NFP.EVENT == command and worker.is_ready():\n        client = msg.pop(0)\n        empty = msg.pop(0)  # noqa\n        self.send_to_client(client, NFP.EVENT, worker.service.name, msg)\n    elif NFP.STREAM == command and worker.is_ready():\n        client = msg.pop(0)\n        empty = msg.pop(0)  # noqa\n        self.send_to_client(client, NFP.STREAM, worker.service.name, msg)\n    elif not worker.is_ready():\n        self.delete_worker(worker, disconnect=True)\n    else:\n        log.error(\n            f\"NFPBroker - invalid message: {msg}, command: {command}, sender: {sender}\"\n        )\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker.require_worker","title":"<code>require_worker(address: bytes) -&gt; NFPWorker</code>","text":"<p>Finds the worker associated with the given address, creating a new worker if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address of the worker to find or create.</p> required <p>Returns:</p> Name Type Description <code>NFPWorker</code> <code>NFPWorker</code> <p>The worker associated with the given address.</p> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def require_worker(self, address: bytes) -&gt; NFPWorker:\n    \"\"\"\n    Finds the worker associated with the given address, creating a new worker if necessary.\n\n    Args:\n        address (str): The address of the worker to find or create.\n\n    Returns:\n        NFPWorker: The worker associated with the given address.\n    \"\"\"\n    if not self.workers.get(address):\n        self.workers[address] = NFPWorker(\n            address=address,\n            socket=self.socket,\n            multiplier=self.multiplier,\n            keepalive=self.keepalive,\n            socket_lock=self.socket_lock,\n        )\n        log.info(\n            f\"NFPBroker - registered new worker {address.decode(encoding='utf-8')}\"\n        )\n\n    return self.workers[address]\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker.require_service","title":"<code>require_service(name: bytes) -&gt; NFPService</code>","text":"<p>Locates the service by name, creating it if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the service to locate or create.</p> required <p>Returns:</p> Name Type Description <code>NFPService</code> <code>NFPService</code> <p>The located or newly created service instance.</p> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def require_service(self, name: bytes) -&gt; NFPService:\n    \"\"\"\n    Locates the service by name, creating it if necessary.\n\n    Args:\n        name (str): The name of the service to locate or create.\n\n    Returns:\n        NFPService: The located or newly created service instance.\n    \"\"\"\n    if not self.services.get(name):\n        service = NFPService(name)\n        self.services[name] = service\n        log.debug(\n            f\"NFPBroker - registered new service {name.decode(encoding='utf-8')}\"\n        )\n\n    return self.services[name]\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker.process_client","title":"<code>process_client(sender: str, msg: list) -&gt; None</code>","text":"<p>Process a request coming from a client.</p> <p>Parameters:</p> Name Type Description Default <code>sender</code> <code>str</code> <p>The identifier of the client sending the request.</p> required <code>msg</code> <code>list</code> <p>The message received from the client, expected to be a list where the first five elements are: - command (str): The command issued by the client. - service (str): The service to which the command is directed. - target (str): The target of the command. - uuid (str): The unique identifier for the request. - data (any): The data associated with the request.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the command is not recognized as a valid client command.</p> The method processes the command by <ul> <li>Checking if the command is valid.</li> <li>Routing the request to the appropriate service handler based on the service specified.</li> <li>Sending an error message back to the client if the command is unsupported.</li> </ul> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def process_client(self, sender: str, msg: list) -&gt; None:\n    \"\"\"\n    Process a request coming from a client.\n\n    Args:\n        sender (str): The identifier of the client sending the request.\n        msg (list): The message received from the client, expected to be a list where the first five elements are:\n            - command (str): The command issued by the client.\n            - service (str): The service to which the command is directed.\n            - target (str): The target of the command.\n            - uuid (str): The unique identifier for the request.\n            - data (any): The data associated with the request.\n\n    Raises:\n        ValueError: If the command is not recognized as a valid client command.\n\n    The method processes the command by:\n        - Checking if the command is valid.\n        - Routing the request to the appropriate service handler based on the service specified.\n        - Sending an error message back to the client if the command is unsupported.\n    \"\"\"\n    command = msg.pop(0)\n    service = msg.pop(0)\n    target = msg.pop(0)\n    uuid = msg.pop(0)\n    data = msg.pop(0)\n\n    # check if valid command from client\n    if command not in NFP.client_commands:\n        message = f\"NFPBroker - Unsupported client command '{command}'\"\n        log.error(message)\n        self.send_to_client(\n            sender, NFP.RESPONSE, service, [message.encode(\"utf-8\")]\n        )\n    # Management Interface\n    elif service == b\"mmi.service.broker\":\n        self.mmi_service(sender, command, target, uuid, data)\n    elif service == b\"sid.service.broker\":\n        self.inventory_service(sender, command, target, uuid, data)\n    else:\n        self.dispatch(\n            sender, command, self.require_service(service), target, uuid, data\n        )\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker.filter_workers","title":"<code>filter_workers(target: bytes, service: NFPService) -&gt; list</code>","text":"<p>Helper function to filter workers</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>bytes</code> <p>bytest string, workers target</p> required <code>service</code> <code>NFPService</code> <p>NFPService object</p> required Source code in <code>norfab\\core\\broker.py</code> <pre><code>def filter_workers(self, target: bytes, service: NFPService) -&gt; list:\n    \"\"\"\n    Helper function to filter workers\n\n    Args:\n        target: bytest string, workers target\n        service: NFPService object\n    \"\"\"\n    ret = []\n\n    if service.name == b\"all\":\n        if target == b\"all\":\n            ret = self.workers.values()\n        elif target == b\"any\":\n            ret = [\n                s.workers[random.randint(0, len(s.workers) - 1)]\n                for s in self.services.values()\n                if s.workers\n            ]\n        elif target in self.workers:  # single worker\n            ret = [self.workers[target]]\n        else:  # target list of workers\n            try:\n                target = json.loads(target)\n                if isinstance(target, list):\n                    for w in target:\n                        w = w.encode(\"utf-8\")\n                        if w in self.workers:\n                            ret.append(self.workers[w])\n                    ret = list(set(ret))  # dedup workers\n            except Exception as e:\n                log.error(\n                    f\"NFPBroker - Failed to load target '{target}' with error '{e}'\"\n                )\n    elif not service.workers:\n        log.warning(\n            f\"NFPBroker - '{service.name}' has no active workers registered, try later\"\n        )\n        ret = []\n    elif target == b\"any\":\n        ret = [service.workers[random.randint(0, len(service.workers) - 1)]]\n    elif target == b\"all\":\n        ret = service.workers\n    elif target in self.workers:  # single worker\n        ret = [self.workers[target]]\n    else:  # target list of workers\n        try:\n            target = json.loads(target)\n            if isinstance(target, list):\n                for w in target:\n                    w = w.encode(\"utf-8\")\n                    if w in self.workers:\n                        ret.append(self.workers[w])\n                ret = list(set(ret))  # dedup workers\n        except Exception as e:\n            log.error(\n                f\"NFPBroker - Failed to load target '{target}' with error '{e}'\"\n            )\n    return ret\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker.dispatch","title":"<code>dispatch(sender: str, command: bytes, service: NFPService, target: Union[str, List[str]], uuid: str, data: Any) -&gt; None</code>","text":"<p>Dispatch requests to waiting workers as possible</p> <p>Parameters:</p> Name Type Description Default <code>sender</code> <code>str</code> <p>The sender of the request.</p> required <code>command</code> <code>bytes</code> <p>The command to be executed by the workers.</p> required <code>service</code> <code>Service</code> <p>The service object associated with the request.</p> required <code>target</code> <code>str</code> <p>A string indicating the addresses of the workers to dispatch to.</p> required <code>uuid</code> <code>str</code> <p>A unique identifier for the request.</p> required <code>data</code> <code>Any</code> <p>The data to be sent to the workers.</p> required Source code in <code>norfab\\core\\broker.py</code> <pre><code>def dispatch(\n    self,\n    sender: str,\n    command: bytes,\n    service: NFPService,\n    target: Union[str, List[str]],\n    uuid: str,\n    data: Any,\n) -&gt; None:\n    \"\"\"\n    Dispatch requests to waiting workers as possible\n\n    Args:\n        sender (str): The sender of the request.\n        command (bytes): The command to be executed by the workers.\n        service (Service): The service object associated with the request.\n        target (str): A string indicating the addresses of the workers to dispatch to.\n        uuid (str): A unique identifier for the request.\n        data (Any): The data to be sent to the workers.\n    \"\"\"\n    log.debug(\n        f\"NFPBroker - dispatching request to workers or clients: sender '{sender}', \"\n        f\"command '{command}', service '{service.name}', target '{target}', \"\n        f\"data '{data}', uuid '{uuid}'\"\n    )\n    self.purge_workers()\n    workers = self.filter_workers(target, service)\n\n    # handle case when service has no workers registered\n    if not workers:\n        message = f\"NFPBroker - {service.name} service failed to target workers '{target}'\"\n        log.error(message)\n        self.send_to_client(\n            sender,\n            NFP.RESPONSE,\n            service.name,\n            [\n                uuid,\n                b\"400\",\n                json.dumps(\n                    {\n                        \"workers\": None,\n                        \"uuid\": uuid.decode(\"utf-8\"),\n                        \"target\": target.decode(\"utf-8\"),\n                        \"status\": \"FAILED\",\n                        \"service\": service.name.decode(\"utf-8\"),\n                        \"errors\": [message],\n                    }\n                ).encode(\"utf-8\"),\n            ],\n        )\n        return\n\n    # inform client that JOB dispatched for POST / GET requests\n    if command in [NFP.POST, NFP.GET]:\n        w_addresses = [w.address.decode(\"utf-8\") for w in workers]\n        self.send_to_client(\n            sender,\n            NFP.RESPONSE,\n            service.name,\n            [\n                uuid,\n                b\"202\",\n                json.dumps(\n                    {\n                        \"workers\": w_addresses,\n                        \"uuid\": uuid.decode(\"utf-8\"),\n                        \"target\": target.decode(\"utf-8\"),\n                        \"status\": \"DISPATCHED\",\n                        \"service\": service.name.decode(\"utf-8\"),\n                    }\n                ).encode(\"utf-8\"),\n            ],\n        )\n\n    # send job to workers\n    for worker in workers:\n        self.send_to_worker(worker, command, sender, uuid, data)\n</code></pre>"},{"location":"api_reference_core_norfab_broker/#norfab.core.broker.NFPBroker.mmi_service","title":"<code>mmi_service(sender, command, target, uuid, data)</code>","text":"<p>Handle internal broker Management Interface (MMI) service tasks.</p> <p>Parameters:</p> Name Type Description Default <code>sender</code> <code>str</code> <p>The sender of the request.</p> required <code>command</code> <code>str</code> <p>The command to be executed.</p> required <code>target</code> <code>str</code> <p>The target of the command.</p> required <code>uuid</code> <code>str</code> <p>The unique identifier for the request.</p> required <code>data</code> <code>str</code> <p>The data payload in JSON format.</p> required <p>Supported MMI Tasks:</p> <ul> <li>\"show_workers\": Returns a list of workers with their details.</li> <li>\"show_broker\": Returns broker details including endpoint, status, keepalives, workers count, services count, directories, and security.</li> <li>\"show_broker_version\": Returns the version of various packages and the platform.</li> <li>\"show_broker_inventory\": Returns the broker's inventory.</li> </ul> <p>The response is sent back to the client in a format of JSON formatted string.</p> Source code in <code>norfab\\core\\broker.py</code> <pre><code>def mmi_service(self, sender, command, target, uuid, data):\n    \"\"\"\n    Handle internal broker Management Interface (MMI) service tasks.\n\n    Parameters:\n        sender (str): The sender of the request.\n        command (str): The command to be executed.\n        target (str): The target of the command.\n        uuid (str): The unique identifier for the request.\n        data (str): The data payload in JSON format.\n\n    Supported MMI Tasks:\n\n    - \"show_workers\": Returns a list of workers with their details.\n    - \"show_broker\": Returns broker details including endpoint, status, keepalives, workers count, services count, directories, and security.\n    - \"show_broker_version\": Returns the version of various packages and the platform.\n    - \"show_broker_inventory\": Returns the broker's inventory.\n\n    The response is sent back to the client in a format of JSON formatted string.\n    \"\"\"\n    log.debug(\n        f\"mmi.service.broker - processing request: sender '{sender}', \"\n        f\"command '{command}', target '{target}'\"\n        f\"data '{data}', uuid '{uuid}'\"\n    )\n    data = json.loads(data)\n    task = data.get(\"task\")\n    kwargs = data.get(\"kwargs\", {})\n    ret = f\"Unsupported task '{task}'\"\n    if task == \"show_workers\":\n        if self.workers:\n            ret = [\n                {\n                    \"name\": w.address.decode(\"utf-8\"),\n                    \"service\": w.service.name.decode(\"utf-8\"),\n                    \"status\": \"alive\" if w.keepaliver.is_alive() else \"dead\",\n                    \"holdtime\": str(w.keepaliver.show_holdtime()),\n                    \"keepalives tx/rx\": f\"{w.keepaliver.keepalives_send} / {w.keepaliver.keepalives_received}\",\n                    \"alive (s)\": str(w.keepaliver.show_alive_for()),\n                }\n                for k, w in self.workers.items()\n            ]\n            # filter reply\n            service = kwargs.get(\"service\")\n            status = kwargs.get(\"status\")\n            if service and service != \"all\":\n                ret = [w for w in ret if w[\"service\"] == service]\n            if status in [\"alive\", \"dead\"]:\n                ret = [w for w in ret if w[\"status\"] == status]\n            if not ret:\n                ret = [{\"name\": \"\", \"service\": \"\", \"status\": \"\"}]\n        else:\n            ret = [{\"name\": \"\", \"service\": \"\", \"status\": \"\"}]\n    elif task == \"show_broker\":\n        ret = {\n            \"endpoint\": self.socket.getsockopt_string(zmq.LAST_ENDPOINT),\n            \"status\": \"active\",\n            \"keepalives\": {\n                \"interval\": self.keepalive,\n                \"multiplier\": self.multiplier,\n            },\n            \"workers count\": len(self.workers),\n            \"services count\": len(self.services),\n            \"directories\": {\n                \"base-dir\": self.base_dir,\n                \"private-keys-dir\": self.private_keys_dir,\n                \"public-keys-dir\": self.public_keys_dir,\n            },\n            \"security\": {\n                \"broker-private-key-file\": self.broker_private_key_file,\n                \"broker-public-key-file\": self.broker_public_key_file,\n                \"zmq-auth\": self.zmq_auth,\n            },\n        }\n    elif task == \"show_broker_version\":\n        ret = {\n            \"norfab\": \"\",\n            \"pyyaml\": \"\",\n            \"pyzmq\": \"\",\n            \"psutil\": \"\",\n            \"pydantic\": \"\",\n            \"tornado\": \"\",\n            \"jinja2\": \"\",\n            \"python\": sys.version.split(\" \")[0],\n            \"platform\": sys.platform,\n        }\n        # get version of packages installed\n        for pkg in ret.keys():\n            try:\n                ret[pkg] = importlib.metadata.version(pkg)\n            except importlib.metadata.PackageNotFoundError:\n                pass\n    elif task == \"show_broker_inventory\":\n        ret = self.inventory.dict()\n    reply = json.dumps(ret).encode(\"utf-8\")\n    self.send_to_client(\n        sender, NFP.RESPONSE, b\"mmi.service.broker\", [uuid, b\"200\", reply]\n    )\n</code></pre>"},{"location":"api_reference_core_norfab_client/","title":"Client","text":""},{"location":"api_reference_core_norfab_client/#norfab.core.client.ClientJobDatabase","title":"<code>ClientJobDatabase(db_path: str, jobs_compress: bool = True)</code>","text":"<p>Lightweight client-side job and events store.</p> Source code in <code>norfab\\core\\client.py</code> <pre><code>def __init__(self, db_path: str, jobs_compress: bool = True):\n    self.db_path = db_path\n    self.jobs_compress = jobs_compress\n    self._local = threading.local()\n    self._lock = threading.Lock()\n    self._initialize_database()\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.ClientJobDatabase.fetch_jobs","title":"<code>fetch_jobs(statuses: List[str], limit: int = 10, min_poll_age: float = 0) -&gt; List[dict]</code>","text":"<p>Fetch jobs by status, optionally filtering by last poll age.</p> <p>Parameters:</p> Name Type Description Default <code>statuses</code> <code>List[str]</code> <p>List of job statuses to fetch</p> required <code>limit</code> <code>int</code> <p>Maximum number of jobs to return</p> <code>10</code> <code>min_poll_age</code> <code>float</code> <p>Minimum seconds since last poll (for rate limiting GET requests)</p> <code>0</code> Source code in <code>norfab\\core\\client.py</code> <pre><code>def fetch_jobs(\n    self,\n    statuses: List[str],\n    limit: int = 10,\n    min_poll_age: float = 0,\n) -&gt; List[dict]:\n    \"\"\"Fetch jobs by status, optionally filtering by last poll age.\n\n    Args:\n        statuses: List of job statuses to fetch\n        limit: Maximum number of jobs to return\n        min_poll_age: Minimum seconds since last poll (for rate limiting GET requests)\n    \"\"\"\n    placeholders = \",\".join([\"?\"] * len(statuses))\n    poll_threshold = time.time() - min_poll_age\n    with self._transaction(write=False) as conn:\n        cur = conn.execute(\n            f\"\"\"\n              SELECT uuid, service, task, args, kwargs, workers_requested, timeout, deadline,\n                  workers_dispatched, workers_started, workers_completed, status,\n                  last_poll_timestamp\n            FROM jobs\n            WHERE status IN ({placeholders})\n              AND (last_poll_timestamp IS NULL OR last_poll_timestamp &lt;= ?)\n            ORDER BY created_at ASC\n            LIMIT ?\n            \"\"\",\n            (*statuses, poll_threshold, limit),\n        )\n        rows = cur.fetchall()\n    return [self._hydrate(row) for row in rows]\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.NFPClient","title":"<code>NFPClient(inventory: NorFabInventory, broker: str, name: str, exit_event: Optional[threading.Event] = None, event_queue: Optional[queue.Queue] = None)</code>","text":"<p>               Bases: <code>object</code></p> <p>NFPClient is a client class for interacting with a broker using ZeroMQ for messaging. It handles sending and receiving messages, managing connections, and performing tasks.</p> <p>Attributes:</p> Name Type Description <code>broker</code> <code>str</code> <p>The broker address.</p> <code>ctx</code> <code>Context</code> <p>The ZeroMQ context.</p> <code>broker_socket</code> <code>Socket</code> <p>The ZeroMQ socket for communication with the broker.</p> <code>poller</code> <code>Poller</code> <p>The ZeroMQ poller for managing socket events.</p> <code>name</code> <code>str</code> <p>The name of the client.</p> <code>stats_send_to_broker</code> <code>int</code> <p>Counter for messages sent to the broker.</p> <code>stats_recv_from_broker</code> <code>int</code> <p>Counter for messages received from the broker.</p> <code>stats_reconnect_to_broker</code> <code>int</code> <p>Counter for reconnections to the broker.</p> <code>stats_recv_event_from_broker</code> <code>int</code> <p>Counter for events received from the broker.</p> <code>client_private_key_file</code> <code>str</code> <p>Path to the client's private key file.</p> <code>broker_public_key_file</code> <code>str</code> <p>Path to the broker's public key file.</p> <p>Methods:</p> Name Description <code>ensure_bytes</code> <p>Helper function to convert workers target to bytes.</p> <code>reconnect_to_broker</code> <p>Connects or reconnects to the broker.</p> <code>send_to_broker</code> <p>Sends a message to the broker.</p> <code>rcv_from_broker</code> <p>Waits for a response from the broker.</p> <code>post</code> <p>Sends a job request to the broker and returns the result.</p> <code>get</code> <p>Sends a job reply message to the broker requesting job results.</p> <code>get_iter</code> <p>Sends a job reply message to the broker requesting job results and yields results iteratively.</p> <code>fetch_file</code> <p>Downloads a file from the Broker File Sharing Service.</p> <code>run_job</code> <p>Runs a job and returns results produced by workers.</p> <code>run_job_iter</code> <p>Runs a job and yields results produced by workers iteratively.</p> <code>destroy</code> <p>Cleans up and destroys the client instance.</p> <p>Parameters:</p> Name Type Description Default <code>inventory</code> <code>NorFabInventory</code> <p>The inventory object containing base directory information.</p> required <code>broker</code> <code>str</code> <p>The broker object for communication.</p> required <code>name</code> <code>str</code> <p>The name of the client.</p> required <code>exit_event</code> <code>Event</code> <p>An event to signal client exit. Defaults to None.</p> <code>None</code> <code>event_queue</code> <code>Queue</code> <p>A queue for handling events. Defaults to None.</p> <code>None</code> Source code in <code>norfab\\core\\client.py</code> <pre><code>def __init__(\n    self,\n    inventory: NorFabInventory,\n    broker: str,\n    name: str,\n    exit_event: Optional[threading.Event] = None,\n    event_queue: Optional[queue.Queue] = None,\n):\n    self.inventory = inventory\n    self.name = name\n    self.zmq_name = f\"{self.name}-{uuid4().hex}\"\n    self.broker = broker\n    self.base_dir = os.path.join(\n        self.inventory.base_dir, \"__norfab__\", \"files\", \"client\", self.name\n    )\n    self.file_transfers = {}  # file transfers tracker\n    self.zmq_auth = self.inventory.broker.get(\"zmq_auth\", True)\n    self.socket_lock = threading.Lock()  # used to protect socket object\n    self.build_message = NFP.MessageBuilder()\n\n    # create base directories\n    os.makedirs(self.base_dir, exist_ok=True)\n\n    self.job_db = ClientJobDatabase(\n        os.path.join(self.base_dir, f\"{self.name}.db\"),\n        jobs_compress=True,\n    )\n\n    # generate certificates and create directories\n    if self.zmq_auth is not False:\n        generate_certificates(\n            self.base_dir,\n            cert_name=self.name,\n            broker_keys_dir=os.path.join(\n                self.inventory.base_dir,\n                \"__norfab__\",\n                \"files\",\n                \"broker\",\n                \"public_keys\",\n            ),\n            inventory=self.inventory,\n        )\n        self.public_keys_dir = os.path.join(self.base_dir, \"public_keys\")\n        self.private_keys_dir = os.path.join(self.base_dir, \"private_keys\")\n\n    self.ctx = zmq.Context()\n    self.poller = zmq.Poller()\n    self.reconnect_to_broker()\n\n    self.exit_event = threading.Event() if exit_event is None else exit_event\n    self.destroy_event = (\n        threading.Event()\n    )  # destroy event, used by worker to stop its client\n    self.recv_queue = queue.Queue(maxsize=0)\n    self.event_queue = event_queue or queue.Queue(maxsize=1000)\n\n    # Configuration for dispatcher\n    self.poll_interval = 0.5  # Seconds between GET polls for same job (throttling)\n    self.dispatch_batch_size = 10  # Max jobs to process per dispatch cycle\n\n    # start receiver thread - handles all incoming messages\n    self.recv_thread = threading.Thread(\n        target=recv, daemon=True, name=f\"{self.name}_recv_thread\", args=(self,)\n    )\n    self.recv_thread.start()\n\n    # start dispatcher thread - sends POST/GET requests asynchronously\n    self.dispatcher_thread = threading.Thread(\n        target=dispatcher,\n        daemon=True,\n        name=f\"{self.name}_dispatcher\",\n        args=(self,),\n    )\n    self.dispatcher_thread.start()\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.NFPClient.ensure_bytes","title":"<code>ensure_bytes(value: Any) -&gt; bytes</code>","text":"<p>Helper function to convert value to bytes.</p> Source code in <code>norfab\\core\\client.py</code> <pre><code>def ensure_bytes(self, value: Any) -&gt; bytes:\n    \"\"\"\n    Helper function to convert value to bytes.\n    \"\"\"\n    if isinstance(value, bytes):\n        return value\n    # transform string to bytes\n    if isinstance(value, str):\n        return value.encode(\"utf-8\")\n    # convert value to json string\n    else:\n        return json.dumps(value).encode(\"utf-8\")\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.NFPClient.reconnect_to_broker","title":"<code>reconnect_to_broker()</code>","text":"<p>Connect or reconnect to the broker.</p> <p>This method handles the connection or reconnection to the broker by:</p> <ul> <li>Closing the existing broker socket if it exists.</li> <li>Creating a new DEALER socket.</li> <li>Setting the socket options including the identity and linger.</li> <li>Loading the client's private and public keys for CURVE encryption.</li> <li>Loading the broker's public key for CURVE encryption.</li> <li>Connecting the socket to the broker.</li> <li>Registering the socket with the poller for incoming messages.</li> <li>Logging the connection status.</li> <li>Incrementing the reconnect statistics counter.</li> </ul> Source code in <code>norfab\\core\\client.py</code> <pre><code>def reconnect_to_broker(self):\n    \"\"\"\n    Connect or reconnect to the broker.\n\n    This method handles the connection or reconnection to the broker by:\n\n    - Closing the existing broker socket if it exists.\n    - Creating a new DEALER socket.\n    - Setting the socket options including the identity and linger.\n    - Loading the client's private and public keys for CURVE encryption.\n    - Loading the broker's public key for CURVE encryption.\n    - Connecting the socket to the broker.\n    - Registering the socket with the poller for incoming messages.\n    - Logging the connection status.\n    - Incrementing the reconnect statistics counter.\n    \"\"\"\n    if self.broker_socket:\n        self.poller.unregister(self.broker_socket)\n        self.broker_socket.close()\n\n    self.broker_socket = self.ctx.socket(zmq.DEALER)\n    self.broker_socket.setsockopt_unicode(zmq.IDENTITY, self.zmq_name, \"utf8\")\n    self.broker_socket.linger = 0\n\n    if self.zmq_auth is not False:\n        # We need two certificates, one for the client and one for\n        # the server. The client must know the server's public key\n        # to make a CURVE connection.\n        self.client_private_key_file = os.path.join(\n            self.private_keys_dir, f\"{self.name}.key_secret\"\n        )\n        client_public, client_secret = zmq.auth.load_certificate(\n            self.client_private_key_file\n        )\n        self.broker_socket.curve_secretkey = client_secret\n        self.broker_socket.curve_publickey = client_public\n\n        # The client must know the server's public key to make a CURVE connection.\n        self.broker_public_key_file = os.path.join(\n            self.public_keys_dir, \"broker.key\"\n        )\n        server_public, _ = zmq.auth.load_certificate(self.broker_public_key_file)\n        self.broker_socket.curve_serverkey = server_public\n\n    self.broker_socket.connect(self.broker)\n    self.poller.register(self.broker_socket, zmq.POLLIN)\n    log.debug(f\"{self.name} - client connected to broker at '{self.broker}'\")\n    self.stats_reconnect_to_broker += 1\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.NFPClient.send_to_broker","title":"<code>send_to_broker(command, service, workers, uuid, request)</code>","text":"<p>Sends a command to the broker.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>The command to send (e.g., NFP.POST, NFP.GET).</p> required <code>service</code> <code>str</code> <p>The service to which the command is related.</p> required <code>workers</code> <code>str</code> <p>The workers involved in the command.</p> required <code>uuid</code> <code>str</code> <p>The unique identifier for the request.</p> required <code>request</code> <code>str</code> <p>The request payload to be sent.</p> required Source code in <code>norfab\\core\\client.py</code> <pre><code>def send_to_broker(self, command, service, workers, uuid, request):\n    \"\"\"\n    Sends a command to the broker.\n\n    Args:\n        command (str): The command to send (e.g., NFP.POST, NFP.GET).\n        service (str): The service to which the command is related.\n        workers (str): The workers involved in the command.\n        uuid (str): The unique identifier for the request.\n        request (str): The request payload to be sent.\n    \"\"\"\n    if command == NFP.POST:\n        msg = self.build_message.client_to_broker_post(\n            command=command,\n            service=service,\n            workers=workers,\n            uuid=uuid,\n            request=request,\n        )\n    elif command == NFP.GET:\n        msg = self.build_message.client_to_broker_get(\n            command=command,\n            service=service,\n            workers=workers,\n            uuid=uuid,\n            request=request,\n        )\n    elif command == NFP.PUT:\n        msg = self.build_message.client_to_broker_put(\n            command=command,\n            service=service,\n            workers=workers,\n            uuid=uuid,\n            request=request,\n        )\n    else:\n        log.error(\n            f\"{self.name} - cannot send '{command}' to broker, command unsupported\"\n        )\n        return\n\n    log.debug(f\"{self.name} - sending '{msg}'\")\n\n    with self.socket_lock:\n        self.broker_socket.send_multipart(msg)\n        self.stats_send_to_broker += 1\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.NFPClient.rcv_from_broker","title":"<code>rcv_from_broker(command: bytes, service: bytes, uuid: bytes) -&gt; Tuple[Any, Any]</code>","text":"<p>Wait for a response from the broker for a given command, service, and uuid.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>The command sent to the broker.</p> required <code>service</code> <code>str</code> <p>The service to which the command is sent.</p> required <code>uuid</code> <code>str</code> <p>The unique identifier for the request.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Any, Any]</code> <p>A tuple containing the reply status and the reply task result.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the reply header, command, or service does not match the expected values.</p> Source code in <code>norfab\\core\\client.py</code> <pre><code>def rcv_from_broker(\n    self, command: bytes, service: bytes, uuid: bytes\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Wait for a response from the broker for a given command, service, and uuid.\n\n    Args:\n        command (str): The command sent to the broker.\n        service (str): The service to which the command is sent.\n        uuid (str): The unique identifier for the request.\n\n    Returns:\n        tuple: A tuple containing the reply status and the reply task result.\n\n    Raises:\n        AssertionError: If the reply header, command, or service does not match the expected values.\n    \"\"\"\n    retries = 3\n    while retries &gt; 0:\n        # check if need to stop\n        if self.exit_event.is_set() or self.destroy_event.is_set():\n            break\n        try:\n            msg = self.recv_queue.get(block=True, timeout=3)\n            self.recv_queue.task_done()\n        except queue.Empty:\n            if retries:\n                log.warning(\n                    f\"{self.name} - '{uuid}:{service}:{command}' job, \"\n                    f\"no reply from broker '{self.broker}', reconnecting\"\n                )\n                self.reconnect_to_broker()\n            retries -= 1\n            continue\n\n        (\n            empty,\n            reply_header,\n            reply_command,\n            reply_service,\n            reply_uuid,\n            reply_status,\n            reply_task_result,\n        ) = msg\n\n        # find message from recv queue for given uuid\n        if reply_uuid == uuid:\n            assert (\n                reply_header == NFP.BROKER\n            ), f\"Was expecting broker header '{NFP.BROKER}' received '{reply_header}'\"\n            assert (\n                reply_command == command\n            ), f\"Was expecting reply command '{command}' received '{reply_command}'\"\n            if service != b\"all\":\n                assert (\n                    reply_service == service\n                ), f\"Was expecting reply from '{service}' but received reply from '{reply_service}' service\"\n\n            return reply_status, reply_task_result\n        else:\n            self.recv_queue.put(msg)\n    else:\n        log.error(\n            f\"{self.name} - '{uuid}:{service}:{command}' job, \"\n            f\"client {retries} retries attempts exceeded\"\n        )\n        return b\"408\", b'{\"status\": \"Request Timeout\"}'\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.NFPClient.post","title":"<code>post(service: str, task: str, args: list = None, kwargs: dict = None, workers: str = 'all', uuid: hex = None, timeout: int = 600) -&gt; dict</code>","text":"<p>Send a job POST request to the broker.</p> <p>Parameters:</p> Name Type Description Default <code>service</code> <code>str</code> <p>The name of the service to send the request to.</p> required <code>task</code> <code>str</code> <p>The task to be executed by the service.</p> required <code>args</code> <code>list</code> <p>A list of positional arguments to pass to the task. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>A dictionary of keyword arguments to pass to the task. Defaults to None.</p> <code>None</code> <code>workers</code> <code>str</code> <p>The workers to handle the task. Defaults to \"all\".</p> <code>'all'</code> <code>uuid</code> <code>hex</code> <p>The unique identifier for the job. Defaults to None.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The timeout for the request in seconds. Defaults to 600.</p> <code>600</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the <code>status</code>, <code>workers</code>, <code>errors</code>, and <code>uuid</code> keys of the request:</p> <ul> <li><code>status</code>: Status of the request.</li> <li><code>uuid</code>: Unique identifier of the request.</li> <li><code>errors</code>: List of error strings.</li> <li><code>workers</code>: A list of worker names who acknowledged this POST request.</li> </ul> Source code in <code>norfab\\core\\client.py</code> <pre><code>def post(\n    self,\n    service: str,\n    task: str,\n    args: list = None,\n    kwargs: dict = None,\n    workers: str = \"all\",\n    uuid: hex = None,\n    timeout: int = 600,\n) -&gt; dict:\n    \"\"\"\n    Send a job POST request to the broker.\n\n    Args:\n        service (str): The name of the service to send the request to.\n        task (str): The task to be executed by the service.\n        args (list, optional): A list of positional arguments to pass to the task. Defaults to None.\n        kwargs (dict, optional): A dictionary of keyword arguments to pass to the task. Defaults to None.\n        workers (str, optional): The workers to handle the task. Defaults to \"all\".\n        uuid (hex, optional): The unique identifier for the job. Defaults to None.\n        timeout (int, optional): The timeout for the request in seconds. Defaults to 600.\n\n    Returns:\n        A dictionary containing the ``status``, ``workers``, ``errors``, and ``uuid`` keys of the request:\n\n            - ``status``: Status of the request.\n            - ``uuid``: Unique identifier of the request.\n            - ``errors``: List of error strings.\n            - ``workers``: A list of worker names who acknowledged this POST request.\n    \"\"\"\n    uuid = uuid or uuid4().hex\n    args = args or []\n    kwargs = kwargs or {}\n    ret = {\"status\": b\"200\", \"workers\": [], \"errors\": [], \"uuid\": uuid}\n\n    service = self.ensure_bytes(service)\n    uuid = self.ensure_bytes(uuid)\n    workers = self.ensure_bytes(workers)\n\n    request = self.ensure_bytes(\n        {\"task\": task, \"kwargs\": kwargs or {}, \"args\": args or []}\n    )\n\n    # run POST response loop\n    start_time = time.time()\n    while timeout &gt; time.time() - start_time:\n        # check if need to stop\n        if self.exit_event.is_set() or self.destroy_event.is_set():\n            return ret\n        self.send_to_broker(\n            NFP.POST, service, workers, uuid, request\n        )  # 1 send POST to broker\n        status, post_response = self.rcv_from_broker(\n            NFP.RESPONSE, service, uuid\n        )  # 2 receive RESPONSE from broker\n        if status == b\"202\":  # 3 go over RESPONSE status and decide what to do\n            break\n        else:\n            msg = f\"{self.name} - '{uuid}' job, POST Request not accepted by broker '{post_response}'\"\n            log.error(msg)\n            ret[\"errors\"].append(msg)\n            ret[\"status\"] = status\n            return ret\n    else:\n        msg = f\"{self.name} - '{uuid}' job, broker POST Request Timeout\"\n        log.error(msg)\n        ret[\"errors\"].append(msg)\n        ret[\"status\"] = b\"408\"\n        return ret\n\n    # get a list of workers where job was dispatched to\n    post_response = json.loads(post_response)\n\n    assert (\n        \"workers\" in post_response\n    ), f\"{self.name} - '{uuid}' job, POST response missing 'workers' {post_response}\"\n\n    workers_dispatched = set(post_response[\"workers\"])\n    log.debug(\n        f\"{self.name} - broker dispatched job '{uuid}' POST request to workers {workers_dispatched}\"\n    )\n\n    # wait workers to ACK POSTed job\n    start_time = time.time()\n    workers_acked = set()\n    while timeout &gt; time.time() - start_time:\n        # check if need to stop\n        if self.exit_event.is_set() or self.destroy_event.is_set():\n            return ret\n        status, response = self.rcv_from_broker(NFP.RESPONSE, service, uuid)\n        response = json.loads(response)\n        if status == b\"201\":  # CREATED JOB\n            log.debug(\n                f\"{self.name} - '{uuid}' job, acknowledged by worker '{response}'\"\n            )\n            workers_acked.add(response[\"worker\"])\n            if workers_acked == workers_dispatched:\n                break\n        else:\n            msg = (\n                f\"{self.name} - '{uuid}:{service}:{task}' job, \"\n                f\"unexpected POST request status '{status}', response '{response}'\"\n            )\n            log.error(msg)\n            ret[\"errors\"].append(msg)\n    else:\n        msg = (\n            f\"{self.name} - '{uuid}' job, POST request timeout exceeded, these workers did not \"\n            f\"acknowledge the job {workers_dispatched - workers_acked}\"\n        )\n        log.error(msg)\n        ret[\"errors\"].append(msg)\n        ret[\"status\"] = b\"408\"\n\n    ret[\"workers\"] = list(workers_acked)\n    ret[\"status\"] = ret[\"status\"].decode(\"utf-8\")\n\n    log.debug(f\"{self.name} - '{uuid}' job POST request completed '{ret}'\")\n\n    return ret\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.NFPClient.get","title":"<code>get(service: str, task: str = None, args: list = None, kwargs: dict = None, workers: Union[str, list] = 'all', uuid: hex = None, timeout: int = 600) -&gt; dict</code>","text":"<p>Send job GET request message to broker requesting job results.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>service task name to run</p> <code>None</code> <code>args</code> <code>list</code> <p>list of positional arguments for the task</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>dictionary of keyword arguments for the task</p> <code>None</code> <code>workers</code> <code>list</code> <p>workers to target - <code>all</code>, <code>any</code>, or list of workers' names</p> <code>'all'</code> <code>timeout</code> <code>int</code> <p>job timeout in seconds, for how long client waits for job result before giving up</p> <code>600</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing <code>status</code>, <code>results</code>, <code>errors</code>, and <code>workers</code> keys:</p> <ul> <li><code>status</code>: Status of the request.</li> <li><code>results</code>: Dictionary keyed by workers' names containing the results.</li> <li><code>errors</code>: List of error strings.</li> <li><code>workers</code>: Dictionary containing worker states (requested, done, dispatched, pending).</li> </ul> Source code in <code>norfab\\core\\client.py</code> <pre><code>def get(\n    self,\n    service: str,\n    task: str = None,\n    args: list = None,\n    kwargs: dict = None,\n    workers: Union[str, list] = \"all\",\n    uuid: hex = None,\n    timeout: int = 600,\n) -&gt; dict:\n    \"\"\"\n    Send job GET request message to broker requesting job results.\n\n    Args:\n        task (str): service task name to run\n        args (list): list of positional arguments for the task\n        kwargs (dict): dictionary of keyword arguments for the task\n        workers (list): workers to target - ``all``, ``any``, or list of workers' names\n        timeout (int): job timeout in seconds, for how long client waits for job result before giving up\n\n    Returns:\n        Dictionary containing ``status``, ``results``, ``errors``, and ``workers`` keys:\n\n            - ``status``: Status of the request.\n            - ``results``: Dictionary keyed by workers' names containing the results.\n            - ``errors``: List of error strings.\n            - ``workers``: Dictionary containing worker states (requested, done, dispatched, pending).\n    \"\"\"\n    uuid = uuid or uuid4().hex\n    args = args or []\n    kwargs = kwargs or {}\n    wkrs = {\n        \"requested\": workers,\n        \"done\": set(),\n        \"dispatched\": set(),\n        \"pending\": set(),\n    }\n    ret = {\"status\": b\"200\", \"results\": {}, \"errors\": [], \"workers\": wkrs}\n\n    service = self.ensure_bytes(service)\n    uuid = self.ensure_bytes(uuid)\n    workers = self.ensure_bytes(workers)\n\n    request = self.ensure_bytes(\n        {\"task\": task, \"kwargs\": kwargs or {}, \"args\": args or []}\n    )\n\n    # run GET response loop\n    start_time = time.time()\n    while timeout &gt; time.time() - start_time:\n        # check if need to stop\n        if self.exit_event.is_set() or self.destroy_event.is_set():\n            return None\n        # dispatch GET request to workers\n        self.send_to_broker(NFP.GET, service, workers, uuid, request)\n        status, get_response = self.rcv_from_broker(NFP.RESPONSE, service, uuid)\n        # ret[\"status\"] = status\n        # received actual GET request results from broker e.g. MMI, SID or FSS services\n        if status == b\"200\":\n            ret[\"results\"] = json.loads(get_response.decode(\"utf-8\"))\n            break\n        # received non DISPATCH response from broker\n        if status != b\"202\":\n            msg = f\"{status}, {self.name} job '{uuid}' GET Request not accepted by broker '{get_response}'\"\n            log.error(msg)\n            ret[\"status\"] = status\n            ret[\"errors\"].append(msg)\n            break\n        get_response = json.loads(get_response)\n        wkrs[\"dispatched\"] = set(get_response[\"workers\"])\n        # collect GET responses from individual workers\n        workers_responded = set()\n        while timeout &gt; time.time() - start_time:\n            # check if need to stop\n            if self.exit_event.is_set() or self.destroy_event.is_set():\n                return None\n            status, response = self.rcv_from_broker(NFP.RESPONSE, service, uuid)\n            log.debug(\n                f\"{self.name} - job '{uuid}' response from worker '{response}'\"\n            )\n            response = json.loads(response)  # dictionary keyed by worker name\n            if status == b\"200\":  # OK\n                ret[\"results\"].update(response)\n                log.debug(\n                    f\"{self.name} - job '{uuid}' results returned by worker '{response}'\"\n                )\n                for w in response.keys():\n                    wkrs[\"done\"].add(w)\n                    workers_responded.add(w)\n                    if w in wkrs[\"pending\"]:\n                        wkrs[\"pending\"].remove(w)\n                if wkrs[\"done\"] == wkrs[\"dispatched\"]:\n                    break\n            elif status == b\"300\":  # PENDING\n                wkrs[\"pending\"].add(response[\"worker\"])\n                workers_responded.add(response[\"worker\"])\n            else:\n                if response.get(\"worker\"):\n                    workers_responded.add(response[\"worker\"])\n                msg = (\n                    f\"{self.name} - '{uuid}:{service}:{task}' job, \"\n                    f\"unexpected GET Response status '{status}', response '{response}'\"\n                )\n                log.error(msg)\n                ret[\"errors\"].append(msg)\n            if workers_responded == wkrs[\"dispatched\"]:\n                break\n        if wkrs[\"done\"] == wkrs[\"dispatched\"]:\n            break\n        time.sleep(0.2)\n    else:\n        msg = f\"{self.name} - '{uuid}' job, broker {timeout}s GET request timeout expired\"\n        log.info(msg)\n        ret[\"errors\"].append(msg)\n        # set status to pending if at least one worker is pending\n        if wkrs[\"pending\"]:\n            ret[\"status\"] = b\"300\"  # PENDING\n        else:\n            ret[\"status\"] = b\"408\"  # TIMEOUT\n\n    if ret[\"status\"] == b\"408\":\n        ret[\"status\"] = \"408\"\n    # set status to pending if at least one worker is pending\n    elif wkrs[\"pending\"]:\n        ret[\"status\"] = \"300\"\n    else:\n        ret[\"status\"] = ret[\"status\"].decode(\"utf-8\")\n\n    return ret\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.NFPClient.delete_fetched_files","title":"<code>delete_fetched_files(filepath: str = '*') -&gt; dict</code>","text":"<p>Delete files and folders matching the filepath glob pattern.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Glob pattern to match files/folders. Default is \"*\" (all files).</p> <code>'*'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with 'deleted' list of deleted paths and 'errors' list of error messages.</p> Source code in <code>norfab\\core\\client.py</code> <pre><code>def delete_fetched_files(self, filepath: str = \"*\") -&gt; dict:\n    \"\"\"\n    Delete files and folders matching the filepath glob pattern.\n\n    Args:\n        filepath (str): Glob pattern to match files/folders. Default is \"*\" (all files).\n\n    Returns:\n        dict: Dictionary with 'deleted' list of deleted paths and 'errors' list of error messages.\n    \"\"\"\n    files_folder = os.path.join(self.base_dir, \"fetchedfiles\")\n\n    result = {\"deleted\": [], \"errors\": []}\n\n    # Build full pattern path\n    pattern = os.path.join(files_folder, filepath)\n\n    # Find all matching files and folders\n    matches = glob.glob(pattern, recursive=True)\n\n    # Sort by depth (deepest first) to avoid deleting parent before children\n    matches.sort(key=lambda x: x.count(os.sep), reverse=True)\n\n    for match in matches:\n        try:\n            if os.path.isfile(match):\n                os.remove(match)\n                result[\"deleted\"].append(match)\n                log.debug(f\"{self.name} - deleted file: {match}\")\n            elif os.path.isdir(match):\n                shutil.rmtree(match)\n                result[\"deleted\"].append(match)\n                log.debug(f\"{self.name} - deleted folder: {match}\")\n        except Exception as e:\n            error_msg = f\"Failed to delete {match}: {str(e)}\"\n            result[\"errors\"].append(error_msg)\n            log.error(f\"{self.name} - {error_msg}\")\n\n    return result\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.NFPClient.fetch_file","title":"<code>fetch_file(url: str, chunk_size: int = 256000, pipeline: int = 10, timeout: int = 600, read: bool = False) -&gt; Tuple[str, Any]</code>","text":"<p>Fetches a file from a given URL and saves it to a specified destination.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to be fetched.</p> required <code>chunk_size</code> <code>int</code> <p>The size of each chunk to be fetched. Default is 250000 bytes.</p> <code>256000</code> <code>pipeline</code> <code>int</code> <p>The number of chunks to be fetched in transit. Default is 10.</p> <code>10</code> <code>timeout</code> <code>int</code> <p>The maximum time (in seconds) to wait for the file to be fetched. Default is 600 seconds.</p> <code>600</code> <code>read</code> <code>bool</code> <p>If True, the file content is read and returned. If False, the file path is returned. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[str, Any]</code> <p>A tuple containing the status code (str) and the reply (str). The reply can be the file content, file path, or an error message.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error in fetching the file or if the file's MD5 hash does not match the expected hash.</p> Source code in <code>norfab\\core\\client.py</code> <pre><code>def fetch_file(\n    self,\n    url: str,\n    chunk_size: int = 256000,\n    pipeline: int = 10,\n    timeout: int = 600,\n    read: bool = False,\n) -&gt; Tuple[str, Any]:\n    \"\"\"\n    Fetches a file from a given URL and saves it to a specified destination.\n\n    Parameters:\n        url (str): The URL of the file to be fetched.\n        chunk_size (int, optional): The size of each chunk to be fetched. Default is 250000 bytes.\n        pipeline (int, optional): The number of chunks to be fetched in transit. Default is 10.\n        timeout (int, optional): The maximum time (in seconds) to wait for the file to be fetched. Default is 600 seconds.\n        read (bool, optional): If True, the file content is read and returned. If False, the file path is returned. Default is False.\n\n    Returns:\n        tuple: A tuple containing the status code (str) and the reply (str). The reply can be the file content, file path, or an error message.\n\n    Raises:\n        Exception: If there is an error in fetching the file or if the file's MD5 hash does not match the expected hash.\n    \"\"\"\n    # round up digit e.g. if 2.0 -&gt; 2 if 2.1 -&gt; 3 if 0.01 -&gt; 1\n    def round_up(num):\n        return max(1, (int(num) + (not num.is_integer())))\n\n    uuid = uuid4().hex\n    result = {\"status\": \"200\", \"content\": None, \"error\": None}\n    downloaded = False\n\n    # run sanity checks\n    if not url.startswith(\"nf://\"):\n        result[\"status\"] = \"500\"\n        result[\"error\"] = \"Invalid url format\"\n        return result\n\n    # prevent path traversal / absolute paths\n    url_path = url.replace(\"nf://\", \"\")\n    url_path = url_path.lstrip(\"/\\\\\")\n    destination = os.path.abspath(\n        os.path.join(self.base_dir, \"fetchedfiles\", *url_path.split(\"/\"))\n    )\n    fetched_root = os.path.abspath(os.path.join(self.base_dir, \"fetchedfiles\"))\n    if os.path.commonpath([fetched_root, destination]) != fetched_root:\n        result[\"status\"] = \"500\"\n        result[\"error\"] = \"Invalid url path\"\n        return result\n\n    os.makedirs(os.path.split(destination)[0], exist_ok=True)\n\n    self.file_transfers[uuid] = {\n        \"total_bytes_received\": 0,  # Total bytes received\n        \"offset\": 0,  # Offset of next chunk request\n        \"credit\": pipeline,  # Up to PIPELINE chunks in transit\n        \"chunk_size\": chunk_size,\n        \"file_hash\": hashlib.md5(),\n    }\n\n    # get file details\n    file_details = self.run_job(\n        service=\"filesharing\",\n        workers=\"all\",\n        task=\"file_details\",\n        kwargs={\"url\": url},\n        timeout=timeout,\n    )\n    for w_name, w_res in file_details.items():\n        if not w_res[\"failed\"]:\n            file_details = w_res[\"result\"]\n            self.file_transfers[uuid].update(file_details)\n            self.file_transfers[uuid][\"w_name\"] = w_name\n            self.file_transfers[uuid][\"chunk_requests_remaining\"] = round_up(\n                file_details[\"size_bytes\"] / chunk_size\n            )\n            break\n    else:\n        result[\"status\"] = \"404\"\n        result[\"error\"] = \"File download failed - file not found\"\n        _ = self.file_transfers.pop(uuid)\n        return result\n\n    log.debug(f\"{self.name}:fetch_file - retrieved file details - {file_details}\")\n\n    # check if file already downloaded\n    if os.path.isfile(destination):\n        file_hash = hashlib.md5()\n        with open(destination, \"rb\") as f:\n            chunk = f.read(8192)\n            while chunk:\n                file_hash.update(chunk)\n                chunk = f.read(8192)\n        md5hash = file_hash.hexdigest()\n        downloaded = md5hash == file_details[\"md5hash\"]\n\n    if file_details[\"exists\"] and not downloaded:\n        self.file_transfers[uuid][\"destination\"] = open(destination, \"wb\")\n        # decrement by 1 because calling fetch_job sends first chunk\n        self.file_transfers[uuid][\"chunk_requests_remaining\"] -= 1\n        # run fetch file job\n        file_fetch_job = self.run_job(\n            uuid=uuid,\n            service=\"filesharing\",\n            workers=[w_name],\n            task=\"fetch_file\",\n            kwargs={\"url\": url, \"offset\": 0, \"chunk_size\": chunk_size},\n            timeout=timeout,\n        )\n        file_fetch_job = file_fetch_job[w_name]\n\n        if file_fetch_job[\"failed\"]:\n            result[\"status\"] = \"404\"\n            result[\"error\"] = file_fetch_job[\"errors\"]\n            downloaded = False\n        else:\n            downloaded = True\n\n    if downloaded:\n        # Verify streaming did not mark job failed (e.g., MD5 mismatch)\n        download_job = self.job_db.get_job(uuid)\n        if download_job and download_job.get(\"status\") == JobStatus.FAILED:\n            result[\"error\"] = (\n                f\"File download job {uuid} failed: {download_job.get('errors', [])}\"\n            )\n            result[\"status\"] = \"400\"\n        elif read:\n            with open(destination, \"r\", encoding=\"utf-8\") as f:\n                result[\"content\"] = f.read()\n        else:\n            result[\"content\"] = destination\n\n    _ = self.file_transfers.pop(uuid)\n\n    return result\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.NFPClient.run_job","title":"<code>run_job(service: str, task: str, uuid: str = None, args: list = None, kwargs: dict = None, workers: Union[str, list] = 'all', timeout: int = 600, markdown: bool = False) -&gt; Any</code>","text":"<p>Run a job on the specified service and task, with optional arguments and timeout settings.</p> <p>This method submits a job to the database and waits for the dispatcher and receiver threads to process it asynchronously. The job progresses through states: NEW -&gt; SUBMITTING -&gt; DISPATCHED -&gt; STARTED -&gt; COMPLETED (or FAILED/STALE)</p> <p>Parameters:</p> Name Type Description Default <code>service</code> <code>str</code> <p>The name of the service to run the job on.</p> required <code>task</code> <code>str</code> <p>The task to be executed.</p> required <code>uuid</code> <code>str</code> <p>A unique identifier for the job. If not provided, a new UUID will be generated. Defaults to None.</p> <code>None</code> <code>args</code> <code>list</code> <p>A list of positional arguments to pass to the task. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>A dictionary of keyword arguments to pass to the task. Defaults to None.</p> <code>None</code> <code>workers</code> <code>str</code> <p>The workers to run the job on. Defaults to \"all\".</p> <code>'all'</code> <code>timeout</code> <code>int</code> <p>The maximum time in seconds to wait for the job to complete. Defaults to 600.</p> <code>600</code> <code>markdown</code> <code>bool</code> <p>Convert results to markdown representation</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the job if successful, or None if the job failed, timed out, or became stale.</p> Source code in <code>norfab\\core\\client.py</code> <pre><code>def run_job(\n    self,\n    service: str,\n    task: str,\n    uuid: str = None,\n    args: list = None,\n    kwargs: dict = None,\n    workers: Union[str, list] = \"all\",\n    timeout: int = 600,\n    markdown: bool = False,\n) -&gt; Any:\n    \"\"\"\n    Run a job on the specified service and task, with optional arguments and timeout settings.\n\n    This method submits a job to the database and waits for the dispatcher and receiver\n    threads to process it asynchronously. The job progresses through states:\n    NEW -&gt; SUBMITTING -&gt; DISPATCHED -&gt; STARTED -&gt; COMPLETED (or FAILED/STALE)\n\n    Args:\n        service (str): The name of the service to run the job on.\n        task (str): The task to be executed.\n        uuid (str, optional): A unique identifier for the job. If not provided, a new UUID will be generated. Defaults to None.\n        args (list, optional): A list of positional arguments to pass to the task. Defaults to None.\n        kwargs (dict, optional): A dictionary of keyword arguments to pass to the task. Defaults to None.\n        workers (str, optional): The workers to run the job on. Defaults to \"all\".\n        timeout (int, optional): The maximum time in seconds to wait for the job to complete. Defaults to 600.\n        markdown (bool, optional): Convert results to markdown representation\n\n    Returns:\n        Any: The result of the job if successful, or None if the job failed, timed out, or became stale.\n    \"\"\"\n    uuid = uuid or uuid4().hex\n    args = args or []\n    kwargs = kwargs or {}\n    result = None\n    job = None\n    deadline = time.time() + timeout\n\n    self.job_db.add_job(\n        uuid, service, task, workers, args, kwargs, timeout, deadline\n    )\n\n    while time.time() &lt; deadline:\n        if self.exit_event.is_set() or self.destroy_event.is_set():\n            break\n        job = self.job_db.get_job(uuid)\n        if not job:\n            break\n        if job[\"status\"] == JobStatus.COMPLETED:\n            result = job.get(\"result_data\")\n            break\n        if job[\"status\"] == JobStatus.FAILED:\n            log.warning(f\"{self.name} - job {uuid} failed: {job.get('errors', [])}\")\n            break\n        if job[\"status\"] == JobStatus.STALE:\n            log.warning(\n                f\"{self.name} - job {uuid} became stale: {job.get('errors', [])}\"\n            )\n            break\n        time.sleep(0.2)\n\n    return markdown_results(job, service, task, kwargs) if markdown else result\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.NFPClient.destroy","title":"<code>destroy()</code>","text":"<p>Gracefully shuts down the client.</p> <p>This method logs an interrupt message, sets the destroy event, and destroys the client context to ensure a clean shutdown.</p> Source code in <code>norfab\\core\\client.py</code> <pre><code>def destroy(self):\n    \"\"\"\n    Gracefully shuts down the client.\n\n    This method logs an interrupt message, sets the destroy event, and\n    destroys the client context to ensure a clean shutdown.\n    \"\"\"\n    log.info(f\"{self.name} - client interrupt received, killing client\")\n    self.destroy_event.set()\n    self.job_db.close()\n    self.ctx.destroy()\n    # close all file transfer files\n    for file_transfer in self.file_transfers.values():\n        file_transfer[\"destination\"].close()\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.recv","title":"<code>recv(client)</code>","text":"<p>Receiver thread: processes all incoming messages from the broker and updates the database.</p> <p>This function continuously polls the client's broker socket for messages until the client's exit event is set. It handles: - EVENT messages: stored in the events table - RESPONSE messages: updates job status in the database based on response type</p> <p>The receiver thread is the ONLY thread that reads from the socket, eliminating contention issues. All job state changes are persisted to the database.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>object</code> <p>The client instance containing the broker socket,              poller, job_db, and configuration.</p> required Source code in <code>norfab\\core\\client.py</code> <pre><code>def recv(client):\n    \"\"\"\n    Receiver thread: processes all incoming messages from the broker and updates the database.\n\n    This function continuously polls the client's broker socket for messages\n    until the client's exit event is set. It handles:\n    - EVENT messages: stored in the events table\n    - RESPONSE messages: updates job status in the database based on response type\n\n    The receiver thread is the ONLY thread that reads from the socket, eliminating\n    contention issues. All job state changes are persisted to the database.\n\n    Args:\n        client (object): The client instance containing the broker socket,\n                         poller, job_db, and configuration.\n    \"\"\"\n    while not client.exit_event.is_set() and not client.destroy_event.is_set():\n        # Poll socket for messages every 500ms interval\n        try:\n            items = client.poller.poll(500)\n        except KeyboardInterrupt:\n            break\n        except Exception:\n            continue\n\n        if not items:\n            continue\n\n        with client.socket_lock:\n            try:\n                msg = client.broker_socket.recv_multipart(zmq.NOBLOCK)\n            except zmq.Again:\n                continue\n\n        client.stats_recv_from_broker += 1\n\n        # Message format: [empty, header, command, service, uuid, status, payload]\n        if len(msg) &lt; 7:\n            log.error(f\"{client.name} - received malformed message: {msg}\")\n            continue\n\n        command = msg[2]\n        juuid = msg[4].decode(\"utf-8\")\n        status = msg[5].decode(\"utf-8\")\n\n        log.debug(\n            f\"{client.name} - received '{command}' message from broker, juuid {juuid}, status {status}\"\n        )\n\n        if command == NFP.STREAM:\n            payload = msg[6]  # payload is a chunk of bytes\n            handle_stream(client, juuid, status, payload)\n            continue\n\n        try:\n            payload = json.loads(msg[6].decode(\"utf-8\"))\n        except Exception as e:\n            log.error(\n                f\"{client.name} - failed to parse message, error '{e}'\", exc_info=True\n            )\n            continue\n\n        # Handle EVENT messages\n        if command == NFP.EVENT:\n            handle_event(client, juuid, payload, msg)\n\n        # Handle RESPONSE messages\n        if command == NFP.RESPONSE:\n            handle_response(client, juuid, status, payload)\n            # Also put in queue for synchronous callers (backwards compatibility)\n            client.recv_queue.put(msg)\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.handle_event","title":"<code>handle_event(client: object, juuid: str, payload: dict, msg: list)</code>","text":"<p>Handle EVENT messages and update job database accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>object</code> <p>The client instance</p> required <code>juuid</code> <code>str</code> <p>Job UUID</p> required <code>payload</code> <code>dict</code> <p>Event payload dictionary</p> required <code>msg</code> <code>list</code> <p>Original message multipart for queue</p> required Source code in <code>norfab\\core\\client.py</code> <pre><code>def handle_event(client: object, juuid: str, payload: dict, msg: list):\n    \"\"\"\n    Handle EVENT messages and update job database accordingly.\n\n    Args:\n        client: The client instance\n        juuid: Job UUID\n        payload: Event payload dictionary\n        msg: Original message multipart for queue\n    \"\"\"\n    client.event_queue.put(msg)\n    client.stats_recv_event_from_broker += 1\n    client.job_db.add_event(\n        job_uuid=juuid,\n        message=payload.get(\"message\", \"\"),\n        severity=payload.get(\"severity\", \"INFO\"),\n        task=payload.get(\"task\"),\n        event_data=payload,\n    )\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.handle_response","title":"<code>handle_response(client, juuid: str, status: str, payload: dict)</code>","text":"<p>Handle RESPONSE messages and update job database accordingly.</p> <p>Uses job status to determine context: - SUBMITTING: expecting broker 202 with workers list - DISPATCHED/STARTED: expecting worker ACKs (202), results (200), or pending (300)</p> <p>Status codes: - 202: Accepted (POST acknowledged by broker or worker) - 200: OK (GET completed with results) - 300: Pending (job still in progress) - 4xx: Client errors - 5xx: Server errors</p> Source code in <code>norfab\\core\\client.py</code> <pre><code>def handle_response(client, juuid: str, status: str, payload: dict):\n    \"\"\"\n    Handle RESPONSE messages and update job database accordingly.\n\n    Uses job status to determine context:\n    - SUBMITTING: expecting broker 202 with workers list\n    - DISPATCHED/STARTED: expecting worker ACKs (202), results (200), or pending (300)\n\n    Status codes:\n    - 202: Accepted (POST acknowledged by broker or worker)\n    - 200: OK (GET completed with results)\n    - 300: Pending (job still in progress)\n    - 4xx: Client errors\n    - 5xx: Server errors\n    \"\"\"\n    job = client.job_db.get_job(juuid)\n    if not job:\n        log.debug(f\"{client.name} - received response for unknown job {juuid}\")\n        return\n\n    # Broker accepted POST - contains dispatched workers list\n    if status == \"202\":  # ACCEPTED\n        workers_list = payload[\"workers\"]\n        client.job_db.update_job(\n            juuid,\n            status=JobStatus.DISPATCHED,\n            workers_dispatched=workers_list,\n            started_ts=time.ctime(),\n        )\n        log.debug(f\"{client.name} - job {juuid} dispatched to workers: {workers_list}\")\n        return\n\n    # Worker created the job\n    if status == \"201\":  # JOB CREATED\n        worker_single = payload[\"worker\"]\n        started = set(job.get(\"workers_started\", []))\n        started.add(worker_single)\n        client.job_db.update_job(\n            juuid,\n            status=JobStatus.STARTED,\n            workers_started=list(started),\n        )\n        log.debug(\n            f\"{client.name} - job {juuid} acknowledged by worker: {worker_single}\"\n        )\n        return\n\n        # GET dispatched to workers (broker 202 response to GET)\n        if workers_list:\n            log.debug(\n                f\"{client.name} - job {juuid} GET dispatched to workers: {workers_list}\"\n            )\n        return\n\n    # Handle 200 OK - GET completed with results\n    if status == \"200\":\n        dispatched = set(job.get(\"workers_dispatched\", []))\n        completed = set(job.get(\"workers_completed\", []))\n        existing_results = job.get(\"result_data\") or {}\n\n        # Merge new results with existing (results keyed by worker name)\n        if isinstance(payload, dict):\n            for worker_name in payload.keys():\n                completed.add(worker_name)\n            existing_results.update(payload)\n\n        is_complete = completed == dispatched and len(dispatched) &gt; 0\n\n        client.job_db.update_job(\n            juuid,\n            status=JobStatus.COMPLETED if is_complete else JobStatus.STARTED,\n            workers_completed=list(completed),\n            result_data=existing_results,\n            completed_ts=time.ctime() if is_complete else None,\n        )\n\n        if is_complete:\n            log.debug(f\"{client.name} - job {juuid} completed\")\n        return\n\n    # Handle 300 Pending - job still in progress\n    if status == \"300\":\n        worker = payload.get(\"worker\")\n        if worker and worker not in job[\"workers_started\"]:\n            job[\"workers_started\"].append(worker)\n            client.job_db.update_job(\n                juuid,\n                status=JobStatus.STARTED,\n                workers_started=job[\"workers_started\"],\n            )\n        return\n\n    # Handle error statuses (4xx, 5xx)\n    if status.startswith(\"4\") or status.startswith(\"5\"):\n        error_msg = payload.get(\"error\", payload.get(\"status\", f\"Error {status}\"))\n        client.job_db.update_job(\n            juuid,\n            status=JobStatus.FAILED,\n            append_errors=[error_msg],\n            completed_ts=time.ctime(),\n        )\n        log.error(f\"{client.name} - job {juuid} failed: {error_msg}\")\n        return\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.dispatch_new_jobs","title":"<code>dispatch_new_jobs(client)</code>","text":"<p>Find NEW jobs and send POST requests to broker. Non-blocking: sends request and updates status to SUBMITTING.</p> Source code in <code>norfab\\core\\client.py</code> <pre><code>def dispatch_new_jobs(client):\n    \"\"\"\n    Find NEW jobs and send POST requests to broker.\n    Non-blocking: sends request and updates status to SUBMITTING.\n    \"\"\"\n    for job in client.job_db.fetch_jobs(\n        [JobStatus.NEW], limit=client.dispatch_batch_size\n    ):\n        juuid = job[\"uuid\"]\n\n        try:\n            # Send POST request (non-blocking)\n            service = client.ensure_bytes(job[\"service\"])\n            uuid_bytes = client.ensure_bytes(juuid)\n            workers = client.ensure_bytes(job[\"workers_requested\"])\n            request = client.ensure_bytes(\n                {\n                    \"task\": job[\"task\"],\n                    \"kwargs\": job[\"kwargs\"] or {},\n                    \"args\": job[\"args\"] or [],\n                }\n            )\n\n            client.send_to_broker(NFP.POST, service, workers, uuid_bytes, request)\n\n            # Update status - receiver will handle the response\n            client.job_db.update_job(\n                juuid,\n                status=JobStatus.SUBMITTING,\n                last_poll_ts=time.time(),\n            )\n            log.debug(f\"{client.name} - dispatched POST for job {juuid}\")\n\n        except Exception as e:\n            msg = f\"{client.name} - failed to dispatch job {juuid}: {e}\"\n            log.error(msg, exc_info=True)\n            client.job_db.update_job(\n                juuid,\n                status=JobStatus.FAILED,\n                errors=[msg],\n                completed_ts=time.ctime(),\n            )\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.poll_active_jobs","title":"<code>poll_active_jobs(client)</code>","text":"<p>Find active jobs and send GET requests to poll for results. Non-blocking: sends request with 5-second throttling via last_poll_timestamp.</p> Source code in <code>norfab\\core\\client.py</code> <pre><code>def poll_active_jobs(client):\n    \"\"\"\n    Find active jobs and send GET requests to poll for results.\n    Non-blocking: sends request with 5-second throttling via last_poll_timestamp.\n    \"\"\"\n    # Jobs that are ready for GET polling (dispatched or started)\n    active_statuses = [JobStatus.DISPATCHED, JobStatus.STARTED]\n\n    # fetch_jobs filters by min_poll_age to enforce polling throttle\n    for job in client.job_db.fetch_jobs(\n        active_statuses,\n        limit=client.dispatch_batch_size,\n        min_poll_age=client.poll_interval,\n    ):\n        juuid = job[\"uuid\"]\n        deadline = job[\"deadline\"]\n        now = time.time()\n\n        # Check if job has exceeded deadline\n        if now &gt;= deadline:\n            client.job_db.update_job(\n                juuid,\n                status=JobStatus.STALE,\n                errors=[\"Job deadline reached without completion\"],\n                completed_ts=time.ctime(),\n            )\n            continue\n\n        try:\n            # Send GET request (non-blocking)\n            service = client.ensure_bytes(job[\"service\"])\n            uuid_bytes = client.ensure_bytes(juuid)\n            workers = client.ensure_bytes(job[\"workers_dispatched\"])\n            request = client.ensure_bytes(\n                {\n                    \"task\": job[\"task\"],\n                    \"kwargs\": job[\"kwargs\"] or {},\n                    \"args\": job[\"args\"] or [],\n                }\n            )\n\n            client.send_to_broker(NFP.GET, service, workers, uuid_bytes, request)\n\n            # Update last_poll_ts to enforce 5-second throttle\n            client.job_db.update_job(\n                juuid,\n                last_poll_ts=time.time(),\n            )\n            log.debug(f\"{client.name} - sent GET poll for job {juuid}\")\n\n        except Exception as e:\n            log.error(f\"{client.name} - failed to poll job {juuid}: {e}\", exc_info=True)\n</code></pre>"},{"location":"api_reference_core_norfab_client/#norfab.core.client.dispatcher","title":"<code>dispatcher(client)</code>","text":"<p>Dispatcher thread: sends POST and GET requests asynchronously.</p> <p>This thread: 1. Finds NEW jobs and sends POST requests to broker 2. Finds DISPATCHED/STARTED jobs and sends GET requests to poll for results</p> <p>It does NOT wait for responses - the receiver thread handles all incoming messages and updates the database.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>object</code> <p>The client instance containing job_db, exit_event, and configuration.</p> required Source code in <code>norfab\\core\\client.py</code> <pre><code>def dispatcher(client):\n    \"\"\"\n    Dispatcher thread: sends POST and GET requests asynchronously.\n\n    This thread:\n    1. Finds NEW jobs and sends POST requests to broker\n    2. Finds DISPATCHED/STARTED jobs and sends GET requests to poll for results\n\n    It does NOT wait for responses - the receiver thread handles all incoming\n    messages and updates the database.\n\n    Args:\n        client (object): The client instance containing job_db, exit_event, and configuration.\n    \"\"\"\n    while not client.exit_event.is_set() and not client.destroy_event.is_set():\n        try:\n            dispatch_new_jobs(client)\n            poll_active_jobs(client)\n        except Exception as e:\n            log.error(f\"{client.name} - dispatcher error: {e}\", exc_info=True)\n        time.sleep(0.1)\n</code></pre>"},{"location":"api_reference_core_norfab_exceptions/","title":"Exceptions","text":""},{"location":"api_reference_core_norfab_exceptions/#norfab.core.exceptions.UnsupportedPluginError","title":"<code>UnsupportedPluginError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception to raise when specified plugin not supported</p>"},{"location":"api_reference_core_norfab_exceptions/#norfab.core.exceptions.UnsupportedServiceError","title":"<code>UnsupportedServiceError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception to raise when specified service not supported</p>"},{"location":"api_reference_core_norfab_exceptions/#norfab.core.exceptions.NorfabJobFailedError","title":"<code>NorfabJobFailedError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception to raise when job failed</p>"},{"location":"api_reference_core_norfab_exceptions/#norfab.core.exceptions.ServicePluginAlreadyRegistered","title":"<code>ServicePluginAlreadyRegistered</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when trying to register an already registered plugin</p>"},{"location":"api_reference_core_norfab_exceptions/#norfab.core.exceptions.ServicePluginNotRegistered","title":"<code>ServicePluginNotRegistered</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when trying to access a plugin that is not registered</p>"},{"location":"api_reference_core_norfab_nfapi/","title":"NFAPI (Python API)","text":"<p>NorFab is a class that provides an interface for interacting with the NorFab system.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>NFPClient</code> <p>The client instance for interfacing with the broker.</p> <code>broker</code> <code>Process</code> <p>The process instance for the broker.</p> <code>inventory</code> <code>NorFabInventory</code> <p>The inventory instance containing configuration data.</p> <code>workers_processes</code> <code>dict</code> <p>A dictionary mapping worker names to their process instances and initialization events.</p> <code>worker_plugins</code> <code>dict</code> <p>A dictionary mapping service names to their worker plugins.</p> <p>Parameters:</p> Name Type Description Default <code>inventory</code> <code>str</code> <p>OS path to NorFab inventory YAML file</p> <code>'./inventory.yaml'</code> <code>inventory_data</code> <code>dict</code> <p>dictionary with NorFab inventory</p> <code>None</code> <code>base_dir</code> <code>str</code> <p>OS path to base directory to anchor NorFab at</p> <code>None</code> <code>log_level</code> <code>str</code> <p>one or supported logging levels - <code>CRITICAL</code>, <code>ERROR</code>, <code>WARNING</code>, <code>INFO</code>, <code>DEBUG</code></p> <code>None</code> <p>Example:</p> <pre><code>from norfab.core.nfapi import NorFab\n\nnf = NorFab(inventory=\"./inventory.yaml\")\nnf.start(run_broker=True, run_workers=[\"my-worker-1\"])\nNFCLIENT = nf.make_client()\n</code></pre> <p>Example using dictionary inventory data:</p> <pre><code>from norfab.core.nfapi import NorFab\n\ndata = {\n    'broker': {'endpoint': 'tcp://127.0.0.1:5555'},\n    'workers': {'my-worker-1': ['workers/common.yaml'],\n}\n\nnf = NorFab(inventory_data=data, base_dir=\"./\")\nnf.start(run_broker=True, run_workers=[\"my-worker-1\"])\nNFCLIENT = nf.make_client()\n</code></pre> Source code in <code>norfab\\core\\nfapi.py</code> <pre><code>def __init__(\n    self,\n    inventory: str = \"./inventory.yaml\",\n    inventory_data: dict = None,\n    base_dir: str = None,\n    log_level: str = None,\n    run_broker: bool = True,\n    run_workers: Union[bool, list[str]] = True,\n) -&gt; None:\n    self.exiting = False  # flag to signal that Norfab is exiting\n    self.inventory = NorFabInventory(\n        path=inventory, data=inventory_data, base_dir=base_dir\n    )\n    self.run_broker = run_broker\n    self.run_workers = run_workers\n    self.log_queue = Queue()\n    self.log_level = log_level\n    self.broker_endpoint = self.inventory.broker[\"endpoint\"]\n    self.workers_init_timeout = self.inventory.topology.get(\n        \"workers_init_timeout\", 300\n    )\n    self.broker_exit_event = Event()\n    self.workers_exit_event = Event()\n    self.clients_exit_event = Event()\n\n    # create needed folders to kickstart the logs\n    os.makedirs(\n        os.path.join(self.inventory.base_dir, \"__norfab__\", \"files\"), exist_ok=True\n    )\n    os.makedirs(\n        os.path.join(self.inventory.base_dir, \"__norfab__\", \"logs\"), exist_ok=True\n    )\n\n    self.setup_logging()\n    # to fix ValueError: signal only works in main thread of the main interpreter\n    # when trying to use nfapi to instantiate a client from different process\n    try:\n        signal.signal(signal.SIGINT, self.handle_ctrl_c)\n    except Exception:\n        pass\n\n    # find all workers plugins\n    self.register_plugins()\n</code></pre>"},{"location":"api_reference_core_norfab_nfapi/#norfab.core.nfapi.NorFab.register_plugins","title":"<code>register_plugins() -&gt; None</code>","text":"<p>Registers worker plugins by iterating through the entry points in the 'norfab.workers' group and registering each worker plugin.</p> <p>This method loads each entry point and registers it using the <code>register_worker_plugin</code> method.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Any exceptions raised by the entry point loading or registration process.</p> Source code in <code>norfab\\core\\nfapi.py</code> <pre><code>def register_plugins(self) -&gt; None:\n    \"\"\"\n    Registers worker plugins by iterating through the entry points in the\n    'norfab.workers' group and registering each worker plugin.\n\n    This method loads each entry point and registers it using the\n    `register_worker_plugin` method.\n\n    Raises:\n        Exception: Any exceptions raised by the entry point loading or registration process.\n    \"\"\"\n    # register worker plugins from entrypoints\n    eps = entry_points()\n    for entry_point in eps.select(group=\"norfab.workers\"):\n        self.register_worker_plugin(entry_point.name, entry_point)\n\n    # register worker plugins from inventory\n    for service_name, service_data in self.inventory.plugins.items():\n        if service_data.get(\"worker\"):\n            self.register_worker_plugin(service_name, service_data[\"worker\"])\n</code></pre>"},{"location":"api_reference_core_norfab_nfapi/#norfab.core.nfapi.NorFab.register_worker_plugin","title":"<code>register_worker_plugin(service_name: str, worker_plugin: Union[EntryPoint, str]) -&gt; None</code>","text":"<p>Registers a worker plugin for a given service.</p> <p>This method registers a worker plugin under the specified service name. If a plugin is already registered under the same service name and it is different from the provided plugin, an exception is raised.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the service to register the plugin for.</p> required <code>worker_plugin</code> <code>object</code> <p>The worker plugin to be registered.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If a different plugin is already registered under the same service name.</p> Source code in <code>norfab\\core\\nfapi.py</code> <pre><code>def register_worker_plugin(\n    self, service_name: str, worker_plugin: Union[EntryPoint, str]\n) -&gt; None:\n    \"\"\"\n    Registers a worker plugin for a given service.\n\n    This method registers a worker plugin under the specified service name.\n    If a plugin is already registered under the same service name and it is\n    different from the provided plugin, an exception is raised.\n\n    Args:\n        service_name (str): The name of the service to register the plugin for.\n        worker_plugin (object): The worker plugin to be registered.\n\n    Raises:\n        Exception: If a different plugin is already registered under the same service name.\n    \"\"\"\n    existing_plugin = self.worker_plugins.get(service_name)\n    if existing_plugin is None:\n        self.worker_plugins[service_name] = worker_plugin\n    else:\n        log.debug(\n            f\"Worker plugin {worker_plugin} can't be registered for \"\n            f\"service '{service_name}' because plugin '{existing_plugin}' \"\n            f\"was already registered under this service.\"\n        )\n</code></pre>"},{"location":"api_reference_core_norfab_nfapi/#norfab.core.nfapi.NorFab.handle_ctrl_c","title":"<code>handle_ctrl_c(signum, frame) -&gt; None</code>","text":"<p>Handle the CTRL-C signal (SIGINT) to gracefully exit the application.</p> <p>This method is called when the user interrupts the program with a CTRL-C signal. It logs the interruption, performs necessary cleanup by calling <code>self.destroy()</code>, and then signals termination to the main process.</p> <p>Parameters:</p> Name Type Description Default <code>signum</code> <code>int</code> <p>The signal number (should be SIGINT).</p> required <code>frame</code> <code>FrameType</code> <p>The current stack frame.</p> required Note <p>This method reassigns the SIGINT signal to the default handler and sends the SIGINT signal to the current process to ensure proper termination.</p> Source code in <code>norfab\\core\\nfapi.py</code> <pre><code>def handle_ctrl_c(self, signum, frame) -&gt; None:\n    \"\"\"\n    Handle the CTRL-C signal (SIGINT) to gracefully exit the application.\n\n    This method is called when the user interrupts the program with a CTRL-C\n    signal. It logs the interruption, performs necessary cleanup by calling\n    `self.destroy()`, and then signals termination to the main process.\n\n    Args:\n        signum (int): The signal number (should be SIGINT).\n        frame (FrameType): The current stack frame.\n\n    Note:\n        This method reassigns the SIGINT signal to the default handler and\n        sends the SIGINT signal to the current process to ensure proper\n        termination.\n    \"\"\"\n    if self.exiting is False:\n        msg = \"CTRL-C, NorFab exiting, interrupted by user...\"\n        print(f\"\\n{msg}\")\n        log.info(msg)\n        self.destroy()\n        # signal termination to main process\n        signal.signal(signal.SIGINT, signal.default_int_handler)\n        os.kill(os.getpid(), signal.SIGINT)\n</code></pre>"},{"location":"api_reference_core_norfab_nfapi/#norfab.core.nfapi.NorFab.setup_logging","title":"<code>setup_logging() -&gt; None</code>","text":"<p>Sets up logging configuration and starts a log queue listener.</p> <p>This method updates the logging levels for all handlers based on the inventory, configures the logging system using the provided inventory, and starts a log queue listener to process logs from child processes.</p> Source code in <code>norfab\\core\\nfapi.py</code> <pre><code>def setup_logging(self) -&gt; None:\n    \"\"\"\n    Sets up logging configuration and starts a log queue listener.\n\n    This method updates the logging levels for all handlers based on the\n    inventory, configures the logging system using the provided\n    inventory, and starts a log queue listener to process logs from child\n    processes.\n    \"\"\"\n    # update logging levels for all handlers\n    if self.log_level is not None:\n        self.inventory[\"logging\"][\"root\"][\"level\"] = self.log_level\n        for handler in self.inventory[\"logging\"][\"handlers\"].values():\n            handler[\"level\"] = self.log_level\n    # configure logging\n    logging.config.dictConfig(self.inventory[\"logging\"])\n    # start logs queue listener thread to process logs from child processes\n    self.log_listener = logging.handlers.QueueListener(\n        self.log_queue,\n        *logging.getLogger(\"root\").handlers,\n        respect_handler_level=True,\n    )\n    self.log_listener.start()\n</code></pre>"},{"location":"api_reference_core_norfab_nfapi/#norfab.core.nfapi.NorFab.start_broker","title":"<code>start_broker() -&gt; None</code>","text":"<p>Starts the broker process if a broker endpoint is defined. This method initializes and starts a separate process for the broker using the provided broker endpoint. It waits for the broker to signal that it has fully initiated, with a timeout of 30 seconds. If the broker fails to start within this time, the method logs an error message and raises a SystemExit exception.</p> <p>Raises:</p> Type Description <code>SystemExit</code> <p>If the broker fails to start within 30 seconds.</p> Logs <p>Info: When the broker starts successfully. Error: If no broker endpoint is defined or if the broker fails to start.</p> Source code in <code>norfab\\core\\nfapi.py</code> <pre><code>def start_broker(self) -&gt; None:\n    \"\"\"\n    Starts the broker process if a broker endpoint is defined.\n    This method initializes and starts a separate process for the broker using the\n    provided broker endpoint. It waits for the broker to signal that it has fully\n    initiated, with a timeout of 30 seconds. If the broker fails to start within\n    this time, the method logs an error message and raises a SystemExit exception.\n\n    Raises:\n        SystemExit: If the broker fails to start within 30 seconds.\n\n    Logs:\n        Info: When the broker starts successfully.\n        Error: If no broker endpoint is defined or if the broker fails to start.\n    \"\"\"\n    if self.broker_endpoint:\n        init_done_event = Event()  # for worker to signal if its fully initiated\n\n        self.broker = Process(\n            target=start_broker_process,\n            args=(\n                self.broker_endpoint,\n                self.broker_exit_event,\n                self.inventory,\n                self.log_level,\n                self.log_queue,\n                init_done_event,\n            ),\n        )\n        self.broker.start()\n\n        # wait for broker to start\n        start_time = time.time()\n        while 30 &gt; time.time() - start_time:\n            if init_done_event.is_set():\n                break\n            time.sleep(0.1)\n        else:\n            log.info(\n                f\"Broker failed to start in 30 seconds on '{self.broker_endpoint}'\"\n            )\n            raise SystemExit()\n\n        log.info(\n            f\"Started broker, broker listening for connections on '{self.broker_endpoint}'\"\n        )\n    else:\n        log.error(\"Failed to start broker, no broker endpoint defined\")\n</code></pre>"},{"location":"api_reference_core_norfab_nfapi/#norfab.core.nfapi.NorFab.start_worker","title":"<code>start_worker(worker_name, worker_data) -&gt; None</code>","text":"<p>Starts a worker process if it is not already running.</p> <p>Parameters:</p> Name Type Description Default <code>worker_name</code> <code>str</code> <p>The name of the worker to start.</p> required <code>worker_data</code> <code>dict</code> <p>A dictionary containing data about the worker, including any dependencies.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a dependent process is not alive.</p> <code>ServicePluginNotRegistered</code> <p>If no worker plugin is registered for the worker's service.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>norfab\\core\\nfapi.py</code> <pre><code>def start_worker(self, worker_name, worker_data) -&gt; None:\n    \"\"\"\n    Starts a worker process if it is not already running.\n\n    Args:\n        worker_name (str): The name of the worker to start.\n        worker_data (dict): A dictionary containing data about the worker, including any dependencies.\n\n    Raises:\n        RuntimeError: If a dependent process is not alive.\n        norfab_exceptions.ServicePluginNotRegistered: If no worker plugin is registered for the worker's service.\n\n    Returns:\n        None\n    \"\"\"\n    if not self.workers_processes.get(worker_name):\n        log.debug(f\"NFAPI PID {os.getpid()} {worker_name} starting worker process\")\n        worker_inventory = self.inventory[worker_name]\n        init_done_event = Event()  # for worker to signal if its fully initiated\n\n        # check dependent processes\n        if worker_data.get(\"depends_on\"):\n            # check if all dependent processes are alive\n            for w in worker_data[\"depends_on\"]:\n                if not self.workers_processes[w][\"process\"].is_alive():\n                    raise RuntimeError(f\"Dependent process is dead '{w}'\")\n            # check if all depended process fully initialized\n            if not all(\n                self.workers_processes[w][\"init_done\"].is_set()\n                for w in worker_data[\"depends_on\"]\n            ):\n                return\n\n        if not self.worker_plugins.get(worker_inventory[\"service\"]):\n            raise norfab_exceptions.ServicePluginNotRegistered(\n                f\"No worker plugin registered for service '{worker_inventory['service']}'\"\n            )\n        worker_plugin = self.worker_plugins[worker_inventory[\"service\"]]\n\n        self.workers_processes[worker_name] = {\n            \"process\": Process(\n                target=start_worker_process,\n                args=(\n                    worker_plugin,\n                    self.inventory,\n                    self.broker_endpoint,\n                    worker_name,\n                    self.workers_exit_event,\n                    self.log_level,\n                    self.log_queue,\n                    init_done_event,\n                ),\n            ),\n            \"init_done\": init_done_event,\n        }\n\n        self.workers_processes[worker_name][\"process\"].start()\n\n        log.debug(f\"NFAPI PID {os.getpid()} {worker_name} worker process started\")\n</code></pre>"},{"location":"api_reference_core_norfab_nfapi/#norfab.core.nfapi.NorFab.start","title":"<code>start(run_broker: bool = None, run_workers: Union[bool, list] = None) -&gt; None</code>","text":"<p>Starts the broker and specified workers.</p> <p>Parameters:</p> Name Type Description Default <code>run_broker</code> <code>bool</code> <p>If True, starts the broker if it is defined in the inventory topology.</p> <code>None</code> <code>run_workers</code> <code>Union[bool, list]</code> <p>Determines which workers to start. If True, starts all workers defined in the inventory topology.                          If False or None, no workers are started. If a list, starts the specified workers.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a worker fails to start due to missing inventory data.</p> <code>FileNotFoundError</code> <p>If a worker fails to start because the inventory file is not found.</p> <code>Exception</code> <p>If a worker fails to start due to any other error.</p> Notes <ul> <li>The method waits for all workers to initialize within a specified timeout period.</li> <li>If the initialization timeout expires, an error is logged and the system is destroyed.</li> <li>After starting the workers, any startup hooks defined in the inventory are executed.</li> </ul> Source code in <code>norfab\\core\\nfapi.py</code> <pre><code>def start(\n    self,\n    run_broker: bool = None,\n    run_workers: Union[bool, list] = None,\n) -&gt; None:\n    \"\"\"\n    Starts the broker and specified workers.\n\n    Args:\n        run_broker (bool): If True, starts the broker if it is defined in the inventory topology.\n        run_workers (Union[bool, list]): Determines which workers to start. If True, starts all workers defined in the inventory topology.\n                                     If False or None, no workers are started. If a list, starts the specified workers.\n\n    Returns:\n        None\n\n    Raises:\n        KeyError: If a worker fails to start due to missing inventory data.\n        FileNotFoundError: If a worker fails to start because the inventory file is not found.\n        Exception: If a worker fails to start due to any other error.\n\n    Notes:\n        - The method waits for all workers to initialize within a specified timeout period.\n        - If the initialization timeout expires, an error is logged and the system is destroyed.\n        - After starting the workers, any startup hooks defined in the inventory are executed.\n    \"\"\"\n    run_broker = run_broker or self.run_broker\n    run_workers = run_workers or self.run_workers\n    workers_to_start = set()\n\n    # start the broker\n    if run_broker is True and self.inventory.topology.get(\"broker\") is True:\n        # update inventory to include build in services\n        self.add_built_in_workers_inventory()\n        self.start_broker()\n\n    # decide on a set of workers to start\n    if run_workers is False or run_workers is None:\n        run_workers = []\n    elif isinstance(run_workers, list) and run_workers:\n        run_workers = [w.strip() for w in run_workers if w.strip()]\n    # start workers defined in inventory\n    elif run_workers is True:\n        run_workers = self.inventory.topology.get(\"workers\", [])\n\n    # exit if no workers\n    if not run_workers:\n        return\n\n    # form a list of workers to start\n    for worker_name in run_workers:\n        if isinstance(worker_name, dict):\n            worker_name = tuple(worker_name)[0]\n        if worker_name:\n            workers_to_start.add(worker_name)\n        else:\n            log.error(f\"'{worker_name}' - worker name is bad, skipping..\")\n            continue\n\n    while workers_to_start != set(self.workers_processes.keys()):\n        for worker in run_workers:\n            # extract worker name and data/params\n            if isinstance(worker, dict):\n                worker_name = tuple(worker)[0]\n                worker_data = worker[worker_name]\n            elif worker:\n                worker_name = worker\n                worker_data = {}\n            else:\n                continue\n            # verify if need to start this worker\n            if worker_name not in workers_to_start:\n                continue\n            # start worker\n            try:\n                self.start_worker(worker_name, worker_data)\n            # if failed to start remove from workers to start\n            except KeyError:\n                workers_to_start.discard(worker_name)\n                log.error(\n                    f\"'{worker_name}' - failed to start worker, no inventory data found\"\n                )\n            except FileNotFoundError as e:\n                workers_to_start.discard(worker_name)\n                log.error(\n                    f\"'{worker_name}' - failed to start worker, inventory file not found '{e}'\"\n                )\n            except Exception as e:\n                workers_to_start.discard(worker_name)\n                log.exception(\n                    f\"'{worker_name}' - failed to start worker, error '{e}'\"\n                )\n\n        time.sleep(0.01)\n\n    # wait for workers to initialize\n    start_time = time.time()\n    while self.workers_init_timeout &gt; time.time() - start_time:\n        if all(w[\"init_done\"].is_set() for w in self.workers_processes.values()):\n            break\n    else:\n        log.error(\n            f\"TimeoutError - {self.workers_init_timeout}s workers init timeout expired\"\n        )\n        self.destroy()\n\n    # run startup hooks\n    for f in self.inventory.hooks.get(\"startup\", []):\n        f[\"function\"](self, *f.get(\"args\", []), **f.get(\"kwargs\", {}))\n</code></pre>"},{"location":"api_reference_core_norfab_nfapi/#norfab.core.nfapi.NorFab.run","title":"<code>run()</code>","text":"<p>Runs the main loop until a termination signal (CTRL+C) is received. This method checks if there are any broker or worker processes running. If none are detected, it logs a critical message and exits. Otherwise, it enters a loop that continues to run until the <code>exiting</code> flag is set to True.</p> Source code in <code>norfab\\core\\nfapi.py</code> <pre><code>def run(self):\n    \"\"\"\n    Runs the main loop until a termination signal (CTRL+C) is received.\n    This method checks if there are any broker or worker processes running.\n    If none are detected, it logs a critical message and exits.\n    Otherwise, it enters a loop that continues to run until the `exiting` flag is set to True.\n    \"\"\"\n    if not self.broker and not self.workers_processes:\n        log.critical(\n            \"NorFab detected no broker or worker processes running, exiting..\"\n        )\n        return\n\n    while self.exiting is False:\n        time.sleep(0.1)\n</code></pre>"},{"location":"api_reference_core_norfab_nfapi/#norfab.core.nfapi.NorFab.destroy","title":"<code>destroy() -&gt; None</code>","text":"<p>Gracefully stop all NORFAB processes and clean up resources.</p> <p>This method performs the following steps:</p> <ol> <li>Executes any registered exit hooks.</li> <li>Sets the <code>exiting</code> flag to indicate that NORFAB is shutting down.</li> <li>Stops all client processes.</li> <li>Stops all worker processes and waits for them to terminate.</li> <li>Stops the broker process and waits for it to terminate.</li> <li>Stops the logging queue listener.</li> </ol> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>norfab\\core\\nfapi.py</code> <pre><code>def destroy(self) -&gt; None:\n    \"\"\"\n    Gracefully stop all NORFAB processes and clean up resources.\n\n    This method performs the following steps:\n\n    1. Executes any registered exit hooks.\n    2. Sets the `exiting` flag to indicate that NORFAB is shutting down.\n    3. Stops all client processes.\n    4. Stops all worker processes and waits for them to terminate.\n    5. Stops the broker process and waits for it to terminate.\n    6. Stops the logging queue listener.\n\n    Returns:\n        None\n    \"\"\"\n    # run exit hooks\n    for f in self.inventory.hooks.get(\"exit\", []):\n        f[\"function\"](self, *f.get(\"args\", []), **f.get(\"kwargs\", {}))\n\n    if self.exiting is not True:\n        self.exiting = True  # indicate that NorFab already exiting\n        # stop client\n        log.info(\"NorFab is exiting, stopping clients\")\n        self.clients_exit_event.set()\n        if self.client:\n            self.client.destroy()\n        # stop workers\n        log.info(\"NorFab is exiting, stopping workers\")\n        self.workers_exit_event.set()\n        while self.workers_processes:\n            wname, w = self.workers_processes.popitem()\n            w[\"process\"].join()\n            log.info(f\"NorFab is exiting, stopped {wname} worker\")\n        # stop broker\n        log.info(\"NorFab is exiting, stopping broker\")\n        self.broker_exit_event.set()\n        if self.broker:\n            self.broker.join()\n        # stop logging thread\n        log.info(\"NorFab is exiting, stopping logging queue listener\")\n</code></pre>"},{"location":"api_reference_core_norfab_nfapi/#norfab.core.nfapi.NorFab.make_client","title":"<code>make_client(broker_endpoint: str = None) -&gt; NFPClient</code>","text":"<p>Creates and returns an NFPClient instance.</p> <p>Parameters:</p> Name Type Description Default <code>broker_endpoint</code> <code>str</code> <p>The broker endpoint to connect to. If not provided, the instance's broker_endpoint attribute will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>NFPClient</code> <code>NFPClient</code> <p>The created client instance if a broker endpoint is defined.</p> <code>None</code> <code>NFPClient</code> <p>If no broker endpoint is defined.</p> Notes <p>If this is the first client being created, it will be assigned to the instance's client attribute.</p> Source code in <code>norfab\\core\\nfapi.py</code> <pre><code>def make_client(self, broker_endpoint: str = None) -&gt; NFPClient:\n    \"\"\"\n    Creates and returns an NFPClient instance.\n\n    Args:\n        broker_endpoint (str, optional): The broker endpoint to connect to.\n            If not provided, the instance's broker_endpoint attribute will be used.\n\n    Returns:\n        NFPClient: The created client instance if a broker endpoint is defined.\n        None: If no broker endpoint is defined.\n\n    Notes:\n        If this is the first client being created, it will be assigned to the\n        instance's client attribute.\n    \"\"\"\n    if broker_endpoint or self.broker_endpoint:\n        client = NFPClient(\n            self.inventory,\n            broker_endpoint or self.broker_endpoint,\n            \"NFPClient\",\n            self.clients_exit_event,\n        )\n        if self.client is None:  # own the first client\n            self.client = client\n        return client\n    else:\n        log.error(\"Failed to make client, no broker endpoint defined\")\n        return None\n</code></pre>"},{"location":"api_reference_core_norfab_simple_inventory/","title":"Simple Inventory","text":"<p>Simple Local Inventory is an inventory plugin to load inventory data from locally stored files.</p> <p>Sample inventory file</p> <pre><code>broker:\n  endpoint: \"tcp://127.0.0.1:5555\"\n\nlogging:\n  handlers:\n    terminal:\n      level: CRITICAL\n    file:\n      level: DEBUG\n\nworkers:\n  nornir-*:\n    - nornir/common.yaml\n  nornir-worker-1:\n    - nornir/nornir-worker-1.yaml\n\ntopology:\n  broker: True\n  workers:\n    - nornir-worker-1\n</code></pre> <p>where <code>nornir/common.yaml</code> contains</p> <pre><code>service: nornir\nbroker_endpoint: \"tcp://127.0.0.1:5555\"\nrunner:\n  plugin: RetryRunner\n  options:\n    num_workers: 100\n    num_connectors: 10\n    connect_retry: 3\n    connect_backoff: 1000\n    connect_splay: 100\n    task_retry: 3\n    task_backoff: 1000\n    task_splay: 100\n    reconnect_on_fail: True\n    task_timeout: 600\n</code></pre> <p>and <code>nornir/nornir-worker-1.yaml</code> contains</p> <pre><code>hosts:\n  csr1000v-1:\n    hostname: sandbox-1.lab.com\n    platform: cisco_ios\n    username: developer\n    password: secretpassword\n  csr1000v-2:\n    hostname: sandbox-2.lab.com\n    platform: cisco_ios\n    username: developer\n    password: secretpassword\ngroups: {}\ndefaults: {}\n</code></pre> <p>Whenever inventory queried to provide data for worker with name <code>nornir-worker-1</code> Simple Inventory iterates over <code>workers</code> dictionary and recursively merges data for keys (glob patterns) that matched worker name.</p>"},{"location":"api_reference_core_norfab_simple_inventory/#norfab.core.inventory.WorkersInventory","title":"<code>WorkersInventory(path: str, data: dict)</code>","text":"<p>Class to collect and server NorFab workers inventory data, forming it by recursively merging all data files that associated with the name of worker requesting inventory data.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>OS path to the top folder with workers inventory data.</p> <code>data</code> <code>dict</code> <p>Dictionary keyed by glob patterns matching workers' names and values being a list of OS paths to files or dictionaries with workers' inventory data.</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>str) -&gt; Any: Retrieves and merges inventory data for the specified worker name. Raises a KeyError if no inventory data is found for the given name.</p> Source code in <code>norfab\\core\\inventory.py</code> <pre><code>def __init__(self, path: str, data: dict) -&gt; None:\n    self.path = path\n    self.data = data\n</code></pre>"},{"location":"api_reference_core_norfab_simple_inventory/#norfab.core.inventory.NorFabInventory","title":"<code>NorFabInventory(path: str = None, data: dict = None, base_dir: str = None)</code>","text":"<p>Initialize NorFab Simple Inventory object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to the inventory YAML file. Defaults to None.</p> <code>None</code> <code>data</code> <code>dict</code> <p>The inventory data dictionary. Defaults to None.</p> <code>None</code> <code>base_dir</code> <code>str</code> <p>The base directory for the inventory. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If neither path nor data is provided.</p> Source code in <code>norfab\\core\\inventory.py</code> <pre><code>def __init__(\n    self, path: str = None, data: dict = None, base_dir: str = None\n) -&gt; None:\n    \"\"\"\n    Initialize NorFab Simple Inventory object.\n\n    Args:\n        path (str, optional): The file path to the inventory YAML file. Defaults to None.\n        data (dict, optional): The inventory data dictionary. Defaults to None.\n        base_dir (str, optional): The base directory for the inventory. Defaults to None.\n\n    Raises:\n        RuntimeError: If neither path nor data is provided.\n    \"\"\"\n    self.broker = {}\n    self.workers = {}\n    self.topology = {}\n    self.logging = {}\n    self.hooks = {}\n    self.plugins = {}\n\n    if data:\n        self.base_dir = base_dir or os.path.split(os.getcwd())[0]\n        self.load_data(data)\n    elif path:\n        path = os.path.abspath(path)\n        self.base_dir = base_dir or os.path.split(path)[0]\n        self.load_path(path)\n    else:\n        raise RuntimeError(\n            \"Either path to inventory.yaml or inventory data dictionary must be provided.\"\n        )\n</code></pre>"},{"location":"api_reference_core_norfab_simple_inventory/#norfab.core.inventory.NorFabInventory.load_data","title":"<code>load_data(data) -&gt; None</code>","text":"<p>Load and initialize various components from the provided data dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A dictionary containing configuration data for initializing          the broker, workers, topology, logging, hooks, and plugins.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>norfab\\core\\inventory.py</code> <pre><code>def load_data(self, data) -&gt; None:\n    \"\"\"\n    Load and initialize various components from the provided data dictionary.\n\n    Args:\n        data (dict): A dictionary containing configuration data for initializing\n                     the broker, workers, topology, logging, hooks, and plugins.\n\n    Returns:\n        None\n    \"\"\"\n    self.broker = data.pop(\"broker\", {})\n    self.workers = WorkersInventory(self.base_dir, data.pop(\"workers\", {}))\n    self.topology = data.pop(\"topology\", {})\n    self.logging = make_logging_config(self.base_dir, data.pop(\"logging\", {}))\n    self.hooks = make_hooks(self.base_dir, data.pop(\"hooks\", {}))\n    self.plugins = data.pop(\"plugins\", {})\n</code></pre>"},{"location":"api_reference_core_norfab_simple_inventory/#norfab.core.inventory.NorFabInventory.load_plugin","title":"<code>load_plugin(service: str) -&gt; dict</code>","text":"<p>Loads and returns a plugin instance for the specified service.</p> <p>Parameters:</p> Name Type Description Default <code>service</code> <code>str</code> <p>The name of the service for which the plugin should be loaded.</p> required <p>Returns:</p> Name Type Description <code>dictionary</code> <code>dict</code> <p>Dictionary with plugin corresponding to the given service.</p> Source code in <code>norfab\\core\\inventory.py</code> <pre><code>def load_plugin(self, service: str) -&gt; dict:\n    \"\"\"\n    Loads and returns a plugin instance for the specified service.\n\n    Args:\n        service (str): The name of the service for which the plugin should be loaded.\n\n    Returns:\n        dictionary: Dictionary with plugin corresponding to the given service.\n    \"\"\"\n    return make_plugin(self.base_dir, self.plugins, service)\n</code></pre>"},{"location":"api_reference_core_norfab_simple_inventory/#norfab.core.inventory.NorFabInventory.load_path","title":"<code>load_path(path: str) -&gt; None</code>","text":"<p>Loads inventory data from a specified file path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to the inventory.yaml file.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist at the specified path.</p> <code>AssertionError</code> <p>If the path does not point to a file.</p> Source code in <code>norfab\\core\\inventory.py</code> <pre><code>def load_path(self, path: str) -&gt; None:\n    \"\"\"\n    Loads inventory data from a specified file path.\n\n    Args:\n        path (str): The file path to the inventory.yaml file.\n\n    Raises:\n        FileNotFoundError: If the file does not exist at the specified path.\n        AssertionError: If the path does not point to a file.\n    \"\"\"\n    if not os.path.exists(path):\n        msg = f\"inventory.yaml file not found under provided path `{path}`\"\n        log.critical(msg)\n        raise FileNotFoundError(msg)\n\n    assert os.path.isfile(path), \"Path not pointing to a file\"\n\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        rendered = render_jinja2_template(f.read())\n        data = yaml.safe_load(rendered)\n\n    self.load_data(data)\n</code></pre>"},{"location":"api_reference_core_norfab_simple_inventory/#norfab.core.inventory.NorFabInventory.__getitem__","title":"<code>__getitem__(key: str) -&gt; Any</code>","text":"<p>Retrieve an item from the inventory.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the item to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value associated with the given key.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the key is not found in the inventory.</p> Source code in <code>norfab\\core\\inventory.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Any:\n    \"\"\"\n    Retrieve an item from the inventory.\n\n    Args:\n        key (str): The key of the item to retrieve.\n\n    Returns:\n        Any: The value associated with the given key.\n\n    Raises:\n        KeyError: If the key is not found in the inventory.\n    \"\"\"\n    if key in self.__slots__:\n        return getattr(self, key)\n    else:\n        return self.workers[key]\n</code></pre>"},{"location":"api_reference_core_norfab_simple_inventory/#norfab.core.inventory.NorFabInventory.get","title":"<code>get(item: str, default: Any = None) -&gt; Any</code>","text":"<p>Retrieve the value of the specified item from the inventory.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>str</code> <p>The name of the item to retrieve.</p> required <code>default</code> <code>Any</code> <p>The value to return if the item is not found. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value of the specified item if it exists, otherwise the default value.</p> Source code in <code>norfab\\core\\inventory.py</code> <pre><code>def get(self, item: str, default: Any = None) -&gt; Any:\n    \"\"\"\n    Retrieve the value of the specified item from the inventory.\n\n    Args:\n        item (str): The name of the item to retrieve.\n        default (Any, optional): The value to return if the item is not found. Defaults to None.\n\n    Returns:\n        Any: The value of the specified item if it exists, otherwise the default value.\n    \"\"\"\n    if item in self.__slots__:\n        return getattr(self, item)\n    else:\n        return default\n</code></pre>"},{"location":"api_reference_core_norfab_simple_inventory/#norfab.core.inventory.NorFabInventory.dict","title":"<code>dict() -&gt; Dict[str, Any]</code>","text":"<p>Convert the inventory object to a dictionary representation.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the inventory details: - broker (str): The broker information. - workers (Any): The data related to workers. - topology (Any): The topology information. - logging (Any): The logging configuration. - hooks (dict): A dictionary containing startup and exit hooks, where each     hook's function is represented by its name.</p> Source code in <code>norfab\\core\\inventory.py</code> <pre><code>def dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert the inventory object to a dictionary representation.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the inventory details:\n            - broker (str): The broker information.\n            - workers (Any): The data related to workers.\n            - topology (Any): The topology information.\n            - logging (Any): The logging configuration.\n            - hooks (dict): A dictionary containing startup and exit hooks, where each\n                hook's function is represented by its name.\n    \"\"\"\n    ret = {\n        \"broker\": self.broker,\n        \"workers\": self.workers.data,\n        \"topology\": self.topology,\n        \"logging\": self.logging,\n        \"hooks\": {},\n        \"plugins\": {},\n    }\n\n    # add hooks replacing hook function with its name\n    for attachpoint, hooks in self.hooks.items():\n        ret[\"hooks\"][attachpoint] = []\n        for hook in hooks:\n            ret[\"hooks\"][attachpoint].append(\n                {**hook, \"function\": hook[\"function\"].__name__}\n            )\n\n    # add plugins replacing plugin classes with their name\n    for service_name, service_data in self.plugins.items():\n        ret[\"plugins\"][service_name] = {**service_data}\n        if service_data.get(\"worker\"):\n            ret[\"plugins\"][service_name][\"worker\"] = service_data[\"worker\"]\n        if service_data.get(\"nfcli\"):\n            ret[\"plugins\"][service_name][\"nfcli\"][\"shell_model\"] = service_data[\n                \"nfcli\"\n            ][\"shell_model\"]\n\n    return ret\n</code></pre>"},{"location":"api_reference_core_norfab_simple_inventory/#norfab.core.inventory.make_logging_config","title":"<code>make_logging_config(base_dir: str, inventory: dict) -&gt; dict</code>","text":"<p>Combines the inventory logging section with a predefined logging configuration. This function updates the predefined logging configuration with the settings provided in the inventory dictionary. It ensures that the log file is stored in the specified base directory and merges handlers, formatters, and root logger settings from the inventory into the predefined configuration.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>str</code> <p>The base directory where the log file will be stored.</p> required <code>inventory</code> <code>dict</code> <p>A dictionary containing logging configuration settings.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The combined logging configuration.</p> Source code in <code>norfab\\core\\inventory.py</code> <pre><code>def make_logging_config(base_dir: str, inventory: dict) -&gt; dict:\n    \"\"\"\n    Combines the inventory logging section with a predefined logging configuration.\n    This function updates the predefined logging configuration with the settings\n    provided in the inventory dictionary. It ensures that the log file is stored\n    in the specified base directory and merges handlers, formatters, and root logger\n    settings from the inventory into the predefined configuration.\n\n    Args:\n        base_dir (str): The base directory where the log file will be stored.\n        inventory (dict): A dictionary containing logging configuration settings.\n\n    Returns:\n        dict: The combined logging configuration.\n    \"\"\"\n    logging_config_listener[\"handlers\"][\"file\"][\"filename\"] = os.path.join(\n        base_dir, \"__norfab__\", \"logs\", \"norfab.log\"\n    )\n\n    if not inventory:\n        return logging_config_listener\n\n    log_cfg = copy.deepcopy(inventory)\n    ret = copy.deepcopy(logging_config_listener)\n\n    # merge handlers\n    ret[\"handlers\"][\"terminal\"].update(log_cfg.get(\"handlers\", {}).pop(\"terminal\", {}))\n    ret[\"handlers\"][\"file\"].update(log_cfg.get(\"handlers\", {}).pop(\"file\", {}))\n    ret[\"handlers\"].update(log_cfg.pop(\"handlers\", {}))\n    # merge formatters\n    ret[\"formatters\"][\"default\"].update(\n        log_cfg.get(\"formatters\", {}).pop(\"default\", {})\n    )\n    ret[\"formatters\"].update(log_cfg.pop(\"formatters\", {}))\n    # merge root logger\n    ret[\"root\"].update(log_cfg.pop(\"root\", {}))\n    if \"file\" not in ret[\"root\"][\"handlers\"]:\n        ret[\"root\"][\"handlers\"].append(\"file\")\n    if \"terminal\" not in ret[\"root\"][\"handlers\"]:\n        ret[\"root\"][\"handlers\"].append(\"terminal\")\n    # merge remaining config\n    ret.update(log_cfg)\n    ret[\"disable_existing_loggers\"] = False\n\n    return ret\n</code></pre>"},{"location":"api_reference_core_norfab_simple_inventory/#norfab.core.inventory.merge_recursively","title":"<code>merge_recursively(data: dict, merge: dict) -&gt; None</code>","text":"<p>Function to merge two dictionaries recursively.</p> <p>This function takes two dictionaries and merges the second dictionary into the first one. If both dictionaries have a key with a dictionary as its value, the function will merge those dictionaries recursively. If both dictionaries have a key with a list as its value, the function will append the elements of the second list to the first list, avoiding duplicates. For other types of values, the function will override the value in the first dictionary with the value from the second dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The primary dictionary to be merged into.</p> required <code>merge</code> <code>dict</code> <p>The dictionary to merge into the primary dictionary.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If either of the inputs is not a dictionary.</p> Source code in <code>norfab\\core\\inventory.py</code> <pre><code>def merge_recursively(data: dict, merge: dict) -&gt; None:\n    \"\"\"\n    Function to merge two dictionaries recursively.\n\n    This function takes two dictionaries and merges the second dictionary into the first one.\n    If both dictionaries have a key with a dictionary as its value, the function will merge\n    those dictionaries recursively. If both dictionaries have a key with a list as its value,\n    the function will append the elements of the second list to the first list, avoiding duplicates.\n    For other types of values, the function will override the value in the first dictionary\n    with the value from the second dictionary.\n\n    Args:\n        data: The primary dictionary to be merged into.\n        merge: The dictionary to merge into the primary dictionary.\n\n    Raises:\n        AssertionError: If either of the inputs is not a dictionary.\n    \"\"\"\n    assert isinstance(data, dict) and isinstance(\n        merge, dict\n    ), f\"Only supports dictionary/dictionary data merges, not {type(data)}/{type(merge)}\"\n    for k, v in merge.items():\n        if k in data:\n            # merge two lists\n            if isinstance(data[k], list) and isinstance(v, list):\n                for i in v:\n                    if i not in data[k]:\n                        data[k].append(i)\n            # recursively merge dictionaries\n            elif isinstance(data[k], dict) and isinstance(v, dict):\n                merge_recursively(data[k], v)\n            # rewrite existing value with new data\n            else:\n                data[k] = v\n        else:\n            data[k] = v\n</code></pre>"},{"location":"api_reference_core_norfab_simple_inventory/#norfab.core.inventory.make_hooks","title":"<code>make_hooks(base_dir: str, hooks: List) -&gt; Dict[str, List]</code>","text":"<p>Load and organize hook functions from specified modules.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>str</code> <p>The base directory to include in the search path for modules.</p> required <code>hooks</code> <code>list</code> <p>A list of dictionaries, each containing: - \"function\" (str): The full path to the hook function in the format 'module.submodule.function'. - \"attachpoint\" (str): The key to which the hook function should be attached.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, List]</code> <p>A dictionary where keys are attach points and values are lists of hook function dictionaries.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error importing or loading a hook function.</p> Source code in <code>norfab\\core\\inventory.py</code> <pre><code>def make_hooks(base_dir: str, hooks: List) -&gt; Dict[str, List]:\n    \"\"\"\n    Load and organize hook functions from specified modules.\n\n    Args:\n        base_dir (str): The base directory to include in the search path for modules.\n        hooks (list): A list of dictionaries, each containing:\n            - \"function\" (str): The full path to the hook function in the format 'module.submodule.function'.\n            - \"attachpoint\" (str): The key to which the hook function should be attached.\n\n    Returns:\n        dict: A dictionary where keys are attach points and values are lists of hook function dictionaries.\n\n    Raises:\n        Exception: If there is an error importing or loading a hook function.\n    \"\"\"\n    ret = {}\n\n    # make sure to include current and base_dir directories in search path\n    if os.getcwd() not in sys.path:\n        sys.path.append(os.getcwd())\n    if base_dir not in sys.path:\n        sys.path.append(base_dir)\n\n    # load hook functions one by one\n    for attachpoint, hooks in hooks.items():\n        ret[attachpoint] = []\n        for hook in hooks:\n            try:\n                imp_str, hook_function_name = hook[\"function\"].split(\":\")\n                log.info(f\"Importing hook '{imp_str}' function '{hook_function_name}'\")\n                hook_module = __import__(imp_str, fromlist=[\"\"])\n                hook[\"function\"] = getattr(hook_module, hook_function_name)\n                ret[attachpoint].append(hook)\n                log.info(f\"Successfully loaded hook function {hook['function']}\")\n            except Exception:\n                log.exception(f\"Failed loading hook {hook}\")\n\n    return ret\n</code></pre>"},{"location":"api_reference_core_norfab_simple_inventory/#norfab.core.inventory.make_plugin","title":"<code>make_plugin(base_dir: str, plugins: Dict, service: str) -&gt; Dict</code>","text":"<p>Loads and initializes plugin function for the given service.</p> <p>This function ensures that the current working directory and the specified base directory are included in the Python search path. It then iterates through the list of worker plugins provided in the <code>plugins</code> dictionary, dynamically imports the specified plugin classes, and adds them to the returned dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>str</code> <p>The base directory to include in the search path.</p> required <code>plugins</code> <code>dict</code> <p>A dictionary containing plugin definition. The             dictionary should have a key \"workers\" which maps to a             list of plugin import path. Each configuration should             include a \"service\" key and a \"plugin\" key in the format             \"module:ClassName\".</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict</code> <p>A dictionary with a key \"workers\" mapping to worker plugin and an \"nfcli\" key with a mapping to NFCLI shell plugin details.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error loading any of the plugin classes, an        exception is logged and the function continues with the next        plugin.</p> Example <p>plugins = {     \"workers\": [         {             \"service\": \"example_service\",             \"plugin\": \"my.path.to.example_module:ExamplePluginClass\"         }     ] } result = make_plugin(\"/path/to/base_dir\", plugins, example_service)</p> Source code in <code>norfab\\core\\inventory.py</code> <pre><code>def make_plugin(base_dir: str, plugins: Dict, service: str) -&gt; Dict:\n    \"\"\"\n    Loads and initializes plugin function for the given service.\n\n    This function ensures that the current working directory and the specified\n    base directory are included in the Python search path. It then iterates\n    through the list of worker plugins provided in the `plugins` dictionary,\n    dynamically imports the specified plugin classes, and adds them to the\n    returned dictionary.\n\n    Args:\n        base_dir (str): The base directory to include in the search path.\n        plugins (dict): A dictionary containing plugin definition. The\n                        dictionary should have a key \"workers\" which maps to a\n                        list of plugin import path. Each configuration should\n                        include a \"service\" key and a \"plugin\" key in the format\n                        \"module:ClassName\".\n\n    Returns:\n        dict: A dictionary with a key \"workers\" mapping to worker plugin and an\n            \"nfcli\" key with a mapping to NFCLI shell plugin details.\n\n    Raises:\n        Exception: If there is an error loading any of the plugin classes, an\n                   exception is logged and the function continues with the next\n                   plugin.\n\n    Example:\n        plugins = {\n            \"workers\": [\n                {\n                    \"service\": \"example_service\",\n                    \"plugin\": \"my.path.to.example_module:ExamplePluginClass\"\n                }\n            ]\n        }\n        result = make_plugin(\"/path/to/base_dir\", plugins, example_service)\n    \"\"\"\n    ret = {}\n\n    # make sure to include current and base_dir directories in search path\n    if os.getcwd() not in sys.path:\n        sys.path.append(os.getcwd())\n    if base_dir not in sys.path:\n        sys.path.append(base_dir)\n\n    # load service plugins\n    service_data = copy.deepcopy(plugins[service])\n    ret[service] = service_data\n    # import worker plugin\n    if service_data.get(\"worker\"):\n        try:\n            imp_str, plugin_class_name = service_data[\"worker\"].split(\":\")\n            log.info(\n                f\"Importing '{plugin_class_name}' worker plugin class from '{imp_str}' module\"\n            )\n            plugin_module = __import__(imp_str, fromlist=[\"\"])\n            service_data[\"worker\"] = getattr(plugin_module, plugin_class_name)\n            log.info(\n                f\"Successfully loaded worker plugin {plugin_class_name} for service {service}\"\n            )\n        except Exception:\n            log.exception(f\"Failed loading worker plugin '{service_data['worker']}'\")\n    # import nfcli pydantic model\n    if service_data.get(\"nfcli\"):\n        try:\n            imp_str, plugin_class_name = service_data[\"nfcli\"][\"shell_model\"].split(\":\")\n            log.info(\n                f\"Importing '{plugin_class_name}' nfcli pydantic model plugin class from '{imp_str}' module\"\n            )\n            plugin_module = __import__(imp_str, fromlist=[\"\"])\n            service_data[\"nfcli\"][\"shell_model\"] = getattr(\n                plugin_module, plugin_class_name\n            )\n            log.info(\n                f\"Successfully loaded nfcli pydantic model plugin class {plugin_class_name} for service {service}\"\n            )\n        except Exception:\n            log.exception(\n                f\"Failed loading nfcli pydantic model plugin class '{service_data['nfcli']}'\"\n            )\n\n    return ret\n</code></pre>"},{"location":"api_reference_core_norfab_simple_inventory/#norfab.core.inventory.render_jinja2_template","title":"<code>render_jinja2_template(template: str, context: dict = None, filters: dict = None) -&gt; List[str]</code>","text":"<p>Renders a Jinja2 template with the given context and custom filters.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>The Jinja2 template as a string.</p> required <code>context</code> <code>dict</code> <p>A dictionary containing the context variables for the template. Defaults to None.</p> <code>None</code> <code>filters</code> <code>dict</code> <p>A dictionary containing custom Jinja2 filters. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The rendered template as a string.</p> <p>Raises:</p> Type Description <code>TemplateError</code> <p>If there is an error in rendering the template.</p> Source code in <code>norfab\\core\\inventory.py</code> <pre><code>def render_jinja2_template(\n    template: str, context: dict = None, filters: dict = None\n) -&gt; List[str]:\n    \"\"\"\n    Renders a Jinja2 template with the given context and custom filters.\n\n    Args:\n        template (str): The Jinja2 template as a string.\n        context (dict, optional): A dictionary containing the context variables for the template. Defaults to None.\n        filters (dict, optional): A dictionary containing custom Jinja2 filters. Defaults to None.\n\n    Returns:\n        List[str]: The rendered template as a string.\n\n    Raises:\n        TemplateError: If there is an error in rendering the template.\n    \"\"\"\n    rendered = \"\"\n    filters = filters or {}\n    context = context or {}\n\n    # get OS environment variables\n    context[\"env\"] = {k: v for k, v in os.environ.items()}\n\n    # render template\n    j2env = Environment(loader=\"BaseLoader\")\n    j2env.filters.update(filters)  # add custom filters\n    renderer = j2env.from_string(template)\n    rendered = renderer.render(**context)\n\n    return rendered\n</code></pre>"},{"location":"api_reference_core_norfab_worker/","title":"Worker Base","text":""},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.Job","title":"<code>Job(worker: object = None, juuid: str = None, client_address: str = None, timeout: int = None, args: list = None, kwargs: dict = None, task: str = None, client_input_queue: object = None)</code>","text":"Source code in <code>norfab\\core\\worker.py</code> <pre><code>def __init__(\n    self,\n    worker: object = None,\n    juuid: str = None,\n    client_address: str = None,\n    timeout: int = None,\n    args: list = None,\n    kwargs: dict = None,\n    task: str = None,\n    client_input_queue: object = None,\n):\n    self.worker = worker\n    self.juuid = juuid\n    self.client_address = client_address\n    self.timeout = timeout\n    self.args = args or []\n    self.kwargs = kwargs or {}\n    self.task = task\n    self.client_input_queue = client_input_queue\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.Job.event","title":"<code>event(message: str, **kwargs: Any)</code>","text":"<p>Handles an event by forwarding it to the worker.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message describing the event.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to include in the event.</p> <code>{}</code> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def event(self, message: str, **kwargs: Any):\n    \"\"\"\n    Handles an event by forwarding it to the worker.\n\n    Args:\n        message (str): The message describing the event.\n        **kwargs: Additional keyword arguments to include in the event.\n    \"\"\"\n    kwargs.setdefault(\"task\", self.task)\n    if self.kwargs.get(\"progress\", False) and self.juuid and self.worker:\n        self.worker.event(\n            message=message,\n            juuid=self.juuid,\n            client_address=self.client_address,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.Job.stream","title":"<code>stream(data: bytes) -&gt; None</code>","text":"<p>Streams data to the broker.</p> <p>This method sends a message containing the client address, a unique identifier (UUID), a status code, and the provided data to the broker.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The data to be streamed to the broker.</p> required Source code in <code>norfab\\core\\worker.py</code> <pre><code>def stream(self, data: bytes) -&gt; None:\n    \"\"\"\n    Streams data to the broker.\n\n    This method sends a message containing the client address, a unique\n    identifier (UUID), a status code, and the provided data to the broker.\n\n    Args:\n        data (bytes): The data to be streamed to the broker.\n    \"\"\"\n    msg = [\n        self.client_address.encode(\"utf-8\"),\n        b\"\",\n        self.juuid.encode(\"utf-8\"),\n        b\"200\",\n        data,\n    ]\n    self.worker.send_to_broker(NFP.STREAM, msg)\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.Job.wait_client_input","title":"<code>wait_client_input(timeout: int = 10) -&gt; Any</code>","text":"<p>Waits for input from the client within a specified timeout period if no item is available within the specified timeout, it returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>The maximum time (in seconds) to wait for input</p> <code>10</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The item retrieved from the <code>client_input_queue</code></p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def wait_client_input(self, timeout: int = 10) -&gt; Any:\n    \"\"\"\n    Waits for input from the client within a specified timeout period if no item\n    is available within the specified timeout, it returns `None`.\n\n    Args:\n        timeout (int, optional): The maximum time (in seconds) to wait for input\n\n    Returns:\n        Any: The item retrieved from the `client_input_queue`\n    \"\"\"\n    try:\n        return self.client_input_queue.get(block=True, timeout=timeout)\n    except queue.Empty:\n        pass\n\n    return None\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.Task","title":"<code>Task(input: Optional[BaseModel] = None, output: Optional[BaseModel] = None, description: Optional[str] = None, fastapi: Optional[dict] = None, mcp: Optional[dict] = None)</code>","text":"<p>Validate is a class-based decorator that accept arguments, designed to validate the input arguments of a task function using a specified Pydantic model. It ensures that the arguments passed to the decorated function conform to the schema defined in the model.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>BaseModel</code> <p>A Pydantic model used to validate the function arguments.</p> <code>name</code> <code>str</code> <p>The name of the task, which is used to register the task for calling, by default set equal to the name of decorated function.</p> <code>result_model</code> <code>BaseModel</code> <p>A Pydantic model used to validate the function's return value.</p> <code>fastapi</code> <code>dict</code> <p>Dictionary with parameters for FastAPI <code>app.add_api_route</code> method</p> <code>mcp</code> <code>dict</code> <p>Dictionary with parameters for MCP <code>mcp.types.Tool</code> class</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Callable) -&gt; Callable: Wraps the target function and validates its arguments before execution.</p> <code>merge_args_to_kwargs</code> <p>List, kwargs: Dict) -&gt; Dict: Merges positional arguments (<code>args</code>) and keyword arguments (<code>kwargs</code>) into a single dictionary, mapping positional arguments to their corresponding parameter names based on the function's signature.</p> <code>validate_input</code> <p>List, kwargs: Dict) -&gt; None: Validates merged arguments against Pydantic model. If validation fails, an exception is raised.</p> Usage <p>@Task()(input=YourPydanticModel) def your_function(arg1, arg2, ...):     # Function implementation     pass</p> Notes <ul> <li>The decorator uses <code>inspect.getfullargspec</code> to analyze the function's signature   and properly map arguments for validation.</li> </ul> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def __init__(\n    self,\n    input: Optional[BaseModel] = None,\n    output: Optional[BaseModel] = None,\n    description: Optional[str] = None,\n    fastapi: Optional[dict] = None,\n    mcp: Optional[dict] = None,\n) -&gt; None:\n    self.input = input\n    self.output = output or Result\n    self.description = description\n    if fastapi is False:\n        self.fastapi = False\n    else:\n        self.fastapi = fastapi or {}\n    if mcp is False:\n        self.mcp = False\n    else:\n        self.mcp = mcp or {}\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.Task.__call__","title":"<code>__call__(function: Callable) -&gt; Callable</code>","text":"<p>Decorator to register a function as a worker task with input/output validation and optional argument filtering.</p> <p>This method wraps the provided function, validates its input arguments and output, and registers it as a task. It also removes 'job' and 'progress' keyword arguments if the wrapped function does not accept them.</p> <p>Side Effects:</p> <pre><code>- Sets self.function, self.description, and self.name based on the provided function.\n- Initializes input model if not already set.\n- Updates the global NORFAB_WORKER_TASKS with the task schema.\n</code></pre> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def __call__(self, function: Callable) -&gt; Callable:\n    \"\"\"\n    Decorator to register a function as a worker task with input/output\n    validation and optional argument filtering.\n\n    This method wraps the provided function, validates its input arguments\n    and output, and registers it as a task. It also removes 'job' and\n    'progress' keyword arguments if the wrapped function does not accept them.\n\n    Side Effects:\n\n        - Sets self.function, self.description, and self.name based on the provided function.\n        - Initializes input model if not already set.\n        - Updates the global NORFAB_WORKER_TASKS with the task schema.\n    \"\"\"\n    self.function = function\n    self.description = self.description or function.__doc__\n    self.name = function.__name__\n\n    if self.input is None:\n        self.make_input_model()\n\n    @functools.wraps(self.function)\n    def wrapper(*args, **kwargs):\n        # remove `job` argument if function does not expect it\n        if self.is_need_argument(function, \"job\") is False:\n            _ = kwargs.pop(\"job\", None)\n\n        # remove `progress` argument if function does not expect it\n        if self.is_need_argument(function, \"progress\") is False:\n            _ = kwargs.pop(\"progress\", None)\n\n        # validate input arguments\n        self.validate_input(args, kwargs)\n\n        ret = self.function(*args, **kwargs)\n\n        # validate result\n        self.validate_output(ret)\n\n        return ret\n\n    NORFAB_WORKER_TASKS.update(self.make_task_schema(wrapper))\n\n    log.debug(\n        f\"{function.__module__} PID {os.getpid()} registered task '{function.__name__}'\"\n    )\n\n    return wrapper\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.Task.make_input_model","title":"<code>make_input_model()</code>","text":"<p>Dynamically creates a Pydantic input model for the worker's function by inspecting its signature.</p> <p>This method uses <code>inspect.getfullargspec</code> to extract the function's argument names, default values, keyword-only arguments, and type annotations. It then constructs a dictionary of field specifications, giving preference to type annotations where available, and excluding special parameters such as 'self', 'return', 'job', and any args or *kwargs. The resulting specification is used to create a Pydantic model, which is assigned to <code>self.input</code>.</p> <p>The generated model used for input validation.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def make_input_model(self):\n    \"\"\"\n    Dynamically creates a Pydantic input model for the worker's function by inspecting its signature.\n\n    This method uses `inspect.getfullargspec` to extract the function's argument names, default values,\n    keyword-only arguments, and type annotations. It then constructs a dictionary of field specifications,\n    giving preference to type annotations where available, and excluding special parameters such as 'self',\n    'return', 'job', and any *args or **kwargs. The resulting specification is used to create a Pydantic\n    model, which is assigned to `self.input`.\n\n    The generated model used for input validation.\n    \"\"\"\n    (\n        fun_args,  # list of the positional parameter names\n        fun_varargs,  # name of the * parameter or None\n        fun_varkw,  # name of the ** parameter or None\n        fun_defaults,  # tuple of default argument values of the last n positional parameters\n        fun_kwonlyargs,  # list of keyword-only parameter names\n        fun_kwonlydefaults,  # dictionary mapping kwonlyargs parameter names to default values\n        fun_annotations,  # dictionary mapping parameter names to annotations\n    ) = inspect.getfullargspec(self.function)\n\n    # form a dictionary keyed by args with their default values\n    args_with_defaults = dict(\n        zip(reversed(fun_args or []), reversed(fun_defaults or []))\n    )\n\n    # form a dictionary keyed by args that has no defaults with values set to\n    # (Any, None) tuple if make_optional is True else set to (Any, ...)\n    args_no_defaults = {\n        k: (Any, ...) for k in fun_args if k not in args_with_defaults\n    }\n\n    # form dictionary keyed by args with annotations and tuple values\n    args_with_hints = {\n        k: (v, args_with_defaults.get(k, ...)) for k, v in fun_annotations.items()\n    }\n\n    # form merged kwargs giving preference to type hint annotations\n    merged_kwargs = {**args_no_defaults, **args_with_defaults, **args_with_hints}\n\n    # form final dictionary of fields\n    fields_spec = {\n        k: v\n        for k, v in merged_kwargs.items()\n        if k not in [\"self\", \"return\", \"job\", fun_varargs, fun_varkw]\n    }\n\n    log.debug(\n        f\"NorFab worker {self.name} task creating Pydantic input \"\n        f\"model using fields spec: {fields_spec}\"\n    )\n\n    # create Pydantic model\n    self.input = create_model(self.name, **fields_spec)\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.Task.make_task_schema","title":"<code>make_task_schema(wrapper) -&gt; dict</code>","text":"<p>Generates a task schema dictionary for the current worker.</p> <p>Parameters:</p> Name Type Description Default <code>wrapper</code> <code>Callable</code> <p>The function wrapper to be associated with the task.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the task's metadata, including: - function: The provided wrapper function. - module: The module name where the original function is defined. - schema: A dictionary with the following keys:     - name (str): The name of the task.     - description (str): The description of the task.     - inputSchema (dict): The JSON schema for the input model.     - outputSchema (dict): The JSON schema for the output model.     - fastapi: FastAPI-specific metadata.     - mcp: Model Context protocol metadata</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def make_task_schema(self, wrapper) -&gt; dict:\n    \"\"\"\n    Generates a task schema dictionary for the current worker.\n\n    Args:\n        wrapper (Callable): The function wrapper to be associated with the task.\n\n    Returns:\n        dict: A dictionary containing the task's metadata, including:\n            - function: The provided wrapper function.\n            - module: The module name where the original function is defined.\n            - schema: A dictionary with the following keys:\n                - name (str): The name of the task.\n                - description (str): The description of the task.\n                - inputSchema (dict): The JSON schema for the input model.\n                - outputSchema (dict): The JSON schema for the output model.\n                - fastapi: FastAPI-specific metadata.\n                - mcp: Model Context protocol metadata\n    \"\"\"\n    input_json_schema = self.input.model_json_schema()\n    _ = input_json_schema.pop(\"title\")\n    output_json_schema = self.output.model_json_schema()\n\n    return {\n        self.name: {\n            \"function\": wrapper,\n            \"module\": self.function.__module__,\n            \"schema\": {\n                \"name\": str(self.name),\n                \"description\": self.description,\n                \"inputSchema\": input_json_schema,\n                \"outputSchema\": output_json_schema,\n                \"fastapi\": self.fastapi,\n                \"mcp\": self.mcp,\n            },\n        }\n    }\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.Task.is_need_argument","title":"<code>is_need_argument(function: callable, argument: str) -&gt; bool</code>","text":"<p>Determines whether a given argument name is required by the function.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def is_need_argument(self, function: callable, argument: str) -&gt; bool:\n    \"\"\"\n    Determines whether a given argument name is required by the function.\n    \"\"\"\n    fun_args, *_ = inspect.getfullargspec(function)\n    return argument in fun_args\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.Task.merge_args_to_kwargs","title":"<code>merge_args_to_kwargs(args: List, kwargs: Dict) -&gt; Dict</code>","text":"<p>Merges positional arguments (<code>args</code>) and keyword arguments (<code>kwargs</code>) into a single dictionary.</p> <p>This function uses the argument specification of the decorated function to ensure that all arguments are properly combined into a dictionary. This is particularly useful for scenarios where **kwargs need to be passed to another function or model (e.g., for validation purposes).</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>list</code> <p>A list of positional arguments passed to the decorated function.</p> required <code>kwargs</code> <code>dict</code> <p>A dictionary of keyword arguments passed to the decorated function.</p> required Return <p>dict: A dictionary containing the merged arguments, where positional arguments       are mapped to their corresponding parameter names.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def merge_args_to_kwargs(self, args: List, kwargs: Dict) -&gt; Dict:\n    \"\"\"\n    Merges positional arguments (`args`) and keyword arguments (`kwargs`)\n    into a single dictionary.\n\n    This function uses the argument specification of the decorated function\n    to ensure that all arguments are properly combined into a dictionary.\n    This is particularly useful for scenarios where **kwargs need to be passed\n    to another function or model (e.g., for validation purposes).\n\n    Arguments:\n        args (list): A list of positional arguments passed to the decorated function.\n        kwargs (dict): A dictionary of keyword arguments passed to the decorated function.\n\n    Return:\n        dict: A dictionary containing the merged arguments, where positional arguments\n              are mapped to their corresponding parameter names.\n    \"\"\"\n    merged_kwargs = {}\n\n    (\n        fun_args,  # list of the positional parameter names\n        fun_varargs,  # name of the * parameter or None\n        *_,  # ignore the rest\n    ) = inspect.getfullargspec(self.function)\n\n    # \"def foo(a, b):\" - combine \"foo(1, 2)\" args with \"a, b\" fun_args\n    args_to_kwargs = dict(zip(fun_args, args))\n\n    # \"def foo(a, *b):\" - combine \"foo(1, 2, 3)\" 2|3 args with \"*b\" fun_varargs\n    if fun_varargs:\n        args_to_kwargs[fun_varargs] = args[len(fun_args) :]\n\n    merged_kwargs = {**kwargs, **args_to_kwargs}\n\n    # remove reference to self if decorating class method\n    _ = merged_kwargs.pop(\"self\", None)\n\n    return merged_kwargs\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.Task.validate_input","title":"<code>validate_input(args: List, kwargs: Dict) -&gt; None</code>","text":"<p>Function to validate provided arguments against model</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def validate_input(self, args: List, kwargs: Dict) -&gt; None:\n    \"\"\"Function to validate provided arguments against model\"\"\"\n    merged_kwargs = self.merge_args_to_kwargs(args, kwargs)\n    log.debug(f\"{self.name} validating input arguments: {merged_kwargs}\")\n    # if below step succeeds, kwargs passed model validation\n    _ = self.input(**merged_kwargs)\n    log.debug(\n        f\"Validated input kwargs: {merged_kwargs} for function {self.function} using model {self.input}\"\n    )\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.JobDatabase","title":"<code>JobDatabase(db_path: str, jobs_compress: bool = True)</code>","text":"<p>Thread-safe SQLite database manager for worker jobs.</p> <p>Handles all job persistence operations with proper thread safety through connection-level locking and WAL mode for concurrent reads.</p> <p>Attributes:</p> Name Type Description <code>db_path</code> <code>str</code> <p>Path to the SQLite database file.</p> <code>_local</code> <code>local</code> <p>Thread-local storage for database connections.</p> <code>_lock</code> <code>Lock</code> <p>Lock for write operations to ensure thread safety.</p> <p>Initialize the job database.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>Path to the SQLite database file.</p> required <code>jobs_compress</code> <code>bool</code> <p>If True, compress args, kwargs, and result_data fields. Defaults to True.</p> <code>True</code> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def __init__(self, db_path: str, jobs_compress: bool = True):\n    \"\"\"\n    Initialize the job database.\n\n    Args:\n        db_path (str): Path to the SQLite database file.\n        jobs_compress (bool): If True, compress args, kwargs, and result_data fields. Defaults to True.\n    \"\"\"\n    self.db_path = db_path\n    self.jobs_compress = jobs_compress\n    self._local = threading.local()\n    self._lock = threading.Lock()\n    self._initialize_database()\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.JobDatabase.add_job","title":"<code>add_job(uuid: str, client_address: str, task: str, args: list, kwargs: dict, timeout: int, timestamp: str) -&gt; None</code>","text":"<p>Add a new job to the database.</p> <p>Parameters:</p> Name Type Description Default <code>uuid</code> <code>str</code> <p>Job UUID.</p> required <code>client_address</code> <code>str</code> <p>Client address.</p> required <code>task</code> <code>str</code> <p>Task name.</p> required <code>args</code> <code>list</code> <p>Task arguments.</p> required <code>kwargs</code> <code>dict</code> <p>Task keyword arguments.</p> required <code>timeout</code> <code>int</code> <p>Job timeout.</p> required <code>timestamp</code> <code>str</code> <p>Received timestamp.</p> required Source code in <code>norfab\\core\\worker.py</code> <pre><code>def add_job(\n    self,\n    uuid: str,\n    client_address: str,\n    task: str,\n    args: list,\n    kwargs: dict,\n    timeout: int,\n    timestamp: str,\n) -&gt; None:\n    \"\"\"\n    Add a new job to the database.\n\n    Args:\n        uuid (str): Job UUID.\n        client_address (str): Client address.\n        task (str): Task name.\n        args (list): Task arguments.\n        kwargs (dict): Task keyword arguments.\n        timeout (int): Job timeout.\n        timestamp (str): Received timestamp.\n    \"\"\"\n    with self._transaction(write=True) as conn:\n        # Compress args and kwargs if compression is enabled\n        if self.jobs_compress:\n            compressed_args = self._compress_data({\"args\": args})\n            compressed_kwargs = self._compress_data({\"kwargs\": kwargs})\n        else:\n            compressed_args = json.dumps(args)\n            compressed_kwargs = json.dumps(kwargs)\n\n        conn.execute(\n            \"\"\"\n            INSERT INTO jobs (uuid, client_address, task, args, kwargs, timeout,\n                             status, received_timestamp)\n            VALUES (?, ?, ?, ?, ?, ?, 'PENDING', ?)\n        \"\"\",\n            (\n                uuid,\n                client_address,\n                task,\n                compressed_args,\n                compressed_kwargs,\n                timeout,\n                timestamp,\n            ),\n        )\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.JobDatabase.get_next_pending_job","title":"<code>get_next_pending_job() -&gt; tuple</code>","text":"<p>Get the next pending job and mark it as STARTED.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>(uuid, received_timestamp) or None if no pending jobs.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def get_next_pending_job(self) -&gt; tuple:\n    \"\"\"\n    Get the next pending job and mark it as STARTED.\n\n    Returns:\n        tuple: (uuid, received_timestamp) or None if no pending jobs.\n    \"\"\"\n    with self._transaction(write=True) as conn:\n        # order jobs by their creation timestamp in ascending order - oldest first\n        cursor = conn.execute(\n            \"\"\"\n            SELECT uuid, received_timestamp FROM jobs\n            WHERE status = 'PENDING'\n            ORDER BY created_at ASC\n            LIMIT 1\n        \"\"\"\n        )\n        row = cursor.fetchone()\n        if row:\n            uuid = row[\"uuid\"]\n            conn.execute(\n                \"\"\"\n                UPDATE jobs SET status = 'STARTED', started_timestamp = ?\n                WHERE uuid = ?\n            \"\"\",\n                (time.ctime(), uuid),\n            )\n            return uuid, row[\"received_timestamp\"]\n        return None\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.JobDatabase.complete_job","title":"<code>complete_job(uuid: str, result_data: dict) -&gt; None</code>","text":"<p>Mark a job as completed and store its result.</p> <p>Parameters:</p> Name Type Description Default <code>uuid</code> <code>str</code> <p>Job UUID.</p> required <code>result_data</code> <code>dict</code> <p>Result data as dictionary.</p> required Source code in <code>norfab\\core\\worker.py</code> <pre><code>def complete_job(self, uuid: str, result_data: dict) -&gt; None:\n    \"\"\"\n    Mark a job as completed and store its result.\n\n    Args:\n        uuid (str): Job UUID.\n        result_data (dict): Result data as dictionary.\n    \"\"\"\n    with self._transaction(write=True) as conn:\n        conn.execute(\n            \"\"\"\n            UPDATE jobs\n            SET status = 'COMPLETED', completed_timestamp = ?, result_data = ?\n            WHERE uuid = ?\n        \"\"\",\n            (time.ctime(), self._compress_data(result_data), uuid),\n        )\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.JobDatabase.fail_job","title":"<code>fail_job(uuid: str, result_data: dict) -&gt; None</code>","text":"<p>Mark a job as failed and store its result.</p> <p>Parameters:</p> Name Type Description Default <code>uuid</code> <code>str</code> <p>Job UUID.</p> required <code>result_data</code> <code>dict</code> <p>Result data as dictionary.</p> required Source code in <code>norfab\\core\\worker.py</code> <pre><code>def fail_job(self, uuid: str, result_data: dict) -&gt; None:\n    \"\"\"\n    Mark a job as failed and store its result.\n\n    Args:\n        uuid (str): Job UUID.\n        result_data (dict): Result data as dictionary.\n    \"\"\"\n    with self._transaction(write=True) as conn:\n        conn.execute(\n            \"\"\"\n            UPDATE jobs\n            SET status = 'FAILED', completed_timestamp = ?, result_data = ?\n            WHERE uuid = ?\n        \"\"\",\n            (time.ctime(), self._compress_data(result_data), uuid),\n        )\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.JobDatabase.get_job_info","title":"<code>get_job_info(uuid: str, include_result: bool = False, include_events: bool = False) -&gt; dict</code>","text":"<p>Get comprehensive job information including status, execution data, and optionally result data and events.</p> <p>Parameters:</p> Name Type Description Default <code>uuid</code> <code>str</code> <p>Job UUID.</p> required <code>include_result</code> <code>bool</code> <p>If True, include result_data in the response. Defaults to False.</p> <code>False</code> <code>include_events</code> <code>bool</code> <p>If True, include job events. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Job information with the following fields: - uuid: Job UUID - status: Job status (PENDING, STARTED, COMPLETED, FAILED, WAITING_CLIENT_INPUT) - received_timestamp: When job was received - started_timestamp: When job started execution - completed_timestamp: When job completed - client_address: Client address - task: Task name - args: Parsed task arguments list - kwargs: Parsed task keyword arguments dict - timeout: Job timeout</p> <code>dict</code> <p>If include_result=True, also includes: - result_data: Result data dictionary (if available)</p> <code>dict</code> <p>If include_events=True, also includes: - job_events: List of event dictionaries</p> <code>dict</code> <p>Returns None if job not found.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def get_job_info(\n    self,\n    uuid: str,\n    include_result: bool = False,\n    include_events: bool = False,\n) -&gt; dict:\n    \"\"\"\n    Get comprehensive job information including status, execution data, and optionally result data and events.\n\n    Args:\n        uuid (str): Job UUID.\n        include_result (bool): If True, include result_data in the response. Defaults to False.\n        include_events (bool): If True, include job events. Defaults to False.\n\n    Returns:\n        dict: Job information with the following fields:\n            - uuid: Job UUID\n            - status: Job status (PENDING, STARTED, COMPLETED, FAILED, WAITING_CLIENT_INPUT)\n            - received_timestamp: When job was received\n            - started_timestamp: When job started execution\n            - completed_timestamp: When job completed\n            - client_address: Client address\n            - task: Task name\n            - args: Parsed task arguments list\n            - kwargs: Parsed task keyword arguments dict\n            - timeout: Job timeout\n\n        If include_result=True, also includes:\n            - result_data: Result data dictionary (if available)\n\n        If include_events=True, also includes:\n            - job_events: List of event dictionaries\n\n        Returns None if job not found.\n    \"\"\"\n    with self._transaction(write=False) as conn:\n        # Build SELECT clause based on requested fields\n        select_fields = [\n            \"uuid\",\n            \"status\",\n            \"received_timestamp\",\n            \"started_timestamp\",\n            \"completed_timestamp\",\n            \"client_address\",\n            \"task\",\n            \"args\",\n            \"kwargs\",\n            \"timeout\",\n        ]\n\n        if include_result:\n            select_fields.append(\"result_data\")\n\n        query = f\"\"\"\n            SELECT {', '.join(select_fields)}\n            FROM jobs WHERE uuid = ?\n        \"\"\"\n\n        cursor = conn.execute(query, (uuid,))\n        row = cursor.fetchone()\n\n        if not row:\n            return None\n\n        result = dict(row)\n        result.setdefault(\"result_data\", None)\n        result.setdefault(\"job_events\", [])\n\n        # Decompress args and kwargs\n        if self.jobs_compress:\n            result[\"args\"] = self._decompress_data(row[\"args\"]).get(\"args\", [])\n            result[\"kwargs\"] = self._decompress_data(row[\"kwargs\"]).get(\n                \"kwargs\", {}\n            )\n        else:\n            result[\"args\"] = json.loads(row[\"args\"])\n            result[\"kwargs\"] = json.loads(row[\"kwargs\"])\n\n        # Include result data if requested\n        if include_result and row[\"result_data\"]:\n            if self.jobs_compress:\n                result[\"result_data\"] = self._decompress_data(row[\"result_data\"])\n            else:\n                result[\"result_data\"] = row[\"result_data\"]\n\n        # Include events if requested\n        if include_events:\n            result[\"job_events\"] = self.get_job_events(uuid)\n\n        return result\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.JobDatabase.add_event","title":"<code>add_event(job_uuid: str, message: str, severity: str, task: str, event_data: dict) -&gt; None</code>","text":"<p>Add an event for a job.</p> <p>Parameters:</p> Name Type Description Default <code>job_uuid</code> <code>str</code> <p>Job UUID.</p> required <code>message</code> <code>str</code> <p>Event message.</p> required <code>severity</code> <code>str</code> <p>Event severity.</p> required <code>task</code> <code>str</code> <p>Task name.</p> required <code>event_data</code> <code>dict</code> <p>Event data dictionary.</p> required Source code in <code>norfab\\core\\worker.py</code> <pre><code>def add_event(\n    self, job_uuid: str, message: str, severity: str, task: str, event_data: dict\n) -&gt; None:\n    \"\"\"\n    Add an event for a job.\n\n    Args:\n        job_uuid (str): Job UUID.\n        message (str): Event message.\n        severity (str): Event severity.\n        task (str): Task name.\n        event_data (dict): Event data dictionary.\n    \"\"\"\n    with self._transaction(write=True) as conn:\n        conn.execute(\n            \"\"\"\n            INSERT INTO events (job_uuid, message, severity, task, event_data)\n            VALUES (?, ?, ?, ?, ?)\n        \"\"\",\n            (job_uuid, message, severity, task, json.dumps(event_data)),\n        )\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.JobDatabase.get_job_events","title":"<code>get_job_events(uuid: str) -&gt; list</code>","text":"<p>Get all events for a job.</p> <p>Parameters:</p> Name Type Description Default <code>uuid</code> <code>str</code> <p>Job UUID.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of event dictionaries.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def get_job_events(self, uuid: str) -&gt; list:\n    \"\"\"\n    Get all events for a job.\n\n    Args:\n        uuid (str): Job UUID.\n\n    Returns:\n        list: List of event dictionaries.\n    \"\"\"\n    with self._transaction(write=False) as conn:\n        cursor = conn.execute(\n            \"\"\"\n            SELECT message, severity, task, event_data, created_at\n            FROM events WHERE job_uuid = ?\n            ORDER BY created_at ASC\n        \"\"\",\n            (uuid,),\n        )\n        return [\n            {\n                \"message\": row[\"message\"],\n                \"severity\": row[\"severity\"],\n                \"task\": row[\"task\"],\n                **json.loads(row[\"event_data\"]),\n                \"created_at\": row[\"created_at\"],\n            }\n            for row in cursor.fetchall()\n        ]\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.JobDatabase.list_jobs","title":"<code>list_jobs(pending: bool = True, completed: bool = True, task: str = None, last: int = None, client: str = None, uuid: str = None) -&gt; list</code>","text":"<p>List jobs based on filters.</p> <p>Parameters:</p> Name Type Description Default <code>pending</code> <code>bool</code> <p>Include pending jobs.</p> <code>True</code> <code>completed</code> <code>bool</code> <p>Include completed jobs.</p> <code>True</code> <code>task</code> <code>str</code> <p>Filter by task name.</p> <code>None</code> <code>last</code> <code>int</code> <p>Return only last N jobs.</p> <code>None</code> <code>client</code> <code>str</code> <p>Filter by client address.</p> <code>None</code> <code>uuid</code> <code>str</code> <p>Filter by specific UUID.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of job dictionaries.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def list_jobs(\n    self,\n    pending: bool = True,\n    completed: bool = True,\n    task: str = None,\n    last: int = None,\n    client: str = None,\n    uuid: str = None,\n) -&gt; list:\n    \"\"\"\n    List jobs based on filters.\n\n    Args:\n        pending (bool): Include pending jobs.\n        completed (bool): Include completed jobs.\n        task (str): Filter by task name.\n        last (int): Return only last N jobs.\n        client (str): Filter by client address.\n        uuid (str): Filter by specific UUID.\n\n    Returns:\n        list: List of job dictionaries.\n    \"\"\"\n    with self._transaction(write=False) as conn:\n        # Build query\n        conditions = []\n        params = []\n\n        if uuid:\n            conditions.append(\"uuid = ?\")\n            params.append(uuid)\n        else:\n            status_conditions = []\n            if pending:\n                status_conditions.append(\"status IN ('PENDING', 'STARTED')\")\n            if completed:\n                status_conditions.append(\"status IN ('COMPLETED', 'FAILED')\")\n            if status_conditions:\n                conditions.append(f\"({' OR '.join(status_conditions)})\")\n\n            if task:\n                conditions.append(\"task = ?\")\n                params.append(task)\n            if client:\n                conditions.append(\"client_address = ?\")\n                params.append(client)\n\n        where_clause = f\"WHERE {' AND '.join(conditions)}\" if conditions else \"\"\n        query = f\"\"\"\n            SELECT uuid, client_address, task, status,\n                   received_timestamp, started_timestamp, completed_timestamp\n            FROM jobs {where_clause}\n            ORDER BY created_at DESC\n        \"\"\"\n\n        if last:\n            query += f\" LIMIT {last}\"\n\n        cursor = conn.execute(query, params)\n        return [dict(row) for row in cursor.fetchall()]\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.JobDatabase.close","title":"<code>close()</code>","text":"<p>Close all database connections.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def close(self):\n    \"\"\"Close all database connections.\"\"\"\n    if hasattr(self._local, \"conn\"):\n        self._local.conn.close()\n        delattr(self._local, \"conn\")\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.WorkerWatchDog","title":"<code>WorkerWatchDog(worker)</code>","text":"<p>               Bases: <code>Thread</code></p> <p>Class to monitor worker performance.</p> <p>Attributes:</p> Name Type Description <code>worker</code> <code>object</code> <p>The worker instance being monitored.</p> <code>worker_process</code> <code>Process</code> <p>The process of the worker.</p> <code>watchdog_interval</code> <code>int</code> <p>Interval in seconds for the watchdog to check the worker's status.</p> <code>memory_threshold_mbyte</code> <code>int</code> <p>Memory usage threshold in megabytes.</p> <code>memory_threshold_action</code> <code>str</code> <p>Action to take when memory threshold is exceeded (\"log\" or \"shutdown\").</p> <code>runs</code> <code>int</code> <p>Counter for the number of times the watchdog has run.</p> <code>watchdog_tasks</code> <code>list</code> <p>List of additional tasks to run during each watchdog interval.</p> <p>Methods:</p> Name Description <code>check_ram</code> <p>Checks the worker's RAM usage and takes action if it exceeds the threshold.</p> <code>get_ram_usage</code> <p>Returns the worker's RAM usage in megabytes.</p> <code>run</code> <p>Main loop of the watchdog thread, periodically checks the worker's status and runs tasks.</p> <p>Parameters:</p> Name Type Description Default <code>worker</code> <code>object</code> <p>The worker object containing inventory attributes.</p> required Source code in <code>norfab\\core\\worker.py</code> <pre><code>def __init__(self, worker):\n    super().__init__()\n    self.worker = worker\n    self.worker_process = psutil.Process(os.getpid())\n\n    # extract inventory attributes\n    self.watchdog_interval = worker.inventory.get(\"watchdog_interval\", 30)\n    self.memory_threshold_mbyte = worker.inventory.get(\n        \"memory_threshold_mbyte\", 1000\n    )\n    self.memory_threshold_action = worker.inventory.get(\n        \"memory_threshold_action\", \"log\"\n    )\n\n    # initiate variables\n    self.runs = 0\n    self.watchdog_tasks = []\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.WorkerWatchDog.check_ram","title":"<code>check_ram()</code>","text":"<p>Checks the current RAM usage and performs an action if it exceeds the threshold.</p> <p>This method retrieves the current RAM usage and compares it to the predefined memory threshold. If the RAM usage exceeds the threshold, it performs an action based on the <code>memory_threshold_action</code> attribute. The possible actions are:</p> <ul> <li>\"log\": Logs a warning message.</li> <li>\"shutdown\": Raises a SystemExit exception to terminate the program.</li> </ul> <p>Raises:</p> Type Description <code>SystemExit</code> <p>If the memory usage exceeds the threshold and the action is \"shutdown\".</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def check_ram(self):\n    \"\"\"\n    Checks the current RAM usage and performs an action if it exceeds the threshold.\n\n    This method retrieves the current RAM usage and compares it to the predefined\n    memory threshold. If the RAM usage exceeds the threshold, it performs an action\n    based on the `memory_threshold_action` attribute. The possible actions are:\n\n    - \"log\": Logs a warning message.\n    - \"shutdown\": Raises a SystemExit exception to terminate the program.\n\n    Raises:\n        SystemExit: If the memory usage exceeds the threshold and the action is \"shutdown\".\n    \"\"\"\n    mem_usage = self.get_ram_usage()\n    if mem_usage &gt; self.memory_threshold_mbyte:\n        if self.memory_threshold_action == \"log\":\n            log.warning(\n                f\"{self.name} watchdog, '{self.memory_threshold_mbyte}' \"\n                f\"memory_threshold_mbyte exceeded, memory usage \"\n                f\"{mem_usage}MByte\"\n            )\n        elif self.memory_threshold_action == \"shutdown\":\n            raise SystemExit(\n                f\"{self.name} watchdog, '{self.memory_threshold_mbyte}' \"\n                f\"memory_threshold_mbyte exceeded, memory usage \"\n                f\"{mem_usage}MByte, killing myself\"\n            )\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.WorkerWatchDog.get_ram_usage","title":"<code>get_ram_usage() -&gt; float</code>","text":"<p>Get the RAM usage of the worker process.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The RAM usage in megabytes.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def get_ram_usage(self) -&gt; float:\n    \"\"\"\n    Get the RAM usage of the worker process.\n\n    Returns:\n        float: The RAM usage in megabytes.\n    \"\"\"\n    return self.worker_process.memory_info().rss / 1024000\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.WorkerWatchDog.run","title":"<code>run()</code>","text":"<p>Executes the worker's watchdog main loop, periodically running tasks and checking conditions. The method performs the following steps in a loop until the worker's exit event is set:</p> <ol> <li>Sleeps in increments of 0.1 seconds until the total sleep time reaches the watchdog interval.</li> <li>Runs built-in tasks such as checking RAM usage.</li> <li>Executes additional tasks provided by child classes.</li> <li>Updates the run counter.</li> <li>Resets the sleep counter to start the cycle again.</li> </ol> <p>Attributes:</p> Name Type Description <code>slept</code> <code>float</code> <p>The total time slept in the current cycle.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def run(self):\n    \"\"\"\n    Executes the worker's watchdog main loop, periodically running tasks and checking conditions.\n    The method performs the following steps in a loop until the worker's exit event is set:\n\n    1. Sleeps in increments of 0.1 seconds until the total sleep time reaches the watchdog interval.\n    2. Runs built-in tasks such as checking RAM usage.\n    3. Executes additional tasks provided by child classes.\n    4. Updates the run counter.\n    5. Resets the sleep counter to start the cycle again.\n\n    Attributes:\n        slept (float): The total time slept in the current cycle.\n    \"\"\"\n    slept = 0\n    while not self.worker.exit_event.is_set():\n        # continue sleeping for watchdog_interval\n        if slept &lt; self.watchdog_interval:\n            time.sleep(0.1)\n            slept += 0.1\n            continue\n\n        # run built in tasks:\n        self.check_ram()\n\n        # run child classes tasks\n        for task in self.watchdog_tasks:\n            task()\n\n        # update counters\n        self.runs += 1\n\n        slept = 0  # reset to go to sleep\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker","title":"<code>NFPWorker(inventory: NorFabInventory, broker: str, service: str, name: str, exit_event: object, log_level: str = None, log_queue: object = None, multiplier: int = 6, keepalive: int = 2500)</code>","text":"<p>NFPWorker class is responsible for managing worker operations, including connecting to a broker, handling jobs,  and maintaining keepalive connections. It interacts with the broker using ZeroMQ and manages job queues and events.</p> <p>Parameters:</p> Name Type Description Default <code>inventory</code> <code>NorFabInventory</code> <p>The inventory object containing base directory information.</p> required <code>broker</code> <code>str</code> <p>The broker address.</p> required <code>service</code> <code>str</code> <p>The service name.</p> required <code>name</code> <code>str</code> <p>The name of the worker.</p> required <code>exit_event</code> <code>object</code> <p>The event used to signal the worker to exit.</p> required <code>log_level</code> <code>str</code> <p>The logging level. Defaults to None.</p> <code>None</code> <code>log_queue</code> <code>object</code> <p>The logging queue. Defaults to None.</p> <code>None</code> <code>multiplier</code> <code>int</code> <p>The multiplier value. Defaults to 6.</p> <code>6</code> <code>keepalive</code> <code>int</code> <p>The keepalive interval in milliseconds. Defaults to 2500.</p> <code>2500</code> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def __init__(\n    self,\n    inventory: NorFabInventory,\n    broker: str,\n    service: str,\n    name: str,\n    exit_event: object,\n    log_level: str = None,\n    log_queue: object = None,\n    multiplier: int = 6,\n    keepalive: int = 2500,\n):\n    self.setup_logging(log_queue, log_level)\n    self.inventory = inventory\n    self.max_concurrent_jobs = max(1, inventory.get(\"max_concurrent_jobs\", 5))\n    self.jobs_compress = inventory.get(\"jobs_compress\", True)\n    self.broker = broker\n    self.service = service.encode(\"utf-8\") if isinstance(service, str) else service\n    self.name = name\n    self.exit_event = exit_event\n    self.broker_socket = None\n    self.multiplier = multiplier\n    self.keepalive = keepalive\n    self.socket_lock = (\n        threading.Lock()\n    )  # used for keepalives to protect socket object\n    self.zmq_auth = self.inventory.broker.get(\"zmq_auth\", True)\n    self.build_message = NFP.MessageBuilder()\n\n    # create base directories\n    self.base_dir = os.path.join(\n        self.inventory.base_dir, \"__norfab__\", \"files\", \"worker\", self.name\n    )\n    os.makedirs(self.base_dir, exist_ok=True)\n\n    # Initialize SQLite database for job management\n    db_path = os.path.join(self.base_dir, f\"{self.name}.db\")\n    self.db = JobDatabase(db_path, jobs_compress=self.jobs_compress)\n\n    # dictionary to store currently running jobs\n    self.running_jobs = {}\n\n    # create events and queues\n    self.destroy_event = threading.Event()\n    self.request_thread = None\n    self.reply_thread = None\n    self.recv_thread = None\n    self.event_thread = None\n    self.put_thread = None\n\n    self.post_queue = queue.Queue(maxsize=0)\n    self.get_queue = queue.Queue(maxsize=0)\n    self.delete_queue = queue.Queue(maxsize=0)\n    self.event_queue = queue.Queue(maxsize=0)\n    self.put_queue = queue.Queue(maxsize=0)\n\n    # generate certificates and create directories\n    if self.zmq_auth is not False:\n        generate_certificates(\n            self.base_dir,\n            cert_name=self.name,\n            broker_keys_dir=os.path.join(\n                self.inventory.base_dir,\n                \"__norfab__\",\n                \"files\",\n                \"broker\",\n                \"public_keys\",\n            ),\n            inventory=self.inventory,\n        )\n        self.public_keys_dir = os.path.join(self.base_dir, \"public_keys\")\n        self.secret_keys_dir = os.path.join(self.base_dir, \"private_keys\")\n\n    self.ctx = zmq.Context()\n    self.poller = zmq.Poller()\n    self.reconnect_to_broker()\n\n    self.client = NFPClient(\n        self.inventory,\n        self.broker,\n        name=f\"{self.name}-NFPClient\",\n        exit_event=self.exit_event,\n    )\n\n    self.tasks = NORFAB_WORKER_TASKS\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.setup_logging","title":"<code>setup_logging(log_queue, log_level: str) -&gt; None</code>","text":"<p>Configures logging for the worker.</p> <p>This method sets up the logging configuration using a provided log queue and log level. It updates the logging configuration dictionary with the given log queue and log level, and then applies the configuration using <code>logging.config.dictConfig</code>.</p> <p>Parameters:</p> Name Type Description Default <code>log_queue</code> <code>Queue</code> <p>The queue to be used for logging.</p> required <code>log_level</code> <code>str</code> <p>The logging level to be set. If None, the default level is used.</p> required Source code in <code>norfab\\core\\worker.py</code> <pre><code>def setup_logging(self, log_queue, log_level: str) -&gt; None:\n    \"\"\"\n    Configures logging for the worker.\n\n    This method sets up the logging configuration using a provided log queue and log level.\n    It updates the logging configuration dictionary with the given log queue and log level,\n    and then applies the configuration using `logging.config.dictConfig`.\n\n    Args:\n        log_queue (queue.Queue): The queue to be used for logging.\n        log_level (str): The logging level to be set. If None, the default level is used.\n    \"\"\"\n    logging_config_producer[\"handlers\"][\"queue\"][\"queue\"] = log_queue\n    if log_level is not None:\n        logging_config_producer[\"root\"][\"level\"] = log_level\n    logging.config.dictConfig(logging_config_producer)\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.reconnect_to_broker","title":"<code>reconnect_to_broker()</code>","text":"<p>Connect or reconnect to the broker.</p> <p>This method handles the connection or reconnection process to the broker. It performs the following steps:</p> <ol> <li>If there is an existing broker socket, it sends a disconnect message,    unregisters the socket from the poller, and closes the socket.</li> <li>Creates a new DEALER socket and sets its identity.</li> <li>Loads the client's secret and public keys for CURVE authentication.</li> <li>Loads the server's public key for CURVE authentication.</li> <li>Connects the socket to the broker.</li> <li>Registers the socket with the poller for incoming messages.</li> <li>Sends a READY message to the broker to register the service.</li> <li>Starts or restarts the keepalive mechanism to maintain the connection.</li> <li>Increments the reconnect statistics counter.</li> <li>Logs the successful registration to the broker.</li> </ol> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def reconnect_to_broker(self):\n    \"\"\"\n    Connect or reconnect to the broker.\n\n    This method handles the connection or reconnection process to the broker.\n    It performs the following steps:\n\n    1. If there is an existing broker socket, it sends a disconnect message,\n       unregisters the socket from the poller, and closes the socket.\n    2. Creates a new DEALER socket and sets its identity.\n    3. Loads the client's secret and public keys for CURVE authentication.\n    4. Loads the server's public key for CURVE authentication.\n    5. Connects the socket to the broker.\n    6. Registers the socket with the poller for incoming messages.\n    7. Sends a READY message to the broker to register the service.\n    8. Starts or restarts the keepalive mechanism to maintain the connection.\n    9. Increments the reconnect statistics counter.\n    10. Logs the successful registration to the broker.\n    \"\"\"\n    if self.broker_socket:\n        self.send_to_broker(NFP.DISCONNECT)\n        self.poller.unregister(self.broker_socket)\n        self.broker_socket.close()\n\n    self.broker_socket = self.ctx.socket(zmq.DEALER)\n    self.broker_socket.setsockopt_unicode(zmq.IDENTITY, self.name, \"utf8\")\n    self.broker_socket.linger = 0\n\n    if self.zmq_auth is not False:\n        # We need two certificates, one for the client and one for\n        # the server. The client must know the server's public key\n        # to make a CURVE connection.\n        client_secret_file = os.path.join(\n            self.secret_keys_dir, f\"{self.name}.key_secret\"\n        )\n        client_public, client_secret = zmq.auth.load_certificate(client_secret_file)\n        self.broker_socket.curve_secretkey = client_secret\n        self.broker_socket.curve_publickey = client_public\n\n        # The client must know the server's public key to make a CURVE connection.\n        server_public_file = os.path.join(self.public_keys_dir, \"broker.key\")\n        server_public, _ = zmq.auth.load_certificate(server_public_file)\n        self.broker_socket.curve_serverkey = server_public\n\n    self.broker_socket.connect(self.broker)\n    self.poller.register(self.broker_socket, zmq.POLLIN)\n\n    # Register service with broker\n    self.send_to_broker(NFP.READY)\n    log.debug(f\"{self.name} - NFP.READY sent to broker '{self.broker}'\")\n\n    # start keepalives\n    if self.keepaliver is not None:\n        self.keepaliver.restart(self.broker_socket)\n    else:\n        self.keepaliver = KeepAliver(\n            address=None,\n            socket=self.broker_socket,\n            multiplier=self.multiplier,\n            keepalive=self.keepalive,\n            exit_event=self.destroy_event,\n            service=self.service,\n            whoami=NFP.WORKER,\n            name=self.name,\n            socket_lock=self.socket_lock,\n        )\n        self.keepaliver.start()\n\n    self.stats_reconnect_to_broker += 1\n    log.info(\n        f\"{self.name} - registered to broker at '{self.broker}', \"\n        f\"service '{self.service.decode('utf-8')}'\"\n    )\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.send_to_broker","title":"<code>send_to_broker(command, msg: list = None)</code>","text":"<p>Send a message to the broker.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>The command to send to the broker. Must be one of NFP.READY, NFP.DISCONNECT, NFP.RESPONSE, or NFP.EVENT.</p> required <code>msg</code> <code>list</code> <p>The message to send. If not provided, a default message will be created based on the command.</p> <code>None</code> Logs <p>Logs an error if the command is unsupported. Logs a debug message with the message being sent.</p> Thread Safety <p>This method is thread-safe and uses a lock to ensure that the broker socket is accessed by only one thread at a time.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def send_to_broker(self, command, msg: list = None):\n    \"\"\"\n    Send a message to the broker.\n\n    Parameters:\n        command (str): The command to send to the broker. Must be one of NFP.READY, NFP.DISCONNECT, NFP.RESPONSE, or NFP.EVENT.\n        msg (list, optional): The message to send. If not provided, a default message will be created based on the command.\n\n    Logs:\n        Logs an error if the command is unsupported.\n        Logs a debug message with the message being sent.\n\n    Thread Safety:\n        This method is thread-safe and uses a lock to ensure that the broker socket is accessed by only one thread at a time.\n    \"\"\"\n    if command == NFP.READY:\n        msg = self.build_message.worker_to_broker_ready(service=self.service)\n    elif command == NFP.DISCONNECT:\n        msg = self.build_message.worker_to_broker_disconnect(service=self.service)\n    elif command == NFP.RESPONSE:\n        msg = self.build_message.worker_to_broker_response(response_data=msg)\n    elif command == NFP.EVENT:\n        msg = self.build_message.worker_to_broker_event(event_data=msg)\n    elif command == NFP.STREAM:\n        msg = self.build_message.worker_to_broker_stream(data=msg)\n    else:\n        log.error(\n            f\"{self.name} - cannot send '{command}' to broker, command unsupported\"\n        )\n        return\n\n    log.debug(f\"{self.name} - sending '{msg}'\")\n\n    with self.socket_lock:\n        self.broker_socket.send_multipart(msg)\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.load_inventory","title":"<code>load_inventory() -&gt; dict</code>","text":"<p>Load inventory data from the broker for this worker.</p> <p>This function retrieves inventory data from the broker service using the worker's name. It logs the received inventory data and returns the results if available.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The inventory data results if available, otherwise an empty dictionary.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def load_inventory(self) -&gt; dict:\n    \"\"\"\n    Load inventory data from the broker for this worker.\n\n    This function retrieves inventory data from the broker service using the worker's name.\n    It logs the received inventory data and returns the results if available.\n\n    Returns:\n        dict: The inventory data results if available, otherwise an empty dictionary.\n    \"\"\"\n    inventory_data = self.client.get(\n        \"sid.service.broker\", \"get_inventory\", kwargs={\"name\": self.name}\n    )\n\n    log.debug(f\"{self.name} - worker received inventory data {inventory_data}\")\n\n    if inventory_data[\"results\"]:\n        return inventory_data[\"results\"]\n    else:\n        return {}\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.worker_exit","title":"<code>worker_exit() -&gt; None</code>","text":"<p>Method to override in child classes with a set of actions to perform on exit call.</p> <p>This method should be implemented by subclasses to define any cleanup or finalization tasks that need to be performed when the worker is exiting.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def worker_exit(self) -&gt; None:\n    \"\"\"\n    Method to override in child classes with a set of actions to perform on exit call.\n\n    This method should be implemented by subclasses to define any cleanup or finalization\n    tasks that need to be performed when the worker is exiting.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.get_inventory","title":"<code>get_inventory(job: Job) -&gt; Result</code>","text":"<p>Retrieve the worker's inventory.</p> <p>This method should be overridden in child classes to provide the specific implementation for retrieving the inventory of a worker.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Result</code> <p>A dictionary representing the worker's inventory.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not overridden in a child class.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_inventory(self, job: Job) -&gt; Result:\n    \"\"\"\n    Retrieve the worker's inventory.\n\n    This method should be overridden in child classes to provide the specific\n    implementation for retrieving the inventory of a worker.\n\n    Returns:\n        Dict: A dictionary representing the worker's inventory.\n\n    Raises:\n        NotImplementedError: If the method is not overridden in a child class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.get_version","title":"<code>get_version() -&gt; Result</code>","text":"<p>Retrieve the version report of the worker.</p> <p>This method should be overridden in child classes to provide the specific version report of the worker.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Result</code> <p>A dictionary containing the version information of the worker.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not overridden in a child class.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_version(self) -&gt; Result:\n    \"\"\"\n    Retrieve the version report of the worker.\n\n    This method should be overridden in child classes to provide the specific\n    version report of the worker.\n\n    Returns:\n        Dict: A dictionary containing the version information of the worker.\n\n    Raises:\n        NotImplementedError: If the method is not overridden in a child class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.destroy","title":"<code>destroy(message=None)</code>","text":"<p>Cleanly shuts down the worker by performing the following steps:</p> <ol> <li>Calls the worker_exit method to handle any worker-specific exit procedures.</li> <li>Sets the destroy_event to signal that the worker is being destroyed.</li> <li>Calls the destroy method on the client to clean up client resources.</li> <li>Joins all the threads (request_thread, reply_thread, event_thread, recv_thread) if they are not None, ensuring they have finished execution.</li> <li>Closes the database connections.</li> <li>Destroys the context with a linger period of 0 to immediately close all sockets.</li> <li>Stops the keepaliver to cease any keepalive signals.</li> <li>Logs an informational message indicating that the worker has been destroyed, including an optional message.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>An optional message to include in the log when the worker is destroyed.</p> <code>None</code> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def destroy(self, message=None):\n    \"\"\"\n    Cleanly shuts down the worker by performing the following steps:\n\n    1. Calls the worker_exit method to handle any worker-specific exit procedures.\n    2. Sets the destroy_event to signal that the worker is being destroyed.\n    3. Calls the destroy method on the client to clean up client resources.\n    4. Joins all the threads (request_thread, reply_thread, event_thread, recv_thread) if they are not None, ensuring they have finished execution.\n    5. Closes the database connections.\n    6. Destroys the context with a linger period of 0 to immediately close all sockets.\n    7. Stops the keepaliver to cease any keepalive signals.\n    8. Logs an informational message indicating that the worker has been destroyed, including an optional message.\n\n    Args:\n        message (str, optional): An optional message to include in the log when the worker is destroyed.\n    \"\"\"\n    self.worker_exit()\n    self.destroy_event.set()\n    self.client.destroy()\n\n    # join all the threads\n    if self.request_thread is not None:\n        self.request_thread.join()\n    if self.reply_thread is not None:\n        self.reply_thread.join()\n    if self.event_thread is not None:\n        self.event_thread.join()\n    if self.recv_thread:\n        self.recv_thread.join()\n\n    # close database\n    self.db.close()\n\n    self.ctx.destroy(0)\n\n    # stop keepalives\n    self.keepaliver.stop()\n\n    log.info(f\"{self.name} - worker destroyed, message: '{message}'\")\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.is_url","title":"<code>is_url(url: str) -&gt; bool</code>","text":"<p>Check if the given string is a URL supported by NorFab File Service.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the URL supported by NorFab File Service, False otherwise.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def is_url(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if the given string is a URL supported by NorFab File Service.\n\n    Args:\n        url (str): The URL to check.\n\n    Returns:\n        bool: True if the URL supported by NorFab File Service, False otherwise.\n    \"\"\"\n    return any(str(url).startswith(k) for k in [\"nf://\"])\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.fetch_file","title":"<code>fetch_file(url: str, raise_on_fail: bool = False, read: bool = True) -&gt; str</code>","text":"<p>Function to download file from broker File Sharing Service</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>file location string in <code>nf://&lt;filepath&gt;</code> format</p> required <code>raise_on_fail</code> <code>bool</code> <p>raise FIleNotFoundError if download fails</p> <code>False</code> <code>read</code> <code>bool</code> <p>if True returns file content, return OS path to saved file otherwise</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>File content if read is True, otherwise OS path to the saved file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If raise_on_fail is True and the download fails.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def fetch_file(\n    self, url: str, raise_on_fail: bool = False, read: bool = True\n) -&gt; str:\n    \"\"\"\n    Function to download file from broker File Sharing Service\n\n    Args:\n        url: file location string in ``nf://&lt;filepath&gt;`` format\n        raise_on_fail: raise FIleNotFoundError if download fails\n        read: if True returns file content, return OS path to saved file otherwise\n\n    Returns:\n        str: File content if read is True, otherwise OS path to the saved file.\n\n    Raises:\n        FileNotFoundError: If raise_on_fail is True and the download fails.\n    \"\"\"\n    if not self.is_url(url):\n        raise ValueError(f\"Invalid URL format: {url}\")\n\n    result = self.client.fetch_file(url=url, read=read)\n    status = result[\"status\"]\n    file_content = result[\"content\"]\n    msg = f\"{self.name} - worker '{url}' fetch file failed with status '{status}'\"\n\n    if status == \"200\":\n        return file_content\n    elif raise_on_fail is True:\n        raise FileNotFoundError(msg)\n    else:\n        log.error(msg)\n        return None\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.jinja2_render_templates","title":"<code>jinja2_render_templates(templates: list[str], context: dict = None, filters: dict = None) -&gt; str</code>","text":"<p>Renders a list of Jinja2 templates with the given context and optional filters.</p> <p>Parameters:</p> Name Type Description Default <code>templates</code> <code>list[str]</code> <p>A list of Jinja2 template strings or NorFab file paths.</p> required <code>context</code> <code>dict</code> <p>A dictionary containing the context variables for rendering the templates.</p> <code>None</code> <code>filters</code> <code>dict</code> <p>A dictionary of custom Jinja2 filters to be used during rendering.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The rendered templates concatenated into a single string.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def jinja2_render_templates(\n    self, templates: list[str], context: dict = None, filters: dict = None\n) -&gt; str:\n    \"\"\"\n    Renders a list of Jinja2 templates with the given context and optional filters.\n\n    Args:\n        templates (list[str]): A list of Jinja2 template strings or NorFab file paths.\n        context (dict): A dictionary containing the context variables for rendering the templates.\n        filters (dict, optional): A dictionary of custom Jinja2 filters to be used during rendering.\n\n    Returns:\n        str: The rendered templates concatenated into a single string.\n    \"\"\"\n    rendered = []\n    filters = filters or {}\n    context = context or {}\n    for template in templates:\n        j2env = Environment(loader=\"BaseLoader\")\n        j2env.filters.update(filters)  # add custom filters\n        renderer = j2env.from_string(template)\n        template = renderer.render(**context)\n        # download template file and render it again\n        if template.startswith(\"nf://\"):\n            filepath = self.jinja2_fetch_template(template)\n            searchpath, filename = os.path.split(filepath)\n            j2env = Environment(loader=FileSystemLoader(searchpath))\n            j2env.filters.update(filters)  # add custom filters\n            renderer = j2env.get_template(filename)\n            rendered.append(renderer.render(**context))\n        # template content is fully rendered\n        else:\n            rendered.append(template)\n\n    return \"\\n\".join(rendered)\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.jinja2_fetch_template","title":"<code>jinja2_fetch_template(url: str) -&gt; str</code>","text":"<p>Helper function to recursively download a Jinja2 template along with other templates referenced using \"include\" statements.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>A URL in the format <code>nf://file/path</code> to download the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The file path of the downloaded Jinja2 template.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file download fails.</p> <code>Exception</code> <p>If Jinja2 template parsing fails.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def jinja2_fetch_template(self, url: str) -&gt; str:\n    \"\"\"\n    Helper function to recursively download a Jinja2 template along with\n    other templates referenced using \"include\" statements.\n\n    Args:\n        url (str): A URL in the format ``nf://file/path`` to download the file.\n\n    Returns:\n        str: The file path of the downloaded Jinja2 template.\n\n    Raises:\n        FileNotFoundError: If the file download fails.\n        Exception: If Jinja2 template parsing fails.\n    \"\"\"\n    filepath = self.fetch_file(url, read=False)\n    if filepath is None:\n        msg = f\"{self.name} - file download failed '{url}'\"\n        raise FileNotFoundError(msg)\n\n    # download Jinja2 template \"include\"-ed files\n    content = self.fetch_file(url, read=True)\n    j2env = Environment(loader=\"BaseLoader\")\n    try:\n        parsed_content = j2env.parse(content)\n    except Exception as e:\n        msg = f\"{self.name} - Jinja2 template parsing failed '{url}', error: '{e}'\"\n        raise Exception(msg)\n\n    # run recursion on include statements\n    for node in parsed_content.find_all(Include):\n        include_file = node.template.value\n        base_path = os.path.split(url)[0]\n        self.jinja2_fetch_template(os.path.join(base_path, include_file))\n\n    return filepath\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.event","title":"<code>event(message: str, juuid: str, task: str, client_address: str, **kwargs: Any) -&gt; None</code>","text":"<p>Handles the creation and emission of an event.</p> <p>This method takes event data, processes it, and sends it to the event queue. It also saves the event data to the database for future reference.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The event message</p> required <code>juuid</code> <code>str</code> <p>Job ID for which this event is generated</p> required <code>task</code> <code>str</code> <p>Task name</p> required <code>client_address</code> <code>str</code> <p>Client address</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed when creating a NorFabEvent instance</p> <code>{}</code> Logs <p>Error: Logs an error message if the event data cannot be formed.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def event(\n    self,\n    message: str,\n    juuid: str,\n    task: str,\n    client_address: str,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Handles the creation and emission of an event.\n\n    This method takes event data, processes it, and sends it to the event queue.\n    It also saves the event data to the database for future reference.\n\n    Args:\n        message: The event message\n        juuid: Job ID for which this event is generated\n        task: Task name\n        client_address: Client address\n        **kwargs: Additional keyword arguments to be passed when creating a NorFabEvent instance\n\n    Logs:\n        Error: Logs an error message if the event data cannot be formed.\n    \"\"\"\n    # construct NorFabEvent\n    try:\n        event_data = NorFabEvent(\n            message=message,\n            juuid=juuid,\n            client_address=client_address,\n            task=task,\n            **kwargs,\n        )\n    except Exception as e:\n        log.error(f\"Failed to form event data, error {e}\")\n        return\n    event_dict = event_data.model_dump(exclude_none=True)\n\n    # emit event to the broker\n    self.event_queue.put(event_dict)\n\n    # check if need to emit log for this event\n    if self.inventory[\"logging\"].get(\"log_events\", False):\n        event_log = f\"EVENT {self.name}:{task} - {message}\"\n        severity = event_dict.get(\"severity\", \"INFO\")\n        if severity == \"INFO\":\n            log.info(event_log)\n        elif severity == \"DEBUG\":\n            log.debug(event_log)\n        elif severity == \"WARNING\":\n            log.warning(event_log)\n        elif severity == \"CRITICAL\":\n            log.critical(event_log)\n        elif severity == \"ERROR\":\n            log.error(event_log)\n\n    # save event to database\n    try:\n        self.db.add_event(\n            job_uuid=juuid,\n            message=message,\n            severity=event_dict.get(\"severity\", \"INFO\"),\n            task=task,\n            event_data=event_dict,\n        )\n    except Exception as e:\n        log.error(f\"Failed to save event to database: {e}\")\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.job_details","title":"<code>job_details(uuid: str = None, result: bool = True, events: bool = True) -&gt; Result</code>","text":"<p>Method to get job details by UUID for completed jobs.</p> <p>Parameters:</p> Name Type Description Default <code>uuid</code> <code>str</code> <p>The job UUID to return details for.</p> <code>None</code> <code>result</code> <code>bool</code> <p>If True, return job result.</p> <code>True</code> <code>events</code> <code>bool</code> <p>If True, return job events.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>A Result object with the job details.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef job_details(\n    self,\n    uuid: str = None,\n    result: bool = True,\n    events: bool = True,\n) -&gt; Result:\n    \"\"\"\n    Method to get job details by UUID for completed jobs.\n\n    Args:\n        uuid (str): The job UUID to return details for.\n        result (bool): If True, return job result.\n        events (bool): If True, return job events.\n\n    Returns:\n        Result: A Result object with the job details.\n    \"\"\"\n    job = self.db.get_job_info(\n        uuid=uuid, include_result=result, include_events=events\n    )\n\n    if job:\n        return Result(\n            task=f\"{self.name}:job_details\",\n            result=job,\n        )\n    else:\n        raise RuntimeError(f\"{self.name} - job with UUID '{uuid}' not found\")\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.job_list","title":"<code>job_list(pending: bool = True, completed: bool = True, task: str = None, last: int = None, client: str = None, uuid: str = None) -&gt; Result</code>","text":"<p>Method to list worker jobs completed and pending.</p> <p>Parameters:</p> Name Type Description Default <code>pending</code> <code>bool</code> <p>If True or None, return pending jobs. If False, skip pending jobs.</p> <code>True</code> <code>completed</code> <code>bool</code> <p>If True or None, return completed jobs. If False, skip completed jobs.</p> <code>True</code> <code>task</code> <code>str</code> <p>If provided, return only jobs with this task name.</p> <code>None</code> <code>last</code> <code>int</code> <p>If provided, return only the last N completed and last N pending jobs.</p> <code>None</code> <code>client</code> <code>str</code> <p>If provided, return only jobs submitted by this client.</p> <code>None</code> <code>uuid</code> <code>str</code> <p>If provided, return only the job with this UUID.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>Result object with a list of jobs.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef job_list(\n    self,\n    pending: bool = True,\n    completed: bool = True,\n    task: str = None,\n    last: int = None,\n    client: str = None,\n    uuid: str = None,\n) -&gt; Result:\n    \"\"\"\n    Method to list worker jobs completed and pending.\n\n    Args:\n        pending (bool): If True or None, return pending jobs. If False, skip pending jobs.\n        completed (bool): If True or None, return completed jobs. If False, skip completed jobs.\n        task (str, optional): If provided, return only jobs with this task name.\n        last (int, optional): If provided, return only the last N completed and last N pending jobs.\n        client (str, optional): If provided, return only jobs submitted by this client.\n        uuid (str, optional): If provided, return only the job with this UUID.\n\n    Returns:\n        Result: Result object with a list of jobs.\n    \"\"\"\n    jobs = self.db.list_jobs(\n        pending=pending,\n        completed=completed,\n        task=task,\n        last=last,\n        client=client,\n        uuid=uuid,\n    )\n\n    # Add worker and service information to each job\n    for job in jobs:\n        job[\"worker\"] = self.name\n        job[\"service\"] = self.service.decode(\"utf-8\")\n        # Map database status to expected status names\n        if job[\"status\"] in (\"PENDING\", \"STARTED\"):\n            job[\"status\"] = \"PENDING\"\n            job[\"completed_timestamp\"] = None\n        elif job[\"status\"] in (\"COMPLETED\", \"FAILED\"):\n            job[\"status\"] = \"COMPLETED\"\n\n    return Result(\n        task=f\"{self.name}:job_list\",\n        result=jobs,\n    )\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.echo","title":"<code>echo(job: Job, raise_error: Union[bool, int, str] = None, sleep: int = None, *args: Any, **kwargs: Any) -&gt; Result</code>","text":"<p>Echoes the job information and optional arguments, optionally sleeping or raising an error.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job instance containing job details.</p> required <code>raise_error</code> <code>str</code> <p>If provided, raises a RuntimeError with this message.</p> <code>None</code> <code>sleep</code> <code>int</code> <p>If provided, sleeps for the specified number of seconds.</p> <code>None</code> <code>*args</code> <code>Any</code> <p>Additional positional arguments to include in the result.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to include in the result.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing job details and any provided arguments.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>raise_error</code> is provided.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"POST\"]},\n    input=models.WorkerEchoIn,\n    output=models.WorkerEchoOut,\n)\ndef echo(\n    self,\n    job: Job,\n    raise_error: Union[bool, int, str] = None,\n    sleep: int = None,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Echoes the job information and optional arguments, optionally sleeping or raising an error.\n\n    Args:\n        job (Job): The job instance containing job details.\n        raise_error (str, optional): If provided, raises a RuntimeError with this message.\n        sleep (int, optional): If provided, sleeps for the specified number of seconds.\n        *args: Additional positional arguments to include in the result.\n        **kwargs: Additional keyword arguments to include in the result.\n\n    Returns:\n        Result: An object containing job details and any provided arguments.\n\n    Raises:\n        RuntimeError: If `raise_error` is provided.\n    \"\"\"\n    if sleep:\n        time.sleep(sleep)\n    if raise_error:\n        raise RuntimeError(raise_error)\n    return Result(\n        result={\n            \"juuid\": job.juuid,\n            \"client_address\": job.client_address,\n            \"timeout\": job.timeout,\n            \"task\": job.task,\n            \"args\": args,\n            \"kwargs\": kwargs,\n        }\n    )\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.list_tasks","title":"<code>list_tasks(name: Union[None, str] = None, brief: bool = False) -&gt; Result</code>","text":"<p>Lists tasks supported by worker.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of a specific task to retrieve</p> <code>None</code> <code>brief</code> <code>bool</code> <p>If True, returns only the list of task names</p> <code>False</code> <p>Returns:</p> Type Description <code>Result</code> <p>Results returned controlled by this logic:</p> <ul> <li>If brief is True returns a list of task names</li> <li>If name is provided returns list with single item - OpenAPI schema of the specified task</li> <li>Otherwise returns a list of schemas for all tasks</li> </ul> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a specific task name is provided but not registered in NORFAB_WORKER_TASKS.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef list_tasks(self, name: Union[None, str] = None, brief: bool = False) -&gt; Result:\n    \"\"\"\n    Lists tasks supported by worker.\n\n    Args:\n        name (str, optional): The name of a specific task to retrieve\n        brief (bool, optional): If True, returns only the list of task names\n\n    Returns:\n        Results returned controlled by this logic:\n\n            - If brief is True returns a list of task names\n            - If name is provided returns list with single item - OpenAPI schema of the specified task\n            - Otherwise returns a list of schemas for all tasks\n\n    Raises:\n        KeyError: If a specific task name is provided but not registered in NORFAB_WORKER_TASKS.\n    \"\"\"\n    ret = Result()\n    if brief:\n        ret.result = list(sorted(NORFAB_WORKER_TASKS.keys()))\n    elif name:\n        if name not in NORFAB_WORKER_TASKS:\n            raise KeyError(f\"{name} - task not registered\")\n        ret.result = [NORFAB_WORKER_TASKS[name][\"schema\"]]\n    else:\n        ret.result = [t[\"schema\"] for t in NORFAB_WORKER_TASKS.values()]\n    return ret\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.start_threads","title":"<code>start_threads() -&gt; None</code>","text":"<p>Starts multiple daemon threads required for the worker's operation.</p> This method initializes and starts the following threads <ul> <li>request_thread: Handles posting requests using the _post function.</li> <li>reply_thread: Handles receiving replies using the _get function.</li> <li>event_thread: Handles event processing using the _event function.</li> <li>recv_thread: Handles receiving data using the recv function.</li> </ul> <p>Each thread is started as a daemon and is provided with the necessary arguments, including queues and events as required.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def start_threads(self) -&gt; None:\n    \"\"\"\n    Starts multiple daemon threads required for the worker's operation.\n\n    This method initializes and starts the following threads:\n        - request_thread: Handles posting requests using the _post function.\n        - reply_thread: Handles receiving replies using the _get function.\n        - event_thread: Handles event processing using the _event function.\n        - recv_thread: Handles receiving data using the recv function.\n\n    Each thread is started as a daemon and is provided with the necessary arguments,\n    including queues and events as required.\n\n    Returns:\n        None\n    \"\"\"\n    # Start threads\n    self.request_thread = threading.Thread(\n        target=_post,\n        daemon=True,\n        name=f\"{self.name}_post_thread\",\n        args=(\n            self,\n            self.post_queue,\n            self.destroy_event,\n        ),\n    )\n    self.request_thread.start()\n    self.reply_thread = threading.Thread(\n        target=_get,\n        daemon=True,\n        name=f\"{self.name}_get_thread\",\n        args=(self, self.get_queue, self.destroy_event),\n    )\n    self.reply_thread.start()\n    self.event_thread = threading.Thread(\n        target=_event,\n        daemon=True,\n        name=f\"{self.name}_event_thread\",\n        args=(self, self.event_queue, self.destroy_event),\n    )\n    self.event_thread.start()\n    self.put_thread = threading.Thread(\n        target=_put,\n        daemon=True,\n        name=f\"{self.name}_put_thread\",\n        args=(\n            self,\n            self.put_queue,\n            self.destroy_event,\n        ),\n    )\n    self.put_thread.start()\n    # start receive thread after other threads\n    self.recv_thread = threading.Thread(\n        target=recv,\n        daemon=True,\n        name=f\"{self.name}_recv_thread\",\n        args=(\n            self,\n            self.destroy_event,\n        ),\n    )\n    self.recv_thread.start()\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.run_next_job","title":"<code>run_next_job(uuid: str)</code>","text":"<p>Processes the next job from the database.</p> <p>This method performs the following steps:</p> <ol> <li>Loads job data from the database.</li> <li>Parses the job data to extract the task name, arguments, keyword arguments, and timeout.</li> <li>Executes the specified task method on the worker instance with the provided arguments.</li> <li>Handles any exceptions raised during task execution, logging errors and creating a failed Result object if needed.</li> <li>Saves the result of the job execution to the database.</li> <li>Marks the job as completed or failed in the database.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>uuid</code> <code>str</code> <p>The job UUID to process.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the executed task does not return a Result object.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def run_next_job(self, uuid: str):\n    \"\"\"\n    Processes the next job from the database.\n\n    This method performs the following steps:\n\n    1. Loads job data from the database.\n    2. Parses the job data to extract the task name, arguments, keyword arguments, and timeout.\n    3. Executes the specified task method on the worker instance with the provided arguments.\n    4. Handles any exceptions raised during task execution, logging errors and creating a failed Result object if needed.\n    5. Saves the result of the job execution to the database.\n    6. Marks the job as completed or failed in the database.\n\n    Args:\n        uuid (str): The job UUID to process.\n\n    Raises:\n        TypeError: If the executed task does not return a Result object.\n    \"\"\"\n    log.debug(f\"{self.name} - processing job request {uuid}\")\n\n    # Load job data from database\n    job_data = self.db.get_job_info(uuid)\n    if not job_data:\n        log.error(f\"{self.name} - job {uuid} not found in database\")\n        return\n\n    client_address = job_data[\"client_address\"]\n    task = job_data[\"task\"]\n    args = job_data[\"args\"]\n    kwargs = job_data[\"kwargs\"]\n    timeout = job_data[\"timeout\"]\n\n    job = Job(\n        worker=self,\n        client_address=client_address,\n        juuid=uuid,\n        task=task,\n        timeout=timeout,\n        args=copy.deepcopy(args),\n        kwargs=copy.deepcopy(kwargs),\n        client_input_queue=queue.Queue(maxsize=0),\n    )\n    self.running_jobs[uuid] = job\n\n    log.debug(\n        f\"{self.name} - doing task '{task}', timeout: '{timeout}', \"\n        f\"args: '{args}', kwargs: '{kwargs}', client: '{client_address}', \"\n        f\"job uuid: '{uuid}'\"\n    )\n\n    # inform client that job started\n    job.event(message=\"starting\", status=\"running\")\n\n    # run the actual job\n    try:\n        task_started = time.ctime()\n        result = NORFAB_WORKER_TASKS[task][\"function\"](\n            self, *args, job=job, **kwargs\n        )\n        task_completed = time.ctime()\n        if not isinstance(result, Result):\n            raise TypeError(\n                f\"{self.name} - task '{task}' did not return Result object, \"\n                f\"args: '{args}', kwargs: '{kwargs}', client: '{client_address}', \"\n                f\"job uuid: '{uuid}'; task returned '{type(result)}'\"\n            )\n        result.task = result.task or f\"{self.name}:{task}\"\n        result.status = result.status or \"completed\"\n        result.juuid = result.juuid or uuid\n        result.service = self.service.decode(\"utf-8\")\n        job_failed = False\n    except Exception as e:\n        task_completed = time.ctime()\n        result = Result(\n            task=f\"{self.name}:{task}\",\n            errors=[traceback.format_exc()],\n            messages=[f\"Worker experienced error: '{e}'\"],\n            failed=True,\n            juuid=uuid,\n        )\n        log.error(\n            f\"{self.name} - worker experienced error:\\n{traceback.format_exc()}\"\n        )\n        job_failed = True\n\n    result.task_started = task_started\n    result.task_completed = task_completed\n\n    # Prepare result data for database storage as JSON-serializable dict\n    result_data = {\n        \"client_address\": client_address,\n        \"uuid\": uuid,\n        \"status_code\": \"200\",\n        \"result\": {self.name: result.model_dump()},\n        \"worker\": self.name,\n        \"service\": self.service.decode(\"utf-8\"),\n    }\n\n    # Save job result to database\n    if job_failed:\n        self.db.fail_job(uuid, result_data)\n    else:\n        self.db.complete_job(uuid, result_data)\n\n    # remove job from running jobs\n    _ = self.running_jobs.pop(uuid)\n\n    # inform client that job completed\n    job.event(message=\"completed\", status=\"completed\")\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.NFPWorker.work","title":"<code>work()</code>","text":"<p>Executes the main worker loop, managing job execution using a thread pool.</p> <p>This method starts necessary background threads, then enters a loop where it:</p> <ul> <li>Queries the database for the next pending job.</li> <li>Atomically marks the job as started in the database.</li> <li>Submits the job to a thread pool executor for concurrent processing.</li> <li>Waits briefly if no pending jobs are found.</li> <li>Continues until either the exit or destroy event is set.</li> </ul> <p>Upon exit, performs cleanup by calling the <code>destroy</code> method with a status message.</p> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def work(self):\n    \"\"\"\n    Executes the main worker loop, managing job execution using a thread pool.\n\n    This method starts necessary background threads, then enters a loop where it:\n\n    - Queries the database for the next pending job.\n    - Atomically marks the job as started in the database.\n    - Submits the job to a thread pool executor for concurrent processing.\n    - Waits briefly if no pending jobs are found.\n    - Continues until either the exit or destroy event is set.\n\n    Upon exit, performs cleanup by calling the `destroy` method with a status message.\n    \"\"\"\n\n    self.start_threads()\n\n    # start job threads and submit jobs in an infinite loop\n    with concurrent.futures.ThreadPoolExecutor(\n        max_workers=self.max_concurrent_jobs,\n        thread_name_prefix=f\"{self.name}-job-thread\",\n    ) as executor:\n        while not self.exit_event.is_set() and not self.destroy_event.is_set():\n            # Get next pending job from database\n            job_info = self.db.get_next_pending_job()\n\n            if job_info is None:\n                # No pending jobs, wait a bit\n                time.sleep(0.1)\n                continue\n\n            uuid, received_timestamp = job_info\n            log.debug(f\"{self.name} - submitting job {uuid} to executor\")\n\n            # Submit the job to workers\n            executor.submit(self.run_next_job, uuid)\n\n    # make sure to clean up\n    self.destroy(\n        f\"{self.name} - exit event is set '{self.exit_event.is_set()}', \"\n        f\"destroy event is set '{self.destroy_event.is_set()}'\"\n    )\n</code></pre>"},{"location":"api_reference_core_norfab_worker/#norfab.core.worker.recv","title":"<code>recv(worker, destroy_event)</code>","text":"<p>Thread to process receive messages from broker.</p> <p>This function runs in a loop, polling the worker's broker socket for messages every second. When a message is received, it processes the message based on the command type and places it into the appropriate queue or handles it accordingly. If the keepaliver thread is not alive, it logs a warning and attempts to reconnect to the broker.</p> <p>Parameters:</p> Name Type Description Default <code>worker</code> <code>Worker</code> <p>The worker instance that contains the broker socket and queues.</p> required <code>destroy_event</code> <code>Event</code> <p>An event to signal the thread to stop.</p> required Commands <ul> <li>NFP.POST: Places the message into the post_queue.</li> <li>NFP.DELETE: Places the message into the delete_queue.</li> <li>NFP.GET: Places the message into the get_queue.</li> <li>NFP.KEEPALIVE: Processes a keepalive heartbeat.</li> <li>NFP.DISCONNECT: Attempts to reconnect to the broker.</li> <li>Other: Logs an invalid input message.</li> </ul> Source code in <code>norfab\\core\\worker.py</code> <pre><code>def recv(worker, destroy_event):\n    \"\"\"\n    Thread to process receive messages from broker.\n\n    This function runs in a loop, polling the worker's broker socket for messages every second.\n    When a message is received, it processes the message based on the command type and places\n    it into the appropriate queue or handles it accordingly. If the keepaliver thread is not\n    alive, it logs a warning and attempts to reconnect to the broker.\n\n    Args:\n        worker (Worker): The worker instance that contains the broker socket and queues.\n        destroy_event (threading.Event): An event to signal the thread to stop.\n\n    Commands:\n        - NFP.POST: Places the message into the post_queue.\n        - NFP.DELETE: Places the message into the delete_queue.\n        - NFP.GET: Places the message into the get_queue.\n        - NFP.KEEPALIVE: Processes a keepalive heartbeat.\n        - NFP.DISCONNECT: Attempts to reconnect to the broker.\n        - Other: Logs an invalid input message.\n    \"\"\"\n    while not destroy_event.is_set():\n        # Poll socket for messages every 1000ms\n        try:\n            items = worker.poller.poll(1000)\n        except KeyboardInterrupt:\n            break  # Interrupted\n        if items:\n            with worker.socket_lock:\n                msg = worker.broker_socket.recv_multipart()\n            log.debug(f\"{worker.name} - received '{msg}'\")\n            empty = msg.pop(0)  # noqa\n            header = msg.pop(0)\n            command = msg.pop(0)\n\n            if command == NFP.POST:\n                worker.post_queue.put(msg)\n            elif command == NFP.DELETE:\n                worker.delete_queue.put(msg)\n            elif command == NFP.GET:\n                worker.get_queue.put(msg)\n            elif command == NFP.PUT:\n                worker.put_queue.put(msg)\n            elif command == NFP.KEEPALIVE:\n                worker.keepaliver.received_heartbeat([header] + msg)\n            elif command == NFP.DISCONNECT:\n                worker.reconnect_to_broker()\n            else:\n                log.debug(\n                    f\"{worker.name} - invalid input, header '{header}', command '{command}', message '{msg}'\"\n                )\n\n        if not worker.keepaliver.is_alive():\n            log.warning(f\"{worker.name} - '{worker.broker}' broker keepalive expired\")\n            worker.reconnect_to_broker()\n</code></pre>"},{"location":"clients_nfcli_overview/","title":"NORFAB CLI Overview","text":"<p>NORFAB comes with an interactive command line shell interface invoked using <code>nfcli</code> to work with the system. The CLI provides a powerful and flexible way to interact with NORFAB, enabling users to manage and automate network operations efficiently.</p> <p>NORFAB CLI designed as a modal operating system. The term modal  describes a system that has various modes of operation, each having its own  domain of operation. The CLI uses a hierarchical structure for the modes.</p> <p>You can access a lower-level mode only from a higher-level mode. For example,  to access the Nornir mode, you must be in the privileged EXEC mode. Each mode  is used to accomplish particular tasks and has a specific set of commands that  are available in this mode. For example, to configure a router interface, you  must be in Nornir configuration mode. All configurations that you enter in  configuration mode apply only to this function.</p> <p>NORFAB CLI build using PICLE package.</p> <p>It is important to remember that in PICLE Shell, when you enter a command, the  command is executed. If you enter an incorrect command in a production environment,  it can negatively impact it.</p>","tags":["nfcli"]},{"location":"clients_overview/","title":"NORFAB Clients Overview","text":"<p>NORFAB provides multiple client interfaces to interact with automation fabric, catering to different use cases and user preferences. These clients include:</p>"},{"location":"clients_overview/#1-robot-framework-client","title":"1. Robot Framework Client","text":"<p>The Robot Framework Client integrates NORFAB with the Robot Framework, enabling users to define and execute workflows using a domain-specific language (DSL). It is ideal for users who prefer a keyword-driven approach to automation.</p> <p>Key Features: - Supports ROBOT keywords for targeting hosts, running tests, and executing CLI or configuration tasks. - Seamlessly integrates with Robot Framework's test suite.</p> <p>Refer to Robot Client Documentation for more details.</p>"},{"location":"clients_overview/#2-fastapi-rest-api-client","title":"2. FastAPI REST API Client","text":"<p>The FastAPI Service provides a RESTful API interface to interact with NORFAB. It is designed for developers who prefer using HTTP-based APIs for automation and integration with other systems.</p> <p>Key Features: - High-performance REST API built with FastAPI. - Automatic API documentation with Swagger UI and ReDoc. - Secure access using bearer token authentication.</p> <p>Refer to FastAPI Service Documentation for more details.</p>"},{"location":"clients_overview/#3-command-line-interface-cli","title":"3. Command Line Interface (CLI)","text":"<p>The NORFAB CLI (<code>nfcli</code>) is an interactive shell interface for managing and automating network operations. It is suitable for users who prefer a command-line approach.</p> <p>Key Features: - Modal design with hierarchical modes for specific tasks. - Built using the PICLE package for a robust shell experience.</p> <p>Refer to CLI Documentation for more details.</p>"},{"location":"clients_overview/#4-python-api-client","title":"4. Python API Client","text":"<p>The Python API Client provides a programmatic interface for developers to integrate NORFAB capabilities into their Python applications. It is ideal for advanced automation and custom integrations.</p> <p>Key Features: - Direct access to NORFAB's core functionality via Python. - Flexible and extensible for custom automation workflows.</p> <p>Refer to Python API Documentation for more details.</p>"},{"location":"clients_overview/#conclusion","title":"Conclusion","text":"<p>Each NORFAB client is tailored to specific use cases, ensuring flexibility and ease of use for different types of users. Whether you prefer a graphical interface, command-line tools, or programmatic APIs, NORFAB has a client to meet your needs.</p>"},{"location":"clients_python_api_overview/","title":"NORFAB Python API","text":"<p>The NORFAB Python API provides a programmatic interface to interact with the NORFAB automation fabric. It is designed for developers who want to integrate NORFAB capabilities into their Python applications for advanced network automation and management tasks.</p> <p>NorFab Python API can be used to interact with automations fabric. To start working with Python API need to import <code>NorFab</code> object and instantiate it.</p> <pre><code>from norfab.core.nfapi import NorFab\n\nnf = NorFab(inventory=\"./inventory.yaml\")\nnf.start()\nnf.destroy()\n</code></pre> <p>Refer to Getting Started section on  how to construct  <code>inventory.yaml</code> file.</p> <p>All interaction with NorFab happens via client, to create a client need to call <code>make_client</code> method:</p> <pre><code>import pprint\nfrom norfab.core.nfapi import NorFab\n\nnf = NorFab(inventory=\"./inventory.yaml\")\nnf.start()\n\nclient = nf.make_client()\n\nresult = nf.client.run_job(\n    service=\"nornir\",\n    task=\"cli\",\n    kwargs={\"commands\": [\"show version\", \"show clock\"]}\n)\n\npprint.pprint(ret)\n\nnf.destroy()\n</code></pre> <p>Calling <code>destroy</code> method will kill all the clients as well.</p>"},{"location":"clients_robot_client_overview/","title":"NorFab Robot Client","text":"<p>NORFAB Robot Client integrates with ROBOT framework to interact  with NORFAB, allowing to construct workflows and tasks using  ROBOT domain specific language (DSL).</p> <p>Robot Framework needs to be installed on the client:</p> <pre><code>pip install norfab[robot]\n</code></pre>","tags":["robot"]},{"location":"clients_robot_client_overview/#supported-robot-keywords","title":"Supported ROBOT Keywords","text":"<ul> <li><code>Hosts</code> - <code>Fx</code> filters to target specific hosts, if not      provided targets all hosts</li> <li><code>Workers</code> - names of the workers to target, default is <code>all</code></li> <li><code>nr.test</code> - run Nornir Service <code>test</code> task using      provided Nornir tests suite</li> <li><code>nr.cli</code> - run Nornir Service <code>cli</code> task using      provided show commands and arguments</li> <li><code>nr.cfg</code> - run Nornir Service <code>cfg</code> task using      provided configuration commands and arguments</li> </ul>","tags":["robot"]},{"location":"clients_robot_client_overview/#nornir-tests-examples","title":"Nornir Tests Examples","text":"<p>This ROBOT framework test suite runs two tests using <code>nr.test</code>:</p> /path/to/robot_suite.robot<pre><code>*** Settings ***\nLibrary    norfab.clients.robot_client.NorFabRobot\n\n*** Test Cases ***\nTest NTP\n    nr.test    suite=nf://tests/test_ntp_config.yaml\n\nTest Software Version\n    Hosts      FM=arista_eos\n    nr.test    suite=nf://tests/test_version.yaml\n</code></pre> <p>Run test suite from client using <code>robot</code> command line tool:</p> <pre><code>robot /path/to/robot_suite.robot\n</code></pre>","tags":["robot"]},{"location":"norfab_changelog/","title":"Changelog","text":""},{"location":"norfab_changelog/#0151","title":"0.15.1","text":""},{"location":"norfab_changelog/#bugs","title":"BUGS","text":"<ol> <li>Nornir service watchdog - fixing <code>connection_idle_timeout</code> handling</li> <li>Netbox service - fixing <code>ssl_verify</code> handling to suppress <code>InsecureRequestWarning</code></li> <li>Norfab shell - fixed references to deprecated broker fss calls, replaced with calls to <code>filesharing</code> service</li> </ol>"},{"location":"norfab_changelog/#0150","title":"0.15.0","text":""},{"location":"norfab_changelog/#changes","title":"CHANGES","text":"<ol> <li>Introduce sqlite3 DB into client for jobs state persistence</li> <li>Updated NFP semantics for better performance and readability</li> <li>Worker - added support for <code>job.stream</code> capability to stream a set of bytes back to client, used for file transfers, added new <code>NFP.STREAM</code> command as part of this effort</li> <li>Added support for new <code>NFP.PUT</code> command for client to update running jobs on worker, currently used by client to command worker a number of file offsets to stream back, but can be extended to provide user input mid job execution, e.g. agent requesting input from user.</li> <li>Client - refactored <code>fetch_file</code> method to use new stream and put capabilities</li> <li>Nornir test markdown - various markdown output improvements such as added total tests to summary, added test number column to the table, added host name next to every collected command, removed support for hosts inventory as it bogged browser memory for being too long.</li> </ol>"},{"location":"norfab_changelog/#features","title":"FEATURES","text":"<ol> <li>Client and worker - added <code>delete_fetched_files</code> task to remove files fetched from broker</li> <li>Created new <code>filesharing</code> service worker to host files, by default broker runs 1 such worker locally, this improves norfab hosting capabilities and open paths toward integrating with external file sharing resources e.g. github, s3, http etc.</li> </ol>"},{"location":"norfab_changelog/#0140","title":"0.14.0","text":""},{"location":"norfab_changelog/#changes_1","title":"CHANGES","text":"<ol> <li>Netbox sync_device_interfaces - refactored to use bulk update and bulk create operations</li> <li>Netbox service - deprecated support for Netbox below 4.4.0 version</li> <li>Netbox service - <code>get_interfaces</code> added <code>label</code> and <code>mark_connected</code> fields</li> <li>Netbox service - <code>get_connections</code> added <code>remote_device_status</code> field</li> <li>Nornir service - test task added nornir hosts inventory to results if extensive set to true</li> <li>Nornir tests markdown - added hosts expandable inventory section</li> <li>Nornir tests markdown - added total tests number for detailed section for each host</li> <li>Nornir tests markdown - updated headings and paragraphs content</li> <li>Enhanced message construction for NFP protocol by adding message builder</li> <li>Enhanced client, worker and broker socket handling by adding thread locks</li> </ol>"},{"location":"norfab_changelog/#features_1","title":"FEATURES","text":"<ol> <li>Netbox service - added <code>create_device_interfaces</code> task</li> <li>Netbox service - added <code>branch_create_timeout</code> inventory argument to control timer waiting for new branch to be created</li> <li>Netbox service - added integration with Netbox BGP Plugin in a form of <code>get_bgp_peerings</code> task to fetch BGP sessions data for devices, added nfcli shell command to call <code>get_bgp_peerings</code> task.</li> <li>Netbox service - <code>get_nornir_inventory</code> task added support for <code>bgp_sessions</code> argument. if True, fetched devices' BGP peerings from netbox and stores them under <code>bgp_sessions</code> key in host's data.</li> </ol>"},{"location":"norfab_changelog/#bugs_1","title":"BUGS","text":"<ol> <li>Nornir tests markdown - fixed detailed output handling to not collect suite into summary table</li> <li>Nfcli - fixed show client command function calling</li> </ol>"},{"location":"norfab_changelog/#0130","title":"0.13.0","text":""},{"location":"norfab_changelog/#bug-fixes","title":"BUG FIXES","text":"<ol> <li>Fixing event severity handling in worker.</li> </ol>"},{"location":"norfab_changelog/#changes_2","title":"CHANGES","text":"<ol> <li>Changed worker to use sqlite database for job persistent storage instead of json files</li> <li>Netbox sync device facts - enhanced sync logic to use bulk device updates</li> </ol>"},{"location":"norfab_changelog/#features_2","title":"FEATURES","text":"<ol> <li>Nornir test task - added <code>extensive</code> argument to return detailed results</li> <li>Client - added <code>markdown</code> argument to <code>run_job</code> method to support rendering results into markdown output</li> <li>Client - added Nornir test task markdown render function to return tests results in a form of markdown report.</li> <li>Client - added generic markdown render function to return results in a form of markdown report.</li> </ol>"},{"location":"norfab_changelog/#0127","title":"0.12.7","text":""},{"location":"norfab_changelog/#features_3","title":"FEATURES","text":"<ol> <li>NFAPI logging - added support for <code>logging-&gt;log_events</code> parameter to emit events as syslog messages</li> <li>Added context manager support for NFAPI to simplify invocation from python scripts</li> </ol>"},{"location":"norfab_changelog/#changes_3","title":"CHANGES","text":"<ol> <li>NFAPI changed start method arguments from <code>start_broker</code> to <code>run_broker</code> and <code>workers</code> to <code>run_workers</code></li> <li>Nornir task - enhanced task import logic</li> <li>Nornir worker - added filter hosts method to reduce duplicate code</li> <li>NorFab client - added ensure_bytes method to reduce duplicate code</li> </ol>"},{"location":"norfab_changelog/#0126","title":"0.12.6","text":""},{"location":"norfab_changelog/#bugs_2","title":"BUGS","text":"<ol> <li>Netbox service - fixed get_circuits handling bug, was not returning circuits data due to recent code refactoring, updated test to catch this kind of an issue.</li> </ol>"},{"location":"norfab_changelog/#0125","title":"0.12.5","text":""},{"location":"norfab_changelog/#bugs_3","title":"BUGS","text":"<ol> <li>Containerlab - fixing error handling when result is None</li> <li>Workflow - fixed emitting event progress for cli shell</li> <li>Netbox - bulk ip create updated to sort interfaces names to make ip allocation order deterministic</li> </ol>"},{"location":"norfab_changelog/#changes_4","title":"CHANGES","text":"<ol> <li>Updated dependencies:</li> <li>TTP 0.9.5 -&gt; 0.10.0</li> <li> <p>nornir-salt 0.22 -&gt; 0.23</p> </li> <li> <p>Renamed Netbox service tasks:</p> </li> <li>update_device_facts -&gt; sync_device_facts</li> <li>update_device_interfaces -&gt; sync_device_interfaces</li> <li>update_device_ip -&gt; sync_device_ip</li> </ol>"},{"location":"norfab_changelog/#features_4","title":"FEATURES","text":"<ol> <li>Netbox service - get_connections enhanced to retrieve power outlet connections.</li> </ol>"},{"location":"norfab_changelog/#0124","title":"0.12.4","text":""},{"location":"norfab_changelog/#bugs_4","title":"BUGS","text":"<ol> <li>Netbox worker - fixing handling of Netbox instance for child subnet creation</li> </ol>"},{"location":"norfab_changelog/#features_5","title":"FEATURES","text":"<ol> <li>Netbox worker - update interface descriptions task now can supply dictionary of interface descriptions</li> </ol>"},{"location":"norfab_changelog/#0123","title":"0.12.3","text":""},{"location":"norfab_changelog/#bugs_5","title":"BUGS","text":"<ol> <li>Fixing handling of entry points for sourcing norfab workers plugins for Py3.10+</li> </ol>"},{"location":"norfab_changelog/#changes_5","title":"CHANGES","text":"<ol> <li>Docker images updated to use Py3.11</li> </ol>"},{"location":"norfab_changelog/#0122","title":"0.12.2","text":""},{"location":"norfab_changelog/#changes_6","title":"CHANGES","text":"<ol> <li>Updated Python version dependencies</li> <li>Adding FastMCP worker</li> <li>Updating agent worker - work in progress</li> <li>Adding Streamlit worker to host WEB UI - work in progress</li> </ol>"},{"location":"norfab_changelog/#0121","title":"0.12.1","text":""},{"location":"norfab_changelog/#bugs_6","title":"BUGS","text":"<ol> <li>Netbox service - restored Netbox v4.2.0 support</li> <li>Fixing Picle shell netbox get interfaces to have <code>interface_regex</code> argument</li> <li>Fixing Picle check netbox get connection arguments</li> </ol>"},{"location":"norfab_changelog/#0120","title":"0.12.0","text":""},{"location":"norfab_changelog/#bugs_7","title":"BUGS","text":"<ol> <li>FastAPI Service - fixing JSON references for OpenAPI schema, which previously broken led to error in swagger and redoc UIs rendering.</li> </ol>"},{"location":"norfab_changelog/#features_6","title":"FEATURES","text":"<ol> <li>Netbox service - <code>get_interfaces</code> task added <code>interfaces_regex</code> filter</li> <li>Netbox service - <code>get_connections</code> task added <code>interfaces_regex</code> filter</li> <li>Netbox service - <code>get_connections</code> added support to retrieve virtual interfaces connections for physical and lag interfaces</li> <li>Netbox service - <code>create_ip</code> added support to automatically assign IPs to link peers or use peers IP addresses prefixes to allocate next available IP address</li> <li>Netbox service - <code>create_ip</code> added support to create child subnet within parent prefix and assign IP out of it</li> <li>Netbox service - added new task <code>create_ip_bulk</code> to create IP addresses for devices using interfaces regex match</li> <li>FastMCP service - created NorFab MCP service</li> </ol>"},{"location":"norfab_changelog/#0112","title":"0.11.2","text":""},{"location":"norfab_changelog/#changes_7","title":"CHANGES","text":"<ol> <li>Moving to Python 3.10 as primary supported version instead of 3.9 due to addition of FastMCP service which only supports Py3.10 and up.</li> <li>Updated build dockerfiles to use <code>python:3.10-slim-trixie</code> as a base image</li> </ol>"},{"location":"norfab_changelog/#bugs_8","title":"BUGS","text":"<ol> <li>Fixing FastAPI worker argument handling by setting it to <code>all</code> by default</li> </ol>"},{"location":"norfab_changelog/#0111","title":"0.11.1","text":""},{"location":"norfab_changelog/#bugs_9","title":"BUGS","text":"<ol> <li>Fixed containerlab show lab outputting</li> <li>Enhanced nfcli logic to be able to start broker and workers only without a client</li> <li>Updated aio Dockerfile to start broker and workers only</li> </ol>"},{"location":"norfab_changelog/#0110","title":"0.11.0","text":""},{"location":"norfab_changelog/#features_7","title":"FEATURES","text":"<ol> <li>FastAPI Services - enhanced to generate API endpoints for all services tasks automatically using <code>@Task</code> decorator data</li> <li>NFCLI - Picle show containerlab containers now emits output with nested tables</li> <li>Netbox Service - <code>create_ip</code> task enhanced to source prefixes to allocate next IP from using prefix description string</li> <li>Netbox Service -  Added <code>create_prefix</code> task to allocate next available prefix</li> <li>Nornir Service - Adding <code>nb_create_prefix</code> Jinja2 filter allocate next available prefix during templates rendering</li> <li>Worker - Added <code>fastapi</code> argument to <code>@Task</code> decorator to control FastAPI REST API endpoints auto-generation</li> <li>Containerlab Service - added support for Containerlab 0.69+</li> <li>Netbox Service - added support for branching plugin, made create and update tasks be branch aware, updated nfcli shells to support <code>branch</code> argument</li> <li>Netbox Service - added <code>delete_branch</code> task</li> </ol>"},{"location":"norfab_changelog/#bugs_10","title":"BUGS","text":"<ol> <li>Fixing nornir test picle shell test task handling for verbose-result and dry-run</li> <li>Fixing nornir test handling for when suite renders to empty tests for a host</li> <li>Fixed Netbox service <code>instance</code> variable options sourcing for CLI shells</li> </ol>"},{"location":"norfab_changelog/#changes_8","title":"CHANGES","text":"<ol> <li>Upgrading NAPALM library dependency from 5.0.0 to 5.1.0</li> <li>Upgrading PICLE library dependency from 0.9.0 to 0.9.1</li> <li>Upgrading Pynetbox library dependency from 7.4.0 to 7.5.0</li> <li>Refactoring Netbox service pydantic models</li> <li>BREAKING CHANGE: Starting NorFab 0.11.0 containerlab service only supports Containerlab 0.69+</li> <li>Dependencies updates.</li> </ol>"},{"location":"norfab_changelog/#0100","title":"0.10.0","text":""},{"location":"norfab_changelog/#features_8","title":"FEATURES","text":"<ol> <li>Adding support for Netbox &gt;= 4.3.0</li> <li>Enhanced Netbox service inventory device filters to support GraphQL query string for <code>device_list</code> queries.</li> <li>Added Netbox service <code>create_ip</code> task to allocate new or source existing IP from prefix</li> <li>Added <code>nb_create_ip</code> Jinja2 filter to Nornir service to source IP allocations during templates rendering</li> <li>Added nfcli shell <code>netbox create ip</code> command to run IP allocations from interactive command line</li> </ol>"},{"location":"norfab_changelog/#091","title":"0.9.1","text":""},{"location":"norfab_changelog/#bugs_11","title":"BUGS","text":"<ol> <li>Fixing list and dict annotations to also allow None values for workers tasks.</li> </ol>"},{"location":"norfab_changelog/#090","title":"0.9.0","text":""},{"location":"norfab_changelog/#features_9","title":"FEATURES","text":"<ol> <li>Adding concurrency to worker jobs execution, adding new worker inventory parameter <code>max_concurrent_jobs</code></li> <li>Adding <code>@Task()</code> decorator to expose worker methods as tasks, this decorator performs automatic type checking using type annotation, alternatively it supports input/output pydantic models to verify input arguments and return results.</li> <li>Passing on <code>job</code> argument to all NorFab tasks, <code>job</code> is an object that contains relevant metadata - client address, juuid, args, kwargs etc. Job object can be used to emit events.</li> <li>Adding workers <code>echo</code> task to perform tests, added respective nfcli commands tree <code>workers.ping</code>.</li> <li>Adding workers <code>list_tasks</code> method to return information about tasks in MCP compatible format.</li> <li>Added picle shell <code>man.tasks</code> command to retrieve information about NorFab services tasks</li> </ol>"},{"location":"norfab_changelog/#changes_9","title":"CHANGES","text":"<ol> <li>Improved Netbox device update nfcli to include Fx hosts filtering for nornir datasource</li> <li>Result object added <code>task_started</code> and <code>task_completed</code> timestamp and <code>service</code> parameters</li> </ol>"},{"location":"norfab_changelog/#breaking-changes","title":"BREAKING CHANGES","text":"<ol> <li>Instead of <code>self.event</code> worker now need to use <code>job.event</code></li> <li>To add pydantic input / output models for tasks need to use <code>@Task()</code> decorator instead of <code>@Task</code> decorator</li> <li>Nornir Jinja2 templates context <code>nornir.cli</code> removed, need to use <code>norfab.run_job</code> instead</li> </ol>"},{"location":"norfab_changelog/#082","title":"0.8.2","text":""},{"location":"norfab_changelog/#bugs_12","title":"BUGS","text":"<ol> <li>Fixed nornir inventory load from containerlab handling</li> </ol>"},{"location":"norfab_changelog/#081","title":"0.8.1","text":""},{"location":"norfab_changelog/#bugs_13","title":"BUGS","text":"<ol> <li>Fixed <code>show containerlab inventory</code> command</li> <li>Fixed Nornir Worker inventory load handling</li> <li>Netbox interface update improving mac address handling</li> </ol>"},{"location":"norfab_changelog/#features_10","title":"FEATURES","text":"<ol> <li>Netbox service added <code>get_containerlab_inventory</code> task </li> <li>Containerlab service added <code>deploy_netbox</code> task</li> </ol>"},{"location":"norfab_changelog/#changes_10","title":"CHANGES","text":"<ol> <li>Improved client post job retry logic</li> <li><code>FN</code> filter argument for Nornir add presence handling for nfcli</li> </ol>"},{"location":"norfab_changelog/#080","title":"0.8.0","text":""},{"location":"norfab_changelog/#changes_11","title":"CHANGES","text":"<ol> <li>Restructuring pydantic models structures for better following DRY principles:</li> <li>Moved FastAPI models under norfab.models.fastapi<ol> <li>Added norfab.models.nornir pydantic models</li> <li>Events and results models moved under norfab.models</li> </ol> </li> <li>Added broker <code>zmq_auth</code> inventory parameter to turn zero mq authentication and encryption off</li> <li>Added <code>verbose-result</code> command line argument to relevant tasks to emit result details</li> <li>Updated CLI shells to support PICLE 0.9.0</li> <li>Enhanced Netbox service to support working with instances of Netbox of different major and minor releases</li> </ol>"},{"location":"norfab_changelog/#bugs_14","title":"BUGS","text":"<ol> <li>Fixed broker to allow workers reconnect on restart.</li> </ol>"},{"location":"norfab_changelog/#features_11","title":"FEATURES","text":"<ol> <li>Improved worker jinja2 templates rendering logic to allow render URL first and next download its content</li> <li>Added <code>nornir refresh</code> CLI command to refresh Nornir workers instances and reload inventory</li> <li>Added support for Netbox 4.2</li> <li>Added support for Nornir service to pull hosts inventory from Containerlab service</li> </ol>"},{"location":"norfab_changelog/#070","title":"0.7.0","text":""},{"location":"norfab_changelog/#features_12","title":"FEATURES","text":"<ol> <li>Added new <code>workflow</code> service to run simple workflows constructed using YAML DSL</li> </ol>"},{"location":"norfab_changelog/#changes_12","title":"CHANGES","text":"<ol> <li>NFCLI shells - updated to use nested outputter where appropriate</li> <li>Nornir worker - updated to set failed flag for its tasks according to test results</li> <li>Worker - Added <code>status</code> field to worker result object to reflect job execution status</li> <li>Nornir Service - replaced <code>cfg_dry_run</code> and <code>cli_dry_run</code> arguments with <code>dry_run</code> argument</li> <li>NFCLI shell - added aliases to use dash instead of underscore</li> <li>NFCLI shell - Moved Nornir service show commands under <code>show nornir xyz</code> path  </li> </ol>"},{"location":"norfab_changelog/#060","title":"0.6.0","text":""},{"location":"norfab_changelog/#features_13","title":"FEATURES","text":"<ol> <li>Added support for worker plugins</li> <li>Added support for nfcli custom shells</li> </ol>"},{"location":"norfab_changelog/#changes_13","title":"CHANGES","text":"<ol> <li>All workers loaded into NorFab using entrypoints implementing lazy loading - workers classes only imported when they being used, in some cases allowing to save on startup time.</li> </ol>"},{"location":"norfab_changelog/#050","title":"0.5.0","text":""},{"location":"norfab_changelog/#features_14","title":"FEATURES","text":"<ol> <li>FastAPI service added bearer authentication support</li> <li>Added hooks attachpoints <code>nornir-startup</code> and <code>nornir-exit</code> to influence Nornir service workers startup and exit</li> </ol>"},{"location":"norfab_changelog/#040","title":"0.4.0","text":""},{"location":"norfab_changelog/#changes_14","title":"CHANGES","text":"<ol> <li>Improved netbox get_circuits logic.</li> <li>Standardised worker <code>get_version</code> and <code>get_inventory</code> methods</li> </ol>"},{"location":"norfab_changelog/#features_15","title":"Features","text":"<ol> <li>Added <code>runtime_inventory</code> task to Nornir service, #6</li> <li>Added support to configure <code>startup</code> and <code>exit</code> hook functions in inventory to be executed by nfapi on start and on exit.</li> </ol>"},{"location":"norfab_changelog/#031","title":"0.3.1","text":""},{"location":"norfab_changelog/#changes_15","title":"CHANGES","text":"<ol> <li>Improved logging handling for NFAPI if it failing to start a worker</li> <li>Update client <code>get</code> method to return result as a dictionary for broker MMI, file and inventory services</li> <li>Enhanced Netbox <code>update_device_facts</code> and <code>update_device_interface</code> to support <code>batch_size</code> argument - a number of devices to process at a time</li> <li>Improved nfcli shell for Netbox service to provide more arguments for <code>netbox update device facts</code> command</li> </ol>"},{"location":"norfab_changelog/#features_16","title":"FEATURES","text":"<ol> <li>Added Netbox Service <code>update_device_ip</code> task to retrieve device interface IP addresses and create them in Netbox</li> <li>Added support to NorFab simple inventory and nfapi to load inventory from dictionary data as well as to explicitly provide <code>base_dir</code> information where to anchor NorFab environment</li> <li>Added support for NorFab inventory workers section items to be dictionaries in addition to OS path to YAML files allowing to construct workers inventory out of dictionaries and/or YAML files.</li> </ol>"},{"location":"norfab_changelog/#030","title":"0.3.0","text":""},{"location":"norfab_changelog/#features_17","title":"FEATURES","text":"<ol> <li>Added \"show version\" support for nfcli client to display versions of locally installed libraries, fixes. #4</li> <li>Added \"show broker version\" support for nfcli client to  retrieve broker report of the version of libraries broker is running on, fixes. #4</li> <li>Added support \"show broker inventory\" command to display broker inventory</li> <li>Simple inventory added support to produce a serialized dictionary output</li> <li>Broker added \"show_broker_inventory\" and \"show_broker_version\" MMI endpoints</li> <li>Added support for simple inventory service to render inventory using Jinja2, renderer passed on <code>env</code> variable that contains operating system environment variables, allowing to source any env data into NorFab inventory for both broker and workers. #5</li> <li>Created <code>fastapi</code> service to host REST API for NorFab</li> </ol>"},{"location":"norfab_changelog/#024","title":"0.2.4","text":""},{"location":"norfab_changelog/#bugs_15","title":"BUGS","text":"<ol> <li>Fixed nfcli <code>--workers-list</code> handling</li> <li>Fixed <code>job_data</code> url handling for nornir cli/cfg/test tasks</li> <li>Fixed nfapi handling of empty worker name</li> </ol>"},{"location":"norfab_changelog/#features_18","title":"FEATURES","text":"<ol> <li>Added a set of confirmed commit shell commands to nornir cfg netmiko plugin</li> </ol>"},{"location":"norfab_changelog/#023","title":"0.2.3","text":""},{"location":"norfab_changelog/#features_19","title":"FEATURES","text":"<ol> <li>Added nfcli <code>--workers-list</code> option to specify a list of workers to start</li> </ol>"},{"location":"norfab_changelog/#changes_16","title":"CHANGES","text":"<ol> <li>Fixed handling of jinja2 import for the worker to make it optional </li> </ol>"},{"location":"norfab_changelog/#021","title":"0.2.1","text":""},{"location":"norfab_changelog/#changes_17","title":"CHANGES","text":"<ol> <li>Improved libs imports handling to account for distributed deployment</li> <li>Improved logging handling</li> <li>Fixed nfcli issue with starting components on NorFab #2</li> <li>Changed CTRL+C handling to trigger graceful NorFab exit</li> </ol>"},{"location":"norfab_changelog/#features_20","title":"FEATURES","text":"<ol> <li>Added <code>broker -&gt; shared_secret</code> parameter in <code>inventory.yaml</code> to configure clients and workers broker shared secret key</li> <li>Added and tested docker files</li> </ol>"},{"location":"norfab_changelog/#020","title":"0.2.0","text":""},{"location":"norfab_changelog/#changes_18","title":"CHANGES","text":"<ol> <li>refactored <code>get_circuits</code> to use <code>ThreadPoolExecutor</code> to fetch circuits path from netbox</li> <li>adding <code>job_data</code> json load to nornir cli, cfg and test tasks</li> </ol>"},{"location":"norfab_changelog/#bugs_16","title":"BUGS","text":"<ol> <li>Fixing netbox <code>get_devices</code> dry run test</li> <li>Fixed netbox <code>get_circuits</code> devices site retrieval handling</li> </ol>"},{"location":"norfab_changelog/#features_21","title":"FEATURES","text":"<ol> <li>Added cache to Netbox <code>get_circuits</code> and <code>get_devices</code> tasks</li> <li>Added new <code>agent</code> worker to start working on use cases to interface with LLMs</li> </ol>"},{"location":"norfab_changelog/#011","title":"0.1.1","text":""},{"location":"norfab_changelog/#bugs_17","title":"BUGS","text":"<ol> <li>Fixed Netbox CLI Shell handling of NFCLIENT</li> </ol>"},{"location":"norfab_changelog/#changes_19","title":"CHANGES","text":"<ol> <li>Updated and tested dependencies for Netmiko 4.5.0</li> <li>Updated and tested dependencies for Nornir 3.5.0</li> <li>Updated and tested dependencies for Nornir-Salt 0.22.1</li> </ol>"},{"location":"norfab_changelog/#010","title":"0.1.0","text":""},{"location":"norfab_changelog/#changes_20","title":"Changes","text":"<ol> <li>Changes to Nornir service module files structure</li> <li>PICLE dependency updated: 0.7. -&gt; 0.8.</li> <li>Made Nornir Service <code>progress</code> argument set to <code>True</code> by default to emit and display events for all Nornir Jobs</li> <li>Nornir tests changed <code>table</code> argument to be set to <code>True</code> by default</li> <li>Improved <code>nfapi</code> broker start logic to wait until broker fully initialized before proceeding to start workers</li> </ol>"},{"location":"norfab_changelog/#features_22","title":"Features","text":"<ol> <li>Added support for Nornir parse task to source TTP template from file with autocompletion</li> <li>Added Nornir File Copy task to copy files to devices using SCP</li> <li>Added support for logs to be collected into single file from all NorFab local processes</li> <li>Added to NorFab worker <code>job_list</code> and <code>job_details</code> methods</li> <li>Added <code>show jobs summary</code> and <code>show jobs details</code> commands to NorFab shell and to Nornir shell</li> <li>Added <code>--create-env</code> argument to nfcli utility to create NorFab folders and files to make it easier to get started using norfab</li> </ol>"},{"location":"norfab_changelog/#bugs_18","title":"BUGS","text":"<ol> <li>Fixed Nornir Service Watchdog to clean up dead connections from hosts data</li> </ol>"},{"location":"norfab_changelog/#000","title":"0.0.0","text":"<p>Initial Release</p>"},{"location":"norfab_changelog/#notable-features","title":"Notable Features","text":"<ol> <li>NorFAB Broker, Client and Worker base classes</li> <li>Nornir Service</li> <li>Network Service</li> <li>Simple Inventory Datastore Service</li> <li>File service</li> <li>ZeroMQ encryption</li> </ol>"},{"location":"norfab_distributed_deployment/","title":"Distributed Deployment","text":"<p>TBD</p>"},{"location":"norfab_docker_deployment/","title":"Docker Deployment","text":"<p>NorFab comes with a set of docker files to get NorFab up and running on the Docker.</p> <p></p> <p>Broker and workers deployed in docker environment, while clients can run on Windows, Linux or MAC machine connecting to the broker instance.</p> <p>Prerequisites:</p> <ol> <li>Docker setup is independent from this guide and assumption is that docker is installed and running </li> <li>Docker compose utility is available on the docker host</li> <li>GIT also need to be installed on the docker host</li> </ol> <p>Assumptions:</p> <ol> <li>Docker host IP address is 192.168.1.130 and it is accessible by clients on TCP port 5555, docker host IP address of course will be different in your setup, adjust clients <code>inventory.yaml</code> file accordingly.</li> </ol>","tags":["norfab"]},{"location":"norfab_docker_deployment/#broker-and-workers-containers-deployment","title":"Broker and Workers Containers Deployment","text":"<p>First, need to clone NorFab repository to the local folder on the docker host:</p> <pre><code>cd ~/\ngit clone https://github.com/norfablabs/NORFAB.git norfab\n</code></pre> <p>To build NorFab docker images we are going to use <code>docker compose</code> utility, it will create broker and nornir service workers containers:</p> <pre><code>cd norfab/docker/norfab-docker\ndocker compose build\n</code></pre> <p>Once build finishes we can start the containers:</p> <pre><code>docker compose start\n</code></pre> <p>After broker reports that it is started and worker shows that it is registered with broker, this is a good indication that things are going ok:</p> <pre><code>root@dockervm:/home/user/norfab/docker/norfab-docker# docker compose up\n[+] Running 2/0\n \u2714 Container norfab-broker          Created                   0.0s \n \u2714 Container norfab-service-nornir  Created                   0.0s \nAttaching to norfab-broker, norfab-service-nornir\nnorfab-broker          | 2025-02-02 10:40:20.453 INFO [norfab.core.nfapi:210 ] -- Started broker, broker listening for connections on 'tcp://10.0.0.100:5555'\nnorfab-service-nornir  | 2025-02-02 10:40:21.528 INFO [norfab.core.worker:557 ] -- nornir-worker-1 - registered to broker at 'tcp://10.0.0.100:5555', service 'nornir'\nnorfab-broker          | 2025-02-02 10:40:21.530 INFO [norfab.core.broker:317 ] -- NFPBroker - registered new worker nornir-worker-1\nnorfab-service-nornir  | 2025-02-02 10:40:21.554 INFO [nornir_salt.plugins.inventory.DictInventory:138 ] -- nornir-salt.DictInventory inventory data validated\nnorfab-service-nornir  | 2025-02-02 10:40:21.566 INFO [norfab.workers.nornir_worker:246 ] -- nornir-worker-1 - Started\n...\n</code></pre> <p>Folder <code>norfab/docker/norfab/norfab-docker</code> mounted to the containers as a volume under <code>/etc/norfab</code> path and this is the content of <code>inventory.yaml</code> file:</p> <pre><code># broker settings\nbroker:\n  endpoint: \"tcp://10.0.0.100:5555\"\n\n# workers inventory section\nworkers:\n  nornir-*:\n    - nornir/common.yaml  \n  nornir-worker-1:\n    - nornir/nornir-worker-1.yaml\n\n# list what entities we want to start on this node\ntopology:\n  broker: True\n  workers:\n    - nornir-worker-1\n</code></pre> <p>Docker compose starts a broker process on <code>norfab-broker</code> container that uses <code>10.0.0.100</code> IP address and starts a single Nornir Service worker process named <code>nornir-worker-1</code> on a <code>norfab-service-nornir</code> container. </p> <p>File <code>inventory.yaml</code> can be adjusted to configure additional Nornir service workers to make <code>norfab-service-nornir</code> container run as many Nornir Service worker processes as needed.</p> <p>Before we proceed with setting up the client, need to grab encryption public key value from the broker, run these commands:</p> <pre><code>docker exec -it norfab-broker bash\ncd /etc/norfab/\nnfcli --show-broker-shared-key\n</code></pre> <p>Running above commands will produce similar to this output:</p> <pre><code>root@user:/home/user/norfab/docker/norfab-docker# docker exec -it norfab-broker bash\nroot@norfab-broker:/# cd /etc/norfab\nroot@norfab-broker:/etc/norfab# nfcli --show-broker-shared-key\nNorFab broker public key content:\n\n'''\n#   ****  Generated on 2025-02-02 09:31:04.387927 by pyzmq  ****\n#   ZeroMQ CURVE Public Certificate\n#   Exchange securely, or use a secure mechanism to verify the contents\n#   of this file after exchange. Store public certificates in your home\n#   directory, in the .curve subdirectory.\n\nmetadata\ncurve\n    public-key = \"j{5-J9&lt;Qs:!@CVFsO$)UH&gt;mf%mP&lt;05[#%bBf(ofo\"\n\n'''\n\nKey file location: '__norfab__/files/broker/public_keys/broker.key'\n\nCopy above key into NorFab clients and workers 'public_keys/broker.key' file or \nput public-key value into clients and workers inventory.yaml 'broker' section \nunder 'shared_key' parameter:\n\nbroker:\n  shared_key: \"j{5-J9&lt;Qs:!@CVFsO$)UH&gt;mf%mP&lt;05[#%bBf(ofo\"\n\nroot@norfab-broker:/etc/norfab# \n</code></pre> <p>Public key value NorFab clients will need in this case is:</p> <pre><code>\"j{5-J9&lt;Qs:!@CVFsO$)UH&gt;mf%mP&lt;05[#%bBf(ofo\"\n</code></pre>","tags":["norfab"]},{"location":"norfab_docker_deployment/#client-setup","title":"Client Setup","text":"<p>After broker and worker containers are running need to setup a NorFab client, client can run on any machine that can connect to the docker host on its IP address on TCP port 5555.</p> <p>First, lets install NorFab on the client machine:</p> <pre><code>pip install norfab[nfcli]\n</code></pre> <p>Second, need to create a folder to host NorFab files:</p> <pre><code>nfcli --create-env norfab-env\n</code></pre> <p>Next need to configure broker endpoint pointing to docker host IP (<code>192.168.1.130</code> in this example) and broker encryption key in the <code>norfab-env/inventory.yaml</code> under <code>broker</code> section:</p> <pre><code># broker settings\nbroker:\n  endpoint: \"tcp://192.168.1.130:5555\"\n  shared_key: \"j{5-J9&lt;Qs:!@CVFsO$)UH&gt;mf%mP&lt;05[#%bBf(ofo\"\n</code></pre> <p>All the other <code>inventory.yaml</code> file content can be deleted, folder <code>norfab-env/nornir</code> also can be deleted. Client only need <code>inventory.yaml</code> file with broker endpoint and broker shared key details to successfully connect with the broker.</p> <p>Lastly, run <code>nfcli</code> client from within <code>norfab-env</code> folder:</p> <pre><code>cd norfab-env\nnfcli -c\n</code></pre> <p>NorFan interactive shell should start:</p> <pre><code>Welcome to NorFab Interactive Shell.\n\nnf#show broker\n status: active\n keepalives:\n   interval: 2500\n   multiplier: 6\n workers count: 1\n services count: 1\n directories:\n   base-dir: /etc/norfab\n   private-keys-dir: /etc/norfab/__norfab__/files/broker/private_keys\n   public-keys-dir: /etc/norfab/__norfab__/files/broker/public_keys\n security:\n   broker-private-key-file: /etc/norfab/__norfab__/files/broker/private_keys/broker.key_secret\n   broker-public-key-file: /etc/norfab/__norfab__/files/broker/public_keys/broker.key\nnf#show workers\n name             service  status  holdtime  keepalives tx/rx  alive (s) \n nornir-worker-1  nornir   alive   12.6      835 / 835         2104\nnf#\n</code></pre> <p>Successfully running <code>show broker</code> and <code>show workers</code> commands is a good indication that everything works well and you did a great job setting up NorFab in a distributed dockerized fashion .</p> <p>Next steps would be to adjust <code>inventory.yaml</code> file on the docker host to configure Nornir Service workers to manage your environment, for further details on how to do it refer to Nornir Service documentations. Good Luck \ud83e\udd1e</p>","tags":["norfab"]},{"location":"norfab_getting_started/","title":"Quickstart","text":"<p>The simplest way to start with NorFab is to do local deployment when broker, workers and client processes all run locally, this is what we going to demonstrate in this guide.</p> <p></p> <p>Client, broker and worker all should be able to run on same machine Windows, Linux or MAC.</p> <p>First, install NorFab using pip:</p> <pre><code>pip install norfab\npip install norfab[nfcli]\npip install norfab[nornirservice]\n</code></pre> <p>Above installs NorFab core components plus extras needed to run nfcli anf Nornir service workers. </p> <p>Once NorFab installed, next step is to create a folder that will hold your environment. Run this command to create NorFab folders and files:</p> <pre><code>nfcli --create-env norfab\n</code></pre> <p>This will create <code>norfab</code> folder and inside of it will create <code>inventory.yaml</code>, file name is important as NORFAB by default searches for <code>inventory.yaml</code>, file content is:</p> inventory.yaml<pre><code>broker: # (1)!\n  endpoint: \"tcp://127.0.0.1:5555\" # (2)!\n\nworkers: # (3)!\n  nornir-*: # (4)!\n    - nornir/common.yaml   \n  nornir-worker-1: # (5)!\n    - nornir/nornir-worker-1.yaml\n\ntopology: # (6)!\n  broker: True # (7)!\n  workers: # (8)!\n    - nornir-worker-1\n</code></pre> <ol> <li>Broker configuration inventory section</li> <li>URL to listen for connections on - <code>localhost</code> port <code>5555</code> in this case</li> <li>Workers configuration inventory section</li> <li>glob pattern that will match all workers with <code>nornir-</code> in the name and map <code>common.yaml</code> file content for each of them</li> <li>Worker definition to map inventory file to a specific worker that has name <code>nornir-worker-1</code></li> <li>Topology section to define what components to run</li> <li>Start broker process</li> <li>List of workers names to start processes for</li> </ol> <p>Command <code>--create-env</code> assumes we are working with Nornir service and creates <code>nornir</code> folder and inside of it creates two files. First file <code>common.yaml</code> to host configuration common for all Nornir service workers:</p> common.yaml<pre><code>service: nornir # (1)!\nbroker_endpoint: \"tcp://127.0.0.1:5555\" # (2)!\n\n# Nornir inventory and configuration\nrunner: # (3)!\n  plugin: RetryRunner\nhosts: {}\ndefault: {} # (4)!\ngroups: {} # (5)!\n</code></pre> <ol> <li>Name of the service this worker hosting</li> <li>Broker URL to initiate connections with</li> <li>Nornir runner plugin configuration</li> <li>Nornir <code>default</code> data section</li> <li>Nornir groups definition section</li> </ol> <p>Second file specific to the worker with name <code>nornir-worker-1</code> which holds Nornir inventory data:</p> nornir-worker-1.yaml<pre><code>hosts:\n  ios-device-1:\n    hostname: 192.168.1.1\n    platform: cisco_ios\n    username: admin\n    password: admin\n</code></pre> <p>This is how files structure will look like:</p> <pre><code>\u2514\u2500\u2500\u2500norfab\n    \u2502   inventory.yaml\n    \u2502\n    \u2514\u2500\u2500\u2500nornir\n            common.yaml\n            nornir-worker-1.yaml\n</code></pre> <p>Now you are ready to start NorFab Interactive Command Line Shell Client. Open terminal window, navigate to the folder where <code>inventory.yaml</code> located and start NFCLI:</p> <pre><code>C:\\&gt;cd norfab\nC:\\norfab&gt;nfcli\nnf#\n</code></pre> <p>this will start the NorFab broker process, Nornir worker process, instantiate NFCLI client and drop you into interactive command line shell </p> <pre><code>nf#? # (1)!\n file      File sharing service\n netbox    Netbox service\n nornir    Nornir service\n show      NorFab show commands\n exit      Exit current shell\n help      Print help message\n pwd       Print current shell path\n top       Exit to top shell\nnf#show workers # (2)!\n name             service  status  holdtime  keepalives tx/rx  alive (s)\n nornir-worker-1  nornir   alive   12.8      58 / 58           149\nnf#\nnf#nornir # (3)!\nnf[nornir]#?\n cfg     Configure devices over CLI interface\n cli     Send CLI commands to devices\n show    Show Nornir service parameters\n task    Run Nornir task\n test    Run network tests\n end     Exit application\n exit    Exit current shell\n help    Print help message\n pwd     Print current shell path\n top     Exit to top shell\nnf[nornir]#top\nnf#show nornir hosts table details\n+-----------------+--------------+------------+-------------+--------+----------+------------+\n| worker          | host         | platform   | hostname    | port   | groups   | username   |\n+=================+==============+============+=============+========+==========+============+\n| nornir-worker-1 | ios-device-1 | cisco_ios  | 192.168.1.1 | None   | []       | admin      |\n+-----------------+--------------+------------+-------------+--------+----------+------------+\nnf# end\nExiting...\n</code></pre> <ol> <li>Question mark plus enter to print commands help</li> <li>Run show command</li> <li>Drop into Nornir Service command shell</li> </ol> <p>NorFab CLI supports Tab completions, question mark help together with sub-shells, read more about NorFab CLI and how to use it here.</p> <p> That's it </p> <p>Where to go next? Checkout tutorials section or read about NORFAB concepts.</p>","tags":["norfab"]},{"location":"norfab_help_with_norfab/","title":"Contact Us","text":""},{"location":"norfab_help_with_norfab/#github","title":"GitHub","text":"<p>For issues and suggestions reach out on GitHub.</p>"},{"location":"norfab_help_with_norfab/#slack","title":"Slack","text":"<p>To have a chat welcome to #norfab channel.</p>"},{"location":"norfab_help_with_norfab/#email","title":"Email","text":"<p>For more information or to schedule a demo, send us an email.</p>"},{"location":"norfab_installation/","title":"Installation","text":""},{"location":"norfab_installation/#install-norfab","title":"Install NorFab","text":"<p>Install NorFab from PyPI</p> <pre><code>pip install norfab\n</code></pre> <p>NorFab core runs equally well on both Windows and Linux. Some  services might work only on one or the other, in that case that will be noted in service deployment details.</p>"},{"location":"norfab_installation/#extras","title":"Extras","text":"<p>Several extra installations supported tailoring certain services dependencies that you want to run on a given node.</p> <p>To install all dependencies for all services can use <code>full</code> extras:</p> <pre><code>pip install norfab[full]\n</code></pre>"},{"location":"norfab_installation/#norfab-cli-dependencies","title":"NORFAB CLI Dependencies","text":"<p>To install NorFab Interactive CLI dependencies</p> <pre><code>pip install norfab[nfcli]\n</code></pre>"},{"location":"norfab_installation/#robot-client-dependencies","title":"Robot Client Dependencies","text":"<p>To install Robot library dependencies</p> <pre><code>pip install norfab[robot]\n</code></pre>"},{"location":"norfab_installation/#nornir-service-dependencies","title":"Nornir Service Dependencies","text":"<p>To install Nornir service dependencies</p> <pre><code>pip install norfab[nornirservice]\n</code></pre>"},{"location":"norfab_installation/#netbox-service-dependencies","title":"Netbox Service Dependencies","text":"<p>To install Netbox service dependencies</p> <pre><code>pip install norfab[netboxservice]\n</code></pre>"},{"location":"norfab_installation/#fastapi-service-dependencies","title":"FastAPI Service Dependencies","text":"<p>To install FastAPI service dependencies</p> <pre><code>pip install norfab[fastapiservice]\n</code></pre>"},{"location":"norfab_installation/#ollama-agent-service-dependencies","title":"Ollama Agent Service Dependencies","text":"<p>To install Ollama Agent service dependencies</p> <pre><code>pip install norfab[agentservice]\n</code></pre>"},{"location":"norfab_installation/#operating-systems-support","title":"Operating Systems Support","text":"Component Windows Linux MacOS NorFab Core Nornir Service Netbox Service"},{"location":"norfab_why_use_norfab/","title":"Why Choose NorFab for Network Automation","text":""},{"location":"norfab_why_use_norfab/#accelerate-unify-and-future-proof-your-network-operations","title":"Accelerate, Unify, and Future-Proof Your Network Operations","text":"<p>Modern networks demand automation that is robust, scalable, and adaptable. NorFab (Network Automations Fabric) is engineered to meet these requirements, providing a unified automation platform that bridges the gap between disparate tools, teams, and technologies. NorFab empowers technical teams to deliver reliable, repeatable, and auditable network operations at scale.</p>"},{"location":"norfab_why_use_norfab/#technical-advantages","title":"Technical Advantages","text":""},{"location":"norfab_why_use_norfab/#1-universal-deployment-scalability","title":"1. Universal Deployment &amp; Scalability","text":"<ul> <li>Runs Anywhere: NorFab is OS-agnostic\u2014deploy on Windows, Linux, macOS, containers, or VMs. Architect for centralized, distributed, or hybrid topologies.</li> <li>Horizontal Scalability: Add workers and services as your network grows. NorFab\u2019s brokered architecture ensures high availability and fault tolerance.</li> </ul>"},{"location":"norfab_why_use_norfab/#2-extensible-modular-architecture","title":"2. Extensible, Modular Architecture","text":"<ul> <li>Service-Oriented: Each service is a logical unit, managed by workers that can be deployed independently or in clusters.</li> <li>Plugin Ecosystem: Integrate new protocols, tools, or workflows without modifying the core. Extend NorFab to fit your unique environment.</li> </ul>"},{"location":"norfab_why_use_norfab/#3-seamless-integration","title":"3. Seamless Integration","text":"<ul> <li>Multi-Interface: Expose automation via Python API, REST API, and CLI. Integrate NorFab into CI/CD pipelines, ITSM, or custom portals.</li> <li>Vendor-Neutral: Manage multi-vendor environments with a single automation fabric.</li> </ul>"},{"location":"norfab_why_use_norfab/#4-model-driven-data-centric-automation","title":"4. Model-Driven, Data-Centric Automation","text":"<ul> <li>Pydantic Models: Enforce data validation and schema consistency across APIs and services.</li> <li>Inventory-Driven: Centralized inventory and configuration management ensures accuracy and repeatability.</li> </ul>"},{"location":"norfab_why_use_norfab/#5-reliability-and-observability","title":"5. Reliability and Observability","text":"<ul> <li>Persistent Messaging: Built-in job persistence and reliable delivery, even across network partitions or process restarts.</li> <li>Health Monitoring: Heartbeating, presence detection, and management interfaces for real-time visibility.</li> </ul>"},{"location":"norfab_why_use_norfab/#6-security-and-control","title":"6. Security and Control","text":"<ul> <li>Encryption: All communication between broker, clients and workers is encrypted using CurveZMQ as a built-in security mechanism.</li> </ul>"},{"location":"norfab_why_use_norfab/#norfab-in-context-feature-comparison","title":"NorFab in Context: Feature Comparison","text":"Feature NorFab Ansible Cisco NSO PyATS Nornir Language Python YAML, Python YANG, XML, Python Python Python Extensibility High High Medium Medium High Model-Driven Yes (Pydantic) No Yes (YANG) No No API Support REST, Python REST, Python REST, NETCONF, Python Python Python Multi-vendor Yes Yes Yes Yes Yes Config Management Yes Yes Yes Yes Yes Network Testing Yes Limited Limited Yes Yes Orchestration Yes (distributed, service-based) Limited Yes No Limited Scalability High (distributed, brokered) Medium High Medium Medium Ease of Use Pythonic, CLI, REST, model-driven YAML DSL YANG/XML/Python Python Python Deployment Centralized, Distributed, Hybrid Centralized Centralized Centralized Centralized Templating Jinja2 Jinja2 XML Proprietary Jinja2 Jinja2 Datastorage Database, Text files Text files Database, Files Text files Text files Device Inventory Internal, External Internal, External Internal Internal Internal, External Support Community &amp; Commercial Community &amp; Comm. Community &amp; Comm. Community Community"},{"location":"norfab_why_use_norfab/#business-and-technical-benefits","title":"Business and Technical Benefits","text":"<ul> <li>Operational Efficiency: Automate repetitive and complex workflows, reducing manual effort and risk.</li> <li>Consistency &amp; Compliance: Enforce standards and policies across your network with model-driven validation.</li> <li>Resilience: Built-in reliability and failover mechanisms ensure business continuity.</li> <li>Investment Protection: Integrate with existing tools and processes\u2014no need to rip and replace.</li> <li>Rapid Innovation: Empower teams to deliver new services and capabilities faster.</li> </ul> <p>NorFab is not just another automation tool\u2014it is a platform designed for the realities of modern network engineering. Whether you are consolidating legacy scripts, orchestrating multi-vendor environments, or building the next generation of network services, NorFab provides the foundation you need.</p> <p>For technical details, integration guidance, or to schedule a demo, contact NorFab Support.</p>"},{"location":"reference_architecture_nfp/","title":"NORFAB Protocol","text":"<p>Status: experimental Editor: d.mulyalin@gmail.com Contributors: </p> <p>The NORFAB Protocol (NFP) defines a reliable service-oriented request-reply dialog between a set of client applications, a broker and a set of worker applications representing service managing a set of resources. </p> <p>NFP covers presence, heartbeating, and service-resource-oriented request-reply processing. NFP originated from the MDP pattern defined in Chapter 4 of the ZeroMQ Guide and combined with TSP pattern (developed in same chapter) approach for persistent messaging across a network of arbitrarily connected clients and workers as a design for disk-based reliable messaging. NORFAB allows clients and workers to work without being connected to the network at the same time, and defines handshaking for safe storage of requests, and retrieval of replies.</p>"},{"location":"reference_architecture_nfp/#license","title":"License","text":"<p>Copyright (c) 2024 Denis Mulyalin.</p> <p>This Specification is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.</p> <p>This Specification is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with this program; if not, see http://www.gnu.org/licenses.</p>"},{"location":"reference_architecture_nfp/#change-process","title":"Change Process","text":"<p>This Specification is a free and open standard (see \u201cDefinition of a Free and Open Standard\") and is governed by the Digital Standards Organization\u2019s Consensus-Oriented Specification System (COSS) (see \u201cConsensus Oriented Specification System\").</p>"},{"location":"reference_architecture_nfp/#language","title":"Language","text":"<p>The key words \u201cMUST\u201d, \u201cMUST NOT\u201d, \u201cREQUIRED\u201d, \u201cSHALL\u201d, \u201cSHALL NOT\u201d, \u201cSHOULD\u201d, \u201cSHOULD NOT\u201d, \u201cRECOMMENDED\u201d, \u201cMAY\u201d, and \u201cOPTIONAL\u201d in this document are to be interpreted as described in RFC 2119 (see \u201cKey words for use in RFCs to Indicate Requirement Levels\").</p>"},{"location":"reference_architecture_nfp/#goals","title":"Goals","text":"<p>The NORFAB Protocol (NFP) defines a reliable service-resource-oriented request-reply dialog between a set of client applications, a broker and a set of worker applications. NFP covers presence, heartbeating, and service-oriented request-reply processing. </p> <p>NFP uses name-based service resolution, named based resource targeting and structured protocol commands.</p> <p>The goals of NFP are to:</p> <ul> <li>Allow requests to be routed to workers on the basis of abstract service names.</li> <li>Allow broker and workers to detect disconnection of one another, through the use of heartbeating.</li> <li>ALlow task distribution by clients targeting <code>all</code> (broadcast), <code>any</code> (anycast) or <code>unicast</code> certain workers by names within given service.</li> <li>Allow the broker to recover from dead or disconnected workers by re-sending requests to other workers.</li> <li>Allow workers to manage <code>resource</code> entities, where entities can be dynamically distributed across all workers within the service.</li> <li>Allow workers to have access to inventory data hosted by broker</li> </ul>"},{"location":"reference_architecture_nfp/#architecture","title":"Architecture","text":""},{"location":"reference_architecture_nfp/#overall-topology","title":"Overall Topology","text":"<p>NFP connects a set of client applications, a single broker device and a pool of workers applications. Clients connect to the broker, as do workers. Clients and workers do not see each other, and both can come and go arbitrarily. The broker MAY open two sockets (ports), one front-end for clients, and one back-end for workers. However NFP is also designed to work over a single broker socket.</p> <p>We define \u2018client\u2019 applications as those issuing requests, and \u2018worker\u2019 applications as those processing them. NFP makes these assumptions:</p> <ul> <li>Workers are idempotent, i.e. it is safe to execute the same request more than once.</li> <li>Workers will handle at most one request a time, and will issue exactly one reply for each successful request.</li> <li>The NORFAB broker mediates requests one a per service basis. The broker SHOULD serve clients on a fair basis and SHOULD deliver requests to workers on the basis of targeting specified by client - <code>any</code> worker, <code>all</code> workers or <code>unicast</code> worker identified by name.</li> </ul> <p>NFP consists of four sub-protocols:</p> <ul> <li>NFP/Client, which covers how the NFP broker communicates with client applications.</li> <li>NFP/Worker, which covers how the NFP broker communicates with workers applications.</li> <li>NFP/Worker-PUB, which covers how broker subscribes to events published by workers.</li> <li>NFP/Broker-PUB, which covers how broker publishes collected worker events to clients.</li> </ul> <p>The broker SHOULD be an intermediary (a device) application that mediates Client-Workers communication. The broker SHOULD integrate Management Interface (MMI) service directly into it together with simple disk based Inventory service for workers.</p>"},{"location":"reference_architecture_nfp/#router-addressing","title":"ROUTER Addressing","text":"<p>The broker MUST use a ROUTER socket to accept requests from clients, and connections from workers. The broker MAY use a separate socket for each sub-protocol, or MAY use a single socket for both sub-protocols.</p> <p>From the \u00d8MQ Reference Manual:</p> <p>When receiving messages a ROUTER socket shall prepend a message part containing the identity of the originating peer to the message before passing it to the application. When sending messages a ROUTER socket shall remove the first part of the message and use it to determine the identity of the peer the message shall be routed to.</p> <p>This extra frame is not shown in the sub-protocol commands explained below.</p>"},{"location":"reference_architecture_nfp/#nfp-messages","title":"NFP messages","text":""},{"location":"reference_architecture_nfp/#open","title":"OPEN","text":"<p>A OPEN command consists of 4 frames, formatted on the wire as follows:</p> <pre><code>OPEN command\n---------------------------------------------------------------\nFrame 0: Empty frame\nFrame 1: \u201cNFPC01\u201d or \u201cNFPW01\u201d or \u201cNFPB01\u201d (six bytes, representing NFP/Client or NFP/Worker or NFP/Broker v0.1)\nFrame 2: 0x00 (one byte, representing OPEN)\nFrame 3: Open body (opaque binary)\n</code></pre> <p>Worker and client use OPEN message to introduce itself to broker to negotiate connection parameters. Broker sends OPEN message back to client or worker to confirm the connection.</p>"},{"location":"reference_architecture_nfp/#ready","title":"READY","text":"<p>A READY command consists of a multipart message of 4 frames, formatted on the wire as follows:</p> <pre><code>READY command\n---------------------------------------------------------------\nFrame 0: Empty frame\nFrame 1: \u201cNFPW01\u201d (six bytes, representing NFP/Worker v0.1)\nFrame 2: 0x01 (one byte, representing READY)\nFrame 3: Service name (printable string)\n</code></pre> <p>Worker sends READY command to broker, broker accepts ready request and registers worker with a service.</p>"},{"location":"reference_architecture_nfp/#keepalive","title":"KEEPALIVE","text":"<p>A KEEPALIVE command consists of 4 frames, formatted on the wire as follows:</p> <pre><code>KEEPALIVE command\n---------------------------------------------------------------\nFrame 0: Empty frame\nFrame 1: \u201cNFPB01\u201d or \u201cNFPW01\u201d (six bytes, representing NFP/Broker or NFP/Worker v0.1)\nFrame 2: 0x02 (one byte, representing KEEPALIVE)\nFrame 3: Service name (printable string)\n</code></pre> <p>Broker sends KEEPALIVE messages to workers to indicate broker is still alive.</p> <p>Workers send KEEPALIVE messages to broker to indicate worker is still alive.</p>"},{"location":"reference_architecture_nfp/#disconnect","title":"DISCONNECT","text":"<p>A DISCONNECT command consists of 3 frames, formatted on the wire as follows:</p> <pre><code>DISCONNECT command\n---------------------------------------------------------------\nFrame 0: Empty frame\nFrame 1: \u201cNFPB01\u201d or \u201cNFPW01\u201d (six bytes, representing NFP/Broker or NFP/Worker v0.1)\nFrame 2: 0x03 (one byte, representing DISCONNECT)\nFrame 3: Service name (printable string)\nFrame 4: Disconnect body (opaque binary)\n</code></pre> <p>Broker sends DISCONNECT command to workers to signal the request to disconnect. </p> <p>Workers also can send DISCONNECT command to broker to signal the request to disconnect. </p>"},{"location":"reference_architecture_nfp/#post","title":"POST","text":"<p>A POST command consists of 7 or more frames, formatted on the wire as follows:</p> <pre><code>POST command\n---------------------------------------------------------------\nFrame 0: Empty (zero bytes, invisible to REQ application)\nFrame 1: \u201cNFPC01\u201d or \"NFPB01\" (six bytes, representing NFP/Client or NFP/Broker v0.1)\nFrame 2: 0x04 (one byte, representing POST)\nFrame 3: Service name (printable string)\nFrame 4: Target (printable string) workers, `all` (default), `any` or comma separated `worker names`\nFrame 5: Job UUID (printable string)\nFrames 6: POST body (opaque binary)\n</code></pre> <p>Client sends POST message to broker to distribute job requests among workers. </p> <p>Broker relays POST message to individual workers to publish job request.</p>"},{"location":"reference_architecture_nfp/#response","title":"RESPONSE","text":"<p>A RESPONSE command consists of 7 or more frames, formatted on the wire as follows:</p> <pre><code>RESPONSE command\n---------------------------------------------------------------\nFrame 0: Empty (zero bytes, invisible to REQ application)\nFrame 1: \u201cNFPB01\u201d or \u201cNFPW01\u201d (six bytes, representing NFP/Broker or NFP/Worker v0.1)\nFrame 2: 0x05 (one byte, representing RESPONSE)\nFrame 3: Service name (printable string)\nFrame 4: Job UUID (printable string)\nFrame 5: Status code (explained below)\nFrames 6: Response body (opaque binary)\n</code></pre> <p>Worker sends RESPONSE message to broker with requests status or job results. </p> <p>Broker relays RESPONSE message to client.</p>"},{"location":"reference_architecture_nfp/#get","title":"GET","text":"<p>A GET command consists of 7 or more frames, formatted on the wire as follows:</p> <pre><code>GET command\n---------------------------------------------------------------\nFrame 0: Empty (zero bytes, invisible to REQ application)\nFrame 1: \u201cNFPC01\u201d or \"NFPB01\" (six bytes, representing NFP/Client or NFP/Broker v0.1)\nFrame 2: 0x06 (one byte, representing GET)\nFrame 3: Service name (printable string)\nFrame 4: Target (printable string) workers, `all` (default), `any` or comma separated `worker names`\nFrame 5: Job UUID (printable string)\nFrames 6: GET request body (opaque binary)\n</code></pre> <p>Client sends GET message to broker to retrieve job results. </p> <p>Broker relays GET message to individual workers to request job request.</p>"},{"location":"reference_architecture_nfp/#delete","title":"DELETE","text":"<p>A DELETE command consists of 7 or more frames, formatted on the wire as follows:</p> <pre><code>DELETE command\n---------------------------------------------------------------\nFrame 0: Empty (zero bytes, invisible to REQ application)\nFrame 1: \u201cNFPC01\u201d or \"NFPB01\" (six bytes, representing NFP/Client or NFP/Broker v0.1)\nFrame 2: 0x07 (one byte, representing POST)\nFrame 3: Service name (printable string)\nFrame 4: Target (printable string) workers, `all` (default), `any` or comma separated `worker names`\nFrame 5: Job UUID (printable string)\nFrames 6: DELETE body (opaque binary)\n</code></pre> <p>Client sends DELETE message to broker to distribute job delete requests to workers. </p> <p>Broker relays DELETE message to individual workers to cancel the job.</p>"},{"location":"reference_architecture_nfp/#event","title":"EVENT","text":"<p>A EVENT command consists of 7 or more frames, formatted on the wire as follows:</p> <pre><code>EVENT command\n---------------------------------------------------------------\nFrame 0: Empty (zero bytes, invisible to REQ application)\nFrame 1: \u201cNFPW01\u201d (six bytes, representing NFP/Worker v0.1)\nFrame 2: 0x08 (one byte, representing EVENT)\nFrame 3: Service name (printable string)\nFrame 4: Topic (printable string e.g. Job UUID)\nFrame 5: Status code 200 (explained below)\nFrames 6: Event body (opaque binary)\n</code></pre> <p>Worker sends EVENT message to Broker to supply information about job execution. </p> <p>Broker relays EVENT message to certain Client.</p>"},{"location":"reference_architecture_nfp/#status-frames","title":"Status Frames","text":"<p>Every RESPONSE message contains a status frame followed by zero or more content frames. The status frame contains a string formatted as three digits, optionally followed by a space and descriptive text. A client MUST NOT treat the text as significant in any way. Implementations MAY NOT use status codes that are not defined here:</p> <p>200 - OK. The NORFAB worker executed the request successfully.  202 - ACCEPTED. The NORFAB Broker accepted POST request to dispatch the job. 300 - PENDING. The client SHOULD retry the request at a later time. 400 - UNKNOWN. The client is using an invalid or unknown UUID and SHOULD NOT retry. 408 - REQUEST TIMEOUT. Client did not receive response from broker or worker. 417 - EXPECT FAILED. Client did not receive what it was expecting to receive. 500 - ERROR. The server cannot complete the request due to some internal error. The client SHOULD retry at some later time.</p>"},{"location":"reference_architecture_nfp/#nfpclient","title":"NFP/Client","text":"<p>NFP/Client is a strictly synchronous dialog initiated by the client (where \u2018C\u2019 represents the client, and \u2018B\u2019 represents the broker):</p> <pre><code>C: OPEN\nB: OPEN\n\nRepeat:\n\n    C: POST\n    B: RESPONSE\n    ...\n\n    C: GET\n    B: RESPONSE\n    ...\n</code></pre> <p>Clients SHOULD use a REQ socket when implementing a synchronous request-reply pattern. The REQ socket will silently create frame 0 for outgoing requests, and remove it for replies before passing them to the calling application. </p> <p>Clients MAY use any suitable strategy for recovering from a non-responsive broker. One recommended strategy is:</p> <ul> <li>To use polling instead of blocking receives on the request socket.</li> <li>If there is no reply within some timeout, to close the request socket and open a new socket, and resend the request on that new socket.</li> <li>If there is no reply after several retries, to signal the transaction as failed.</li> <li>The service name is a 0MQ string that matches the service name specified by a worker in its READY command (see NFP/Worker below). The broker SHOULD queue client requests for which service no workers has been registered and SHOULD expire these requests after a reasonable and configurable time if no service's workers has been registered.</li> </ul>"},{"location":"reference_architecture_nfp/#nfpbroker","title":"NFP/Broker","text":"<p>NFP/Broker is a mediator that receives messages from clients and dispatches them out to workers. In return messages from workers routed to clients.</p>"},{"location":"reference_architecture_nfp/#nfpworker","title":"NFP/Worker","text":"<p>NFP/Worker is a mix of a synchronous request-reply dialog, initiated by the service worker, and an asynchronous heartbeat dialog that operates independently in both directions. This is the synchronous dialog (where \u2018W\u2019 represents the service worker, and \u2018B\u2019 represents the broker):</p> <pre><code>W: OPEN\nB: OPEN\nW: READY\n\nRepeat:\n\n    B: POST\n    W: RESPONSE\n    ...\n\n    B: GET\n    W: RESPONSE\n    ... \n</code></pre> <p>The asynchronous heartbeat dialog operates on the same sockets and works thus:</p> <pre><code>Repeat:                 Repeat:\n\n    W: HEARTBEAT            B: HEARTBEAT\n    ...                     ...\n\nW: DISCONNECT           B: DISCONNECT\n</code></pre> <p>NFP/Worker commands all start with an empty frame to allow consistent processing of client and worker frames in a broker, over a single socket. The empty frame has no other significance.</p>"},{"location":"reference_architecture_nfp/#nfpworker-pub","title":"NFP/Worker-PUB","text":"<p>TBD </p>"},{"location":"reference_architecture_nfp/#nfpbroker-pub","title":"NFP/Broker-PUB","text":"<p>TBD</p>"},{"location":"reference_architecture_nfp/#job-persistence","title":"Job Persistence","text":"<p>Workers SHOULD persistently store job requests and job execution results for a configurable amount of time allowing clients (client submitted job request or any other client) to request job execution results on demand.</p> <p>Clients SHOULD persistently store job requests and MAY store job execution results locally for a configurable amount of time.</p>"},{"location":"reference_architecture_nfp/#opening-and-closing-a-connection","title":"Opening and Closing a Connection","text":"<p>The worker is responsible for opening and closing a logical connection. One worker MUST connect to exactly one broker using a single \u00d8MQ DEALER (XREQ) socket.</p> <p>Since \u00d8MQ automatically reconnects peers after a failure, every NFP command includes the protocol header to allow proper validation of all messages that a peer receives.</p> <p>The worker opens the connection to the broker by creating a new socket, connecting it, and then sending a READY command to register to a service. One worker handles precisely one service, and many workers MAY handle the same service. The worker MUST NOT send a further READY.</p> <p>There is no response to a READY. The worker SHOULD assume the registration succeeded until or unless it receives a DISCONNECT, or it detects a broker failure through heartbeating.</p> <p>The worker MAY send DISCONNECT at any time, including before READY. When the broker receives DISCONNECT from a worker it MUST send no further commands to that worker.</p> <p>The broker MAY send DISCONNECT at any time, by definition after it has received at least one command from the worker.</p> <p>The broker MUST respond to any valid but unexpected command by sending DISCONNECT and then no further commands to that worker. The broker SHOULD respond to invalid messages by dropping them and treating that peer as invalid.</p> <p>When the worker receives DISCONNECT it must send no further commands to the broker; it MUST close its socket, and reconnect to the broker on a new socket. This mechanism allows workers to re-register after a broker failure and recovery.</p>"},{"location":"reference_architecture_nfp/#post-and-response-processing","title":"POST and RESPONSE Processing","text":"<p>The POST and the RESPONSE commands MUST contain precisely one client address frame. This frame MUST be followed by an empty (zero sized) frame.</p> <p>The address of each directly connected client is prepended by the ROUTER socket to all request messages coming from clients. That ROUTER socket also expects a client address to be prepended to each reply message sent to a client.</p>"},{"location":"reference_architecture_nfp/#keepaliving","title":"Keepaliving","text":"<p>KEEPALIVE commands are valid at any time, after a READY command.</p> <p>Any received command except DISCONNECT acts as a keepalive. Peers SHOULD NOT send KEEPALIVE commands while also sending other commands.</p> <p>Both broker and worker MUST send heartbeats at regular and agreed-upon intervals. A peer MUST consider the other peer \u201cdisconnected\u201d if no keepalive arrives within some multiple of that interval (usually 3-5).</p> <p>If the worker detects that the broker has disconnected, it SHOULD restart a new conversation.</p> <p>If the broker detects that the worked has disconnected, it SHOULD stop sending messages of any type to that worker.</p>"},{"location":"reference_architecture_nfp/#broker-management-interface-bmmi","title":"Broker Management Interface (BMMI)","text":"<p>Broker SHOULD implement Management interface as a service endpoint for clients to interact with.</p> <p>Broker should use <code>mmi.service.broker</code> service endpoint to listen to client's requests. </p> <p>These MMI functions SHOULD be implemented:</p> <ul> <li><code>show_broker</code> - to return broker status and statistics</li> <li><code>show_workers</code> - to return worker status and statistics </li> <li><code>show_clients</code> - to return clients statistics</li> <li><code>show_services</code> - to return services status and statistics </li> <li><code>restart</code> - restart broker</li> <li><code>shutdown</code> - shutdown broker completely</li> <li><code>disconnect</code> - to disconnect all workers</li> </ul>"},{"location":"reference_architecture_nfp/#worker-management-interface-wmmi","title":"Worker Management Interface (WMMI)","text":"<p>Worker SHOULD implement Management interface as a service endpoint for clients to interact with.</p> <p>Worker should use <code>mmi.service.worker</code> service endpoint to listen to client's requests. </p> <p>These MMI functions SHOULD be implemented:</p> <ul> <li><code>show_broker</code> - to return broker status and statistics</li> <li><code>show_workers</code> - to return worker status and statistics </li> <li><code>show_clients</code> - to return clients statistics</li> <li><code>restart</code> - restart worker</li> <li><code>shutdown</code> - shutdown worker completely</li> <li><code>disconnect</code> - to disconnect worker from broker and re-establish connection</li> </ul>"},{"location":"reference_architecture_nfp/#broker-simple-inventory-datastore-sid","title":"Broker Simple Inventory Datastore (SID)","text":"<p>Broker should implement Inventory Datastore to store and serve configuration to workers as well as arbitrary workers inventory data.</p> <p>Broker should use <code>sid.service.broker</code> service endpoint to listen to worker's requests. </p> <p>Workers willing to make use of broker's inventory datastore should implement <code>NFP/Client</code> protocol defined above to request inventory data.</p> <p>These SID functions SHOULD be implemented:</p> <ul> <li><code>get_inventory</code> - to return inventory content for given worker</li> </ul>"},{"location":"reference_architecture_nfp/#sid-implementation","title":"SID Implementation","text":"<p>TBD</p>"},{"location":"reference_architecture_nfp/#broker-file-sharing-service-fss","title":"Broker File Sharing Service (FSS)","text":"<p>Broker automatically runs <code>filesharing</code> service worker(s) to serve files to clients and workers from local file system using <code>nf://&lt;filepath&gt;</code> URL for supported arguments.</p>"},{"location":"reference_architecture_nfp/#fss-implementation","title":"FSS Implementation","text":"<p>TBD</p>"},{"location":"reference_architecture_nfp/#reliability","title":"Reliability","text":"<p>The NORFAB pattern is designed to extend the basic \u00d8MQ request-reply pattern with the ability to detect and recover from a specific set of failures:</p> <ul> <li>Worker applications which crash, run too slowly, or freeze.</li> <li>Worker applications that are disconnected from the network (temporarily or permanently).</li> <li>Client applications that are temporarily disconnected from the network.</li> <li>A queue broker that crashes and is restarted.</li> <li>A queue broker that suffers a permanent failure.</li> <li>Requests or replies that are lost due to any of these failures.</li> <li>The general approach is to retry and reconnect, using heartbeating when needed. </li> </ul>"},{"location":"reference_architecture_nfp/#scalability-and-performance","title":"Scalability and Performance","text":"<p>NORFAB is designed to be scalable to large numbers (thousands) of workers and clients allowing to manage 10s of thousands resource entities, limited only by system resources on the broker. Partitioning of workers by service allows for multiple applications to share the same broker infrastructure. Workers manage a set of resources defined by system administrator. Same resource can be managed by single or multiple workers, system impose no restrictions on how resource entities distributed across workers.</p> <p>Throughput performance for a single client application will be limited to tens of thousands, not millions, of request-reply transactions per second due to round-trip costs and the extra latency of a broker-based approach. The larger the request and reply messages, the more efficient NORFAB will become. </p> <p>System requirements for the broker are moderate: no more than one outstanding request per client will be queued, and message contents can be switched between clients and workers without copying or processing. A single broker thread can therefore switch several million messages per second.</p>"},{"location":"reference_architecture_nfp/#security","title":"Security","text":""},{"location":"reference_architecture_nfp/#worker-authentication","title":"Worker Authentication","text":"<p>TBD</p>"},{"location":"reference_architecture_nfp/#worker-authorization","title":"Worker Authorization","text":"<p>TBD</p>"},{"location":"reference_architecture_nfp/#client-authentication","title":"Client Authentication","text":"<p>TBD</p>"},{"location":"reference_architecture_nfp/#client-authorization-role-based-access-control-rbac","title":"Client Authorization - Role Based Access Control (RBAC)","text":"<p>TBD</p>"},{"location":"reference_architecture_nfp/#client-encryption","title":"Client Encryption","text":"<p>TBD</p>"},{"location":"reference_architecture_nfp/#worker-encryption","title":"Worker Encryption","text":"<p>TBD</p>"},{"location":"reference_architecture_nfp/#accounting","title":"Accounting","text":"<p>TBD</p>"},{"location":"reference_architecture_nfp/#known-weaknesses","title":"Known Weaknesses","text":"<ul> <li>The heartbeat rate must be set to similar values in broker and worker, or false disconnections will occur. </li> <li>The use of multiple frames for command formatting has a performance impact.</li> </ul>"},{"location":"reference_architecture_norfab/","title":"NORFAB Architecture","text":""},{"location":"reference_architecture_norfab/#high-level-design","title":"High Level Design","text":""},{"location":"reference_architecture_norfab/#low-level-design","title":"Low Level Design","text":"<p>Low level design revolves around resource oriented services - services that manage resources, where resources could be databases, network devices, file system etc.</p> <p></p>"},{"location":"reference_architecture_norfab/#jobs-execution-flow","title":"Jobs Execution Flow","text":"<p>There are multiple job flows implemented:</p> <ul> <li>JOB POST FLOW - for clients to publish jobs to workers</li> <li>JOB LOOP - job execution performed by workers</li> <li>JOB GET FLOW - for clients to retrieve job execution results</li> </ul> <p>Above flows depicted on the diagram.</p> <p></p>"},{"location":"reference_norfab_events_subsystem/","title":"Events","text":"<p>TBD</p>"},{"location":"reference_norfab_inventory/","title":"NorFab Inventory","text":"<p>NorFab comes with Simple Inventory Datastore (SID) hosted by broker, allowing workers to source inventory data from broker.</p> <p>NorFab inventory separated in sections, each responsible for configuring different aspects of the system.</p> inventory.yaml<pre><code>broker: # (1)!\n  endpoint: \"tcp://127.0.0.1:5555\" # (2)!\n\nworkers: # (3)!\n  nornir-*: # (4)!\n    - nornir/common.yaml   \n  nornir-worker-1: # (5)!\n    - nornir/nornir-worker-1.yaml\n\ntopology: # (6)!\n  broker: True # (7)!\n  workers: # (8)!\n    - nornir-worker-1\n\nlogging: # (9)!\n  log_events: True\n  handlers:\n    terminal:\n      level: WARNING\n    file: \n      level: INFO\n</code></pre> <ol> <li>Broker configuration inventory section</li> <li>URL to listen for connections on - <code>localhost</code> port <code>5555</code> in this case</li> <li>Workers configuration inventory section</li> <li>glob pattern that will match all workers with <code>nornir-</code> in the name and map <code>common.yaml</code> file content for each of them</li> <li>Worker definition to map inventory file to a specific worker that has name <code>nornir-worker-1</code></li> <li>Topology section to define what components to run</li> <li>Start broker process</li> <li>List of workers names to start processes for</li> <li>Logging configuration section</li> </ol>"},{"location":"reference_norfab_inventory/#broker-inventory-section","title":"Broker Inventory Section","text":"<p>Broker inventory must have broker <code>endpoint</code> parameter defined for workers and clients to identify how connect with broker, and for broker itself to identify where to listen for connections.</p> inventory.yaml<pre><code># broker settings\nbroker:\n  endpoint: \"tcp://127.0.0.1:5555\"\n  shared_key: \"5z1:yW}]n?UXhGmz+5CeHN1&gt;:S9k!eCh6JyIhJqO\"\n  zmq_auth: True\n</code></pre> <p>In addition these parameters are supported</p> <ol> <li> <p><code>shared_key</code> - broker encryption shared key may or may not be needed depending of type of the setup you are running, in case if all components - broker, client and workers run on same machine, configuring <code>shared_key</code> parameter is options, as <code>nfapi</code> is smart enough to auto-configure all workers and client with correct broker shared key. In case if broker and workers with clients are distributed i.e. running in separate containers or on separate machines, <code>share_key</code> parameter must be configured on all workers and clients to match shared key used by broker.</p> </li> <li> <p><code>zmq_auth</code> - flag to enable or disable ZeroMQ authentication, <code>False</code> - disable authentication and encryption, by default set to <code>True</code> - ZeroMQ authentication and encryption enabled.</p> </li> </ol>"},{"location":"reference_norfab_inventory/#workers-inventory-section","title":"Workers Inventory Section","text":"<p>To understand how Simple Inventory Datastore serves workers inventory it is good to know that each worker has a unique name to identify it.</p> <p>With that in mind, the goal is to map inventory data to individual worker by its name.</p> <p>For example, let's pretend that worker name is <code>nornir-worker-1</code> and we have <code>common.yaml</code> and <code>nornir-worker-1.yaml</code> files with inventory data  that we need to provide worker with.</p> <p>To do the mapping between worker name and inventory files we can put this in NorFab inventory (<code>inventory.yaml</code>) file:</p> inventory.yaml<pre><code>workers:\n  nornir-*:\n    - nornir/common.yaml  \n  nornir-worker-1:\n    - nornir/nornir-worker-1.yaml\n</code></pre> <p>Where files structure would look like this:</p> <pre><code>\u2514\u2500\u2500\u2500rootfolder\n    \u2502   inventory.yaml\n    \u2502\n    \u2514\u2500\u2500\u2500nornir\n            common.yaml\n            nornir-worker-1.yaml\n</code></pre> <p>As you can see, <code>inventory.yaml</code> file contains <code>workers</code> section with a dictionary keyed by glob patterns  to match against workers' names, once worker name matched by the pattern, all items in the list underneaths that pattern being loaded and recursively merged. As such, process continues until all patterns evaluated. Final output of the process is a combined inventory data of all the matched files.</p> <p>The recursive logic of combining inventory data files is pretty  straightforward - each next data file merged into the previous data file  overriding the overlapping values.</p> <p>The glob pattern matching logic allows be as specific as required and  map specific files to individual workers or to map single data file to  multiple workers or map multiple files to multiple workers, all combinations  supported.</p> <p>For example, we have a group of two workers with names <code>netbox-wroker-1.1</code> and <code>netbox-worker-1.2</code> and we want to map <code>netbox_common.yaml</code> to both of the workers, in that case NorFab inventory (<code>inventory.yaml</code>) file could have this content:</p> inventory.yaml<pre><code>workers:\n  netbox-worker-1.*:\n    - nornir/netbox_common.yaml  \n</code></pre> <p>Where files structure would look like this:</p> <pre><code>\u2514\u2500\u2500\u2500rootfolder\n    \u2502   inventory.yaml\n    \u2502\n    \u2514\u2500\u2500\u2500netbox\n            netbox_common.yaml\n</code></pre> <p>Both workers will be served with  <code>netbox_common.yaml</code> file content as an inventory data.</p>"},{"location":"reference_norfab_inventory/#workers-inventory-parameters","title":"Workers Inventory Parameters","text":"<p>Workers inventory can contain these common parameters:</p> <ol> <li><code>service</code> - name of the service this worker belongs to</li> <li><code>max_concurrent_jobs</code> - maximum number of concurrent jobs this worker can run</li> </ol> <p>Sample worker base inventory:</p> <pre><code>service: nornir\nmax_concurrent_jobs: 5\n</code></pre> <p>The rest of the inventory data is worker specific.</p>"},{"location":"reference_norfab_inventory/#topology-inventory-section","title":"Topology Inventory Section","text":"<p>Topology section of NorFab inventory identifies the components that need to be started on the given node.</p>"},{"location":"reference_norfab_inventory/#logging-inventory-section","title":"Logging Inventory Section","text":"<p>Logging inventory section allows to configure logging parameters such as file retention options, logging to remote hosts, logging levels etc. using Python logging configuration dictionary schema. </p> <p>Additional <code>logging</code> section supported attributes:</p> <ul> <li><code>log_events</code> - boolean, if True emit events copy as log messages</li> </ul>"},{"location":"reference_norfab_inventory/#hooks-section","title":"Hooks Section","text":"<p>Hooks section allows to configure a list of functions to run during NorFab execution lifespan events.</p> <p>Supported attach points:</p> <ul> <li><code>startup</code> - list of functions to run right after NorFab nfapi started broker and worker process and fully initialized. Startup hook function must accept <code>norfab</code> object as a single argument.</li> <li><code>exit</code> - list of functions to run right before NorFab nfapi initiates exit sequence. Exit hook function must accept <code>norfab</code> object as a single argument.</li> </ul> <p>Each hook defined as a dictionary that can contain these keys:</p> <ul> <li><code>function</code> - Python import path for hook function</li> <li><code>attachpoint</code> - one of the attach points indicating when to run hook function e.g. <code>startup</code></li> <li><code>args</code> - optional list of function positional arguments</li> <li><code>kwargs</code> - optional dictionary of function key-word arguments</li> </ul> <p>Sample hooks definition:</p> inventory.yaml<pre><code>hooks:\n  - attachpoint: startup\n    function: \"hooks.functions.do_on_startup\"\n    args: []\n    kwargs: {}\n    description: \"Function to run on startup\"\n  - attachpoint: exit\n    function: \"hooks.functions.do_on_exit\"\n    args: []\n    kwargs: {}\n    description: \"Function to run on startup\"\n</code></pre> <p>Where hook functions are:</p> hooks/functions.py<pre><code>def do_on_startup(norfab):\n    print(\"Startup hook executed\")\n\ndef do_on_exit(norfab):\n    print(\"Exit hook executed\")\n</code></pre> <p>Function import path is a dot separated path used to import module file that contains hook functions, where individual function name is a last item in dot separated path definition. For example <code>hooks.functions.do_on_startup</code> path is equivalent of running Python import <code>from hooks.functions import do_on_startup</code>.</p>"},{"location":"reference_norfab_inventory/#jinja2-support","title":"Jinja2 Support","text":"<p>Starting with version 0.3.0 NorFab supports Jinja2 syntax rendering of inventory files, in addition, <code>env</code> dictionary variable available to source environment variables:</p> inventory.yaml<pre><code>logging:\n  handlers:\n    terminal:\n      level: {{ env.get(\"TERMINAL_LOGGING_LEVEL\", \"WARNING\") }}\n    file: \n      level: {{ env.get(\"FILE_LOGGING_LEVEL\", \"INFO\") }}\n</code></pre> <p>Above example demonstrates how terminal and file logging level can be sourced from environment using Jinja2 syntax. </p> <p>All workers inventory files also passed through Jinja2 renderer with access to <code>env</code> dictionary variable:</p> nornir/common.yaml<pre><code>defaults:\n  username: {{ env.get(\"NORNIR_USERNAME\", \"nornir\") }}\n  password: {{ env.get(\"NORNIR_PASSWORD\", \"password\" ) }}\n</code></pre> <p><code>env</code> variable passed onto Jinja2 context as a dictionary that contains environment variables keys and values supporting all Jinja2 dictionary access operations:</p> nornir/common.yaml<pre><code>defaults:\n  username: {{ env[\"NORNIR_USERNAME\"] }}\n  password: {{ env.NORNIR_PASSWORD }}\n  port: {{ env.get(\"NORNIR_PORT\", 22) }}\n</code></pre>"},{"location":"reference_norfab_inventory/#loading-inventory-from-dictionary","title":"Loading Inventory from Dictionary","text":"<p>By default NorFab supports loading inventory from <code>inventory.yaml</code> file together with <code>workers</code> section items referring to a list of OS paths to YAML files with workers inventory data. As an alternative it is possible to load full NorFab and its workers inventory from dictionary, this can be useful when working with NorFab Python API directly:</p> Instantiate NorFab out of Dictionary Inventory<pre><code>from norfab.core.nfapi import NorFab\n\ndata = {\n    \"broker\": {\n        \"endpoint\": \"tcp://127.0.0.1:5555\",\n        \"shared_key\": \"5z1:yW}]n?UXhGmz+5CeHN1&gt;:S9k!eCh6JyIhJqO\",\n    },\n    \"workers\": {\n        \"nornir-*\": [\n            {\n                \"service\": \"nornir\",\n                \"watchdog_interval\": 30,\n                \"runner\": {\n                    \"plugin\": \"RetryRunner\",\n                    \"options\": {\n                        \"num_workers\": 100,\n                        \"num_connectors\": 10,\n                    }\n                }\n            }\n        ],\n        \"nornir-worker-1*\": [\"nornir/nornir-worker-1.yaml\"],\n        \"nornir-worker-2\": [\n            \"nornir/nornir-worker-2.yaml\",\n            \"nornir/nornir-worker-2-extra.yaml\",\n        ],\n    },\n    \"topology\": {\n        \"broker\": True,\n        \"workers\": [\n            \"nornir-worker-1\",\n            \"nornir-worker-2\",\n        ],\n    },\n}\n\nif __name__ == \"__main__\":\n    nf = NorFab(inventory_data=data, base_dir=\"./norfab/\")\n    nf.start()\n    client = nf.make_client()\n\n    job_result = client.run_job(\"nornir\", \"get_nornir_hosts\")\n    print(job_result)\n\n    nf.destroy()\n</code></pre> <p>In above example, <code>data</code> dictionary contains complete NorFab inventory and passed onto <code>NorFab</code> object together with <code>base_dir</code> argument to inform NorFab where to search for inventory YAML files, for example <code>\"nornir/nornir-worker-2.yaml\"</code> file will be searched within this path <code>\"./norfab/nornir/nornir-worker-2.yaml\"</code> since <code>./norfab/</code> is a base directory. Base directory argument is optional and will be automatically set by NorFab to current directory.</p>"},{"location":"reference_norfab_jobs_subsystem/","title":"Jobs","text":"<p>TBD</p>"},{"location":"reference_norfab_mmi_subsystem/","title":"MMI","text":"<p>TBD</p>"},{"location":"services_overview/","title":"Norfab Services Overview","text":"<p>Norfab services are the core components that provide the functionality and value of the Norfab platform. These services enable the management of a diverse set of resources, ranging from network devices to databases, through a unified and automated approach. </p> <p></p>","tags":["services"]},{"location":"services_overview/#key-features-of-norfab-services","title":"Key Features of Norfab Services","text":"<ul> <li>Flexibility: With support for various interfaces and libraries, Norfab services provide the flexibility to interact with different types of resources. </li> <li>Automation: Norfab services enable the automation of routine tasks, reducing the need for manual intervention and minimizing the risk of human errors. </li> <li>Integration: Norfab services can seamlessly integrate with existing tools and systems, allowing you to leverage your current investments while enhancing your automation capabilities. </li> <li>Security: Norfab services include built-in security features to protect your data and ensure that your operations are secure.</li> </ul>","tags":["services"]},{"location":"services_overview/#types-of-norfab-services","title":"Types of Norfab Services","text":"","tags":["services"]},{"location":"services_overview/#network-automation-services","title":"Network Automation Services","text":"<p>Norfab provides a range of network automation services that enable you to manage and automate network devices and configurations. These services include:</p> <ul> <li>Nornir Service: Based on the Nornir library, this service allows you to automate network device operations using popular libraries like Netmiko, NAPALM, and Scrapli. It supports tasks such as configuration management, device diagnostics, and network monitoring.</li> <li>Workflow Service: Enables users to define workflows as YAML files, where each workflow consists of multiple steps. It supports conditional execution, error handling, and integration with other Norfab services, making it ideal for automating complex processes.</li> <li>Agent Service: Leverages AI-based agents to enhance network automation and management. These agents can perform tasks such as configuration compliance checks, fault detection, and performance optimization.</li> <li>Containerlab Service: Integrates with Containerlab tool to manage containerized network topologies for troubleshooting and testing network automations or (c)labbing.</li> </ul>","tags":["services"]},{"location":"services_overview/#data-management-services","title":"Data Management Services","text":"<p>Norfab also provides services for managing and automating data-related tasks. These services include:</p> <ul> <li>Netbox Service: Integrates with Netbox, an open-source tool for network documentation and management. This service allows you to synchronize and update network inventory data, ensuring that your documentation is always accurate and up-to-date.</li> </ul>","tags":["services"]},{"location":"services_overview/#rest-api-services","title":"REST API Services","text":"<ul> <li>FastAPI Service: Provides a set of REST API endpoints to interact with Norfab. It allows users to start, run, list, and retrieve job results, offering a scalable and efficient way to integrate Norfab with external systems.</li> </ul>","tags":["services"]},{"location":"services_overview/#getting-started-with-norfab-services","title":"Getting Started with Norfab Services","text":"<p>To get started with Norfab services, you need to configure your Norfab environment and define the necessary parameters for each service. Refer to the individual service documentation sections for details.</p>","tags":["services"]},{"location":"services_overview/#conclusion","title":"Conclusion","text":"<p>Norfab services provide a powerful and flexible platform for managing and automating a diverse set of resources. By leveraging these services, you can improve operational efficiency, ensure consistency, and enhance the overall management of your infrastructure. Explore the various Norfab services to unlock the full potential of your network and data management operations.</p> <p>For more information or to schedule a demo, contact NorFab Support.</p>","tags":["services"]},{"location":"tags/","title":"Tags","text":"<p>Following is a list of tags:</p>"},{"location":"tags/#tag:fastapi","title":"FastAPI","text":"<ul> <li>            Auth          </li> <li>            Overview          </li> </ul>"},{"location":"tags/#tag:netbox","title":"Netbox","text":"<ul> <li>            Overview          </li> </ul>"},{"location":"tags/#tag:agent","title":"agent","text":"<ul> <li>            Chat          </li> <li>            Overview          </li> </ul>"},{"location":"tags/#tag:containerlab","title":"containerlab","text":"<ul> <li>            Deploy          </li> <li>            Deploy Netbox          </li> <li>            Destroy          </li> <li>            Get Nornir Inventory          </li> <li>            Inspect          </li> <li>            Overview          </li> <li>            Restart          </li> <li>            Save          </li> </ul>"},{"location":"tags/#tag:filesharing","title":"filesharing","text":"<ul> <li>            Fetch File          </li> <li>            File Details          </li> <li>            List Files          </li> <li>            Walk          </li> </ul>"},{"location":"tags/#tag:netbox","title":"netbox","text":"<ul> <li>            Create Device Interfaces          </li> <li>            Create IP          </li> <li>            Create Prefix          </li> <li>            Get BGP Peerings          </li> <li>            Get Circuits          </li> <li>            Get Connections          </li> <li>            Get Containerlab Inventory          </li> <li>            Get Devices          </li> <li>            Get Interfaces          </li> <li>            Get Nornir Inventory          </li> <li>            GraphQL          </li> <li>            REST          </li> <li>            Sync Device Facts          </li> <li>            Sync Device IP          </li> <li>            Sync Device Interfaces          </li> </ul>"},{"location":"tags/#tag:nfcli","title":"nfcli","text":"<ul> <li>            NFCLI Client API          </li> <li>            NFCLI Shell Client          </li> <li>            Overview          </li> </ul>"},{"location":"tags/#tag:norfab","title":"norfab","text":"<ul> <li>            Docker Deployment          </li> <li>            Quickstart          </li> </ul>"},{"location":"tags/#tag:nornir","title":"nornir","text":"<ul> <li>            CFG          </li> <li>            CLI          </li> <li>            Diagram          </li> <li>            File Copy          </li> <li>            Jinja2 Filters          </li> <li>            Network          </li> <li>            Overview          </li> <li>            Parse          </li> <li>            Runtime Inventory          </li> <li>            Task          </li> <li>            Test          </li> </ul>"},{"location":"tags/#tag:plugins","title":"plugins","text":"<ul> <li>            Local Plugin          </li> <li>            Overview          </li> </ul>"},{"location":"tags/#tag:robot","title":"robot","text":"<ul> <li>            ROBOT Client          </li> <li>            ROBOT Client API          </li> </ul>"},{"location":"tags/#tag:services","title":"services","text":"<ul> <li>            Overview          </li> <li>            Services Overview          </li> </ul>"},{"location":"tags/#tag:workflow","title":"workflow","text":"<ul> <li>            Overview          </li> <li>            Run          </li> </ul>"},{"location":"customization/norfab_hooks/","title":"NORFAB Hooks System","text":""},{"location":"customization/norfab_hooks/#norfab-hooks","title":"NORFAB Hooks","text":"<p>Hooks is a set of functions to run during NorFab execution lifespan.</p> <p>NorFab supports definition of hooks inside <code>inventory.yaml</code> file within <code>hooks</code> section.</p> <p>Hooks defined as a dictionary keyed by attachpoint name and value being a list of hook function definitions with these keys:</p> <ul> <li><code>function</code> - Python import path for hook function import</li> <li><code>args</code> - optional list of function positional arguments</li> <li><code>kwargs</code> - optional dictionary of function key-word arguments</li> <li>any other keys - will be ignored by NorFab but will loaded into inventory</li> </ul> <p>Supported attach points:</p> <ul> <li><code>startup</code> - list of functions to run right after NorFab nfapi fully initialized. Startup hook function must accept <code>norfab</code> object as a single argument.</li> <li><code>exit</code> - list of functions to run right before NorFab nfapi initiates exit sequence. Exit hook function must accept <code>norfab</code> object as a single argument.</li> <li><code>nornir-startup</code> - list of functions to run right after Nornir worker fully initialized. Startup hook function must accept <code>worker</code> object as a single argument.</li> <li><code>nornir-exit</code> - list of functions to run right before Nornir worker initiates exit sequence. Exit hook function must accept <code>worker</code> object as a single argument.</li> </ul> <p>Sample hooks definition:</p> inventory.yaml<pre><code>hooks:\n  startup:\n    - function: \"hooks.functions:do_on_startup\"\n      args: []\n      kwargs: {}\n      description: \"Function to run on startup\"\n  exit:\n    - function: \"hooks.functions:do_on_exit\"\n      args: []\n      kwargs: {}\n      description: \"Function to run on startup\"\n</code></pre> <p>Where hook functions are:</p> hooks/functions.py<pre><code>def do_on_startup(norfab):\n    print(\"Startup hook executed\")\n\ndef do_on_exit(norfab):\n    print(\"Exit hook executed\")\n</code></pre> <p>Function import path is a dot separated path used to import module file that contains hook functions, where individual function name is a last item in dot separated path definition. For example <code>hooks.functions:do_on_startup</code> path is equivalent of running Python import <code>from hooks.functions import do_on_startup</code>.</p>"},{"location":"customization/service_plugin_local/","title":"Local Plugin","text":"","tags":["plugins"]},{"location":"customization/service_plugin_local/#norfab-custom-service-plugin-tutorial","title":"NorFab Custom Service Plugin Tutorial","text":"<p>The simplest way to start with NorFab plugins is to create plugin within NorFab base directory, directory where <code>inventory.yaml</code> file resides.</p> <p>In this tutorial we going to create <code>DummyService</code> and its worker. We going to define two tasks that worker can execute - <code>get_version</code> and <code>get_inventory</code> to retrieve worker's version and inventory details. In addition we going to define a set of custom nfcli shell commands to interact with custom <code>DummyService</code> from interactive command line shell.</p> <p>To start with, define these folders structure and create all the files:</p> <pre><code>\u2514\u2500\u2500\u2500norfab\n    \u2502   inventory.yaml\n    \u2502\n    \u2514\u2500\u2500\u2500plugins\n            dummy_worker_inventory.yaml\n            dummy_service.py\n</code></pre> <p>Above are all the files we need, <code>plugins</code> directory name is arbitrary and can be anything, NorFab is not hardcoded to search for any of the directories, all plugins mapping defined within <code>inventory.yaml</code> file.</p>","tags":["plugins"]},{"location":"customization/service_plugin_local/#dummy-service-custom-worker","title":"Dummy Service Custom Worker","text":"<p>Below is the source code of custom worker plugin for dummy service.</p> plugins/dummy_service.py<pre><code>import logging\nimport sys\nimport importlib.metadata\nfrom norfab.core.worker import NFPWorker, Task, Job\nfrom norfab.models import Result\nfrom pydantic import (\n    BaseModel,\n    Field,\n)\nfrom typing import Dict, Callable, Any\nfrom picle.models import Outputters\n\nSERVICE = \"DummyService\"\n\nlog = logging.getLogger(__name__)\n\n# ---------------------------------------------------------------------------------------------\n# DUMMY SERVICE WORKER CLASS\n# ---------------------------------------------------------------------------------------------\n\n\nclass DummyServiceWorker(NFPWorker):\n    def __init__(\n        self,\n        inventory,\n        broker: str,\n        worker_name: str,\n        exit_event=None,\n        init_done_event=None,\n        log_level: str = \"WARNING\",\n        log_queue: object = None,\n    ):\n        \"\"\"\n        Initialize the DummyService.\n\n        Args:\n            inventory: The inventory object.\n            broker (str): The broker address.\n            worker_name (str): The name of the worker.\n            exit_event (threading.Event, optional): Event to signal service exit.\n            init_done_event (threading.Event, optional): Event to signal initialization completion.\n            log_level (str, optional): The logging level.\n            log_queue (object, optional): The logging queue.\n        \"\"\"\n        super().__init__(\n            inventory, broker, SERVICE, worker_name, exit_event, log_level, log_queue\n        )\n        self.init_done_event = init_done_event\n\n        # get inventory from broker\n        self.dummy_inventory = self.load_inventory()\n\n        # signal to NFAPI that finished initializing\n        self.init_done_event.set()\n        log.info(f\"{self.name} - Started\")\n\n    @Task()\n    def get_version(self) -&gt; Dict:\n        \"\"\"\n        Retrieves the version information for specified libraries and the current Python environment.\n\n        Returns:\n            Dict: A dictionary containing the version information for the following keys:\n                - \"norfab\": The version of the 'norfab' package, if installed.\n                - \"python\": The version of the Python interpreter.\n                - \"platform\": The platform on which the Python interpreter is running.\n\n        Note:\n            If the 'norfab' package is not installed, its version will be an empty string.\n        \"\"\"\n\n        libs = {\n            \"norfab\": \"\",\n            \"python\": sys.version.split(\" \")[0],\n            \"platform\": sys.platform,\n        }\n        # get version of packages installed\n        for pkg in libs.keys():\n            try:\n                libs[pkg] = importlib.metadata.version(pkg)\n            except importlib.metadata.PackageNotFoundError:\n                pass\n\n        return Result(result=libs)\n\n    @Task()\n    def get_inventory(self) -&gt; Dict:\n        \"\"\"\n        Retrieves the dummy service inventory.\n\n        Returns:\n            Dict: A dictionary containing the dummy inventory data.\n        \"\"\"\n        return Result(result=self.dummy_inventory)\n\n    @Task()\n    def dummy_task(self) -&gt; Dict:\n        \"\"\"\n        Dummy task\n        \"\"\"\n        return Result(result=\"dummy\")\n\n\n# ---------------------------------------------------------------------------------------------\n# DUMMY SERVICE SHELL SHOW COMMANDS MODELS\n# ---------------------------------------------------------------------------------------------\n\n\nclass DummyServiceShowCommandsModel(BaseModel):\n    inventory: Any = Field(\n        None,\n        description=\"show Dummy service inventory data\",\n        json_schema_extra={\"function\": \"get_inventory\"},\n    )\n    version: Any = Field(\n        None,\n        description=\"show Dummy service version report\",\n        json_schema_extra={\"function\": \"get_version\"},\n    )\n\n    class PicleConfig:\n        outputter = Outputters.outputter_json\n\n    @staticmethod\n    def get_inventory(**kwargs):\n        workers = kwargs.pop(\"workers\", \"all\")\n        result = NFCLIENT.run_job(\"DummyService\", \"get_inventory\", workers=workers)\n        return result\n\n    @staticmethod\n    def get_version(**kwargs):\n        workers = kwargs.pop(\"workers\", \"all\")\n        result = NFCLIENT.run_job(\"DummyService\", \"get_version\", workers=workers)\n        return result\n\n\nclass DummyServiceNfcliShell(BaseModel):\n    show: DummyServiceShowCommandsModel = Field(\n        None, description=\"Show Dummy service parameters\"\n    )\n\n    class PicleConfig:\n        subshell = True\n        prompt = \"nf[dummy]#\"\n</code></pre> <p>NorFab shell creates two commands:</p> <ul> <li><code>dummy show version</code></li> <li><code>dummy show inventory</code></li> </ul>","tags":["plugins"]},{"location":"customization/service_plugin_local/#configure-norfab-inventory","title":"Configure NorFab Inventory","text":"<p>Populate <code>inventory.yaml</code> file with this content to run single <code>DummyService</code> worker:</p> inventory.yaml<pre><code>broker:\n  endpoint: \"tcp://127.0.0.1:5555\"\n\nworkers:\n  dummy-worker-1:\n    - plugins/dummy_worker_inventory.yaml\n\ntopology:\n  broker: True\n  workers:\n    - dummy-worker-1\n\nplugins:\n  DummyService: \n    worker: \"plugins.dummy_service:DummyServiceWorker\"\n    nfcli: \n      mount_path: \"dummy\"\n      shell_model: \"plugins.dummy_service:DummyServiceNfcliShell\"\n</code></pre> <p>We also need to define inventoy for dummy service itself:</p> plugins/dummy_worker_inventory.yaml<pre><code>service: DummyService\n\ndata:\n  any: data\n  goes: here\n\nmore:\n  service: data\n</code></pre>","tags":["plugins"]},{"location":"customization/service_plugin_local/#use-dummy-service-from-nfcli-shell","title":"Use Dummy Service from Nfcli Shell","text":"<p>Run nfcli command to start NorFab. NorFab should detect custom worker plugin and start <code>DummyService</code> worker process. After startup completes, can run dummy service commands.</p> <pre><code>\n</code></pre>","tags":["plugins"]},{"location":"customization/service_plugin_local/#use-dummy-service-from-python-api","title":"Use Dummy Service from Python API","text":"","tags":["plugins"]},{"location":"customization/service_plugin_module/","title":"Python Module Plugin","text":""},{"location":"customization/service_plugin_module/#norfab-custom-service-plugin-python-module-tutorial","title":"NorFab Custom Service Plugin Python Module Tutorial","text":""},{"location":"customization/service_plugin_module/#step-1-set-up-your-development-environment","title":"Step 1: Set Up Your Development Environment","text":"<p>To create custom service plugins for Norfab, you need to set up your development environment. Ensure that you have Python installed and Norfab installed.</p>"},{"location":"customization/service_plugin_module/#step-2-create-a-plugin-directory","title":"Step 2: Create a Plugin Directory","text":"<p>Create a directory for your custom plugin within the Norfab project. This directory will contain all the necessary files for your plugin.</p> <pre><code>mkdir -p /path/to/norfab/plugins/my_custom_plugin\n</code></pre>"},{"location":"customization/service_plugin_module/#step-3-implement-the-plugin","title":"Step 3: Implement the Plugin","text":"<p>Create a Python file for your plugin and implement the necessary functionality. Here is an example of a simple custom plugin:</p> <pre><code>TBD\n</code></pre>"},{"location":"customization/service_plugin_module/#step-4-register-the-plugin","title":"Step 4: Register the Plugin","text":"<p>Register your custom plugin with Norfab by updating entrypoint in pyproject file.</p> <pre><code>TBD\n</code></pre>"},{"location":"customization/service_plugin_module/#step-5-configure-the-plugin","title":"Step 5: Configure the Plugin","text":"<p>Configure your custom plugin by adding any necessary parameters to the Norfab inventory. </p> <pre><code>TBD\n</code></pre>"},{"location":"customization/service_plugin_module/#step-6-use-the-plugin","title":"Step 6: Use the Plugin","text":"<p>Use your custom plugin in your Norfab tasks and workflows. Here is an example of how to call the custom plugin from a Norfab task:</p> <pre><code>TBD\n</code></pre>"},{"location":"customization/service_plugin_overview/","title":"Norfab Custom Service Plugins","text":"<p>Norfab custom service plugins provide a flexible and extensible way to enhance the functionality of Norfab platform. By creating and integrating custom plugins, you can tailor Norfab to meet your specific network automation and management needs. This documentation provides an overview of how to create, configure, and use custom service plugins in Norfab.</p>","tags":["plugins"]},{"location":"customization/service_plugin_overview/#overview","title":"Overview","text":"<p>Custom service plugins used to extend NorFab's capabilities by adding new features, tasks, and integrations. These plugins can be developed to interact with various network devices, data sources, and external systems, providing a seamless and integrated automation experience.</p> <p>To create custom service plugin means to create custom worker class and register it with NorFab to run. NorFab support loading custom service workers using one of these two methods:</p> <ol> <li>Load custom service worker from Python module using entrypoints.</li> <li>Load custom service worker from NorFab base directory, base directory is a directory where <code>inventory.yaml</code> file resides.</li> </ol>","tags":["plugins"]},{"location":"customization/service_plugin_overview/#key-features","title":"Key Features","text":"<ul> <li>Extensibility: Easily extend the functionality of Norfab by creating custom service plugins that integrate with your existing tools and systems.</li> <li>Flexibility: Develop plugins to perform a wide range of tasks, from network device management to data processing and analysis.</li> <li>Modularity: Organize your custom plugins into modular components, making it easy to manage and maintain your codebase.</li> <li>Reusability: Reuse custom plugins across different projects and environments, ensuring consistency and reducing development effort.</li> </ul>","tags":["plugins"]},{"location":"development/documentation_style_guide/","title":"Documentation Style Guide","text":"<p>This guide defines lightweight conventions for NORFAB documentation. The goal is to keep pages consistent, easy to scan, and easy to maintain.</p>"},{"location":"development/documentation_style_guide/#page-types","title":"Page types","text":"<p>Use the page type that matches the reader intent:</p> <ul> <li>Overview: what a component does, when to use it, key concepts.</li> <li>How-to: step-by-step for a specific goal (task-oriented).</li> <li>Reference: complete options/arguments, defaults, and output schema.</li> <li>Troubleshooting: common failure modes and fixes.</li> </ul>"},{"location":"development/documentation_style_guide/#recommended-page-template","title":"Recommended page template","text":"<p>For most service task pages, prefer this order:</p> <ol> <li>Purpose (1\u20132 paragraphs)</li> <li>Inputs (kwargs, required vs optional, defaults)</li> <li>Outputs (what the result looks like)</li> <li>Examples</li> <li>CLI example</li> <li>Python example</li> <li>Notes / Gotchas (timeouts, filters, permissions)</li> <li>Troubleshooting</li> </ol>"},{"location":"development/documentation_style_guide/#writing-style","title":"Writing style","text":"<ul> <li>Prefer short sentences and active voice.</li> <li>Use consistent terminology:</li> <li>Pick one: Service vs Worker wording in headings and keep it consistent per section.</li> <li>Use the same names for the same concept everywhere (e.g. <code>kwargs</code>, <code>workers</code>, <code>service</code>, <code>task</code>).</li> <li>Put the most common path first; hide long outputs behind collapsible blocks.</li> </ul>"},{"location":"development/documentation_style_guide/#headings","title":"Headings","text":"<ul> <li>Use <code>#</code> once per page.</li> <li>Use <code>##</code> for major sections and <code>###</code> for subsections.</li> <li>Keep headings action-oriented for How-to pages (e.g. \u201cRun tests\u201d, \u201cGenerate inventory\u201d).</li> </ul>"},{"location":"development/documentation_style_guide/#code-blocks","title":"Code blocks","text":"<ul> <li>Always specify a language: <code>bash</code>, <code>yaml</code>, <code>json</code>, <code>python</code>.</li> <li>For long outputs, use <code>&lt;details&gt;&lt;summary&gt;\u2026&lt;/summary&gt;\u2026&lt;/details&gt;</code>.</li> <li>If you need nested code fences (code block inside a code block), use longer outer fences:</li> </ul> <pre><code>```python\nprint(\"nested\")\n```\n</code></pre>"},{"location":"development/documentation_style_guide/#examples","title":"Examples","text":"<ul> <li>Provide both CLI and Python examples for user-facing tasks.</li> <li>Use real-looking but safe sample values (avoid private endpoints/keys).</li> <li>If a task supports <code>markdown=True</code>, include one example showing how to render or store the markdown output.</li> </ul>"},{"location":"development/documentation_style_guide/#links","title":"Links","text":"<ul> <li>Prefer relative links to pages that are in <code>nav</code>.</li> <li>Avoid linking to folders (MkDocs does not treat folders as pages). Link to a concrete page.</li> <li>Keep anchor links lowercase (MkDocs generates lowercase header IDs).</li> </ul>"},{"location":"development/documentation_style_guide/#naming-conventions","title":"Naming conventions","text":"<ul> <li>Use consistent filenames:</li> <li><code>services_&lt;service&gt;_service.md</code></li> <li><code>services_&lt;service&gt;_service_inventory.md</code></li> <li><code>services_&lt;service&gt;_service_tasks_&lt;task&gt;.md</code></li> <li>Use consistent nav labels:</li> <li>Task names in Title Case (e.g. \u201cGet Devices\u201d, \u201cFile Copy\u201d).</li> </ul>"},{"location":"development/documentation_style_guide/#when-to-create-a-new-page","title":"When to create a new page","text":"<p>Create a new page when:</p> <ul> <li>The topic is referenced from multiple places (avoid duplication).</li> <li>A task page exceeds ~2\u20133 screens and includes multiple distinct workflows.</li> </ul> <p>Otherwise, keep content in the closest existing page and add anchors.</p>"},{"location":"development/file_streaming_fetch_file/","title":"Client file streaming (<code>fetch_file</code>) and NFP <code>STREAM</code>/<code>PUT</code>","text":"<p>This page documents how the Python client downloads files via the built-in File Sharing Service (<code>service=\"filesharing\"</code>) using a streaming data path.</p> <p>The implementation is split across:</p> <ul> <li>Client: <code>norfab/core/client.py</code></li> <li>Broker routing: <code>norfab/core/broker.py</code>](../../norfab/core/broker.py)`</li> <li>Worker core queues/threads: <code>norfab/core/worker.py</code></li> <li>File Sharing worker tasks: <code>norfab/workers/filesharing_worker/filesharing_worker.py</code></li> <li>Protocol constants/builders: <code>norfab/core/NFP.py</code></li> </ul>"},{"location":"development/file_streaming_fetch_file/#purpose","title":"Purpose","text":"<p><code>NFPClient.fetch_file()</code> downloads <code>nf://...</code> files from a <code>filesharing</code> worker into the client\u2019s local <code>fetchedfiles/</code> folder.</p> <p><code>filesharing</code> is a built-in NORFAB service. When you start NORFAB via <code>NorFab</code> (NFAPI) with broker enabled, NFAPI injects a default File Sharing worker (<code>filesharing-worker-1</code>) into the inventory and adds it to the topology so it is started alongside other workers.</p> <p>Unlike most tasks (which return JSON via <code>RESPONSE</code>), file content is delivered as raw bytes over <code>NFP.STREAM</code>. The client uses <code>NFP.PUT</code> to request subsequent chunks (offset-based), implementing a simple sliding window (\"pipeline\") to avoid uncontrolled buffering.</p>"},{"location":"development/file_streaming_fetch_file/#built-in-service-lifecycle-nfapi","title":"Built-in service lifecycle (NFAPI)","text":"<p>This is implemented in <code>norfab/core/nfapi.py</code>:</p> <ul> <li><code>NorFab.start()</code> calls <code>add_built_in_workers_inventory()</code> when <code>run_broker</code> is enabled and the inventory topology has <code>broker: true</code>.</li> <li><code>add_built_in_workers_inventory()</code> adds <code>filesharing-worker-1</code> with <code>service: filesharing</code> and <code>base_dir: &lt;inventory.base_dir&gt;</code>.</li> <li>The worker name is inserted into <code>inventory.topology[\"workers\"]</code> (at the beginning), so it gets started like any other worker.</li> </ul>"},{"location":"development/file_streaming_fetch_file/#high-level-flow","title":"High-level flow","text":"<ol> <li>Client validates the URL (<code>nf://...</code>) and prepares a destination path under <code>${base_dir}/fetchedfiles/</code>.</li> <li>Client calls <code>filesharing.file_details</code> (normal job/response) to discover:</li> <li><code>exists</code>, <code>size_bytes</code>, <code>md5hash</code></li> <li>and to pick a worker that has the file</li> <li>Client starts a <code>filesharing.fetch_file</code> job on the chosen worker with <code>offset=0</code> and <code>chunk_size=256000</code> (by default but can be adjusted using the <code>chunk_size</code> argument).</li> <li>Worker reads the file and pushes each chunk using <code>job.stream(chunk)</code> (NFP <code>STREAM</code>).</li> <li>Client receiver thread writes each chunk to disk, updates running MD5, and sends <code>PUT</code> messages to request the next offsets until complete.</li> <li>When total received bytes match <code>size_bytes</code>, client closes the file and verifies the MD5.</li> </ol>"},{"location":"development/file_streaming_fetch_file/#flow-diagram","title":"Flow diagram","text":"sequenceDiagram   autonumber   participant C as Client (NFPClient)   participant B as Broker (NFPBroker)   participant W as FileSharingWorker   participant J as Worker Job object (Job)    C-&gt;&gt;B: POST filesharing:file_details {url}   B-&gt;&gt;W: POST {url}   W--&gt;&gt;B: RESPONSE 200 {exists,size_bytes,md5hash}   B--&gt;&gt;C: RESPONSE 200 {\u2026}    Note over C: allocate transfer state (uuid, credit=pipeline)   C-&gt;&gt;B: POST filesharing:fetch_file {url, offset=0, chunk_size}   B-&gt;&gt;W: POST {\u2026}    Note over W,J: Worker creates per-request Job (uuid)    loop For each chunk     W-&gt;&gt;J: job.stream(chunk_bytes)     J--&gt;&gt;B: STREAM (uuid, status=200, bytes)     B--&gt;&gt;C: STREAM (uuid, status=200, bytes)     C-&gt;&gt;B: PUT {offset=next_offset} (0..pipeline in flight)     B-&gt;&gt;W: PUT {offset=next_offset}     W--&gt;&gt;J: enqueue client_input_queue[{offset}]     W-&gt;&gt;J: job.wait_client_input(timeout)     J--&gt;&gt;W: {offset}   end    Note over C: close file, verify MD5"},{"location":"development/file_streaming_fetch_file/#threading-model-what-runs-where","title":"Threading model (what runs where)","text":""},{"location":"development/file_streaming_fetch_file/#client-side-threads","title":"Client-side threads","text":"<p>The client uses a dedicated receiver thread and a dispatcher thread.</p> <ul> <li>Receiver thread: <code>recv(client)</code></li> <li>The only reader of the ZeroMQ socket.</li> <li>Parses incoming multipart frames.</li> <li>For <code>NFP.STREAM</code>: calls <code>handle_stream()</code> and (importantly) sends <code>NFP.PUT</code> requests for further chunks.</li> <li> <p>For <code>NFP.RESPONSE</code>/<code>NFP.EVENT</code>: updates the client-side SQLite job DB and enqueues messages for synchronous consumers.</p> </li> <li> <p>Dispatcher thread: <code>dispatcher(client)</code></p> </li> <li>Finds jobs in the local job DB and sends <code>POST</code> / <code>GET</code> to the broker.</li> <li><code>fetch_file()</code> uses this job pipeline for:<ul> <li><code>filesharing.file_details</code> (a standard JSON response)</li> <li><code>filesharing.fetch_file</code> (which primarily streams data)</li> </ul> </li> </ul>"},{"location":"development/file_streaming_fetch_file/#worker-side-threads","title":"Worker-side threads","text":"<p>Each worker runs a small set of queue-driven threads.</p> <ul> <li><code>recv(worker, destroy_event)</code>: reads from broker socket and fan-outs by command into queues</li> <li><code>_post(...)</code>: handles <code>NFP.POST</code> (creates job record + ACK)</li> <li><code>_get(...)</code>: handles <code>NFP.GET</code> (status/result polling)</li> <li><code>_event(...)</code>: emits <code>NFP.EVENT</code></li> <li><code>_put(...)</code>: handles <code>NFP.PUT</code> and forwards decoded JSON into <code>running_jobs[uuid].client_input_queue</code></li> </ul> <p>The File Sharing worker task <code>fetch_file(job, ...)</code> then consumes those inputs using:</p> <ul> <li><code>job.wait_client_input(timeout=chunk_timeout)</code></li> </ul> <p>This is how the worker learns which offsets to send next.</p>"},{"location":"development/file_streaming_fetch_file/#messaging-formats","title":"Messaging formats","text":"<p>This section describes the multipart frame layout as it appears to each endpoint.</p>"},{"location":"development/file_streaming_fetch_file/#client-broker-dealer-router","title":"Client \u2194 Broker (DEALER \u2194 ROUTER)","text":"<p>The client is a <code>zmq.DEALER</code>. The broker is a <code>zmq.ROUTER</code>.</p> <p>From the client\u2019s perspective (what <code>recv(client)</code> expects):</p> <ul> <li>Incoming from broker:</li> </ul> <pre><code>[ empty, header, command, service, uuid, status, payload ]\n</code></pre> <p>Where: - <code>header</code> is <code>NFP.BROKER</code> - <code>command</code> is typically <code>NFP.RESPONSE</code>, <code>NFP.EVENT</code>, or <code>NFP.STREAM</code> - <code>payload</code> is:   - JSON bytes for <code>RESPONSE</code>/<code>EVENT</code>   - raw file bytes for <code>STREAM</code></p> <p>From the broker\u2019s perspective, messages include an initial identity frame that the client does not see.</p>"},{"location":"development/file_streaming_fetch_file/#worker-broker-dealer-router","title":"Worker \u2194 Broker (DEALER \u2194 ROUTER)","text":"<p>Workers are also <code>zmq.DEALER</code>. The broker is <code>zmq.ROUTER</code>.</p> <p>Broker \u2192 worker (built by <code>NFP.MessageBuilder.broker_to_worker_put</code>):</p> <pre><code>[ worker_id, empty, NFP.BROKER, NFP.PUT, sender_id, empty, uuid, request_json ]\n</code></pre> <p>Worker-side <code>recv(worker, ...)</code> strips the initial <code>empty</code>, reads <code>header</code> + <code>command</code>, then pushes the remaining frames into <code>put_queue</code>.</p> <p>In <code>_put(...)</code>, the queued <code>work</code> list is treated as:</p> <ul> <li><code>work[2]</code> = <code>uuid</code></li> <li><code>work[3]</code> = JSON payload</li> </ul>"},{"location":"development/file_streaming_fetch_file/#stream-payload","title":"<code>STREAM</code> payload","text":"<p>A stream chunk originates from the worker job via <code>Job.stream(data)</code>:</p> <pre><code>msg = [\n  client_address, b\"\", juuid, b\"200\", data_bytes\n]\nworker.send_to_broker(NFP.STREAM, msg)\n</code></pre> <p>The broker forwards <code>STREAM</code> to the client without wrapping the chunk into JSON.</p>"},{"location":"development/file_streaming_fetch_file/#how-put-is-used-client-worker","title":"How <code>PUT</code> is used (client \u2192 worker)","text":"<p>In file download, <code>PUT</code> does not upload file content.</p> <p>Instead, <code>PUT</code> is used as a client-to-worker control channel tied to the same job UUID.</p> <ul> <li>Client sends <code>PUT</code> with JSON: <code>{ \"offset\": &lt;int&gt; }</code></li> <li>Broker routes it to the same worker that is executing the job</li> <li>Worker <code>_put</code> thread places that dict into the running job\u2019s <code>client_input_queue</code></li> <li>FileSharingWorker\u2019s <code>fetch_file</code> task reads it with <code>job.wait_client_input()</code> and updates <code>offset</code></li> </ul> <p>This design provides a simple backpressure mechanism: the worker only sends the next chunk after the client requests it.</p>"},{"location":"development/file_streaming_fetch_file/#sliding-window-pipeline-backpressure","title":"Sliding window / pipeline (backpressure)","text":"<p>Client-side <code>pipeline</code> is implemented as a credit counter in <code>client.file_transfers[uuid]</code>:</p> <ul> <li>Start: <code>credit = pipeline</code></li> <li>Each received chunk: <code>credit += 1</code></li> <li>Each <code>PUT</code> request sent: <code>credit -= 1</code></li> </ul> <p>After writing a chunk, the client sends up to <code>credit</code> additional <code>PUT</code> requests (while there are offsets remaining), which keeps up to ~<code>pipeline</code> requests \"in flight\".</p> <p>This keeps throughput reasonable without letting either side buffer an unbounded amount of data.</p>"},{"location":"development/file_streaming_fetch_file/#completion-and-integrity-checking","title":"Completion and integrity checking","text":"<p>Completion is detected purely by byte count:</p> <ul> <li>Client tracks <code>total_bytes_received</code> and compares to <code>size_bytes</code>.</li> <li>When equal:</li> <li>Close the destination file</li> <li>Verify MD5 hash against <code>file_details.md5hash</code></li> </ul> <p>If MD5 mismatches, the client marks the job as failed and logs an error.</p>"},{"location":"development/file_streaming_fetch_file/#operational-notes-gotchas","title":"Operational notes / gotchas","text":"<ul> <li>Timeouts</li> <li>Worker <code>filesharing.fetch_file</code> defaults <code>chunk_timeout=5</code> seconds.</li> <li>If the client stops sending <code>PUT</code> requests, the worker raises <code>RuntimeError</code>.</li> <li> <p>Client <code>fetch_file(timeout=...)</code> limits the overall <code>run_job()</code> operations (details + streaming job).</p> </li> <li> <p>Local storage path</p> </li> <li> <p>Downloads go to <code>${base_dir}/fetchedfiles/&lt;nf-path&gt;</code>.</p> </li> <li> <p>Binary vs text</p> </li> <li><code>fetch_file(read=True)</code> opens the downloaded file as UTF-8 text (<code>open(..., \"r\", encoding=\"utf-8\")</code>).</li> <li> <p>For binary files, prefer <code>read=False</code> and open the returned path in binary mode.</p> </li> <li> <p>Single worker selection</p> </li> <li><code>file_details</code> is called on <code>workers=\"all\"</code>, but <code>fetch_file</code> streams from the first worker that reports <code>exists</code>.</li> </ul>"},{"location":"development/file_streaming_fetch_file/#debugging-tips","title":"Debugging tips","text":"<ul> <li>Enable debug logging for <code>norfab.core.client</code>, <code>norfab.core.broker</code>, and <code>norfab.core.worker</code>.</li> <li>Look for:</li> <li>Client: <code>received stream for unknown file transfer</code> (mismatched/expired UUID)</li> <li>Worker: <code>chunk timeout reached before received next chunk request</code></li> <li>Client: <code>MD5 hash mismatch</code></li> </ul>"},{"location":"testing/netbox_service_tests/","title":"Netbox Service Tests","text":""},{"location":"testing/netbox_service_tests/#overview","title":"Overview","text":"<p>The Netbox Service Tests (<code>test_netbox_service.py</code>) provide comprehensive testing coverage for NORFAB's Netbox integration service. These tests validate the Netbox worker's ability to interact with Netbox instances.</p>"},{"location":"testing/netbox_service_tests/#test-summary","title":"Test Summary","text":"<p>The test suite is organized into 17 test classes:</p> Test Class Purpose Documentation TestNetboxWorker Core service discovery and version checks Details TestNetboxGrapQL GraphQL query operations Details TestGetInterfaces Interface retrieval and filtering Details TestGetDevices Device retrieval and filtering Details TestGetConnections Device connection queries Details TestGetNornirInventory Nornir inventory generation from Netbox Details TestGetCircuits Circuit information retrieval Details TestGetBgpPeerings BGP peering data Details TestSyncDeviceFacts Device fact synchronization Details TestSyncDeviceInterfaces Device interface synchronization Details TestCreateDeviceInterfaces Device interface creation Details TestSyncDeviceIP IP address synchronization Details TestCreateIP IP address creation Details TestNetboxCache Caching functionality Details TestGetContainerlabInventory Containerlab inventory generation Details TestCreatePrefix IP prefix creation Details TestCreateIPBulk Bulk IP address creation Details"},{"location":"testing/netbox_service_tests/#test-classes","title":"Test Classes","text":""},{"location":"testing/netbox_service_tests/#testnetboxworker","title":"TestNetboxWorker","text":"<p>Core Netbox service functionality and health checks.</p> <p>Purpose: Validate basic service operations, version compatibility, and status monitoring.</p>"},{"location":"testing/netbox_service_tests/#test-get-netbox-inventory","title":"Test Get Netbox Inventory","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_netbox_inventory(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_inventory\",\n        workers=\"any\",\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert all(\n            k in res[\"result\"] for k in [\"service\", \"instances\"]\n        ), f\"{worker} - not all netbox inventory data returned\"\n        assert all(\n            k in res[\"result\"][\"instances\"] for k in [\"dev\", \"preprod\", \"prod\"]\n        ), f\"{worker} - not all netbox instances inventory data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-netbox-version","title":"Test Get Netbox Version","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_netbox_version(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_version\",\n        workers=\"any\",\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert all(\n            k in res[\"result\"]\n            for k in [\"platform\", \"pynetbox\", \"python\", \"requests\"]\n        ), f\"{worker} - not all netbox version data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-netbox-status","title":"Test Get Netbox Status","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_netbox_status(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_netbox_status\",\n        workers=\"any\",\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert all(\n            k in res[\"result\"] for k in [\"dev\", \"preprod\", \"prod\"]\n        ), f\"{worker} - not all netbox instances inventory data returned\"\n        for instance, status_data in res[\"result\"].items():\n            assert all(\n                k in status_data\n                for k in [\n                    \"django-version\",\n                    \"error\",\n                    \"netbox-version\",\n                    \"plugins\",\n                    \"python-version\",\n                    \"rq-workers-running\",\n                    \"status\",\n                ]\n            ), f\"{worker}:{instance} - not all netbox instances status data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-netbox-compatibility","title":"Test Get Netbox Compatibility","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_netbox_compatibility(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_compatibility\",\n        workers=\"any\",\n    )\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert all(\n            k in res[\"result\"] for k in [\"dev\", \"preprod\", \"prod\"]\n        ), f\"{worker} - not all netbox instances inventory data returned\"\n        for instance, compatible in res[\"result\"].items():\n            assert compatible == True, f\"{worker}:{instance} - not compatible\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#testnetboxgrapql","title":"TestNetboxGrapQL","text":"<p>GraphQL query operations against Netbox instances.</p> <p>Purpose: Validate GraphQL query execution, dry-run mode, and error handling.</p>"},{"location":"testing/netbox_service_tests/#test-graphql-query-string","title":"Test Graphql Query String","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_graphql_query_string(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"graphql\",\n        workers=\"any\",\n        kwargs={\"query_string\": \"query DeviceListQuery { device_list { name } }\"},\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert \"device_list\" in res[\"result\"], f\"{worker} no device list returned\"\n        assert isinstance(\n            res[\"result\"][\"device_list\"], list\n        ), f\"{worker} unexpected device list payload type, was expecting list\"\n        assert (\n            len(res[\"result\"][\"device_list\"]) &gt; 0\n        ), f\"{worker} returned no devices in device list\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-graphql-query-string-with-instance","title":"Test Graphql Query String With Instance","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_graphql_query_string_with_instance(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"graphql\",\n        workers=\"any\",\n        kwargs={\n            \"query_string\": \"query DeviceListQuery { device_list { name } }\",\n            \"instance\": \"prod\",\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert \"device_list\" in res[\"result\"], f\"{worker} no device list returned\"\n        assert isinstance(\n            res[\"result\"][\"device_list\"], list\n        ), f\"{worker} unexpected device list payload type, was expecting list\"\n        assert (\n            len(res[\"result\"][\"device_list\"]) &gt; 0\n        ), f\"{worker} returned no devices in device list\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-graphql-query-string-dry-run","title":"Test Graphql Query String Dry Run","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_graphql_query_string_dry_run(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"graphql\",\n        workers=\"any\",\n        kwargs={\n            \"query_string\": \"query DeviceListQuery { device_list { name } }\",\n            \"dry_run\": True,\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert all(\n            k in res[\"result\"] for k in [\"headers\", \"data\", \"verify\", \"url\"]\n        ), f\"{worker} - not all dry run data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-graphql-query-string-error","title":"Test Graphql Query String Error","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_graphql_query_string_error(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"graphql\",\n        workers=\"any\",\n        kwargs={\n            \"query_string\": \"query DeviceListQuery { device_list { name } \",\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert res[\n            \"errors\"\n        ], f\"{worker} did not return errors for malformed graphql query\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-form-graphql-query-dry-run","title":"Test Form Graphql Query Dry Run","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_form_graphql_query_dry_run(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    if self.nb_version[0] == 4:\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"graphql\",\n            workers=\"any\",\n            kwargs={\n                \"obj\": \"device_list\",\n                \"fields\": [\"name\", \"platform {name}\"],\n                \"filters\": {\"q\": \"ceos\", \"platform\": \"arista_eos\"},\n                \"dry_run\": True,\n            },\n        )\n        pprint.pprint(ret, width=200)\n        for worker, res in ret.items():\n            assert res[\"result\"][\"data\"] == (\n                '{\"query\": \"query {device_list(filters: {q: \\\\\"ceos\\\\\", platform: \\\\\"arista_eos\\\\\"}) {name platform {name}}}\"}'\n            ), f\"{worker} did not return correct query string\"\n    elif self.nb_version[0] == 3:\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"graphql\",\n            workers=\"any\",\n            kwargs={\n                \"obj\": \"device_list\",\n                \"fields\": [\"name\", \"platform {name}\"],\n                \"filters\": {\"name__ic\": \"ceos\", \"platform\": \"arista_eos\"},\n                \"dry_run\": True,\n            },\n        )\n        pprint.pprint(ret, width=200)\n        for worker, res in ret.items():\n            assert res[\"result\"][\"data\"] == (\n                '{\"query\": \"query {device_list(name__ic: \\\\\"ceos\\\\\", platform: \\\\\"arista_eos\\\\\") {name platform {name}}}\"}'\n            ), f\"{worker} did not return correct query string\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#testgetinterfaces","title":"TestGetInterfaces","text":"<p>Interface retrieval and filtering operations.</p> <p>Purpose: Validate device interface queries with various filters and parameters.</p>"},{"location":"testing/netbox_service_tests/#test-get-interfaces","title":"Test Get Interfaces","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_interfaces(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_interfaces\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"ceos1\", \"fceos4\"]},\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert \"ceos1\" in res[\"result\"], f\"{worker} returned no results for ceos1\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        for device, interfaces in res[\"result\"].items():\n            assert isinstance(\n                interfaces, dict\n            ), f\"{worker}:{device} did not return interfaces dictionary\"\n            for intf_name, intf_data in interfaces.items():\n                assert all(\n                    k in intf_data\n                    for k in [\n                        \"enabled\",\n                        \"description\",\n                        \"mtu\",\n                        \"parent\",\n                        \"mode\",\n                        \"untagged_vlan\",\n                        \"vrf\",\n                        \"tagged_vlans\",\n                        \"tags\",\n                        \"custom_fields\",\n                        \"last_updated\",\n                        \"bridge\",\n                        \"child_interfaces\",\n                        \"bridge_interfaces\",\n                        \"member_interfaces\",\n                        \"wwn\",\n                        \"duplex\",\n                        \"speed\",\n                    ]\n                ), f\"{worker}:{device}:{intf_name} not all data returned\"\n                if self.nb_version &gt;= (4, 2, 0):\n                    assert \"mac_addresses\" in intf_data\n                else:\n                    assert \"mac_address\" in intf_data\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-interfaces-with-instance","title":"Test Get Interfaces With Instance","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_interfaces_with_instance(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_interfaces\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"ceos1\", \"fceos4\"], \"instance\": \"prod\"},\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert \"ceos1\" in res[\"result\"], f\"{worker} returned no results for ceos1\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        for device, interfaces in res[\"result\"].items():\n            assert isinstance(\n                interfaces, dict\n            ), f\"{worker}:{device} did not return interfaces dictionary\"\n            for intf_name, intf_data in interfaces.items():\n                assert all(\n                    k in intf_data\n                    for k in [\n                        \"enabled\",\n                        \"description\",\n                        \"mtu\",\n                        \"parent\",\n                        \"mode\",\n                        \"untagged_vlan\",\n                        \"vrf\",\n                        \"tagged_vlans\",\n                        \"tags\",\n                        \"custom_fields\",\n                        \"last_updated\",\n                        \"bridge\",\n                        \"child_interfaces\",\n                        \"bridge_interfaces\",\n                        \"member_interfaces\",\n                        \"wwn\",\n                        \"duplex\",\n                        \"speed\",\n                    ]\n                ), f\"{worker}:{device}:{intf_name} not all data returned\"\n                if self.nb_version &gt;= (4, 2, 0):\n                    assert \"mac_addresses\" in intf_data\n                else:\n                    assert \"mac_address\" in intf_data\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-interfaces-dry-run","title":"Test Get Interfaces Dry Run","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_interfaces_dry_run(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_interfaces\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"ceos1\", \"fceos4\"], \"dry_run\": True},\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert res[\"result\"][\"data\"] == (\n            '{\"query\": \"query {interfaces: '\n            + \"interface_list(filters: {device: \"\n            + '{name: {in_list: [\\\\\"ceos1\\\\\", '\n            + '\\\\\"fceos4\\\\\"]}}}) {name enabled '\n            + \"description mtu parent {name} mode \"\n            + \"untagged_vlan {vid name} vrf {name} \"\n            + \"tagged_vlans {vid name} tags {name} \"\n            + \"custom_fields last_updated bridge \"\n            + \"{name} child_interfaces {name} \"\n            + \"bridge_interfaces {name} \"\n            + \"member_interfaces {name} wwn duplex \"\n            + \"speed id device {name} label \"\n            + \"mark_connected mac_addresses \"\n            + '{mac_address}}}\"}'\n        ), f\"{worker} did not return correct query string\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-interfaces-add-ip","title":"Test Get Interfaces Add Ip","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_interfaces_add_ip(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_interfaces\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"ceos1\", \"fceos4\"], \"ip_addresses\": True},\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert \"ceos1\" in res[\"result\"], f\"{worker} returned no results for ceos1\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        for device, interfaces in res[\"result\"].items():\n            assert isinstance(\n                interfaces, dict\n            ), f\"{worker}:{device} did not return interfaces dictionary\"\n            for intf_name, intf_data in interfaces.items():\n                assert (\n                    \"ip_addresses\" in intf_data\n                ), f\"{worker}:{device}:{intf_name} no IP addresses data returned\"\n                for ip in intf_data[\"ip_addresses\"]:\n                    assert all(\n                        k in ip\n                        for k in [\n                            \"address\",\n                            \"status\",\n                            \"role\",\n                            \"dns_name\",\n                            \"description\",\n                            \"custom_fields\",\n                            \"last_updated\",\n                            \"tenant\",\n                            \"tags\",\n                        ]\n                    ), f\"{worker}:{device}:{intf_name} not all IP data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-interfaces-add-inventory-items","title":"Test Get Interfaces Add Inventory Items","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_interfaces_add_inventory_items(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"ceos1\", \"fceos4\"],\n            \"inventory_items\": True,\n        },\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert \"ceos1\" in res[\"result\"], f\"{worker} returned no results for ceos1\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        for device, interfaces in res[\"result\"].items():\n            assert isinstance(\n                interfaces, dict\n            ), f\"{worker}:{device} did not return interfaces dictionary\"\n            for intf_name, intf_data in interfaces.items():\n                assert (\n                    \"inventory_items\" in intf_data\n                ), f\"{worker}:{device}:{intf_name} no inventory items data returned\"\n                for item in intf_data[\"inventory_items\"]:\n                    assert all(\n                        k in item\n                        for k in [\n                            \"name\",\n                            \"role\",\n                            \"manufacturer\",\n                            \"custom_fields\",\n                            \"label\",\n                            \"description\",\n                            \"tags\",\n                            \"asset_tag\",\n                            \"serial\",\n                            \"part_id\",\n                        ]\n                    ), f\"{worker}:{device}:{intf_name} not all inventory item data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-interfaces-with-interface-regex","title":"Test Get Interfaces With Interface Regex","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_interfaces_with_interface_regex(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"ceos1\", \"fceos4\"],\n            \"interface_regex\": \"loop.+\",\n        },\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert \"ceos1\" in res[\"result\"], f\"{worker} returned no results for ceos1\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        for device, interfaces in res[\"result\"].items():\n            assert isinstance(\n                interfaces, dict\n            ), f\"{worker}:{device} did not return interfaces dictionary\"\n            for intf_name, intf_data in interfaces.items():\n                assert (\n                    \"loopback\" in intf_name.lower()\n                ), f\"{worker}:{device}:{intf_name} interface name does not match regex pattern\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#testgetdevices","title":"TestGetDevices","text":"<p>Device retrieval and advanced filtering.</p> <p>Purpose: Validate comprehensive device queries with filtering, selection, and data structure validation.</p>"},{"location":"testing/netbox_service_tests/#test-with-devices-list","title":"Test With Devices List","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_with_devices_list(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_devices\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"ceos1\", \"fceos4\"]},\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert \"ceos1\" in res[\"result\"], f\"{worker} returned no results for ceos1\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        for device, device_data in res[\"result\"].items():\n            assert isinstance(\n                device_data, dict\n            ), f\"{worker}:{device} did not return device data as dictionary\"\n            assert all(\n                k in device_data\n                for k in [\n                    \"last_updated\",\n                    \"custom_field_data\",\n                    \"tags\",\n                    \"device_type\",\n                    \"config_context\",\n                    \"tenant\",\n                    \"platform\",\n                    \"serial\",\n                    \"asset_tag\",\n                    \"site\",\n                    \"location\",\n                    \"rack\",\n                    \"status\",\n                    \"primary_ip4\",\n                    \"primary_ip6\",\n                    \"airflow\",\n                    \"position\",\n                ]\n            ), f\"{worker}:{device} not all data returned\"\n            assert (\n                \"role\" in device_data or \"devcie_role\" in device_data\n            ), f\"{worker}:{device} nodevice role info returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-with-filters","title":"Test With Filters","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_with_filters(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    if (4, 0, 0) &lt;= self.nb_version &lt; (4, 3, 0):\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"get_devices\",\n            workers=\"any\",\n            kwargs={\n                \"filters\": [\n                    {\"name\": '{in_list: [\"ceos1\", \"fceos4\"]}'},\n                    {\"q\": \"390\"},\n                ]\n            },\n        )\n    elif self.nb_version &gt;= (4, 3, 0):\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"get_devices\",\n            workers=\"any\",\n            kwargs={\n                \"filters\": [\n                    {\"name\": '{in_list: [\"ceos1\", \"fceos4\"]}'},\n                    {\"name\": '{i_contains: \"390\"}'},\n                ]\n            },\n        )\n    elif self.nb_version[0] == 3:\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"get_devices\",\n            workers=\"any\",\n            kwargs={\n                \"filters\": [\n                    {\"name\": [\"ceos1\", \"fceos4\"]},\n                    {\"name__ic\": \"390\"},\n                ]\n            },\n        )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert \"ceos1\" in res[\"result\"], f\"{worker} returned no results for ceos1\"\n        assert (\n            \"fceos3_390\" in res[\"result\"]\n        ), f\"{worker} returned no results for fceos3_390\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        for device, device_data in res[\"result\"].items():\n            assert isinstance(\n                device_data, dict\n            ), f\"{worker}:{device} did not return device data as dictionary\"\n            assert all(\n                k in device_data\n                for k in [\n                    \"last_updated\",\n                    \"custom_field_data\",\n                    \"tags\",\n                    \"device_type\",\n                    \"config_context\",\n                    \"tenant\",\n                    \"platform\",\n                    \"serial\",\n                    \"asset_tag\",\n                    \"site\",\n                    \"location\",\n                    \"rack\",\n                    \"status\",\n                    \"primary_ip4\",\n                    \"primary_ip6\",\n                    \"airflow\",\n                    \"position\",\n                ]\n            ), f\"{worker}:{device} not all data returned\"\n            assert (\n                \"role\" in device_data or \"devcie_role\" in device_data\n            ), f\"{worker}:{device} nodevice role info returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-with-filters-dry-run","title":"Test With Filters Dry Run","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_with_filters_dry_run(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_devices\",\n        workers=\"any\",\n        kwargs={\n            \"filters\": [\n                {\"name\": [\"ceos1\", \"fceos4\"]},\n                {\"q\": \"390\"},\n            ],\n            \"dry_run\": True,\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert all(\n            k in res[\"result\"][\"get_devices_dry_run\"]\n            for k in [\"headers\", \"data\", \"verify\", \"url\"]\n        ), f\"{worker} - not all dry run data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-devices-cache","title":"Test Get Devices Cache","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>@pytest.mark.parametrize(\"cache\", cache_options)\ndef test_get_devices_cache(self, nfclient, cache):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    if (4, 0, 0) &lt;= self.nb_version &lt; (4, 3, 0):\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"get_devices\",\n            workers=\"any\",\n            kwargs={\n                \"devices\": [\"ceos1\", \"fceos4\"],\n                \"filters\": [\n                    {\"name\": '{in_list: [\"ceos1\", \"fceos4\"]}'},\n                    {\"q\": \"390\"},\n                ],\n                \"cache\": cache,\n            },\n        )\n    elif self.nb_version &gt;= (4, 3, 0):\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"get_devices\",\n            workers=\"any\",\n            kwargs={\n                \"devices\": [\"ceos1\", \"fceos4\"],\n                \"filters\": [\n                    {\"name\": '{in_list: [\"ceos1\", \"fceos4\"]}'},\n                    {\"name\": '{i_contains: \"390\"}'},\n                ],\n                \"cache\": cache,\n            },\n        )\n    elif self.nb_version[0] == 3:\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"get_devices\",\n            workers=\"any\",\n            kwargs={\n                \"devices\": [\"ceos1\", \"fceos4\"],\n                \"filters\": [\n                    {\"name\": [\"ceos1\", \"fceos4\"]},\n                    {\"name__ic\": \"390\"},\n                ],\n                \"cache\": cache,\n            },\n        )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert \"ceos1\" in res[\"result\"], f\"{worker} returned no results for ceos1\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        for device, device_data in res[\"result\"].items():\n            assert isinstance(\n                device_data, dict\n            ), f\"{worker}:{device} did not return device data as dictionary\"\n            assert all(\n                k in device_data\n                for k in [\n                    \"last_updated\",\n                    \"custom_field_data\",\n                    \"tags\",\n                    \"device_type\",\n                    \"config_context\",\n                    \"tenant\",\n                    \"platform\",\n                    \"serial\",\n                    \"asset_tag\",\n                    \"site\",\n                    \"location\",\n                    \"rack\",\n                    \"status\",\n                    \"primary_ip4\",\n                    \"primary_ip6\",\n                    \"airflow\",\n                    \"position\",\n                ]\n            ), f\"{worker}:{device} not all data returned\"\n            assert (\n                \"role\" in device_data or \"devcie_role\" in device_data\n            ), f\"{worker}:{device} nodevice role info returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#testgetconnections","title":"TestGetConnections","text":"<p>Device connection and cable management queries.</p> <p>Purpose: Validate device connection retrieval and relationship mapping.</p>"},{"location":"testing/netbox_service_tests/#test-get-connections","title":"Test Get Connections","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_connections(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_connections\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"fceos4\", \"fceos5\"],\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert \"fceos5\" in res[\"result\"], f\"{worker} returned no results for fceos5\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        assert (\n            \"ConsolePort1\" in res[\"result\"][\"fceos4\"]\n        ), f\"{worker}:fceos4 no console ports data returned\"\n        assert (\n            \"ConsoleServerPort1\" in res[\"result\"][\"fceos5\"]\n        ), f\"{worker}:fceos5 no console server ports data returned\"\n        assert (\n            \"eth11.123\" in res[\"result\"][\"fceos5\"]\n        ), f\"{worker}:fceos5 no virtual ports data returned\"\n        assert (\n            \"Port-Channel1\" in res[\"result\"][\"fceos4\"]\n        ), f\"{worker}:fceos5 no lag ports data returned\"\n        assert (\n            \"Port-Channel1.101\" in res[\"result\"][\"fceos4\"]\n        ), f\"{worker}:fceos5 no lag virtual ports data returned\"\n        for device, interfaces in res[\"result\"].items():\n            assert isinstance(\n                interfaces, dict\n            ), f\"{worker}:{device} did not return interfaces dictionary\"\n            for intf_name, intf_data in interfaces.items():\n                assert all(\n                    k in intf_data\n                    for k in [\n                        \"remote_device\",\n                        \"remote_interface\",\n                        \"remote_termination_type\",\n                        \"termination_type\",\n                    ]\n                ), f\"{worker}:{device}:{intf_name} not all data returned\"\n                # verify provider network connection handling\n                if device == \"fceos4\" and intf_name == \"eth201\":\n                    assert (\n                        \"provider\" in intf_data\n                    ), f\"{worker}:{device}:{intf_name} no provider data\"\n                    assert intf_data[\"remote_termination_type\"] == \"providernetwork\"\n                    assert intf_data[\"remote_device\"] == None\n                    assert intf_data[\"remote_interface\"] == None\n                # verify breakout handling\n                if device == \"fceos5\" and intf_name == \"eth1\":\n                    assert (\n                        intf_data[\"breakout\"] == True\n                    ), f\"{worker}:{device}:{intf_name} was expecting breakout connection\"\n                    assert isinstance(intf_data[\"remote_interface\"], list)\n                    assert len(intf_data[\"remote_interface\"]) &gt; 1\n                # verify virtual ports handling\n                if device == \"fceos5\" and intf_name == \"eth11.123\":\n                    assert intf_data[\"remote_device\"] == \"fceos4\"\n                    assert intf_data[\"remote_device_status\"] == \"active\"\n                    assert intf_data[\"remote_interface\"] == \"eth11.123\"\n                    assert intf_data[\"termination_type\"] == \"virtual\"\n                # verify lag ports handling\n                if device == \"fceos4\" and intf_name == \"Port-Channel1\":\n                    assert intf_data[\"remote_device\"] == \"fceos5\"\n                    assert intf_data[\"remote_interface\"] == \"ae5\"\n                    assert intf_data[\"termination_type\"] == \"lag\"\n                # verify lag virtual interfaces handling\n                if device == \"fceos4\" and intf_name == \"Port-Channel1.101\":\n                    assert intf_data[\"remote_device\"] == \"fceos5\"\n                    assert intf_data[\"remote_interface\"] == \"ae5.101\"\n                    assert intf_data[\"termination_type\"] == \"virtual\"\n                if device == \"fceos5\" and intf_name == \"ae6.0\":\n                    assert intf_data[\"remote_device\"] == \"fceos4\"\n                    assert intf_data[\"remote_interface\"] == \"Port-Channel2\"\n                    assert intf_data[\"termination_type\"] == \"virtual\"\n                    assert intf_data[\"remote_termination_type\"] == \"lag\"\n                if device == \"fceos4\" and intf_name == \"eth103.0\":\n                    assert intf_data[\"remote_device\"] == \"fceos5\"\n                    assert intf_data[\"remote_interface\"] == \"eth103\"\n                    assert intf_data[\"termination_type\"] == \"virtual\"\n                    assert intf_data[\"remote_termination_type\"] == \"interface\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-connections-physical-interface-regex","title":"Test Get Connections Physical Interface Regex","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_connections_physical_interface_regex(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_connections\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"fceos4\", \"fceos5\"],\n            \"interface_regex\": \"eth10.*\",\n            \"include_virtual\": False,\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert \"fceos5\" in res[\"result\"], f\"{worker} returned no results for fceos5\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        for device, interfaces in res[\"result\"].items():\n            assert interfaces, f\"{worker}:{device} no connections data returned\"\n            for intf_name, intf_data in interfaces.items():\n                assert intf_name.startswith(\n                    \"eth10\"\n                ), f\"{worker}:{device}:{intf_name} not matching regex\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-connections-virtual-interface-regex","title":"Test Get Connections Virtual Interface Regex","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_connections_virtual_interface_regex(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_connections\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"fceos4\", \"fceos5\"],\n            \"interface_regex\": \"(Port-Channel1|ae5).101\",  # match Port-Channel1.101 and ae5.101\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert \"fceos5\" in res[\"result\"], f\"{worker} returned no results for fceos5\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        assert (\n            res[\"result\"][\"fceos4\"][\"Port-Channel1.101\"][\"remote_interface\"]\n            == \"ae5.101\"\n        ), f\"Unexpected interface name\"\n        assert (\n            res[\"result\"][\"fceos5\"][\"ae5.101\"][\"remote_interface\"]\n            == \"Port-Channel1.101\"\n        ), f\"Unexpected interface name\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-connections-dry-run","title":"Test Get Connections Dry Run","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_connections_dry_run(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_connections\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"fceos4\", \"fceos5\"], \"dry_run\": True},\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert all(\n            k in res[\"result\"] for k in [\"headers\", \"data\", \"verify\", \"url\"]\n        ), f\"{worker} - not all dry run data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-connections-and-cables","title":"Test Get Connections And Cables","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_connections_and_cables(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_connections\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"fceos4\", \"fceos5\"], \"cables\": True},\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert \"fceos5\" in res[\"result\"], f\"{worker} returned no results for fceos5\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        for device, interfaces in res[\"result\"].items():\n            assert isinstance(\n                interfaces, dict\n            ), f\"{worker}:{device} did not return interfaces dictionary\"\n            for intf_name, intf_data in interfaces.items():\n                if intf_data[\"termination_type\"] in [\"virtual\", \"lag\"]:\n                    continue\n                assert (\n                    \"cable\" in intf_data\n                ), f\"{worker}:{device}:{intf_name} no cable data returned\"\n                assert all(\n                    k in intf_data[\"cable\"]\n                    for k in [\n                        \"custom_fields\",\n                        \"label\",\n                        \"length\",\n                        \"length_unit\",\n                        \"peer_device\",\n                        \"peer_interface\",\n                        \"peer_termination_type\",\n                        \"status\",\n                        \"tags\",\n                        \"tenant\",\n                        \"type\",\n                    ]\n                ), f\"{worker}:{device}:{intf_name} not all cable data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#testgetnornirinventory","title":"TestGetNornirInventory","text":"<p>Nornir inventory generation from Netbox data.</p> <p>Purpose: Validate conversion of Netbox data into Nornir-compatible inventory format.</p>"},{"location":"testing/netbox_service_tests/#test-with-devices","title":"Test With Devices","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_with_devices(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_nornir_inventory\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"ceos1\", \"fceos4\", \"nonexist\"]},\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert (\n            \"ceos1\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for ceos1\"\n        assert (\n            \"fceos4\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for fceos4\"\n        for device, data in res[\"result\"][\"hosts\"].items():\n            assert all(\n                k in data for k in [\"data\", \"hostname\", \"platform\"]\n            ), f\"{worker}:{device} not all data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-with-filters_1","title":"Test With Filters","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_with_filters(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    if self.nb_version[0] == 4:\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"get_nornir_inventory\",\n            workers=\"any\",\n            kwargs={\n                \"filters\": [\n                    {\"name\": '{in_list: [\"ceos1\"]}'},\n                    {\"name\": '{contains: \"fceos\"}'},\n                ]\n            },\n        )\n    elif self.nb_version[0] == 3:\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"get_nornir_inventory\",\n            workers=\"any\",\n            kwargs={\n                \"filters\": [\n                    {\"name\": [\"ceos1\"]},\n                    {\"name__ic\": \"fceos\"},\n                ]\n            },\n        )\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert (\n            \"ceos1\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for ceos1\"\n        assert (\n            \"fceos4\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for fceos4\"\n        for device, data in res[\"result\"][\"hosts\"].items():\n            assert all(\n                k in data for k in [\"data\", \"hostname\", \"platform\"]\n            ), f\"{worker}:{device} not all data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-source-platform-from-config-context","title":"Test Source Platform From Config Context","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_source_platform_from_config_context(self, nfclient):\n    # for iosxr1 platform data encoded in config context\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_nornir_inventory\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"iosxr1\"]},\n    )\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert (\n            \"iosxr1\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for iosxr1\"\n        for device, data in res[\"result\"][\"hosts\"].items():\n            assert all(\n                k in data for k in [\"data\", \"hostname\", \"platform\"]\n            ), f\"{worker}:{device} not all data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-with-devices-nbdata-is-true","title":"Test With Devices Nbdata Is True","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_with_devices_nbdata_is_true(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_nornir_inventory\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"ceos1\", \"fceos4\"], \"nbdata\": True},\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert (\n            \"ceos1\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for ceos1\"\n        assert (\n            \"fceos4\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for fceos4\"\n        for device, data in res[\"result\"][\"hosts\"].items():\n            assert all(\n                k in data for k in [\"data\", \"hostname\", \"platform\"]\n            ), f\"{worker}:{device} not all device data returned\"\n            assert all(\n                k in data[\"data\"]\n                for k in [\n                    \"airflow\",\n                    \"asset_tag\",\n                    \"config_context\",\n                    \"custom_field_data\",\n                    \"device_type\",\n                    \"last_updated\",\n                    \"location\",\n                    \"platform\",\n                    \"position\",\n                    \"primary_ip4\",\n                    \"primary_ip6\",\n                    \"rack\",\n                    \"role\",\n                    \"serial\",\n                    \"site\",\n                    \"status\",\n                    \"tags\",\n                    \"tenant\",\n                ]\n            ), f\"{worker}:{device} not all nbdata returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-with-devices-add-interfaces","title":"Test With Devices Add Interfaces","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_with_devices_add_interfaces(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_nornir_inventory\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"ceos1\", \"fceos4\"], \"interfaces\": True},\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert (\n            \"ceos1\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for ceos1\"\n        assert (\n            \"fceos4\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for fceos4\"\n        for device, data in res[\"result\"][\"hosts\"].items():\n            assert data[\"data\"][\n                \"interfaces\"\n            ], f\"{worker}:{device} no interfaces data returned\"\n            for intf_name, intf_data in data[\"data\"][\"interfaces\"].items():\n                assert all(\n                    k in intf_data\n                    for k in [\n                        \"vrf\",\n                        \"mode\",\n                        \"description\",\n                    ]\n                ), f\"{worker}:{device}:{intf_name} not all interface data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-with-devices-add-interfaces-with-ip-and-inventory","title":"Test With Devices Add Interfaces With Ip And Inventory","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_with_devices_add_interfaces_with_ip_and_inventory(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_nornir_inventory\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"ceos1\", \"fceos4\"],\n            \"interfaces\": {\"ip_addresses\": True, \"inventory_items\": True},\n        },\n    )\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert (\n            \"ceos1\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for ceos1\"\n        assert (\n            \"fceos4\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for fceos4\"\n        for device, data in res[\"result\"][\"hosts\"].items():\n            assert data[\"data\"][\n                \"interfaces\"\n            ], f\"{worker}:{device} no interfaces data returned\"\n            for intf_name, intf_data in data[\"data\"][\"interfaces\"].items():\n                assert (\n                    \"ip_addresses\" in intf_data\n                ), f\"{worker}:{device}:{intf_name} no ip addresses data returned\"\n                assert (\n                    \"inventory_items\" in intf_data\n                ), f\"{worker}:{device}:{intf_name} no invetnory data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-with-devices-add-connections","title":"Test With Devices Add Connections","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_with_devices_add_connections(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_nornir_inventory\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"fceos4\", \"fceos5\"], \"connections\": True},\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert (\n            \"fceos5\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for fceos5\"\n        assert (\n            \"fceos4\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for fceos4\"\n        for device, data in res[\"result\"][\"hosts\"].items():\n            assert data[\"data\"][\n                \"connections\"\n            ], f\"{worker}:{device} no connections data returned\"\n            for intf_name, intf_data in data[\"data\"][\"connections\"].items():\n                assert all(\n                    k in intf_data\n                    for k in [\n                        \"remote_interface\",\n                        \"remote_device\",\n                    ]\n                ), f\"{worker}:{device}:{intf_name} not all connection data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-with-devices-add-bgp-peerings","title":"Test With Devices Add Bgp Peerings","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_with_devices_add_bgp_peerings(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_nornir_inventory\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"fceos4\", \"fceos5\"], \"bgp_peerings\": True},\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert (\n            \"fceos5\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for fceos5\"\n        assert (\n            \"fceos4\" in res[\"result\"][\"hosts\"]\n        ), f\"{worker} returned no results for fceos4\"\n        for device, data in res[\"result\"][\"hosts\"].items():\n            assert data[\"data\"][\n                \"bgp_peerings\"\n            ], f\"{worker}:{device} no bgp_peerings data returned\"\n            for peering, peering_data in data[\"data\"][\"bgp_peerings\"].items():\n                assert all(\n                    k in peering_data\n                    for k in [\n                        \"id\",\n                        \"name\",\n                    ]\n                ), f\"{worker}:{device}:{peering} not all peerings data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#testgetcircuits","title":"TestGetCircuits","text":"<p>Circuit information retrieval from Netbox.</p> <p>Purpose: Validate circuit data queries and filtering.</p>"},{"location":"testing/netbox_service_tests/#test-get-circuits-dry-run","title":"Test Get Circuits Dry Run","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_circuits_dry_run(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    if self.nb_version[0] == 3:\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"get_circuits\",\n            workers=\"any\",\n            kwargs={\n                \"devices\": [\"fceos4\", \"fceos5\"],\n                \"dry_run\": True,\n            },\n        )\n        pprint.pprint(ret, width=200)\n        for worker, res in ret.items():\n            assert res[\"result\"][\"data\"] == (\n                '{\"query\": \"query {circuit_list(site: '\n                + '[\\\\\"saltnornir-lab\\\\\"]) {cid tags {name} '\n                + \"provider {name} commit_rate description status \"\n                + \"type {name} provider_account {name} tenant \"\n                + \"{name} termination_a {id} termination_z {id} \"\n                + 'custom_fields comments}}\"}'\n            ), f\"{worker} did not return correct query string\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-circuits","title":"Test Get Circuits","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_circuits(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_circuits\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"fceos4\", \"fceos5\"],\n        },\n    )\n    pprint.pprint(ret, width=200)\n    for worker, res in ret.items():\n        assert \"fceos5\" in res[\"result\"], f\"{worker} returned no results for fceos5\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        for device, device_data in res[\"result\"].items():\n            assert device_data, f\"{worker}:{device} no circuit data returned\"\n            for cid, cid_data in device_data.items():\n                if cid == \"CID3\":\n                    assert all(\n                        k in cid_data\n                        for k in [\n                            \"tags\",\n                            \"provider\",\n                            \"commit_rate\",\n                            \"description\",\n                            \"status\",\n                            \"type\",\n                            \"provider_account\",\n                            \"tenant\",\n                            \"custom_fields\",\n                            \"comments\",\n                            \"provider_account\",\n                            \"provider_network\",\n                        ]\n                    ), f\"{worker}:{device}:{cid} not all circuit data returned\"\n                else:\n                    assert all(\n                        k in cid_data\n                        for k in [\n                            \"tags\",\n                            \"provider\",\n                            \"commit_rate\",\n                            \"description\",\n                            \"status\",\n                            \"type\",\n                            \"provider_account\",\n                            \"tenant\",\n                            \"custom_fields\",\n                            \"comments\",\n                            \"remote_device\",\n                            \"remote_interface\",\n                        ]\n                    ), f\"{worker}:{device}:{cid} not all circuit data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-circuits-by-cid","title":"Test Get Circuits By Cid","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_circuits_by_cid(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_circuits\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"fceos4\", \"fceos5\"], \"cid\": [\"CID1\"]},\n    )\n    pprint.pprint(ret, width=200)\n    for worker, res in ret.items():\n        assert \"fceos5\" in res[\"result\"], f\"{worker} returned no results for fceos5\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        for device, device_data in res[\"result\"].items():\n            assert device_data, f\"{worker}:{device} no circuit data returned\"\n            for cid, cid_data in device_data.items():\n                assert (\n                    cid == \"CID1\"\n                ), f\"{worker}:{device}:{cid} wrong circuit returned, was expecting 'CID1' only\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-circuits-cache","title":"Test Get Circuits Cache","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>@pytest.mark.parametrize(\"cache\", cache_options)\ndef test_get_circuits_cache(self, nfclient, cache):\n    print(f\"cache: {cache}\")\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_circuits\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"fceos4\", \"fceos5\"], \"cache\": cache},\n    )\n    pprint.pprint(ret, width=200)\n    for worker, res in ret.items():\n        assert \"fceos5\" in res[\"result\"], f\"{worker} returned no results for fceos5\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} returned no results for fceos4\"\n        for device, device_data in res[\"result\"].items():\n            assert device_data, f\"{worker}:{device} no circuit data returned\"\n            for cid, cid_data in device_data.items():\n                if cid == \"CID3\":\n                    assert all(\n                        k in cid_data\n                        for k in [\n                            \"tags\",\n                            \"provider\",\n                            \"commit_rate\",\n                            \"description\",\n                            \"status\",\n                            \"type\",\n                            \"provider_account\",\n                            \"tenant\",\n                            \"custom_fields\",\n                            \"comments\",\n                            \"provider_account\",\n                            \"provider_network\",\n                        ]\n                    ), f\"{worker}:{device}:{cid} not all circuit data returned\"\n                else:\n                    assert all(\n                        k in cid_data\n                        for k in [\n                            \"tags\",\n                            \"provider\",\n                            \"commit_rate\",\n                            \"description\",\n                            \"status\",\n                            \"type\",\n                            \"provider_account\",\n                            \"tenant\",\n                            \"custom_fields\",\n                            \"comments\",\n                            \"remote_device\",\n                            \"remote_interface\",\n                        ]\n                    ), f\"{worker}:{device}:{cid} not all circuit data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-circuits-cache-content","title":"Test Get Circuits Cache Content","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_circuits_cache_content(self, nfclient):\n    circuits_cache = nfclient.run_job(\n        \"netbox\",\n        \"cache_get\",\n        workers=\"all\",\n        kwargs={\"keys\": \"get_circuits*\"},\n    )\n\n    pprint.pprint(circuits_cache)\n\n    for worker, res in circuits_cache.items():\n        if \"get_circuits::CID1\" in res[\"result\"]:\n            assert (\n                res[\"result\"][\"get_circuits::CID1\"][\"fceos4\"][\"remote_device\"]\n                == \"fceos5\"\n            )\n            assert (\n                res[\"result\"][\"get_circuits::CID1\"][\"fceos5\"][\"remote_device\"]\n                == \"fceos4\"\n            )\n        if \"get_circuits::CID2\" in res[\"result\"]:\n            assert (\n                res[\"result\"][\"get_circuits::CID2\"][\"fceos4\"][\"remote_device\"]\n                == \"fceos5\"\n            )\n            assert (\n                res[\"result\"][\"get_circuits::CID2\"][\"fceos5\"][\"remote_device\"]\n                == \"fceos4\"\n            )\n        if \"get_circuits::CID3\" in res[\"result\"]:\n            assert (\n                res[\"result\"][\"get_circuits::CID3\"][\"fceos4\"][\"provider_network\"]\n                == \"Provider1-Net1\"\n            )\n</code></pre>"},{"location":"testing/netbox_service_tests/#testgetbgppeerings","title":"TestGetBgpPeerings","text":"<p>BGP peering information retrieval.</p> <p>Purpose: Validate BGP peering data extraction and filtering.</p>"},{"location":"testing/netbox_service_tests/#test-get-bgp-peerings","title":"Test Get Bgp Peerings","text":"<p>Test basic BGP peerings retrieval</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_bgp_peerings(self, nfclient):\n    \"\"\"Test basic BGP peerings retrieval\"\"\"\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_bgp_peerings\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"fceos4\", \"fceos5\"]},\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert res[\"result\"], f\"{worker} returned no results\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} missing fceos4 in results\"\n        assert \"fceos5\" in res[\"result\"], f\"{worker} missing fceos5 in results\"\n        # Check that each device has a dictionary (may be empty if no BGP sessions)\n        for device, bgp_sessions in res[\"result\"].items():\n            assert isinstance(\n                bgp_sessions, dict\n            ), f\"{worker}:{device} BGP sessions should be a dictionary\"\n            # If there are BGP sessions, verify the structure\n            for session_name, session_data in bgp_sessions.items():\n                assert isinstance(\n                    session_data, dict\n                ), f\"{worker}:{device}:{session_name} session data should be a dictionary\"\n\n                # Verify required top-level fields\n                required_fields = [\n                    \"id\",\n                    \"name\",\n                    \"description\",\n                    \"device\",\n                    \"local_address\",\n                    \"local_as\",\n                    \"remote_address\",\n                    \"remote_as\",\n                    \"status\",\n                    \"last_updated\",\n                    \"created\",\n                    \"url\",\n                    \"display\",\n                    \"site\",\n                    \"tenant\",\n                    \"tags\",\n                    \"comments\",\n                    \"custom_fields\",\n                ]\n                for field in required_fields:\n                    assert (\n                        field in session_data\n                    ), f\"{worker}:{device}:{session_name} missing field '{field}'\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-bgp-peerings-with-instance","title":"Test Get Bgp Peerings With Instance","text":"<p>Test BGP peerings retrieval with explicit instance</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_bgp_peerings_with_instance(self, nfclient):\n    \"\"\"Test BGP peerings retrieval with explicit instance\"\"\"\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_bgp_peerings\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"fceos4\", \"fceos5\"], \"instance\": \"prod\"},\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert res[\"result\"], f\"{worker} returned no results\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} missing fceos4 in results\"\n        assert \"fceos5\" in res[\"result\"], f\"{worker} missing fceos5 in results\"\n        # Check that each device has a dictionary (may be empty if no BGP sessions)\n        for device, bgp_sessions in res[\"result\"].items():\n            assert isinstance(\n                bgp_sessions, dict\n            ), f\"{worker}:{device} BGP sessions should be a dictionary\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-bgp-peerings-nonexistent-device","title":"Test Get Bgp Peerings Nonexistent Device","text":"<p>Test error handling for non-existent device</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_bgp_peerings_nonexistent_device(self, nfclient):\n    \"\"\"Test error handling for non-existent device\"\"\"\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_bgp_peerings\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"nonexistent-device-12345\"]},\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert res[\"result\"], f\"{worker} returned no results\"\n        assert (\n            \"nonexistent-device-12345\" in res[\"result\"]\n        ), f\"{worker} should have entry for nonexistent device\"\n        # The result for non-existent device should be empty dict\n        assert (\n            res[\"result\"][\"nonexistent-device-12345\"] == {}\n        ), f\"{worker} should return empty dict for nonexistent device\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-bgp-peerings-empty-devices-list","title":"Test Get Bgp Peerings Empty Devices List","text":"<p>Test with empty devices list</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_bgp_peerings_empty_devices_list(self, nfclient):\n    \"\"\"Test with empty devices list\"\"\"\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_bgp_peerings\",\n        workers=\"any\",\n        kwargs={\"devices\": []},\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert isinstance(\n            res[\"result\"], dict\n        ), f\"{worker} should return a dictionary\"\n        assert (\n            len(res[\"result\"]) == 0\n        ), f\"{worker} should return empty dict for empty devices list\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-bgp-peerings-cache-true","title":"Test Get Bgp Peerings Cache True","text":"<p>Test cache content for BGP peerings</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_bgp_peerings_cache_true(self, nfclient):\n    \"\"\"Test cache content for BGP peerings\"\"\"\n    # get cache brief info\n    cache_before = nfclient.run_job(\n        \"netbox\",\n        \"cache_list\",\n        workers=[\"netbox-worker-1.1\"],\n        kwargs={\"keys\": \"get_bgp_peerings*\", \"details\": True},\n    )\n\n    # Clear any existing cache\n    nfclient.run_job(\n        \"netbox\",\n        \"cache_clear\",\n        workers=[\"netbox-worker-1.1\"],\n        kwargs={\"keys\": \"get_bgp_peerings*\"},\n    )\n\n    # cache data\n    nfclient.run_job(\n        \"netbox\",\n        \"get_bgp_peerings\",\n        workers=[\"netbox-worker-1.1\"],\n        kwargs={\"devices\": [\"fceos4\", \"fceos5\"], \"cache\": True},\n    )\n\n    # Now retrieve cache content\n    cache_after = nfclient.run_job(\n        \"netbox\",\n        \"cache_list\",\n        workers=[\"netbox-worker-1.1\"],\n        kwargs={\"keys\": \"get_bgp_peerings*\", \"details\": True},\n    )\n\n    print(\"cache_before:\")\n    pprint.pprint(cache_before, width=200)\n\n    print(\"cache_after:\")\n    pprint.pprint(cache_after, width=200)\n\n    for worker, res in cache_after.items():\n        for cache_item_after in res[\"result\"]:\n            key = cache_item_after[\"key\"]\n            for cache_item_before in cache_before[worker][\"result\"]:\n                if cache_item_before[\"key\"] == key:\n                    assert (\n                        cache_item_before[\"creation\"]\n                        != cache_item_after[\"creation\"]\n                    ), f\"{worker}:{key} cache not re-created\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-bgp-peerings-cache-refresh","title":"Test Get Bgp Peerings Cache Refresh","text":"<p>Test cache content for BGP peerings</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_bgp_peerings_cache_refresh(self, nfclient):\n    \"\"\"Test cache content for BGP peerings\"\"\"\n    # get cache brief info\n    cache_before = nfclient.run_job(\n        \"netbox\",\n        \"cache_list\",\n        workers=[\"netbox-worker-1.1\"],\n        kwargs={\"keys\": \"get_bgp_peerings*\", \"details\": True},\n    )\n\n    # cache data\n    nfclient.run_job(\n        \"netbox\",\n        \"get_bgp_peerings\",\n        workers=[\"netbox-worker-1.1\"],\n        kwargs={\"devices\": [\"fceos4\", \"fceos5\"], \"cache\": \"refresh\"},\n    )\n\n    # Now retrieve cache content\n    cache_after = nfclient.run_job(\n        \"netbox\",\n        \"cache_list\",\n        workers=[\"netbox-worker-1.1\"],\n        kwargs={\"keys\": \"get_bgp_peerings*\", \"details\": True},\n    )\n\n    print(\"cache_before:\")\n    pprint.pprint(cache_before, width=200)\n\n    print(\"cache_after:\")\n    pprint.pprint(cache_after, width=200)\n\n    for worker, res in cache_after.items():\n        for cache_item_after in res[\"result\"]:\n            key = cache_item_after[\"key\"]\n            for cache_item_before in cache_before[worker][\"result\"]:\n                if cache_item_before[\"key\"] == key:\n                    assert (\n                        cache_item_before[\"creation\"]\n                        != cache_item_after[\"creation\"]\n                    ), f\"{worker}:{key} cache not re-created\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-bgp-peerings-cache-force","title":"Test Get Bgp Peerings Cache Force","text":"<p>Test cache force mode (use cache without checking)</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_bgp_peerings_cache_force(self, nfclient):\n    \"\"\"Test cache force mode (use cache without checking)\"\"\"\n    # First, populate cache\n    nfclient.run_job(\n        \"netbox\",\n        \"get_bgp_peerings\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"fceos4\"], \"cache\": True},\n    )\n\n    # Use cache=\"force\" to retrieve from cache only\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_bgp_peerings\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"fceos4\"], \"cache\": \"force\"},\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert res[\"result\"], f\"{worker} returned no results\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} missing fceos4 in results\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-bgp-peerings-cache-false","title":"Test Get Bgp Peerings Cache False","text":"<p>Test with cache disabled</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_bgp_peerings_cache_false(self, nfclient):\n    \"\"\"Test with cache disabled\"\"\"\n    # Clear any existing cache\n    nfclient.run_job(\n        \"netbox\",\n        \"cache_clear\",\n        workers=\"all\",\n        kwargs={\"keys\": \"get_bgp_peerings*\"},\n    )\n\n    # Fetch with cache=False\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_bgp_peerings\",\n        workers=\"any\",\n        kwargs={\"devices\": [\"fceos4\"], \"cache\": False},\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert res[\"result\"], f\"{worker} returned no results\"\n        assert \"fceos4\" in res[\"result\"], f\"{worker} missing fceos4 in results\"\n\n    # Verify nothing was cached\n    bgp_cache = nfclient.run_job(\n        \"netbox\",\n        \"cache_get\",\n        workers=\"all\",\n        kwargs={\"keys\": \"get_bgp_peerings::fceos4\"},\n    )\n\n    for worker, res in bgp_cache.items():\n        # Cache should be empty or the key should not exist\n        assert (\n            \"get_bgp_peerings::fceos4\" not in res[\"result\"]\n        ), f\"{worker} should not have cached data when cache=False\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#testsyncdevicefacts","title":"TestSyncDeviceFacts","text":"<p>Device fact synchronization from external sources to Netbox.</p> <p>Purpose: Validate pushing collected device facts (from Nornir) back to Netbox.</p>"},{"location":"testing/netbox_service_tests/#test-sync-device-facts-basic-update","title":"Test Sync Device Facts Basic Update","text":"<p>Test basic sync with devices list - updates serial numbers</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_facts_basic_update(self, nfclient):\n    \"\"\"Test basic sync with devices list - updates serial numbers\"\"\"\n    # Setup: update serial for spine to force a change\n    pynb = get_pynetbox(nfclient)\n    nb_device = pynb.dcim.devices.get(name=\"ceos-spine-1\")\n    nb_device.serial = \"123456\"\n    nb_device.save()\n\n    # Execute sync job\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_facts\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\", \"ceos-spine-2\"],\n        },\n    )\n    pprint.pprint(ret, width=200)\n\n    # Verify results\n    for worker, res in ret.items():\n        assert not res[\"failed\"], f\"{worker} failed - {res.get('errors')}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"ceos-spine-2\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-2\"\n\n        # ceos-spine-1 should have been updated\n        assert res[\"result\"][\"ceos-spine-1\"][\n            \"sync_device_facts\"\n        ], \"ceos-spine-1 no data returned\"\n        assert res[\"result\"][\"ceos-spine-1\"][\"sync_device_facts\"][\n            \"serial\"\n        ], \"ceos-spine-1 serial not synced\"\n\n        # ceos-spine-2 should be in sync or updated\n        assert res[\"result\"][\"ceos-spine-2\"][\n            \"sync_device_facts\"\n        ], \"ceos-spine-2 no data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-facts-already-in-sync","title":"Test Sync Device Facts Already In Sync","text":"<p>Test when device facts are already synchronized</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_facts_already_in_sync(self, nfclient):\n    \"\"\"Test when device facts are already synchronized\"\"\"\n    # First sync to ensure devices are up to date\n    nfclient.run_job(\n        \"netbox\",\n        \"sync_device_facts\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\"],\n        },\n    )\n\n    # Second sync should show devices in sync\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_facts\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\"],\n        },\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert not res[\"failed\"], f\"{worker} failed\"\n        assert (\n            res[\"result\"][\"ceos-spine-1\"][\"sync_device_facts\"]\n            == \"Device facts in sync\"\n        ), \"Expected device to be in sync\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-facts-with-filters","title":"Test Sync Device Facts With Filters","text":"<p>Test sync using Nornir filters instead of device list</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_facts_with_filters(self, nfclient):\n    \"\"\"Test sync using Nornir filters instead of device list\"\"\"\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_facts\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"FC\": \"spine\",\n        },\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert not res[\"failed\"], f\"{worker} failed - {res.get('errors')}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"ceos-spine-2\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-2\"\n\n        for device, device_data in res[\"result\"].items():\n            assert (\n                \"sync_device_facts\" in device_data\n            ), f\"{worker}:{device} no sync data\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-facts-dry-run","title":"Test Sync Device Facts Dry Run","text":"<p>Test dry run mode - should not modify NetBox</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_facts_dry_run(self, nfclient):\n    \"\"\"Test dry run mode - should not modify NetBox\"\"\"\n    # Setup: change serial to create a difference\n    pynb = get_pynetbox(nfclient)\n    nb_device = pynb.dcim.devices.get(name=\"ceos-spine-1\")\n    original_serial = nb_device.serial\n    nb_device.serial = \"DRY-RUN-TEST-123\"\n    nb_device.save()\n\n    # Execute dry run\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_facts\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\", \"ceos-spine-2\"],\n            \"dry_run\": True,\n        },\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert not res[\"failed\"], f\"{worker} failed\"\n        assert res[\"dry_run\"] is True, \"dry_run flag not set in result\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"ceos-spine-2\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-2\"\n\n        for device, device_data in res[\"result\"].items():\n            assert (\n                \"sync_device_facts_dry_run\" in device_data\n            ), f\"{worker}:{device} no dry run data\"\n\n    # Verify NetBox was not modified\n    nb_device = pynb.dcim.devices.get(name=\"ceos-spine-1\")\n    assert (\n        nb_device.serial == \"DRY-RUN-TEST-123\"\n    ), \"NetBox was modified during dry run\"\n\n    # Cleanup\n    nb_device.serial = original_serial\n    nb_device.save()\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-facts-with-diff","title":"Test Sync Device Facts With Diff","text":"<p>Test that diff is properly populated when changes are made</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_facts_with_diff(self, nfclient):\n    \"\"\"Test that diff is properly populated when changes are made\"\"\"\n    # Setup: force a change\n    pynb = get_pynetbox(nfclient)\n    nb_device = pynb.dcim.devices.get(name=\"ceos-spine-1\")\n    nb_device.serial = \"OLD-SERIAL-123\"\n    nb_device.save()\n\n    # Execute sync\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_facts\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\"],\n        },\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert not res[\"failed\"], f\"{worker} failed\"\n        if (\n            res[\"result\"][\"ceos-spine-1\"][\"sync_device_facts\"]\n            != \"Device facts in sync\"\n        ):\n            assert (\n                \"ceos-spine-1\" in res[\"diff\"]\n            ), \"diff not populated for changed device\"\n            assert (\n                \"serial\" in res[\"diff\"][\"ceos-spine-1\"]\n            ), \"serial field not in diff\"\n            assert (\n                \"-\" in res[\"diff\"][\"ceos-spine-1\"][\"serial\"]\n            ), \"old value (-) not in diff\"\n            assert (\n                \"+\" in res[\"diff\"][\"ceos-spine-1\"][\"serial\"]\n            ), \"new value (+) not in diff\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-facts-with-branch","title":"Test Sync Device Facts With Branch","text":"<p>Test sync with NetBox branching plugin</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_facts_with_branch(self, nfclient):\n    \"\"\"Test sync with NetBox branching plugin\"\"\"\n    delete_branch(\"sync_facts_test_branch\", nfclient)\n\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_facts\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\", \"ceos-spine-2\"],\n            \"branch\": \"sync_facts_test_branch\",\n        },\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert not res[\"failed\"], f\"{worker} failed - {res.get('errors')}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"ceos-spine-2\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-2\"\n\n        for device, device_data in res[\"result\"].items():\n            assert \"branch\" in device_data, f\"{worker}:{device} no branch info\"\n            assert (\n                device_data[\"branch\"] == \"sync_facts_test_branch\"\n            ), f\"Wrong branch name\"\n\n    # Cleanup\n    delete_branch(\"sync_facts_test_branch\", nfclient)\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-facts-with-custom-instance","title":"Test Sync Device Facts With Custom Instance","text":"<p>Test sync with explicit NetBox instance</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_facts_with_custom_instance(self, nfclient):\n    \"\"\"Test sync with explicit NetBox instance\"\"\"\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_facts\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\"],\n            \"instance\": \"prod\",\n        },\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert not res[\"failed\"], f\"{worker} failed\"\n        assert \"prod\" in res[\"resources\"], \"instance not in resources\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-facts-with-batch-size","title":"Test Sync Device Facts With Batch Size","text":"<p>Test sync with custom batch size</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_facts_with_batch_size(self, nfclient):\n    \"\"\"Test sync with custom batch size\"\"\"\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_facts\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\", \"ceos-spine-2\"],\n            \"batch_size\": 1,  # Process one device at a time\n        },\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert not res[\"failed\"], f\"{worker} failed\"\n        assert \"ceos-spine-1\" in res[\"result\"], \"ceos-spine-1 not processed\"\n        assert \"ceos-spine-2\" in res[\"result\"], \"ceos-spine-2 not processed\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-facts-with-timeout","title":"Test Sync Device Facts With Timeout","text":"<p>Test sync with custom timeout</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_facts_with_timeout(self, nfclient):\n    \"\"\"Test sync with custom timeout\"\"\"\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_facts\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\"],\n            \"timeout\": 120,\n        },\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert not res[\"failed\"], f\"{worker} failed\"\n        assert \"ceos-spine-1\" in res[\"result\"], \"ceos-spine-1 not processed\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-facts-non-existing-device","title":"Test Sync Device Facts Non Existing Device","text":"<p>Test error handling when device doesn't exist in NetBox</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_facts_non_existing_device(self, nfclient):\n    \"\"\"Test error handling when device doesn't exist in NetBox\"\"\"\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_facts\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"nonexistent-device-12345\"],\n        },\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        # Should fail or report error\n        assert res[\"errors\"], \"Should report error for non-existent device\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-facts-empty-device-list","title":"Test Sync Device Facts Empty Device List","text":"<p>Test sync with empty device list and no filters</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_facts_empty_device_list(self, nfclient):\n    \"\"\"Test sync with empty device list and no filters\"\"\"\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_facts\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [],\n        },\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        # Should complete without error, just process no devices\n        assert not res[\"failed\"], f\"{worker} failed\"\n        assert (\n            res[\"result\"] == {} or len(res[\"result\"]) == 0\n        ), \"Should have empty result\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-facts-single-device","title":"Test Sync Device Facts Single Device","text":"<p>Test sync with a single device</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_facts_single_device(self, nfclient):\n    \"\"\"Test sync with a single device\"\"\"\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_facts\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\"],\n        },\n    )\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert not res[\"failed\"], f\"{worker} failed\"\n        assert \"ceos-spine-1\" in res[\"result\"], \"Device not in result\"\n        assert len(res[\"result\"]) == 1, \"Should only process one device\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#testsyncdeviceinterfaces","title":"TestSyncDeviceInterfaces","text":"<p>Interface status and configuration synchronization.</p> <p>Purpose: Validate pushing interface data from network devices to Netbox.</p>"},{"location":"testing/netbox_service_tests/#test-sync-device-interfaces","title":"Test Sync Device Interfaces","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_interfaces(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\", \"ceos-spine-2\"],\n        },\n    )\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"ceos-spine-2\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-2\"\n        for device, device_data in res[\"result\"].items():\n            assert device_data[\n                \"sync_device_interfaces\"\n            ], f\"{worker}:{device} no interfaces updated data\"\n            assert (\n                \"created_device_interfaces\" in device_data\n            ), f\"{worker}:{device} no interfaces created data\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-interfaces-dry-run","title":"Test Sync Device Interfaces Dry Run","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_interfaces_dry_run(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\", \"ceos-spine-2\"],\n            \"dry_run\": True,\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"ceos-spine-2\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-2\"\n        for device, device_data in res[\"result\"].items():\n            assert device_data[\n                \"sync_device_interfaces_dry_run\"\n            ], f\"{worker}:{device} no interfaces updated data\"\n            assert (\n                \"created_device_interfaces_dry_run\" in device_data\n            ), f\"{worker}:{device} no interfaces created data\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-interfaces-create","title":"Test Sync Device Interfaces Create","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_interfaces_create(self, nfclient):\n    delete_interfaces(nfclient, \"ceos-spine-2\", \"Loopback123\")\n\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-2\"],\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        for device, device_data in res[\"result\"].items():\n            assert device_data[\n                \"sync_device_interfaces\"\n            ], f\"{worker}:{device} no interfaces updated data\"\n            assert (\n                \"Loopback123\" in device_data[\"created_device_interfaces\"]\n            ), f\"{worker}:{device} Loopback123 interface not created\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-interfaces-update","title":"Test Sync Device Interfaces Update","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_interfaces_update(self, nfclient):\n    resp_get = nfclient.run_job(\n        \"netbox\",\n        \"rest\",\n        workers=\"any\",\n        kwargs={\n            \"method\": \"get\",\n            \"api\": \"/dcim/interfaces/\",\n            \"params\": {\"device\": \"ceos-spine-2\", \"name\": \"Loopback123\"},\n        },\n    )\n    worker, interfaces = tuple(resp_get.items())[0]\n    intf_id = interfaces[\"result\"][\"results\"][0][\"id\"]\n    resp_patch = nfclient.run_job(\n        \"netbox\",\n        \"rest\",\n        workers=\"any\",\n        kwargs={\n            \"method\": \"patch\",\n            \"api\": f\"/dcim/interfaces/{intf_id}\",\n            \"json\": {\"description\": \"foo\"},\n        },\n    )\n    print(\"Updated interface description:\")\n    pprint.pprint(resp_patch)\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-2\"],\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        for device, device_data in res[\"result\"].items():\n            assert device_data[\"sync_device_interfaces\"][\"Loopback123\"][\n                \"description\"\n            ], f\"{worker}:{device} Loopback123 not updated\"\n        assert (\n            res[\"diff\"][\"ceos-spine-2\"][\"Loopback123\"][\"description\"][\"-\"] == \"foo\"\n        ), f\"Interface description diff not populated\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-interfaces-non-existing-device","title":"Test Sync Device Interfaces Non Existing Device","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>@pytest.mark.skip(reason=\"TBD\")\ndef test_sync_device_interfaces_non_existing_device(self, nfclient):\n    pass\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-interfaces-with-branch","title":"Test Sync Device Interfaces With Branch","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_interfaces_with_branch(self, nfclient):\n    delete_branch(\"update_interfaces_branch_1\", nfclient)\n\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\", \"ceos-spine-2\"],\n            \"branch\": \"update_interfaces_branch_1\",\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"ceos-spine-2\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-2\"\n        for device, device_data in res[\"result\"].items():\n            assert device_data[\n                \"sync_device_interfaces\"\n            ], f\"{worker}:{device} no interfaces updated\"\n            assert device_data[\"branch\"] == \"update_interfaces_branch_1\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#testcreatedeviceinterfaces","title":"TestCreateDeviceInterfaces","text":"<p>Device interface creation in Netbox.</p> <p>Purpose: Validate programmatic interface creation and configuration.</p>"},{"location":"testing/netbox_service_tests/#test-create-device-interfaces-single","title":"Test Create Device Interfaces Single","text":"<p>Test creating a single interface on a device</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_device_interfaces_single(self, nfclient):\n    \"\"\"Test creating a single interface on a device\"\"\"\n    delete_interfaces(nfclient, \"ceos-spine-1\", \"TestInterface1\")\n\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"create_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"ceos-spine-1\"],\n            \"interface_name\": \"TestInterface1\",\n            \"interface_type\": \"virtual\",\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"TestInterface1\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n        ), f\"{worker} did not create TestInterface1\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-device-interfaces-multiple-devices","title":"Test Create Device Interfaces Multiple Devices","text":"<p>Test creating interfaces on multiple devices</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_device_interfaces_multiple_devices(self, nfclient):\n    \"\"\"Test creating interfaces on multiple devices\"\"\"\n    delete_interfaces(nfclient, \"ceos-spine-1\", \"TestInterface2\")\n    delete_interfaces(nfclient, \"ceos-spine-2\", \"TestInterface2\")\n\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"create_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"ceos-spine-1\", \"ceos-spine-2\"],\n            \"interface_name\": \"TestInterface2\",\n            \"interface_type\": \"virtual\",\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"ceos-spine-2\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-2\"\n        assert (\n            \"TestInterface2\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n        ), f\"{worker} did not create TestInterface2 on ceos-spine-1\"\n        assert (\n            \"TestInterface2\" in res[\"result\"][\"ceos-spine-2\"][\"created\"]\n        ), f\"{worker} did not create TestInterface2 on ceos-spine-2\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-device-interfaces-with-range-numeric","title":"Test Create Device Interfaces With Range Numeric","text":"<p>Test creating interfaces with numeric range expansion</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_device_interfaces_with_range_numeric(self, nfclient):\n    \"\"\"Test creating interfaces with numeric range expansion\"\"\"\n    for i in range(1, 4):\n        delete_interfaces(nfclient, \"ceos-spine-1\", f\"Loopback{i}\")\n\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"create_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"ceos-spine-1\"],\n            \"interface_name\": \"Loopback[1-3]\",\n            \"interface_type\": \"virtual\",\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            len(res[\"result\"][\"ceos-spine-1\"][\"created\"]) == 3\n        ), f\"{worker} did not create 3 interfaces\"\n        assert (\n            \"Loopback1\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n        ), f\"{worker} did not create Loopback1\"\n        assert (\n            \"Loopback2\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n        ), f\"{worker} did not create Loopback2\"\n        assert (\n            \"Loopback3\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n        ), f\"{worker} did not create Loopback3\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-device-interfaces-with-range-list","title":"Test Create Device Interfaces With Range List","text":"<p>Test creating interfaces with comma-separated list expansion</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_device_interfaces_with_range_list(self, nfclient):\n    \"\"\"Test creating interfaces with comma-separated list expansion\"\"\"\n    delete_interfaces(nfclient, \"ceos-spine-1\", \"ge-0/0/0\")\n    delete_interfaces(nfclient, \"ceos-spine-1\", \"xe-0/0/0\")\n\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"create_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"ceos-spine-1\"],\n            \"interface_name\": \"[ge,xe]-0/0/0\",\n            \"interface_type\": \"1000base-t\",\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            len(res[\"result\"][\"ceos-spine-1\"][\"created\"]) == 2\n        ), f\"{worker} did not create 2 interfaces\"\n        assert (\n            \"ge-0/0/0\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n        ), f\"{worker} did not create ge-0/0/0\"\n        assert (\n            \"xe-0/0/0\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n        ), f\"{worker} did not create xe-0/0/0\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-device-interfaces-with-multiple-ranges","title":"Test Create Device Interfaces With Multiple Ranges","text":"<p>Test creating interfaces with multiple range patterns</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_device_interfaces_with_multiple_ranges(self, nfclient):\n    \"\"\"Test creating interfaces with multiple range patterns\"\"\"\n    for prefix in [\"ge\", \"xe\"]:\n        for i in range(0, 2):\n            delete_interfaces(nfclient, \"ceos-spine-1\", f\"{prefix}-0/0/{i}\")\n\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"create_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"ceos-spine-1\"],\n            \"interface_name\": \"[ge,xe]-0/0/[0-1]\",\n            \"interface_type\": \"1000base-t\",\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            len(res[\"result\"][\"ceos-spine-1\"][\"created\"]) == 4\n        ), f\"{worker} did not create 4 interfaces\"\n        assert \"ge-0/0/0\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n        assert \"ge-0/0/1\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n        assert \"xe-0/0/0\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n        assert \"xe-0/0/1\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-device-interfaces-multiple-names-list","title":"Test Create Device Interfaces Multiple Names List","text":"<p>Test creating multiple interfaces passed as a list</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_device_interfaces_multiple_names_list(self, nfclient):\n    \"\"\"Test creating multiple interfaces passed as a list\"\"\"\n    delete_interfaces(nfclient, \"ceos-spine-1\", \"TestIntf1\")\n    delete_interfaces(nfclient, \"ceos-spine-1\", \"TestIntf2\")\n\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"create_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"ceos-spine-1\"],\n            \"interface_name\": [\"TestIntf1\", \"TestIntf2\"],\n            \"interface_type\": \"virtual\",\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            len(res[\"result\"][\"ceos-spine-1\"][\"created\"]) == 2\n        ), f\"{worker} did not create 2 interfaces\"\n        assert \"TestIntf1\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n        assert \"TestIntf2\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-device-interfaces-skip-existing","title":"Test Create Device Interfaces Skip Existing","text":"<p>Test that existing interfaces are skipped</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_device_interfaces_skip_existing(self, nfclient):\n    \"\"\"Test that existing interfaces are skipped\"\"\"\n    # First create the interface\n    ret1 = nfclient.run_job(\n        \"netbox\",\n        \"create_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"ceos-spine-1\"],\n            \"interface_name\": \"TestExisting\",\n            \"interface_type\": \"virtual\",\n        },\n    )\n\n    # Try to create it again\n    ret2 = nfclient.run_job(\n        \"netbox\",\n        \"create_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"ceos-spine-1\"],\n            \"interface_name\": \"TestExisting\",\n            \"interface_type\": \"virtual\",\n        },\n    )\n\n    pprint.pprint(ret2)\n    for worker, res in ret2.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"TestExisting\" in res[\"result\"][\"ceos-spine-1\"][\"skipped\"]\n        ), f\"{worker} did not skip existing TestExisting interface\"\n        assert (\n            len(res[\"result\"][\"ceos-spine-1\"][\"created\"]) == 0\n        ), f\"{worker} should not have created any interfaces\"\n\n    # Cleanup\n    delete_interfaces(nfclient, \"ceos-spine-1\", \"TestExisting\")\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-device-interfaces-dry-run","title":"Test Create Device Interfaces Dry Run","text":"<p>Test dry run mode</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_device_interfaces_dry_run(self, nfclient):\n    \"\"\"Test dry run mode\"\"\"\n    delete_interfaces(nfclient, \"ceos-spine-1\", \"TestDryRun\")\n\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"create_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"ceos-spine-1\"],\n            \"interface_name\": \"TestDryRun\",\n            \"interface_type\": \"virtual\",\n            \"dry_run\": True,\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"TestDryRun\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n        ), f\"{worker} did not mark TestDryRun for creation in dry run\"\n\n    # Verify interface was not actually created\n    resp_get = nfclient.run_job(\n        \"netbox\",\n        \"rest\",\n        workers=\"any\",\n        kwargs={\n            \"method\": \"get\",\n            \"api\": \"/dcim/interfaces/\",\n            \"params\": {\"device\": \"ceos-spine-1\", \"name\": \"TestDryRun\"},\n        },\n    )\n    worker, interfaces = tuple(resp_get.items())[0]\n    assert (\n        len(interfaces[\"result\"][\"results\"]) == 0\n    ), \"Interface should not exist after dry run\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-device-interfaces-with-branch","title":"Test Create Device Interfaces With Branch","text":"<p>Test creating interfaces with a branch</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_device_interfaces_with_branch(self, nfclient):\n    \"\"\"Test creating interfaces with a branch\"\"\"\n    delete_branch(\"create_interfaces_branch_1\", nfclient)\n    delete_interfaces(nfclient, \"ceos-spine-1\", \"TestBranch\")\n\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"create_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"ceos-spine-1\"],\n            \"interface_name\": \"TestBranch\",\n            \"interface_type\": \"virtual\",\n            \"branch\": \"create_interfaces_branch_1\",\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"TestBranch\" in res[\"result\"][\"ceos-spine-1\"][\"created\"]\n        ), f\"{worker} did not create TestBranch\"\n\n    # Cleanup\n    delete_interfaces(nfclient, \"ceos-spine-1\", \"TestBranch\")\n    delete_branch(\"create_interfaces_branch_1\", nfclient)\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-device-interfaces-non-existing-device","title":"Test Create Device Interfaces Non Existing Device","text":"<p>Test handling of non-existing device</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_device_interfaces_non_existing_device(self, nfclient):\n    \"\"\"Test handling of non-existing device\"\"\"\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"create_device_interfaces\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"nonexistent-device-12345\"],\n            \"interface_name\": \"TestInterface\",\n            \"interface_type\": \"virtual\",\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert len(res[\"errors\"]) &gt; 0, f\"{worker} should have errors\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#testsyncdeviceip","title":"TestSyncDeviceIP","text":"<p>IP address and interface IP synchronization.</p> <p>Purpose: Validate IP address assignment and synchronization to device interfaces.</p>"},{"location":"testing/netbox_service_tests/#test-sync-device-ip","title":"Test Sync Device Ip","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_ip(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_ip\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\", \"ceos-spine-2\"],\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"ceos-spine-2\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-2\"\n        for device, device_data in res[\"result\"].items():\n            assert (\n                \"created_ip\" in device_data\n            ), f\"{worker}:{device} no create ip data\"\n            assert \"sync_ip\" in device_data, f\"{worker}:{device} no update ip data\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-ip-dry-run","title":"Test Sync Device Ip Dry Run","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_ip_dry_run(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_ip\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\", \"ceos-spine-2\"],\n            \"dry_run\": True,\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"ceos-spine-2\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-2\"\n        for device, device_data in res[\"result\"].items():\n            assert (\n                \"created_ip_dry_run\" in device_data\n            ), f\"{worker}:{device} no create ip data\"\n            assert (\n                \"sync_ip_dry_run\" in device_data\n            ), f\"{worker}:{device} no update ip data\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-sync-device-ip-with-branch","title":"Test Sync Device Ip With Branch","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_sync_device_ip_with_branch(self, nfclient):\n    delete_branch(\"sync_device_ip_1\", nfclient)\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"sync_device_ip\",\n        workers=\"any\",\n        kwargs={\n            \"datasource\": \"nornir\",\n            \"devices\": [\"ceos-spine-1\", \"ceos-spine-2\"],\n            \"branch\": \"sync_device_ip_1\",\n        },\n    )\n\n    pprint.pprint(ret)\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} failed - {res}\"\n        assert (\n            \"ceos-spine-1\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-1\"\n        assert (\n            \"ceos-spine-2\" in res[\"result\"]\n        ), f\"{worker} returned no results for ceos-spine-2\"\n        for device, device_data in res[\"result\"].items():\n            assert (\n                \"created_ip\" in device_data\n            ), f\"{worker}:{device} no create ip data\"\n            assert \"sync_ip\" in device_data, f\"{worker}:{device} no update ip data\"\n            assert (\n                device_data[\"branch\"] == \"sync_device_ip_1\"\n            ), f\"{worker}:{device} has no branch info\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#testcreateip","title":"TestCreateIP","text":"<p>IP address creation in Netbox.</p> <p>Purpose: Validate IP address allocation and creation.</p>"},{"location":"testing/netbox_service_tests/#test-create-ip-by-prefix","title":"Test Create Ip By Prefix","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_by_prefix(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_ips(\"10.0.0.0/24\", nfclient)\n\n    rand = random.randint(1, 1000)\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"description\": f\"test create ip {rand}\",\n            },\n        )\n        create_2 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"description\": f\"test create ip {rand}\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n        print(\"create_2\")\n        pprint.pprint(create_2, width=200)\n\n        for worker, res1 in create_1.items():\n            assert res1[\"failed\"] == False, \"Allocation failed\"\n            assert res1[\"result\"][\"address\"], f\"Result has no ip {res1['result']}\"\n\n        worker, res2 = tuple(create_2.items())[0]\n        assert (\n            res1[\"result\"][\"address\"] == res2[\"result\"][\"address\"]\n        ), f\"Should have been same IP address\"\n        assert (\n            res1[\"result\"][\"description\"] == res2[\"result\"][\"description\"]\n        ), f\"Should have been same IP description\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-by-prefix-description","title":"Test Create Ip By Prefix Description","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_by_prefix_description(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_ips(\"10.0.0.0/24\", nfclient)\n\n    rand = random.randint(1, 1000)\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"TEST NEXT IP PREFIX\",\n                \"description\": f\"test create ip {rand}\",\n            },\n        )\n        create_2 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"TEST NEXT IP PREFIX\",\n                \"description\": f\"test create ip {rand}\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n        print(\"create_2\")\n        pprint.pprint(create_2, width=200)\n\n        for worker, res1 in create_1.items():\n            assert res1[\"failed\"] == False, \"Allocation failed\"\n            assert res1[\"result\"][\"address\"], f\"Result has no ip {res1['result']}\"\n\n        worker, res2 = tuple(create_2.items())[0]\n        assert (\n            res1[\"result\"][\"address\"] == res2[\"result\"][\"address\"]\n        ), f\"Should have been same IP address\"\n        assert (\n            res1[\"result\"][\"description\"] == res2[\"result\"][\"description\"]\n        ), f\"Should have been same IP description\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-by-prefix-multiple","title":"Test Create Ip By Prefix Multiple","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_by_prefix_multiple(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_ips(\"10.0.0.0/24\", nfclient)\n\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"description\": f\"test create ip {random.randint(1, 1000)}\",\n            },\n        )\n        create_2 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"description\": f\"test create ip {random.randint(1, 1000)}\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n        print(\"create_2\")\n        pprint.pprint(create_2, width=200)\n\n        worker, res1 = tuple(create_1.items())[0]\n        worker, res2 = tuple(create_2.items())[0]\n\n        assert res1[\"failed\"] == False, \"Allocation failed\"\n        assert res2[\"failed\"] == False, \"Allocation failed\"\n\n        assert res1[\"result\"][\"address\"] != res2[\"result\"][\"address\"]\n        assert res1[\"result\"][\"description\"] != res2[\"result\"][\"description\"]\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-nonexist-prefix","title":"Test Create Ip Nonexist Prefix","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_nonexist_prefix(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"1.2.3.0/24\",\n                \"description\": f\"test create ip {random.randint(1, 1000)}\",\n            },\n        )\n        pprint.pprint(create_1, width=200)\n        worker, res1 = tuple(create_1.items())[0]\n        assert res1[\"failed\"] == True, \"Allocation not failed\"\n        assert \"Unable to source parent prefix from Netbox\" in res1[\"messages\"][0]\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-by-prefix-device-interface","title":"Test Create Ip By Prefix Device Interface","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_by_prefix_device_interface(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_ips(\"10.0.0.0/24\", nfclient)\n\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"device\": \"fceos4\",\n                \"interface\": \"eth1\",\n            },\n        )\n        create_2 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"device\": \"fceos4\",\n                \"interface\": \"eth1\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n        print(\"create_2\")\n        pprint.pprint(create_2, width=200)\n\n        worker, res1 = tuple(create_1.items())[0]\n        worker, res2 = tuple(create_2.items())[0]\n\n        assert res1[\"failed\"] == False, \"Allocation failed\"\n        assert res2[\"failed\"] == False, \"Allocation failed\"\n\n        assert (\n            res1[\"result\"][\"address\"] == res2[\"result\"][\"address\"]\n        ), \"Should be same IP\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-by-prefix-description-device-interface","title":"Test Create Ip By Prefix Description Device Interface","text":"<p>This test should allocate two different IPs since device, interface, description given</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_by_prefix_description_device_interface(self, nfclient):\n    \"This test should allocate two different IPs since device, interface, description given\"\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_ips(\"10.0.0.0/24\", nfclient)\n\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"device\": \"fceos4\",\n                \"interface\": \"eth1\",\n                \"description\": \"foo1\",\n            },\n        )\n        create_2 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"device\": \"fceos4\",\n                \"interface\": \"eth1\",\n                \"description\": \"foo2\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n        print(\"create_2\")\n        pprint.pprint(create_2, width=200)\n\n        worker, res1 = tuple(create_1.items())[0]\n        worker, res2 = tuple(create_2.items())[0]\n\n        assert res1[\"failed\"] == False, \"Allocation failed\"\n        assert res2[\"failed\"] == False, \"Allocation failed\"\n\n        assert (\n            res1[\"result\"][\"address\"] != res2[\"result\"][\"address\"]\n        ), \"Should be different IP cause description is different\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-with-vrf-tags-tenant-role-dnsname-comments","title":"Test Create Ip With Vrf Tags Tenant Role Dnsname Comments","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_with_vrf_tags_tenant_role_dnsname_comments(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_ips(\"10.0.0.0/24\", nfclient)\n\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"device\": \"fceos4\",\n                \"interface\": \"eth1\",\n                \"description\": \"foo1\",\n                \"vrf\": \"VRF1\",\n                \"tags\": [\"NORFAB\", \"ACCESS\"],\n                \"tenant\": \"NORFAB\",\n                \"dns_name\": \"foo1.lab.local\",\n                \"role\": \"anycast\",\n                \"comments\": \"Some comments\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n\n        worker, res1 = tuple(create_1.items())[0]\n\n        assert res1[\"failed\"] == False, \"Allocation failed\"\n        assert res1[\"result\"][\"address\"], f\"No ip allocated\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-non-existing-device","title":"Test Create Ip Non Existing Device","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_non_existing_device(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"device\": \"does_not_exist\",\n                \"interface\": \"eth1\",\n                \"description\": \"foo1\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n\n        worker, res1 = tuple(create_1.items())[0]\n\n        assert res1[\"failed\"] == True, \"Allocation should have failed\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-non-existing-interface","title":"Test Create Ip Non Existing Interface","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_non_existing_interface(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"device\": \"fce0s4\",\n                \"interface\": \"does_not_exist\",\n                \"description\": \"foo1\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n\n        worker, res1 = tuple(create_1.items())[0]\n\n        assert res1[\"failed\"] == True, \"Allocation should have failed\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-is-primary","title":"Test Create Ip Is Primary","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_is_primary(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_ips(\"10.0.0.0/24\", nfclient)\n\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"device\": \"ceos-spine-1\",\n                \"interface\": \"Ethernet1\",\n                \"description\": \"foo1\",\n                \"is_primary\": True,\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n\n        worker, res1 = tuple(create_1.items())[0]\n\n        assert res1[\"failed\"] == False, \"Allocation failed\"\n        assert res1[\"result\"][\"address\"], f\"No ip allocated\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-dry-run-new-ip","title":"Test Create Ip Dry Run New Ip","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_dry_run_new_ip(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_ips(\"10.0.0.0/24\", nfclient)\n\n    if self.nb_version[0] == 4:\n        # test dry run for new ip\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"device\": \"ceos-spine-1\",\n                \"interface\": \"Ethernet1\",\n                \"dry_run\": True,\n            },\n        )\n        create_2 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\"prefix\": \"10.0.0.0/24\", \"description\": \"foo\", \"dry_run\": True},\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n        print(\"create_2\")\n        pprint.pprint(create_2, width=200)\n\n        worker, res1 = tuple(create_1.items())[0]\n        worker, res2 = tuple(create_2.items())[0]\n\n        assert res1[\"failed\"] == False, \"Allocation failed\"\n        assert res1[\"result\"][\"address\"], f\"No ip allocated\"\n        assert res1[\"dry_run\"] is True, \"No dry run flag set to true\"\n        assert res1[\"status\"] == \"unchanged\", \"Unexpected status\"\n\n        assert res2[\"failed\"] == False, \"Allocation failed\"\n        assert res2[\"result\"][\"address\"], f\"No ip allocated\"\n        assert res2[\"dry_run\"] is True, \"No dry run flag set to true\"\n        assert res2[\"status\"] == \"unchanged\", \"Unexpected status\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-dry-run-existing-ip","title":"Test Create Ip Dry Run Existing Ip","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_dry_run_existing_ip(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_ips(\"10.0.0.0/24\", nfclient)\n\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"description\": \"foobar\",\n            },\n        )\n        create_2 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"description\": \"foobar\",\n                \"role\": \"anycast\",\n                \"dry_run\": True,\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n        print(\"create_2\")\n        pprint.pprint(create_2, width=200)\n\n        worker, res1 = tuple(create_1.items())[0]\n        worker, res2 = tuple(create_2.items())[0]\n\n        assert res1[\"failed\"] == False, \"Allocation failed\"\n        assert res1[\"result\"][\"address\"], f\"No ip allocated\"\n        assert res1[\"dry_run\"] is False, \"No dry run flag set to true\"\n        assert res1[\"status\"] == \"created\", \"Unexpected status\"\n\n        assert res2[\"failed\"] == False, \"Allocation failed\"\n        assert res2[\"result\"][\"address\"], f\"No ip allocated\"\n        assert res2[\"dry_run\"] is True, \"No dry run flag set to true\"\n        assert res2[\"status\"] == \"unchanged\", \"Unexpected status\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-with-nb-instance","title":"Test Create Ip With Nb Instance","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_with_nb_instance(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_ips(\"10.0.0.0/24\", nfclient)\n\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"description\": \"foobar\",\n                \"instance\": \"dev\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n\n        worker, res1 = tuple(create_1.items())[0]\n\n        assert res1[\"failed\"] == False, \"Allocation failed\"\n        assert res1[\"result\"][\"address\"], f\"No ip allocated\"\n        assert res1[\"dry_run\"] is False, \"No dry run flag set to true\"\n        assert res1[\"status\"] == \"created\", \"Unexpected status\"\n        assert \"dev\" in res1[\"resources\"]\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-with-branch","title":"Test Create Ip With Branch","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_with_branch(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n    delete_branch(\"create_ip_1\", nfclient)\n    delete_ips(\"10.0.0.0/24\", nfclient)\n\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_ip\",\n            workers=\"any\",\n            kwargs={\n                \"prefix\": \"10.0.0.0/24\",\n                \"device\": \"fceos4\",\n                \"interface\": \"eth1\",\n                \"description\": \"foo1\",\n                \"vrf\": \"VRF1\",\n                \"tags\": [\"NORFAB\", \"ACCESS\"],\n                \"tenant\": \"NORFAB\",\n                \"dns_name\": \"foo1.lab.local\",\n                \"role\": \"anycast\",\n                \"comments\": \"Some comments\",\n                \"branch\": \"create_ip_1\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n\n        worker, res1 = tuple(create_1.items())[0]\n\n        assert res1[\"failed\"] == False, \"Allocation failed\"\n        assert res1[\"result\"][\"address\"], f\"No ip allocated\"\n        assert (\n            res1[\"result\"][\"branch\"] == \"create_ip_1\"\n        ), \"No branch info in results\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-with-mask-len","title":"Test Create Ip With Mask Len","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_with_mask_len(self, nfclient):\n    delete_prefixes_within(\"10.0.0.0/24\", nfclient)\n    delete_ips(\"10.0.0.0/24\", nfclient)\n    create_1 = nfclient.run_job(\n        \"netbox\",\n        \"create_ip\",\n        workers=\"any\",\n        kwargs={\n            \"prefix\": \"10.0.0.0/24\",\n            \"device\": \"fceos4\",\n            \"interface\": \"Port-Channel1\",\n            \"mask_len\": 31,\n        },\n    )\n    print(\"create_1\")\n    pprint.pprint(create_1, width=200)\n    worker, res1 = tuple(create_1.items())[0]\n    assert res1[\"result\"][\"address\"] == \"10.0.0.0/31\", f\"Wrong ip allocated\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-with-mask-len-dry-run","title":"Test Create Ip With Mask Len Dry Run","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_with_mask_len_dry_run(self, nfclient):\n    delete_prefixes_within(\"10.0.0.0/24\", nfclient)\n    delete_ips(\"10.0.0.0/24\", nfclient)\n    create_1 = nfclient.run_job(\n        \"netbox\",\n        \"create_ip\",\n        workers=\"any\",\n        kwargs={\n            \"prefix\": \"10.0.0.0/24\",\n            \"device\": \"fceos4\",\n            \"interface\": \"Port-Channel1\",\n            \"mask_len\": 31,\n            \"dry_run\": True,\n        },\n    )\n    print(\"create_1\")\n    pprint.pprint(create_1, width=200)\n    worker, res1 = tuple(create_1.items())[0]\n    # dry run will allocate first ip within /24 as opposed to /31\n    assert res1[\"result\"][\"address\"] == \"10.0.0.1/24\", f\"Wrong ip allocated\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-check-create-peer-ip","title":"Test Create Ip Check Create Peer Ip","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_check_create_peer_ip(self, nfclient):\n    delete_prefixes_within(\"10.0.0.0/24\", nfclient)\n    delete_ips(\"10.0.0.0/24\", nfclient)\n    create_1 = nfclient.run_job(\n        \"netbox\",\n        \"create_ip\",\n        workers=\"any\",\n        kwargs={\n            \"prefix\": \"10.0.0.0/24\",\n            \"device\": \"fceos4\",\n            \"interface\": \"Port-Channel1.101\",\n            \"mask_len\": 31,\n        },\n    )\n    print(\"create_1\")\n    pprint.pprint(create_1, width=200)\n\n    worker, res1 = tuple(create_1.items())[0]\n\n    assert res1[\"result\"][\"address\"] == \"10.0.0.0/31\", f\"Wrong ip allocated\"\n    assert (\n        res1[\"result\"][\"peer\"][\"address\"] == \"10.0.0.1/31\"\n    ), f\"Wrong ip allocated for peer\"\n    assert (\n        res1[\"result\"][\"peer\"][\"device\"] == \"fceos5\"\n    ), f\"Wrong ip allocated for peer\"\n    assert (\n        res1[\"result\"][\"peer\"][\"interface\"] == \"ae5.101\"\n    ), f\"Wrong ip allocated for peer\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-check-create-peer-ip-with-branch","title":"Test Create Ip Check Create Peer Ip With Branch","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_check_create_peer_ip_with_branch(self, nfclient):\n    delete_prefixes_within(\"10.0.0.0/24\", nfclient)\n    delete_ips(\"10.0.0.0/24\", nfclient)\n    delete_branch(\"create_ip_with_peer\", nfclient)\n    create_1 = nfclient.run_job(\n        \"netbox\",\n        \"create_ip\",\n        workers=\"any\",\n        kwargs={\n            \"prefix\": \"10.0.0.0/24\",\n            \"device\": \"fceos4\",\n            \"interface\": \"Port-Channel1.101\",\n            \"mask_len\": 31,\n            \"branch\": \"create_ip_with_peer\",\n        },\n    )\n    print(\"create_1\")\n    pprint.pprint(create_1, width=200)\n\n    worker, res1 = tuple(create_1.items())[0]\n\n    assert res1[\"result\"][\"address\"] == \"10.0.0.0/31\", f\"Wrong ip allocated\"\n    assert res1[\"result\"][\"branch\"] == \"create_ip_with_peer\", f\"Wrong branch\"\n    assert (\n        res1[\"result\"][\"peer\"][\"address\"] == \"10.0.0.1/31\"\n    ), f\"Wrong ip allocated for peer\"\n    assert (\n        res1[\"result\"][\"peer\"][\"device\"] == \"fceos5\"\n    ), f\"Wrong ip allocated for peer\"\n    assert (\n        res1[\"result\"][\"peer\"][\"interface\"] == \"ae5.101\"\n    ), f\"Wrong ip allocated for peer\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-check-skip-create-peer-ip","title":"Test Create Ip Check Skip Create Peer Ip","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_check_skip_create_peer_ip(self, nfclient):\n    delete_prefixes_within(\"10.0.0.0/24\", nfclient)\n    delete_ips(\"10.0.0.0/24\", nfclient)\n    create_1 = nfclient.run_job(\n        \"netbox\",\n        \"create_ip\",\n        workers=\"any\",\n        kwargs={\n            \"prefix\": \"10.0.0.0/24\",\n            \"device\": \"fceos4\",\n            \"interface\": \"Port-Channel1.101\",\n            \"mask_len\": 31,\n            \"create_peer_ip\": False,\n        },\n    )\n    print(\"create_1\")\n    pprint.pprint(create_1, width=200)\n\n    worker, res1 = tuple(create_1.items())[0]\n\n    assert res1[\"result\"][\"address\"] == \"10.0.0.0/31\", f\"Wrong ip allocated\"\n    assert (\n        \"peer\" not in res1[\"result\"]\n    ), f\"SHould have been skipping peer ip creation\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-use-peer-ip","title":"Test Create Ip Use Peer Ip","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_use_peer_ip(self, nfclient):\n    delete_prefixes_within(\"10.0.0.0/24\", nfclient)\n    delete_ips(\"10.0.0.0/24\", nfclient)\n    create_1 = nfclient.run_job(\n        \"netbox\",\n        \"create_ip\",\n        workers=\"any\",\n        kwargs={\n            \"prefix\": \"10.0.0.0/24\",\n            \"device\": \"fceos4\",\n            \"interface\": \"Port-Channel1.101\",\n            \"mask_len\": 31,\n            \"create_peer_ip\": False,\n        },\n    )\n    create_2 = nfclient.run_job(\n        \"netbox\",\n        \"create_ip\",\n        workers=\"any\",\n        kwargs={\n            \"prefix\": \"10.0.0.0/24\",\n            \"device\": \"fceos5\",\n            \"interface\": \"ae5.101\",\n        },\n    )\n    print(\"create_1\")\n    pprint.pprint(create_1, width=200)\n    print(\"create_2\")\n    pprint.pprint(create_2, width=200)\n\n    worker, res1 = tuple(create_1.items())[0]\n    worker, res2 = tuple(create_2.items())[0]\n\n    assert res1[\"result\"][\"address\"] == \"10.0.0.0/31\", f\"Wrong ip allocated\"\n    assert res2[\"result\"][\"address\"] == \"10.0.0.1/31\", f\"Wrong ip allocated\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-with-link-peer-dry-run","title":"Test Create Ip With Link Peer Dry Run","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_with_link_peer_dry_run(self, nfclient):\n    delete_prefixes_within(\"10.0.0.0/24\", nfclient)\n    delete_ips(\"10.0.0.0/24\", nfclient)\n    create_1 = nfclient.run_job(\n        \"netbox\",\n        \"create_ip\",\n        workers=\"any\",\n        kwargs={\n            \"prefix\": \"10.0.0.0/24\",\n            \"device\": \"fceos4\",\n            \"interface\": \"Port-Channel1.101\",\n            \"mask_len\": 31,\n        },\n    )\n    create_2 = nfclient.run_job(\n        \"netbox\",\n        \"create_ip\",\n        workers=\"any\",\n        kwargs={\n            \"prefix\": \"10.0.0.0/24\",\n            \"device\": \"fceos5\",\n            \"interface\": \"ae5.101\",\n            \"dry_run\": True,\n        },\n    )\n    print(\"create_1\")\n    pprint.pprint(create_1, width=200)\n    print(\"create_2\")\n    pprint.pprint(create_2, width=200)\n\n    worker, res1 = tuple(create_1.items())[0]\n    worker, res2 = tuple(create_2.items())[0]\n\n    assert res1[\"result\"][\"address\"] == \"10.0.0.0/31\", f\"Wrong ip allocated\"\n    assert res2[\"result\"][\"address\"] == \"10.0.0.1/31\", f\"Wrong ip allocated\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-ip-with-link-peer-within-parent","title":"Test Create Ip With Link Peer Within Parent","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_with_link_peer_within_parent(self, nfclient):\n    delete_prefixes_within(\"10.0.0.0/24\", nfclient)\n    delete_ips(\"10.0.0.0/24\", nfclient)\n    create_1 = nfclient.run_job(\n        \"netbox\",\n        \"create_ip\",\n        workers=\"any\",\n        kwargs={\n            \"prefix\": \"10.0.0.0/24\",\n            \"device\": \"fceos4\",\n            \"interface\": \"Port-Channel1.101\",\n        },\n    )\n    create_2 = nfclient.run_job(\n        \"netbox\",\n        \"create_ip\",\n        workers=\"any\",\n        kwargs={\n            \"prefix\": \"10.0.0.0/24\",\n            \"device\": \"fceos5\",\n            \"interface\": \"ae5.101\",\n        },\n    )\n    print(\"create_1\")\n    pprint.pprint(create_1, width=200)\n    print(\"create_2\")\n    pprint.pprint(create_2, width=200)\n\n    worker, res1 = tuple(create_1.items())[0]\n    worker, res2 = tuple(create_2.items())[0]\n\n    assert res1[\"result\"][\"address\"] == \"10.0.0.1/24\", f\"Wrong ip allocated\"\n    assert res2[\"result\"][\"address\"] == \"10.0.0.2/24\", f\"Wrong ip allocated\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#testnetboxcache","title":"TestNetboxCache","text":"<p>Caching functionality and cache management.</p> <p>Purpose: Validate cache operations for improved query performance.</p>"},{"location":"testing/netbox_service_tests/#test-cache-list","title":"Test Cache List","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_cache_list(self, nfclient):\n    # populate the cache\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_devices\",\n        workers=\"all\",\n        kwargs={\"cache\": True, \"devices\": [\"ceos1\", \"fceos4\", \"fceos5\"]},\n    )\n\n    # verify cache is present\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"cache_list\",\n        workers=\"all\",\n        kwargs={},\n    )\n\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} - cache operation failed\"\n        assert len(res[\"result\"]) &gt; 0, f\"{worker} - cache is empty\"\n        assert isinstance(\n            res[\"result\"], list\n        ), f\"{worker} - cache list result is not a list\"\n        assert \"get_devices::fceos5\" in res[\"result\"]\n        assert \"get_devices::ceos1\" in res[\"result\"]\n        assert \"get_devices::fceos4\" in res[\"result\"]\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-cache-list-details","title":"Test Cache List Details","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_cache_list_details(self, nfclient):\n    # populate the cache\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_devices\",\n        workers=\"all\",\n        kwargs={\"cache\": True, \"devices\": [\"ceos1\", \"fceos4\", \"fceos5\"]},\n    )\n\n    # verify cache is present\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"cache_list\",\n        workers=\"all\",\n        kwargs={\"details\": True},\n    )\n\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} - cache operation failed\"\n        assert len(res[\"result\"]) &gt; 0, f\"{worker} - cache is empty\"\n        assert isinstance(\n            res[\"result\"], list\n        ), f\"{worker} - cache list result is not a list\"\n        for item in res[\"result\"]:\n            assert all(\n                key in item for key in [\"age\", \"creation\", \"expires\", \"key\"]\n            ), f\"{worker} - not all cache list data details returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-cache-list-filter","title":"Test Cache List Filter","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_cache_list_filter(self, nfclient):\n    # populate the cache\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_devices\",\n        workers=\"all\",\n        kwargs={\"cache\": True, \"devices\": [\"ceos1\", \"fceos4\", \"fceos5\"]},\n    )\n\n    # verify cache is present\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"cache_list\",\n        workers=\"all\",\n        kwargs={\"keys\": \"*ceos1*\"},\n    )\n\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} - cache operation failed\"\n        assert len(res[\"result\"]) &gt; 0, f\"{worker} - cache is empty\"\n        assert isinstance(\n            res[\"result\"], list\n        ), f\"{worker} - cache list result is not a list\"\n        for key in res[\"result\"]:\n            assert (\n                \"ceos1\" in key\n            ), f\"{worker} - key '{key}' does not contain 'ceos1' pattern \"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-cache-clear-all","title":"Test Cache Clear All","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_cache_clear_all(self, nfclient):\n    # populate the cache\n    ret_populate = nfclient.run_job(\n        \"netbox\",\n        \"get_devices\",\n        workers=\"all\",\n        kwargs={\"cache\": True, \"devices\": [\"ceos1\", \"fceos4\", \"fceos5\"]},\n    )\n\n    # clear cache\n    ret_clear = nfclient.run_job(\n        \"netbox\",\n        \"cache_clear\",\n        workers=\"all\",\n        kwargs={\"keys\": \"*\"},\n    )\n\n    # list cache\n    ret_list = nfclient.run_job(\n        \"netbox\",\n        \"cache_list\",\n        workers=\"all\",\n        kwargs={\"keys\": \"*\"},\n    )\n\n    print(\"\\nret_populate:\")\n    pprint.pprint(ret_populate, width=150)\n\n    print(\"\\nret_clear:\")\n    pprint.pprint(ret_clear, width=150)\n\n    print(\"\\nret_list:\")\n    pprint.pprint(ret_list, width=150)\n\n    for worker, res in ret_populate.items():\n        assert (\n            res[\"failed\"] == False\n        ), f\"{worker} - get_devices populate operation failed\"\n\n    for worker, res in ret_clear.items():\n        assert res[\"failed\"] == False, f\"{worker} - cache clear operation failed\"\n        assert (\n            len(res[\"result\"]) &gt; 0\n        ), f\"{worker} - did not return list of cleared keys\"\n        assert isinstance(\n            res[\"result\"], list\n        ), f\"{worker} - cache clear result is not a list\"\n\n    for worker, res in ret_list.items():\n        assert res[\"failed\"] == False, f\"{worker} - cache list operation failed\"\n        assert len(res[\"result\"]) == 0, f\"{worker} - cache is not empty\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-cache-clear-key","title":"Test Cache Clear Key","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_cache_clear_key(self, nfclient):\n    # populate the cache\n    ret_populate = nfclient.run_job(\n        \"netbox\",\n        \"get_devices\",\n        workers=\"all\",\n        kwargs={\"cache\": True, \"devices\": [\"ceos1\", \"fceos4\", \"fceos5\"]},\n    )\n\n    # list cache\n    ret_list_before = nfclient.run_job(\n        \"netbox\",\n        \"cache_list\",\n        workers=\"all\",\n        kwargs={\"keys\": \"*\"},\n    )\n\n    # clear cache\n    ret_clear = nfclient.run_job(\n        \"netbox\",\n        \"cache_clear\",\n        workers=\"all\",\n        kwargs={\"key\": \"get_devices::ceos1\"},\n    )\n\n    # list cache\n    ret_list_after = nfclient.run_job(\n        \"netbox\",\n        \"cache_list\",\n        workers=\"all\",\n        kwargs={\"keys\": \"*\"},\n    )\n\n    print(\"\\nret_populate:\")\n    pprint.pprint(ret_populate, width=150)\n\n    print(\"\\nret_list_before:\")\n    pprint.pprint(ret_list_before, width=150)\n\n    print(\"\\nret_clear:\")\n    pprint.pprint(ret_clear, width=150)\n\n    print(\"\\nret_list_after:\")\n    pprint.pprint(ret_list_after, width=150)\n\n    for worker, res in ret_populate.items():\n        assert (\n            res[\"failed\"] == False\n        ), f\"{worker} - get_devices populate operation failed\"\n\n    for worker, res in ret_list_before.items():\n        assert res[\"failed\"] == False, f\"{worker} - cache list operation failed\"\n        assert len(res[\"result\"]) &gt; 0, f\"{worker} - cache is empty\"\n        assert (\n            \"get_devices::ceos1\" in res[\"result\"]\n        ), f\"{worker} - cache does not have get_devices::ceos1 key\"\n\n    for worker, res in ret_clear.items():\n        assert res[\"failed\"] == False, f\"{worker} - cache clear operation failed\"\n        assert (\n            len(res[\"result\"]) &gt; 0\n        ), f\"{worker} - did not return list of cleared keys\"\n        assert isinstance(\n            res[\"result\"], list\n        ), f\"{worker} - cache clear result is not a list\"\n\n    for worker, res in ret_list_after.items():\n        assert res[\"failed\"] == False, f\"{worker} - cache list operation failed\"\n        assert len(res[\"result\"]) &gt; 0, f\"{worker} - cache is empty\"\n        assert (\n            \"get_devices::ceos1\" not in res[\"result\"]\n        ), f\"{worker} - cache still has get_devices::ceos1 key\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-cache-get-key","title":"Test Cache Get Key","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_cache_get_key(self, nfclient):\n    # populate the cache\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_devices\",\n        workers=\"all\",\n        kwargs={\"cache\": True, \"devices\": [\"ceos1\", \"fceos4\", \"fceos5\"]},\n    )\n\n    # verify cache is present\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"cache_get\",\n        workers=\"all\",\n        kwargs={\"key\": \"get_devices::ceos1\"},\n    )\n\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} - cache operation failed\"\n        assert len(res[\"result\"]) &gt; 0, f\"{worker} - cache is empty\"\n        assert isinstance(\n            res[\"result\"], dict\n        ), f\"{worker} - cache get result is not a dict\"\n        assert res[\"result\"][\n            \"get_devices::ceos1\"\n        ], f\"{worker} - cache get result key data is empty\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-cache-get-keys","title":"Test Cache Get Keys","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_cache_get_keys(self, nfclient):\n    # populate the cache\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_devices\",\n        workers=\"all\",\n        kwargs={\"cache\": True, \"devices\": [\"ceos1\", \"fceos4\", \"fceos5\"]},\n    )\n\n    # verify cache is present\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"cache_get\",\n        workers=\"all\",\n        kwargs={\"keys\": \"*ceos1*\"},\n    )\n\n    pprint.pprint(ret, width=200)\n\n    for worker, res in ret.items():\n        assert res[\"failed\"] == False, f\"{worker} - cache operation failed\"\n        assert len(res[\"result\"]) &gt; 0, f\"{worker} - cache is empty\"\n        assert isinstance(\n            res[\"result\"], dict\n        ), f\"{worker} - cache get result is not a dict\"\n        for key in res[\"result\"].keys():\n            assert (\n                \"ceos1\" in key\n            ), f\"{worker} - cache key '{key}' does not contain ceos1 pattern\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-cache-false","title":"Test Cache False","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_cache_false(self, nfclient):\n    # clear cache\n    ret_clear = nfclient.run_job(\n        \"netbox\",\n        \"cache_clear\",\n        workers=\"all\",\n        kwargs={\"keys\": \"*\"},\n    )\n\n    # query data with cache set to False\n    ret_query = nfclient.run_job(\n        \"netbox\",\n        \"get_devices\",\n        workers=\"all\",\n        kwargs={\"cache\": False, \"devices\": [\"ceos1\", \"fceos4\", \"fceos5\"]},\n    )\n\n    # verify cache is empty\n    ret_list = nfclient.run_job(\n        \"netbox\",\n        \"cache_list\",\n        workers=\"all\",\n        kwargs={\"keys\": \"*\"},\n    )\n\n    print(\"\\nret_clear:\")\n    pprint.pprint(ret_clear, width=150)\n\n    print(\"\\nret_query:\")\n    pprint.pprint(ret_query, width=150)\n\n    print(\"\\nret_list:\")\n    pprint.pprint(ret_list, width=150)\n\n    for worker, res in ret_clear.items():\n        assert res[\"failed\"] == False, f\"{worker} - cache clear operation failed\"\n\n    for worker, res in ret_query.items():\n        assert res[\"failed\"] == False, f\"{worker} - query netbox operation failed\"\n\n    for worker, res in ret_list.items():\n        assert res[\"failed\"] == False, f\"{worker} - cache list operation failed\"\n        assert isinstance(\n            res[\"result\"], list\n        ), f\"{worker} - cache get result is not a dict\"\n        assert len(res[\"result\"]) == 0, f\"{worker} - cache is not empty\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-cache-refresh","title":"Test Cache Refresh","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_cache_refresh(self, nfclient):\n    # clear cache\n    ret_clear = nfclient.run_job(\n        \"netbox\",\n        \"cache_clear\",\n        workers=\"all\",\n        kwargs={\"keys\": \"*\"},\n    )\n\n    # query data with cache set to True\n    ret_query_true = nfclient.run_job(\n        \"netbox\",\n        \"get_devices\",\n        workers=\"all\",\n        kwargs={\"cache\": True, \"devices\": [\"ceos1\", \"fceos4\", \"fceos5\"]},\n    )\n\n    # get cache creation time\n    ret_list_1st = nfclient.run_job(\n        \"netbox\",\n        \"cache_list\",\n        workers=\"all\",\n        kwargs={\"keys\": \"*\", \"details\": True},\n    )\n\n    # query data with cache set to refresh\n    ret_query_refresh = nfclient.run_job(\n        \"netbox\",\n        \"get_devices\",\n        workers=\"all\",\n        kwargs={\"cache\": \"refresh\", \"devices\": [\"ceos1\", \"fceos4\", \"fceos5\"]},\n    )\n\n    # get cache creation time\n    ret_list_2nd = nfclient.run_job(\n        \"netbox\",\n        \"cache_list\",\n        workers=\"all\",\n        kwargs={\"keys\": \"*\", \"details\": True},\n    )\n\n    print(\"\\nret_clear:\")\n    pprint.pprint(ret_clear, width=150)\n\n    print(\"\\nret_query_true:\")\n    pprint.pprint(ret_query_true, width=150)\n\n    print(\"\\nret_list_1st:\")\n    pprint.pprint(ret_list_1st, width=150)\n\n    print(\"\\nret_query_refresh:\")\n    pprint.pprint(ret_query_refresh, width=150)\n\n    print(\"\\nret_list_2nd:\")\n    pprint.pprint(ret_list_2nd, width=150)\n\n    # verify no errors\n    for worker, res in ret_clear.items():\n        assert res[\"failed\"] == False, f\"{worker} - cache clear operation failed\"\n\n    for worker, res in ret_query_true.items():\n        assert res[\"failed\"] == False, f\"{worker} - query netbox operation failed\"\n\n    for worker, res in ret_list_1st.items():\n        assert res[\"failed\"] == False, f\"{worker} - ret_list_1st operation failed\"\n\n    for worker, res in ret_query_refresh.items():\n        assert (\n            res[\"failed\"] == False\n        ), f\"{worker} - ret_query_refresh operation failed\"\n\n    for worker, res in ret_list_2nd.items():\n        assert res[\"failed\"] == False, f\"{worker} - ret_list_2nd operation failed\"\n\n    # compare 2nd list items expiration time is after the 1st one\n    for worker_2nd, res_2nd in ret_list_2nd.items():\n        for item_2nd in res_2nd[\"result\"]:\n            for item_1st in ret_list_1st[worker_2nd][\"result\"]:\n                if item_2nd[\"key\"] == item_1st[\"key\"]:\n                    assert item_2nd[\"expires\"] &gt; item_1st[\"expires\"]\n</code></pre>"},{"location":"testing/netbox_service_tests/#testgetcontainerlabinventory","title":"TestGetContainerlabInventory","text":"<p>Containerlab topology generation from Netbox.</p> <p>Purpose: Validate containerlab topology file generation from Netbox inventory.</p>"},{"location":"testing/netbox_service_tests/#test-get-containerlab-inventory-devices","title":"Test Get Containerlab Inventory Devices","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_containerlab_inventory_devices(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_containerlab_inventory\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"fceos4\", \"fceos5\"],\n            \"lab_name\": \"foobar\",\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert all(\n            k in res[\"result\"] for k in [\"mgmt\", \"name\", \"topology\"]\n        ), f\"{worker} - not all clab inventory data returned\"\n        assert res[\"result\"][\"topology\"][\n            \"links\"\n        ], f\"{worker} - no clab inventory links data returned\"\n        assert res[\"result\"][\"topology\"][\n            \"nodes\"\n        ], f\"{worker} - no clab inventory nodes data returned\"\n        assert (\n            res[\"result\"][\"name\"] == \"foobar\"\n        ), f\"{worker} - clab inventory name is not foobar\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-containerlab-inventory-non-existing-devices","title":"Test Get Containerlab Inventory Non Existing Devices","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_containerlab_inventory_non_existing_devices(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_containerlab_inventory\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"fceosabc\", \"fceosxyz\"],\n            \"lab_name\": \"foobar\",\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert res[\"errors\"], f\"{worker} - received no error\"\n        assert res[\"failed\"] == True, f\"{worker} - should have failed\"\n        assert all(\n            k in res[\"result\"] for k in [\"mgmt\", \"name\", \"topology\"]\n        ), f\"{worker} - not all clab inventory data returned\"\n        assert (\n            res[\"result\"][\"topology\"][\"links\"] == []\n        ), f\"{worker} - clab inventory links data returned\"\n        assert (\n            res[\"result\"][\"topology\"][\"nodes\"] == {}\n        ), f\"{worker} - clab inventory nodes data returned\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-containerlab-inventory-by-tenant","title":"Test Get Containerlab Inventory By Tenant","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_containerlab_inventory_by_tenant(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    if self.nb_version &gt;= (4, 3, 0):\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"get_containerlab_inventory\",\n            workers=\"any\",\n            kwargs={\n                \"tenant\": \"NORFAB\",\n                \"lab_name\": \"foobar\",\n            },\n        )\n    else:\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"get_containerlab_inventory\",\n            workers=\"any\",\n            kwargs={\n                \"tenant\": \"norfab\",\n                \"lab_name\": \"foobar\",\n            },\n        )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert all(\n            k in res[\"result\"] for k in [\"mgmt\", \"name\", \"topology\"]\n        ), f\"{worker} - not all clab inventory data returned\"\n        assert res[\"result\"][\"topology\"][\n            \"links\"\n        ], f\"{worker} - no clab inventory links data returned\"\n        assert res[\"result\"][\"topology\"][\n            \"nodes\"\n        ], f\"{worker} - no clab inventory nodes data returned\"\n        assert (\n            res[\"result\"][\"name\"] == \"foobar\"\n        ), f\"{worker} - clab inventory name is not foobar\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-containerlab-inventory-by-filters","title":"Test Get Containerlab Inventory By Filters","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_containerlab_inventory_by_filters(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    if self.nb_version &gt;= (4, 3, 0):\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"get_containerlab_inventory\",\n            workers=\"any\",\n            kwargs={\n                \"filters\": [\n                    '{name: {i_contains: \"ceos-spine\"}, status: STATUS_ACTIVE}'\n                ],\n                \"lab_name\": \"foobar\",\n            },\n        )\n    else:\n        ret = nfclient.run_job(\n            \"netbox\",\n            \"get_containerlab_inventory\",\n            workers=\"any\",\n            kwargs={\n                \"filters\": [{\"q\": \"ceos-spine\", \"status\": \"active\"}],\n                \"lab_name\": \"foobar\",\n            },\n        )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert all(\n            k in res[\"result\"] for k in [\"mgmt\", \"name\", \"topology\"]\n        ), f\"{worker} - not all clab inventory data returned\"\n        assert res[\"result\"][\"topology\"][\n            \"links\"\n        ], f\"{worker} - no clab inventory links data returned\"\n        assert (\n            len(res[\"result\"][\"topology\"][\"nodes\"]) == 2\n        ), f\"{worker} - clab inventory nodes data wromg\"\n        assert (\n            res[\"result\"][\"name\"] == \"foobar\"\n        ), f\"{worker} - clab inventory name is not foobar\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-containerlab-inventory-with-nb-instance","title":"Test Get Containerlab Inventory With Nb Instance","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_containerlab_inventory_with_nb_instance(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_containerlab_inventory\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"fceos4\", \"fceos5\"],\n            \"lab_name\": \"foobar\",\n            \"instance\": \"preprod\",\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert all(\n            k in res[\"result\"] for k in [\"mgmt\", \"name\", \"topology\"]\n        ), f\"{worker} - not all clab inventory data returned\"\n        assert res[\"result\"][\"topology\"][\n            \"links\"\n        ], f\"{worker} - no clab inventory links data returned\"\n        assert res[\"result\"][\"topology\"][\n            \"nodes\"\n        ], f\"{worker} - no clab inventory nodes data returned\"\n        assert (\n            res[\"result\"][\"name\"] == \"foobar\"\n        ), f\"{worker} - clab inventory name is not foobar\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-containerlab-inventory-with-image","title":"Test Get Containerlab Inventory With Image","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_containerlab_inventory_with_image(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_containerlab_inventory\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"ceos1\"],\n            \"lab_name\": \"foobar\",\n            \"image\": \"ceos:latest\",\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert all(\n            k in res[\"result\"] for k in [\"mgmt\", \"name\", \"topology\"]\n        ), f\"{worker} - not all clab inventory data returned\"\n        assert res[\"result\"][\"topology\"][\n            \"nodes\"\n        ], f\"{worker} - no clab inventory nodes data returned\"\n        for node_name, node_data in res[\"result\"][\"topology\"][\"nodes\"].items():\n            assert (\n                node_data[\"image\"] == \"ceos:latest\"\n            ), f\"{worker} - node {node_name} image is not ceos:latest\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-containerlab-inventory-run-out-of-ports","title":"Test Get Containerlab Inventory Run Out Of Ports","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_containerlab_inventory_run_out_of_ports(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_containerlab_inventory\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"fceos4\", \"fceos5\"],\n            \"lab_name\": \"foobar\",\n            \"ports\": [10000, 10005],\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert res[\"errors\"], f\"{worker} - received no error\"\n        assert res[\"failed\"] == True, f\"{worker} - should have failed\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-containerlab-inventory-run-out-of-ips","title":"Test Get Containerlab Inventory Run Out Of Ips","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_containerlab_inventory_run_out_of_ips(self, nfclient):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_containerlab_inventory\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"fceos4\", \"fceos5\", \"ceos1\"],\n            \"lab_name\": \"foobar\",\n            \"ipv4_subnet\": \"172.100.100.0/30\",\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert res[\"errors\"], f\"{worker} - received no error\"\n        assert res[\"failed\"] == True, f\"{worker} - should have failed\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-containerlab-inventory-with-ports-map","title":"Test Get Containerlab Inventory With Ports Map","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_get_containerlab_inventory_with_ports_map(self, nfclient):\n    ports_map = {\n        \"fceos4\": [\n            \"10007:22/tcp\",\n            \"10008:23/tcp\",\n            \"10009:80/tcp\",\n            \"10010:161/udp\",\n            \"10011:443/tcp\",\n            \"10012:830/tcp\",\n            \"10013:8080/tcp\",\n        ]\n    }\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_containerlab_inventory\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"fceos4\", \"fceos5\"],\n            \"lab_name\": \"foobar\",\n            \"ports_map\": ports_map,\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert all(\n            k in res[\"result\"] for k in [\"mgmt\", \"name\", \"topology\"]\n        ), f\"{worker} - not all clab inventory data returned\"\n        assert res[\"result\"][\"topology\"][\n            \"nodes\"\n        ], f\"{worker} - no clab inventory nodes data returned\"\n        assert (\n            res[\"result\"][\"topology\"][\"nodes\"][\"fceos4\"][\"ports\"]\n            == ports_map[\"fceos4\"]\n        ), f\"{worker} - ports map not applied for fceos4\"\n        assert res[\"result\"][\"topology\"][\"nodes\"][\"fceos5\"][\n            \"ports\"\n        ], f\"{worker} - no ports allocated for fceos5\"\n        assert all(\n            p.startswith(\"1200\")\n            for p in res[\"result\"][\"topology\"][\"nodes\"][\"fceos5\"][\"ports\"]\n        ), f\"{worker} - ports for fceos5 allocated correctly\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-get-containerlab-inventory-with-cache","title":"Test Get Containerlab Inventory With Cache","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>@pytest.mark.parametrize(\"cache\", cache_options)\ndef test_get_containerlab_inventory_with_cache(self, nfclient, cache):\n    ret = nfclient.run_job(\n        \"netbox\",\n        \"get_containerlab_inventory\",\n        workers=\"any\",\n        kwargs={\n            \"devices\": [\"fceos4\", \"fceos5\"],\n            \"lab_name\": \"foobar\",\n            \"cache\": cache,\n        },\n    )\n    pprint.pprint(ret)\n\n    for worker, res in ret.items():\n        assert not res[\"errors\"], f\"{worker} - received error\"\n        assert all(\n            k in res[\"result\"] for k in [\"mgmt\", \"name\", \"topology\"]\n        ), f\"{worker} - not all clab inventory data returned\"\n        assert res[\"result\"][\"topology\"][\n            \"links\"\n        ], f\"{worker} - no clab inventory links data returned\"\n        assert res[\"result\"][\"topology\"][\n            \"nodes\"\n        ], f\"{worker} - no clab inventory nodes data returned\"\n        assert (\n            res[\"result\"][\"name\"] == \"foobar\"\n        ), f\"{worker} - clab inventory name is not foobar\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#testcreateprefix","title":"TestCreatePrefix","text":"<p>IP prefix creation and management.</p> <p>Purpose: Validate IP prefix allocation and creation.</p>"},{"location":"testing/netbox_service_tests/#test-create-prefix","title":"Test Create Prefix","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_prefix(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_prefixes_within(\"10.1.0.0/24\", nfclient)\n\n    rand = random.randint(1, 1000)\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.1.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n            },\n        )\n        create_2 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.1.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n        print(\"create_2\")\n        pprint.pprint(create_2, width=200)\n\n        for worker, res1 in create_1.items():\n            assert res1[\"failed\"] == False, \"Allocation failed\"\n            assert res1[\"result\"][\n                \"prefix\"\n            ], f\"Result has no prefix {res1['result']}\"\n\n        worker, res2 = tuple(create_2.items())[0]\n        assert (\n            res1[\"result\"][\"prefix\"] == res2[\"result\"][\"prefix\"]\n        ), f\"Should have been same prefix\"\n        assert (\n            res1[\"result\"][\"description\"] == res2[\"result\"][\"description\"]\n        ), f\"Should have been same prefix description\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-prefix-multiple","title":"Test Create Prefix Multiple","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_prefix_multiple(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_prefixes_within(\"10.1.0.0/24\", nfclient)\n\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.1.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {random.randint(1, 100)}\",\n            },\n        )\n        create_2 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.1.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {random.randint(200, 300)}\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n        print(\"create_2\")\n        pprint.pprint(create_2, width=200)\n\n        for worker, res1 in create_1.items():\n            assert res1[\"failed\"] == False, \"Allocation failed\"\n            assert res1[\"result\"][\n                \"prefix\"\n            ], f\"Result has no prefix {res1['result']}\"\n\n        worker, res2 = tuple(create_2.items())[0]\n        assert (\n            res1[\"result\"][\"prefix\"] != res2[\"result\"][\"prefix\"]\n        ), f\"Should have been different prefix\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-prefix-non-exist-parent","title":"Test Create Prefix Non Exist Parent","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_prefix_non_exist_parent(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.1.123.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {random.randint(1, 100)}\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n\n        for worker, res1 in create_1.items():\n            assert res1[\"failed\"] == True, \"Allocation not failed\"\n            assert (\n                \"Unable to source parent prefix from Netbox\" in res1[\"messages\"][0]\n            ), f\"Result has no errors\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-prefix-with-vrf","title":"Test Create Prefix With Vrf","text":"<p>Should create single prefix and handle deduplication within vrf</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_prefix_with_vrf(self, nfclient):\n    \"\"\"Should create single prefix and handle deduplication within vrf\"\"\"\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_prefixes_within(\"10.2.0.0/24\", nfclient)\n\n    rand = random.randint(1, 1000)\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.2.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n                \"vrf\": \"VRF1\",\n            },\n        )\n        create_2 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.2.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n                \"vrf\": \"VRF1\",\n            },\n        )\n        create_3 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.2.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand+1}\",\n                \"vrf\": \"VRF1\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n        print(\"create_2\")\n        pprint.pprint(create_2, width=200)\n        print(\"create_3\")\n        pprint.pprint(create_3, width=200)\n\n        for worker, res1 in create_1.items():\n            assert res1[\"failed\"] == False, \"Allocation failed\"\n            assert res1[\"result\"][\n                \"prefix\"\n            ], f\"Result has no prefix {res1['result']}\"\n\n        worker, res1 = tuple(create_1.items())[0]\n        worker, res2 = tuple(create_2.items())[0]\n        worker, res3 = tuple(create_3.items())[0]\n        assert (\n            res1[\"result\"][\"prefix\"] == res2[\"result\"][\"prefix\"]\n        ), f\"Should have been same prefix\"\n        assert (\n            res1[\"result\"][\"prefix\"] != res3[\"result\"][\"prefix\"]\n        ), f\"Should have been different prefix\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-prefix-with-parent-vrf-mismatch","title":"Test Create Prefix With Parent Vrf Mismatch","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_prefix_with_parent_vrf_mismatch(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_prefixes_within(\"10.1.0.0/24\", nfclient)\n\n    rand = random.randint(1, 1000)\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.1.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n                \"vrf\": \"VRF1\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n\n        for worker, res1 in create_1.items():\n            assert res1[\"failed\"] == True, \"Allocation not failed\"\n            assert \"NetboxAllocationError\" in res1[\"errors\"][0]\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-prefix-by-parent-prefix-name","title":"Test Create Prefix By Parent Prefix Name","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_prefix_by_parent_prefix_name(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_prefixes_within(\"10.1.0.0/24\", nfclient)\n\n    rand = random.randint(1, 1000)\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"TEST CREATE PREFIXES\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n            },\n        )\n        create_2 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"TEST CREATE PREFIXES\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n        print(\"create_2\")\n        pprint.pprint(create_2, width=200)\n\n        for worker, res1 in create_1.items():\n            assert res1[\"failed\"] == False, \"Allocation failed\"\n            assert res1[\"result\"][\n                \"prefix\"\n            ], f\"Result has no prefix {res1['result']}\"\n\n        worker, res2 = tuple(create_2.items())[0]\n        assert (\n            res1[\"result\"][\"prefix\"] == res2[\"result\"][\"prefix\"]\n        ), f\"Should have been same prefix\"\n        assert (\n            res1[\"result\"][\"description\"] == res2[\"result\"][\"description\"]\n        ), f\"Should have been same prefix description\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-prefix-within-vrf-by-parent-prefix-name","title":"Test Create Prefix Within Vrf By Parent Prefix Name","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_prefix_within_vrf_by_parent_prefix_name(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_prefixes_within(\"10.2.0.0/24\", nfclient)\n\n    rand = random.randint(1, 1000)\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"TEST CREATE PREFIXES WITH VRF\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n                \"vrf\": \"VRF1\",\n            },\n        )\n        create_2 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"TEST CREATE PREFIXES WITH VRF\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n                \"vrf\": \"VRF1\",\n            },\n        )\n        create_3 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"TEST CREATE PREFIXES WITH VRF\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand+1}\",\n                \"vrf\": \"VRF1\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n        print(\"create_2\")\n        pprint.pprint(create_2, width=200)\n        print(\"create_3\")\n        pprint.pprint(create_3, width=200)\n\n        for worker, res1 in create_1.items():\n            assert res1[\"failed\"] == False, \"Allocation failed\"\n            assert res1[\"result\"][\n                \"prefix\"\n            ], f\"Result has no prefix {res1['result']}\"\n\n        worker, res1 = tuple(create_1.items())[0]\n        worker, res2 = tuple(create_2.items())[0]\n        worker, res3 = tuple(create_3.items())[0]\n\n        assert (\n            res1[\"result\"][\"prefix\"] == res2[\"result\"][\"prefix\"]\n        ), f\"Should have been same prefix\"\n        assert (\n            res1[\"result\"][\"description\"] == res2[\"result\"][\"description\"]\n        ), f\"Should have been same prefix description\"\n        assert (\n            res1[\"result\"][\"prefix\"] != res3[\"result\"][\"prefix\"]\n        ), f\"Should have been different prefix\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-prefix-dry-run-empty-parent","title":"Test Create Prefix Dry Run Empty Parent","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_prefix_dry_run_empty_parent(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_prefixes_within(\"10.1.0.0/24\", nfclient)\n\n    rand = random.randint(1, 1000)\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.1.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n                \"dry_run\": True,\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n\n        for worker, res1 in create_1.items():\n            assert res1[\"failed\"] == False, \"Allocation failed\"\n            assert res1[\"dry_run\"] == True\n            assert res1[\"status\"] == \"unchanged\"\n            assert res1[\"result\"][\"prefix\"] == \"10.1.0.0/30\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-prefix-dry-run-parent-has-children","title":"Test Create Prefix Dry Run Parent Has Children","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_prefix_dry_run_parent_has_children(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_prefixes_within(\"10.1.0.0/24\", nfclient)\n\n    rand = random.randint(1, 1000)\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.1.0.0/24\",\n                \"prefixlen\": 31,\n                \"description\": f\"test create prefix {rand}\",\n            },\n        )\n        create_dry_run = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.1.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand+1}\",\n                \"dry_run\": True,\n            },\n        )\n        print(\"create_dry_run\")\n        pprint.pprint(create_dry_run, width=200)\n\n        for worker, res1 in create_dry_run.items():\n            assert res1[\"failed\"] == False, \"Allocation failed\"\n            assert res1[\"dry_run\"] == True\n            assert res1[\"status\"] == \"unchanged\"\n            assert res1[\"result\"][\"prefix\"] == \"10.1.0.4/30\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-prefix-dry-run-prefix-exists","title":"Test Create Prefix Dry Run Prefix Exists","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_prefix_dry_run_prefix_exists(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_prefixes_within(\"10.1.0.0/24\", nfclient)\n\n    rand = random.randint(1, 1000)\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.1.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n            },\n        )\n        create_dry_run = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.1.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n                \"dry_run\": True,\n            },\n        )\n        print(\"create_dry_run\")\n        pprint.pprint(create_dry_run, width=200)\n\n        for worker, res1 in create_dry_run.items():\n            assert res1[\"failed\"] == False, \"Allocation failed\"\n            assert res1[\"dry_run\"] == True\n            assert res1[\"status\"] == \"unchanged\"\n            assert res1[\"result\"][\"prefix\"] == \"10.1.0.0/30\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-prefix-test-length-mismatch","title":"Test Create Prefix Test Length Mismatch","text":"<p>We creating first prefix, next creating prefix with same description but different prefix length</p> Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_prefix_test_length_mismatch(self, nfclient):\n    \"\"\"We creating first prefix, next creating prefix with same\n    description but different prefix length\"\"\"\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_prefixes_within(\"10.1.0.0/24\", nfclient)\n\n    rand = random.randint(1, 1000)\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.1.0.0/24\",\n                \"prefixlen\": 31,\n                \"description\": f\"test create prefix {rand}\",\n            },\n        )\n        create_2 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.1.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n        print(\"create_2\")\n        pprint.pprint(create_2, width=200)\n\n        for worker, res in create_2.items():\n            assert res[\"failed\"] == True, \"Allocation not failed\"\n            assert \"NetboxAllocationError\" in res[\"errors\"][0]\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-prefix-with-attributes","title":"Test Create Prefix With Attributes","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_prefix_with_attributes(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_prefixes_within(\"10.2.0.0/24\", nfclient)\n\n    rand = random.randint(1, 1000)\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.2.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n                \"site\": \"NORFAB-LAB\",\n                \"tenant\": \"NORFAB\",\n                \"role\": \"PREFIX_ROLE_1\",\n                \"comments\": \"Some important comment\",\n                \"vrf\": \"VRF1\",\n                \"tags\": [\"NORFAB\"],\n                \"status\": \"reserved\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n\n        for worker, res1 in create_1.items():\n            assert res1[\"failed\"] == False, \"Allocation failed\"\n            assert all(\n                k in res1[\"diff\"]\n                for k in [\n                    \"comments\",\n                    \"description\",\n                    \"role\",\n                    \"site\",\n                    \"status\",\n                    \"tags\",\n                    \"tenant\",\n                ]\n            )\n\n        # retrieve created prefix details from Netbox\n        nb_prefix = nfclient.run_job(\n            \"netbox\",\n            \"rest\",\n            workers=\"any\",\n            kwargs={\n                \"method\": \"get\",\n                \"api\": \"/ipam/prefixes/\",\n                \"params\": {\"prefix\": res1[\"result\"][\"prefix\"]},\n            },\n        )\n        print(\"nb_prefix:\")\n        pprint.pprint(nb_prefix, width=200)\n        worker, created_prefix = tuple(nb_prefix.items())[0]\n        created_prefix = created_prefix[\"result\"][\"results\"][0]\n\n        assert created_prefix[\"role\"][\"name\"] == \"PREFIX_ROLE_1\"\n        assert created_prefix[\"scope\"][\"name\"] == \"NORFAB-LAB\"\n        assert created_prefix[\"tags\"][0][\"name\"] == \"NORFAB\"\n        assert created_prefix[\"tenant\"][\"name\"] == \"NORFAB\"\n        assert created_prefix[\"vrf\"][\"name\"] == \"VRF1\"\n        assert created_prefix[\"description\"]\n        assert created_prefix[\"comments\"]\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-prefix-with-attributes-updates","title":"Test Create Prefix With Attributes Updates","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_prefix_with_attributes_updates(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_prefixes_within(\"10.2.0.0/24\", nfclient)\n\n    rand = random.randint(1, 1000)\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.2.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n                \"site\": \"NORFAB-LAB\",\n                \"tenant\": \"NORFAB\",\n                \"role\": \"PREFIX_ROLE_1\",\n                \"comments\": \"Some important comment\",\n                \"tags\": [\"NORFAB\"],\n                \"status\": \"reserved\",\n            },\n        )\n        create_2 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.2.0.0/24\",\n                \"description\": f\"test create prefix {rand}\",\n                \"site\": \"SALTNORNIR-LAB\",\n                \"tenant\": \"SALTNORNIR\",\n                \"role\": \"PREFIX_ROLE_2\",\n                \"comments\": \"Some important comments updates\",\n                \"tags\": [\"ACCESS\"],\n                \"status\": \"active\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n        print(\"create_2\")\n        pprint.pprint(create_2, width=200)\n\n        # verify has changes\n        for worker, res1 in create_2.items():\n            assert res1[\"failed\"] == False, \"Allocation failed\"\n            assert all(\n                k in res1[\"diff\"]\n                for k in [\n                    \"comments\",\n                    \"role\",\n                    \"site\",\n                    \"status\",\n                    \"tags\",\n                    \"tenant\",\n                ]\n            )\n\n        # retrieve created prefix details from Netbox\n        nb_prefix = nfclient.run_job(\n            \"netbox\",\n            \"rest\",\n            workers=\"any\",\n            kwargs={\n                \"method\": \"get\",\n                \"api\": \"/ipam/prefixes/\",\n                \"params\": {\"prefix\": res1[\"result\"][\"prefix\"]},\n            },\n        )\n        print(\"nb_prefix:\")\n        pprint.pprint(nb_prefix, width=200)\n        worker, created_prefix = tuple(nb_prefix.items())[0]\n        created_prefix = created_prefix[\"result\"][\"results\"][0]\n\n        assert created_prefix[\"role\"][\"name\"] == \"PREFIX_ROLE_2\"\n        assert created_prefix[\"scope\"][\"name\"] == \"SALTNORNIR-LAB\"\n        for tag in created_prefix[\"tags\"]:\n            assert tag[\"name\"] == \"NORFAB\" or tag[\"name\"] == \"ACCESS\"\n        assert created_prefix[\"tenant\"][\"name\"] == \"SALTNORNIR\"\n        assert created_prefix[\"vrf\"][\"name\"] == \"VRF1\"\n        assert created_prefix[\"description\"] == f\"test create prefix {rand}\"\n        assert created_prefix[\"comments\"] == \"Some important comments updates\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-create-prefix-with-branch","title":"Test Create Prefix With Branch","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_prefix_with_branch(self, nfclient):\n    if self.nb_version is None:\n        self.nb_version = get_nb_version(nfclient)\n\n    delete_branch(\"create_prefix_1\", nfclient)\n    delete_prefixes_within(\"10.2.0.0/24\", nfclient)\n\n    rand = random.randint(1, 1000)\n    if self.nb_version[0] == 4:\n        create_1 = nfclient.run_job(\n            \"netbox\",\n            \"create_prefix\",\n            workers=\"any\",\n            kwargs={\n                \"parent\": \"10.2.0.0/24\",\n                \"prefixlen\": 30,\n                \"description\": f\"test create prefix {rand}\",\n                \"site\": \"NORFAB-LAB\",\n                \"tenant\": \"NORFAB\",\n                \"role\": \"PREFIX_ROLE_1\",\n                \"comments\": \"Some important comment\",\n                \"vrf\": \"VRF1\",\n                \"tags\": [\"NORFAB\"],\n                \"status\": \"reserved\",\n                \"branch\": \"create_prefix_1\",\n            },\n        )\n        print(\"create_1\")\n        pprint.pprint(create_1, width=200)\n\n        for worker, res1 in create_1.items():\n            assert res1[\"failed\"] == False, \"Allocation with branch failed\"\n            assert (\n                res1[\"result\"][\"branch\"] == \"create_prefix_1\"\n            ), \"No branch details in result\"\n</code></pre>"},{"location":"testing/netbox_service_tests/#testcreateipbulk","title":"TestCreateIPBulk","text":"<p>Bulk IP address creation operations.</p> <p>Purpose: Validate efficient bulk IP creation for large datasets.</p>"},{"location":"testing/netbox_service_tests/#test-create-ip-bulk","title":"Test Create Ip Bulk","text":"Source code in <code>tests\\test_netbox_service.py</code> <pre><code>def test_create_ip_bulk(self, nfclient):\n    delete_prefixes_within(\"10.0.0.0/24\", nfclient)\n    delete_ips(\"10.0.0.0/24\", nfclient)\n    create_1 = nfclient.run_job(\n        \"netbox\",\n        \"create_ip_bulk\",\n        workers=\"any\",\n        kwargs={\n            \"prefix\": \"10.0.0.0/24\",\n            \"devices\": [\"fceos4\", \"fceos5\"],\n            \"interface_regex\": \"eth103.0|eth11.123|Port-Channel1.101|ae5.101\",\n            \"mask_len\": 31,\n        },\n    )\n    print(\"create_1\")\n    pprint.pprint(create_1, width=200)\n\n    for worker, res1 in create_1.items():\n        assert res1[\"failed\"] == False, \"Allocation failed\"\n        assert res1[\"result\"] == {\n            \"fceos4\": {\n                \"Port-Channel1.101\": {\n                    \"address\": \"10.0.0.0/31\",\n                    \"description\": \"\",\n                    \"device\": \"fceos4\",\n                    \"interface\": \"Port-Channel1.101\",\n                    \"peer\": {\n                        \"address\": \"10.0.0.1/31\",\n                        \"description\": \"\",\n                        \"device\": \"fceos5\",\n                        \"interface\": \"ae5.101\",\n                        \"vrf\": \"None\",\n                    },\n                    \"vrf\": \"None\",\n                },\n                \"eth103.0\": {\n                    \"address\": \"10.0.0.2/31\",\n                    \"description\": \"\",\n                    \"device\": \"fceos4\",\n                    \"interface\": \"eth103.0\",\n                    \"peer\": {\n                        \"address\": \"10.0.0.3/31\",\n                        \"description\": \"\",\n                        \"device\": \"fceos5\",\n                        \"interface\": \"eth103\",\n                        \"vrf\": \"None\",\n                    },\n                    \"vrf\": \"None\",\n                },\n                \"eth11.123\": {\n                    \"address\": \"10.0.0.4/31\",\n                    \"description\": \"\",\n                    \"device\": \"fceos4\",\n                    \"interface\": \"eth11.123\",\n                    \"peer\": {\n                        \"address\": \"10.0.0.5/31\",\n                        \"description\": \"\",\n                        \"device\": \"fceos5\",\n                        \"interface\": \"eth11.123\",\n                        \"vrf\": \"None\",\n                    },\n                    \"vrf\": \"None\",\n                },\n            },\n            \"fceos5\": {\n                \"ae5.101\": {\n                    \"address\": \"10.0.0.1/31\",\n                    \"description\": \"\",\n                    \"device\": \"fceos5\",\n                    \"interface\": \"ae5.101\",\n                    \"vrf\": \"None\",\n                },\n                \"eth11.123\": {\n                    \"address\": \"10.0.0.5/31\",\n                    \"description\": \"\",\n                    \"device\": \"fceos5\",\n                    \"interface\": \"eth11.123\",\n                    \"vrf\": \"None\",\n                },\n            },\n        }\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-execution","title":"Test Execution","text":""},{"location":"testing/netbox_service_tests/#configuration","title":"Configuration","text":"<p>Netbox tests require configuration in <code>netbox_data.py</code>:</p> <pre><code>NB_URL = \"http://netbox-instance:8000\"\nNB_API_TOKEN = \"your-api-token\"\n</code></pre> <p>Tests operate against three Netbox instances configured in inventory: dev, preprod, and prod.</p>"},{"location":"testing/netbox_service_tests/#running-tests","title":"Running Tests","text":"<pre><code># Run all Netbox tests\npytest tests/test_netbox_service.py -v\n\n# Run specific test class\npytest tests/test_netbox_service.py::TestGetDevices -v\n\n# Run specific test method\npytest tests/test_netbox_service.py::TestGetDevices::test_get_devices_with_filters -v\n</code></pre>"},{"location":"testing/netbox_service_tests/#test-utilities","title":"Test Utilities","text":"<p>Helper functions available in test file:</p> <ul> <li><code>get_nb_version(nfclient, instance=None)</code> - Get Netbox version</li> <li><code>delete_branch(branch, nfclient)</code> - Delete transaction branch</li> <li><code>delete_interfaces(nfclient, device, interface)</code> - Delete interface</li> <li><code>delete_prefixes_within(prefix, nfclient)</code> - Delete prefix subtree</li> <li><code>delete_ips(prefix, nfclient)</code> - Delete IPs in prefix</li> <li><code>get_pynetbox(nfclient)</code> - Get pynetbox API instance</li> </ul>"},{"location":"testing/netbox_service_tests/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Connection errors: Verify Netbox is running and <code>netbox_data.py</code> is configured correctly</li> <li>Fixture timeout: Increase sleep time in fixture if workers need more time to start</li> <li>Cache issues: Clear cache between test runs if stale data is observed</li> <li>Branch isolation: Always delete test branches after use to prevent state pollution</li> </ul>"},{"location":"testing/netbox_service_tests/#related-documentation","title":"Related Documentation","text":"<ul> <li>Netbox Service Overview</li> <li>Netbox Service Tasks</li> <li>NORFAB Testing Framework</li> <li>Netbox Worker API Reference</li> </ul>"},{"location":"testing/norfab_testing_framework/","title":"NORFAB Testing Framework","text":""},{"location":"testing/norfab_testing_framework/#overview","title":"Overview","text":"<p>NORFAB employs a comprehensive testing framework built on pytest to ensure code quality, functionality, and reliability across all components. The testing infrastructure includes unit tests, integration tests, and service-specific tests that validate the core framework, workers, and client implementations.</p>"},{"location":"testing/norfab_testing_framework/#testing-architecture","title":"Testing Architecture","text":""},{"location":"testing/norfab_testing_framework/#test-organization","title":"Test Organization","text":"<p>The NORFAB testing framework is organized as follows:</p> <ul> <li>Core Tests (<code>tests/core/</code>): Tests for fundamental NORFAB components</li> <li><code>test_nfapi.py</code> - NFAPI (Python API) tests</li> <li><code>test_client.py</code> - Client functionality tests</li> <li><code>test_worker.py</code> - Worker base class tests</li> <li> <p><code>test_simple_inventory_datastore.py</code> - Inventory datastore tests</p> </li> <li> <p>Service Tests (root <code>tests/</code> directory): Tests for service workers</p> </li> <li><code>test_containerlab_service.py</code> - Containerlab service tests</li> <li><code>test_nornir_service.py</code> - Nornir service tests</li> <li><code>test_netbox_service.py</code> - Netbox service tests</li> <li><code>test_fastapi_service.py</code> - FastAPI service tests</li> <li><code>test_fastmcp_service.py</code> - FastMCP service tests</li> <li><code>test_workflow_service.py</code> - Workflow service tests</li> <li><code>test_dummy_service_plugin.py</code> - Plugin service tests</li> </ul>"},{"location":"testing/norfab_testing_framework/#test-support-files","title":"Test Support Files","text":"<ul> <li><code>conftest.py</code> - Pytest fixtures for test initialization and teardown</li> <li><code>netbox_data.py</code> - Netbox instance configuration and test data</li> <li><code>nf_tests_inventory/</code> - Test inventory files used across test suites</li> </ul>"},{"location":"testing/norfab_testing_framework/#testing-framework","title":"Testing Framework","text":""},{"location":"testing/norfab_testing_framework/#pytest-fixtures","title":"pytest Fixtures","text":"<p>The testing framework uses pytest fixtures to manage test lifecycle:</p>"},{"location":"testing/norfab_testing_framework/#norfab-client-fixture-nfclient","title":"NorFab Client Fixture (<code>nfclient</code>)","text":"<p>Creates a NorFab instance with test inventory, starts all workers, and provides a client object:</p> <pre><code>@pytest.fixture(scope=\"class\")\ndef nfclient():\n    \"\"\"Fixture to start NorFab and return client object\"\"\"\n    nf = NorFab(inventory=\"./nf_tests_inventory/inventory.yaml\")\n    nf.start()\n    time.sleep(3)  # wait for workers to start\n    yield nf.make_client()  # return nf client\n    nf.destroy()  # teardown\n</code></pre> <p>Scope: Class-level (single instance per test class) Teardown: Automatically destroys NorFab instance after tests complete</p>"},{"location":"testing/norfab_testing_framework/#dictionary-based-inventory-fixture-nfclient_dict_inventory","title":"Dictionary-based Inventory Fixture (<code>nfclient_dict_inventory</code>)","text":"<p>Creates a NorFab instance using a programmatically defined inventory dictionary:</p> <pre><code>@pytest.fixture(scope=\"class\")\ndef nfclient_dict_inventory():\n    \"\"\"Fixture to start NorFab with dict inventory\"\"\"\n    data = {\n        \"broker\": {\"endpoint\": \"tcp://127.0.0.1:5555\", ...},\n        \"topology\": {\"broker\": True, \"workers\": [...]},\n        \"workers\": {...}\n    }\n    nf = NorFab(inventory_data=data, base_dir=\"./nf_tests_inventory/\")\n    nf.start()\n    time.sleep(3)\n    yield nf.make_client()\n    nf.destroy()\n</code></pre> <p>Use Case: Testing with custom worker configurations and topology definitions</p>"},{"location":"testing/norfab_testing_framework/#picle-shell-fixture-picle_shell","title":"PICLE Shell Fixture (<code>picle_shell</code>)","text":"<p>Initializes the PICLE shell client for interactive testing:</p> <pre><code>@pytest.fixture(scope=\"class\")\ndef picle_shell():\n    \"\"\"Fixture for PICLE shell testing\"\"\"\n    nf = NorFab(inventory=\"./nf_tests_inventory/inventory.yaml\")\n    nf.start()\n    time.sleep(3)\n    NFCLIENT = nf.make_client()\n    builtins.NFCLIENT = NFCLIENT\n    shell = App(NorFabShell, stdin=mock_stdin, stdout=mock_stdout)\n    mount_shell_plugins(shell, nf.inventory)\n    yield shell, mock_stdout\n    nf.destroy()\n</code></pre> <p>Use Case: Testing NFCLI shell client functionality</p>"},{"location":"testing/norfab_testing_framework/#writing-tests","title":"Writing Tests","text":""},{"location":"testing/norfab_testing_framework/#test-structure","title":"Test Structure","text":"<p>Tests follow standard pytest conventions with class-based organization:</p> <pre><code>import pytest\n\nclass TestServiceName:\n    def test_specific_functionality(self, nfclient):\n        # Arrange\n        # Act\n        ret = nfclient.run_job(\"service\", \"task\", workers=\"any\")\n\n        # Assert\n        assert not ret.errors\n        assert \"expected_key\" in ret.result\n</code></pre>"},{"location":"testing/norfab_testing_framework/#common-testing-patterns","title":"Common Testing Patterns","text":""},{"location":"testing/norfab_testing_framework/#running-a-job","title":"Running a Job","text":"<p>All service tests interact with NORFAB through the client's <code>run_job()</code> method:</p> <pre><code>ret = nfclient.run_job(\n    service_name,  # service name (e.g., \"netbox\", \"nornir\")\n    task_name,     # task name\n    workers=\"any\", # worker filter\n    kwargs={...}   # task-specific parameters\n)\n</code></pre> <p>Response Structure: <pre><code>{\n    \"worker_name\": {\n        \"errors\": [],           # List of error messages\n        \"result\": {...},        # Task result data\n        \"task_id\": \"uuid\",      # Task identifier\n        \"status\": \"done\"        # Task status\n    }\n}\n</code></pre></p>"},{"location":"testing/norfab_testing_framework/#assertions","title":"Assertions","text":"<p>Common assertion patterns in tests:</p> <pre><code># Check for no errors\nassert not res[\"errors\"]\n\n# Validate response structure\nassert all(k in res[\"result\"] for k in [\"expected\", \"keys\"])\n\n# Check specific values\nassert res[\"result\"][\"status\"] == \"success\"\nassert len(res[\"result\"][\"items\"]) &gt; 0\n</code></pre>"},{"location":"testing/norfab_testing_framework/#test-dependencies","title":"Test Dependencies","text":"<p>The testing framework requires:</p> <ul> <li>pytest - Testing framework and runner</li> <li>pyyaml - Inventory file parsing</li> <li>pyzmq - ZeroMQ communication for NorFab</li> <li>All service dependencies - Nornir, Netbox, Containerlab, etc.</li> </ul> <p>Install test dependencies:</p> <pre><code>poetry install  # Install all dependencies including optional ones\n</code></pre>"},{"location":"testing/norfab_testing_framework/#running-tests","title":"Running Tests","text":""},{"location":"testing/norfab_testing_framework/#run-all-tests","title":"Run All Tests","text":"<pre><code>pytest tests/\n</code></pre>"},{"location":"testing/norfab_testing_framework/#run-specific-test-file","title":"Run Specific Test File","text":"<pre><code>pytest tests/test_netbox_service.py\n</code></pre>"},{"location":"testing/norfab_testing_framework/#run-specific-test-class","title":"Run Specific Test Class","text":"<pre><code>pytest tests/test_netbox_service.py::TestNetboxWorker\n</code></pre>"},{"location":"testing/norfab_testing_framework/#run-specific-test-method","title":"Run Specific Test Method","text":"<pre><code>pytest tests/test_netbox_service.py::TestNetboxWorker::test_get_devices\n</code></pre>"},{"location":"testing/norfab_testing_framework/#run-with-verbose-output","title":"Run with Verbose Output","text":"<pre><code>pytest tests/ -v\n</code></pre>"},{"location":"testing/norfab_testing_framework/#test-inventory","title":"Test Inventory","text":"<p>The test inventory is located in <code>tests/nf_tests_inventory/</code> and defines:</p> <ul> <li>Broker configuration - ZeroMQ endpoint and shared key</li> <li>Worker topology - Worker names and assignments</li> <li>Worker configurations - Service and plugin definitions</li> <li>Service inventories - Service-specific configuration files</li> </ul>"},{"location":"testing/norfab_testing_framework/#inventory-files","title":"Inventory Files","text":"<ul> <li><code>inventory.yaml</code> - Main inventory file</li> <li><code>nornir/</code> - Nornir service configurations</li> <li><code>nf_containerlab/</code> - Containerlab test environments</li> </ul>"},{"location":"testing/norfab_testing_framework/#test-setup-and-teardown","title":"Test Setup and Teardown","text":""},{"location":"testing/norfab_testing_framework/#automatic-cleanup","title":"Automatic Cleanup","text":"<p>All fixtures use class-level scope with automatic teardown:</p> <ol> <li>Setup Phase: </li> <li>Create NorFab instance</li> <li>Load inventory</li> <li>Start broker</li> <li>Start workers</li> <li> <p>Wait for initialization</p> </li> <li> <p>Test Phase: </p> </li> <li>Run individual test methods</li> <li> <p>Reuse same NorFab instance across tests</p> </li> <li> <p>Teardown Phase: </p> </li> <li>Destroy NorFab instance</li> <li>Clean up worker connections</li> <li>Release resources</li> </ol>"},{"location":"testing/norfab_testing_framework/#best-practices","title":"Best Practices","text":""},{"location":"testing/norfab_testing_framework/#1-use-appropriate-fixtures","title":"1. Use Appropriate Fixtures","text":"<p>Choose the right fixture for your test type: - <code>nfclient</code> - General service testing - <code>nfclient_dict_inventory</code> - Custom topology testing - <code>picle_shell</code> - CLI client testing</p>"},{"location":"testing/norfab_testing_framework/#2-wait-for-worker-readiness","title":"2. Wait for Worker Readiness","text":"<p>Always allow time for workers to initialize:</p> <pre><code>@pytest.fixture(scope=\"class\")\ndef nfclient():\n    nf = NorFab(...)\n    nf.start()\n    time.sleep(3)  # Critical for worker initialization\n    yield nf.make_client()\n    nf.destroy()\n</code></pre>"},{"location":"testing/norfab_testing_framework/#3-validate-response-structure","title":"3. Validate Response Structure","text":"<p>Always check for errors and expected keys:</p> <pre><code>assert not res[\"errors\"]\nassert all(k in res[\"result\"] for k in expected_keys)\n</code></pre>"},{"location":"testing/norfab_testing_framework/#4-use-descriptive-test-names","title":"4. Use Descriptive Test Names","text":"<p>Test method names should clearly indicate what is being tested:</p> <pre><code>def test_create_device_with_valid_data(self, nfclient):\n    \"\"\"Test creating a device with valid Netbox data\"\"\"\n    pass\n\ndef test_handle_invalid_device_data(self, nfclient):\n    \"\"\"Test error handling for invalid device data\"\"\"\n    pass\n</code></pre>"},{"location":"testing/norfab_testing_framework/#5-clean-up-test-data","title":"5. Clean Up Test Data","text":"<p>Always clean up created resources to prevent test interference:</p> <pre><code>def test_create_and_cleanup(self, nfclient):\n    # Create test data\n    create_result = nfclient.run_job(...)\n\n    try:\n        # Test operations\n        assert create_result[\"success\"]\n    finally:\n        # Always cleanup\n        cleanup_result = nfclient.run_job(...)\n        assert not cleanup_result[\"errors\"]\n</code></pre>"},{"location":"testing/norfab_testing_framework/#6-mock-external-dependencies","title":"6. Mock External Dependencies","text":"<p>For tests requiring external services (Netbox, Containerlab):</p> <pre><code>import unittest.mock\n\n@unittest.mock.patch('external_service.api')\ndef test_with_mocked_service(self, mock_api, nfclient):\n    mock_api.return_value = {\"status\": \"ok\"}\n    # Test with mocked service\n    pass\n</code></pre>"},{"location":"testing/norfab_testing_framework/#continuous-integration","title":"Continuous Integration","text":"<p>The testing framework supports CI/CD pipelines:</p> <pre><code># Run tests with coverage report\npytest tests/ --cov=norfab --cov-report=html\n\n# Run tests with JUnit XML output\npytest tests/ --junit-xml=test-results.xml\n\n# Run tests with specific markers\npytest tests/ -m \"not integration\"\n</code></pre>"},{"location":"testing/norfab_testing_framework/#troubleshooting","title":"Troubleshooting","text":""},{"location":"testing/norfab_testing_framework/#worker-startup-timeout","title":"Worker Startup Timeout","text":"<p>If workers fail to start within the 3-second wait:</p> <pre><code>time.sleep(5)  # Increase wait time\n</code></pre>"},{"location":"testing/norfab_testing_framework/#import-errors","title":"Import Errors","text":"<p>Ensure all dependencies are installed:</p> <pre><code>poetry install  # Install all optional dependencies\n</code></pre>"},{"location":"testing/norfab_testing_framework/#port-conflicts","title":"Port Conflicts","text":"<p>Multiple test runs may conflict on ZeroMQ ports. Clear stale processes:</p> <pre><code>lsof -i :5555  # Find process using port 5555\nkill -9 &lt;PID&gt;  # Kill process\n</code></pre>"},{"location":"testing/norfab_testing_framework/#worker-connection-issues","title":"Worker Connection Issues","text":"<p>Check worker status using the MMI service:</p> <pre><code>status = nfclient.get(\"mmi.service.broker\", \"show_workers\")\n</code></pre>"},{"location":"testing/norfab_testing_framework/#related-documentation","title":"Related Documentation","text":"<ul> <li>NORFAB Getting Started</li> <li>NORFAB Architecture</li> <li>Services and Workers Documentation</li> <li>Service Plugin Development</li> </ul>"},{"location":"workers/agent/api_reference_workers_agent_worker/","title":"Agent Worker","text":""},{"location":"workers/agent/api_reference_workers_agent_worker/#norfab.workers.agent_worker.agent_worker.AgentWorker","title":"<code>AgentWorker(inventory: Any, broker: str, worker_name: str, exit_event: Any = None, init_done_event: Any = None, log_level: str = 'WARNING', log_queue: object = None)</code>","text":"<p>               Bases: <code>NFPWorker</code></p> <p>This class represents a worker that interacts with a language model to handle various tasks such as chatting with users, retrieving inventory, and producing version reports of Python packages.</p> <p>Parameters:</p> Name Type Description Default <code>inventory</code> <code>Any</code> <p>The inventory object to be used by the worker.</p> required <code>broker</code> <code>str</code> <p>The broker URL to connect to.</p> required <code>worker_name</code> <code>str</code> <p>The name of this worker.</p> required <code>exit_event</code> <code>Any</code> <p>An event that, if set, indicates the worker needs to stop/exit.</p> <code>None</code> <code>init_done_event</code> <code>Any</code> <p>An event to set when the worker has finished initializing.</p> <code>None</code> <code>log_level</code> <code>str</code> <p>The logging level of this worker. Defaults to \"WARNING\".</p> <code>'WARNING'</code> <code>log_queue</code> <code>object</code> <p>The logging queue object.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>agent_inventory</code> <p>The inventory loaded from the broker.</p> <code>llm_model</code> <code>str</code> <p>The language model to be used. Defaults to \"llama3.1:8b\".</p> <code>llm_temperature</code> <code>float</code> <p>The temperature setting for the language model. Defaults to 0.5.</p> <code>llm_base_url</code> <code>str</code> <p>The base URL for the language model. Defaults to \"http://127.0.0.1:11434\".</p> <code>llm_flavour</code> <code>str</code> <p>The flavour of the language model. Defaults to \"ollama\".</p> <code>llm</code> <code>str</code> <p>The language model instance.</p> <p>Methods:</p> Name Description <code>worker_exit</code> <p>Placeholder method for worker exit logic.</p> <code>get_version</code> <p>Produces a report of the versions of Python packages.</p> <code>get_inventory</code> <p>Returns the agent's inventory.</p> <code>get_status</code> <p>Returns the status of the worker.</p> <code>_chat_ollama</code> <p>Handles the chat interaction with the Ollama LLM.</p> <code>chat</code> <p>Handles the chat interaction with the user by processing the input through a language model.</p> Source code in <code>norfab\\workers\\agent_worker\\agent_worker.py</code> <pre><code>def __init__(\n    self,\n    inventory: Any,\n    broker: str,\n    worker_name: str,\n    exit_event: Any = None,\n    init_done_event: Any = None,\n    log_level: str = \"WARNING\",\n    log_queue: object = None,\n):\n    super().__init__(\n        inventory, broker, SERVICE, worker_name, exit_event, log_level, log_queue\n    )\n    self.init_done_event = init_done_event\n\n    # get inventory from broker\n    self.agent_inventory = self.load_inventory()\n    self.llms = {}\n\n    self.init_done_event.set()\n    log.info(f\"{self.name} - Started\")\n</code></pre>"},{"location":"workers/agent/api_reference_workers_agent_worker/#norfab.workers.agent_worker.agent_worker.AgentWorker.get_llm","title":"<code>get_llm(model: str = None, provider: str = None, **kwargs) -&gt; object</code>","text":"<p>Retrieve or create an LLM instance.</p> <p>If no model_name is provided, this method consults agent service inventory for the <code>default_model</code> definition.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | None</code> <p>Name of the model to obtain.</p> <code>None</code> <code>provider</code> <code>str</code> <p>Model provider name, e.g. \"ollama\".</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>Any additional model parameters supported by LangChain.</p> <code>{}</code> <p>Returns:</p> Type Description <code>object</code> <p>object | None: The LLM instance (e.g. ChatOllama)</p> Source code in <code>norfab\\workers\\agent_worker\\agent_worker.py</code> <pre><code>def get_llm(self, model: str = None, provider: str = None, **kwargs) -&gt; object:\n    \"\"\"\n    Retrieve or create an LLM instance.\n\n    If no model_name is provided, this method consults agent service inventory for the\n    ``default_model`` definition.\n\n    Args:\n        model (str | None): Name of the model to obtain.\n        provider (str): Model provider name, e.g. \"ollama\".\n        kwargs (dict): Any additional model parameters supported by LangChain.\n\n    Returns:\n        object | None: The LLM instance (e.g. ChatOllama)\n    \"\"\"\n    # use inventory defined model or defaults\n    if model is None:\n        model_data = self.agent_inventory.get(\"default_model\")\n    else:\n        model_data = {\"model\": model, **kwargs}\n\n    # instantiate llm object\n    if model in self.llms:\n        llm = self.llms[model]\n    elif provider == \"ollama\":\n        llm = ChatOllama(**model_data)\n    else:\n        log.error(f\"Unsupported LLM provider '{provider}'\")\n        return None\n\n    # store LLM for future references\n    self.llms.setdefault(model, llm)\n\n    return llm\n</code></pre>"},{"location":"workers/agent/api_reference_workers_agent_worker/#norfab.workers.agent_worker.agent_worker.AgentWorker.get_version","title":"<code>get_version() -&gt; Result</code>","text":"<p>Generate a report of the versions of specific Python packages and system information. This method collects the version information of several Python packages and system details, including the Python version, platform, and a specified language model.</p> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing a dictionary with the package names as keys and their     respective version numbers as values. If a package is not found, its version     will be an empty string.</p> Source code in <code>norfab\\workers\\agent_worker\\agent_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_version(self) -&gt; Result:\n    \"\"\"\n    Generate a report of the versions of specific Python packages and system information.\n    This method collects the version information of several Python packages and system details,\n    including the Python version, platform, and a specified language model.\n\n    Returns:\n        Result: An object containing a dictionary with the package names as keys and their\n                respective version numbers as values. If a package is not found, its version\n                will be an empty string.\n    \"\"\"\n    libs = {\n        \"norfab\": \"\",\n        \"langchain\": \"\",\n        \"langchain-community\": \"\",\n        \"langchain-core\": \"\",\n        \"langchain-ollama\": \"\",\n        \"ollama\": \"\",\n        \"python\": sys.version.split(\" \")[0],\n        \"platform\": sys.platform,\n    }\n    # get version of packages installed\n    for pkg in libs.keys():\n        try:\n            libs[pkg] = importlib.metadata.version(pkg)\n        except importlib.metadata.PackageNotFoundError:\n            pass\n\n    return Result(result=libs)\n</code></pre>"},{"location":"workers/agent/api_reference_workers_agent_worker/#norfab.workers.agent_worker.agent_worker.AgentWorker.get_inventory","title":"<code>get_inventory() -&gt; Result</code>","text":"<p>NorFab task to retrieve the agent's inventory.</p> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An instance of the Result class containing the agent's inventory.</p> Source code in <code>norfab\\workers\\agent_worker\\agent_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_inventory(self) -&gt; Result:\n    \"\"\"\n    NorFab task to retrieve the agent's inventory.\n\n    Returns:\n        Result: An instance of the Result class containing the agent's inventory.\n    \"\"\"\n    return Result(result=self.agent_inventory)\n</code></pre>"},{"location":"workers/agent/api_reference_workers_agent_worker/#norfab.workers.agent_worker.agent_worker.AgentWorker.get_status","title":"<code>get_status() -&gt; Result</code>","text":"<p>NorFab Task that retrieves the status of the agent worker.</p> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the status result with a value of \"OK\".</p> Source code in <code>norfab\\workers\\agent_worker\\agent_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_status(self) -&gt; Result:\n    \"\"\"\n    NorFab Task that retrieves the status of the agent worker.\n\n    Returns:\n        Result: An object containing the status result with a value of \"OK\".\n    \"\"\"\n    return Result(result=\"OK\")\n</code></pre>"},{"location":"workers/agent/services_agent_service/","title":"Agent Service","text":"<p>The Agent Service is a key component of the Network Automations Fabric (NORFAB) that leverages AI-based agents to enhance network automation and management.</p> <p></p>","tags":["agent"]},{"location":"workers/agent/services_agent_service/#using-ollama","title":"Using Ollama","text":"<p>Ollama is an application designed to facilitate the use of large language models (LLMs) on your local machine. It provides a user-friendly interface and REST API for interacting with these models, enabling various AI-driven tasks such as natural language processing, text generation, and more. By running Ollama on your PC, you can leverage the power of LLMs for a wide range of applications, including network automation, data analysis, and intelligent agent services.</p> <p>In the context of the NorFab Agent Service, Ollama acts as a bridge between the AI agents and the LLMs, allowing the agents to connect to and utilize the capabilities of the models specified in the inventory. This integration enhances the functionality of the Agent Service, enabling more advanced and intelligent automation tasks.</p> <p>Follow official Guide on how to run Ollama Models - GitHub link</p> <p>Once Ollama application installed, configure NorFab Agent Worker inventory and proceed with running it by updating NorFab <code>inventory.yaml</code> file:</p> inventory.yaml<pre><code>broker:\n  endpoint: \"tcp://127.0.0.1:5555\"\n\nworkers:\n  agent-worker-1:\n    - agents/agent-worker-1.yaml\n\ntopology:\n  broker: True\n  workers:\n    - agent-worker-1\n</code></pre> <p>Where <code>agent-worker-1.yaml</code> content is:</p> <pre><code>service: agent\nbroker_endpoint: \"tcp://127.0.0.1:5555\"\nllm_flavour: ollama\nllm_model: lama3.1:8b\nllm_temperature: 0.5\nllm_base_url: \"http://127.0.0.1:11434\"\n</code></pre>","tags":["agent"]},{"location":"workers/agent/services_agent_service_inventory/","title":"Agent Worker Inventory","text":"<p>Sample Agent Worker Inventory</p> <pre><code>service: agent\nbroker_endpoint: \"tcp://127.0.0.1:5555\"\nllm_flavour: ollama\nllm_model: lama3.1:8b\nllm_temperature: 0.5\nllm_base_url: \"http://127.0.0.1:11434\"\n</code></pre>"},{"location":"workers/agent/services_agent_service_tasks_chat/","title":"Agent Service Chat Task","text":"","tags":["agent"]},{"location":"workers/agent/services_agent_service_tasks_chat/#agent-chat-sample-usage","title":"Agent Chat Sample Usage","text":"","tags":["agent"]},{"location":"workers/agent/services_agent_service_tasks_chat/#norfab-agent-chat-shell-reference","title":"NORFAB Agent Chat Shell Reference","text":"<p>NorFab shell supports these command options for Agent <code>chat</code> task:</p> <pre><code>nf#man tree agent\nroot\n\u2514\u2500\u2500 agent:    AI Agent service\n    \u251c\u2500\u2500 timeout:    Job timeout\n    \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n    \u251c\u2500\u2500 show:    Show Agent service parameters\n    \u2502   \u251c\u2500\u2500 inventory:    show agent inventory data\n    \u2502   \u251c\u2500\u2500 version:    show agent service version report\n    \u2502   \u2514\u2500\u2500 status:    show agent status\n    \u251c\u2500\u2500 chat:    Chat with the agent\n    \u2514\u2500\u2500 progress:    Emit execution progress, default 'True'\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["agent"]},{"location":"workers/agent/services_agent_service_tasks_chat/#python-api-reference","title":"Python API Reference","text":"Source code in <code>norfab\\workers\\agent_worker\\agent_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef invoke(\n    self,\n    job,\n    instructions: str,\n    name: str = \"NorFab\",\n    verbose_result: bool = False,\n) -&gt; Result:\n    ret = Result()\n    job.event(f\"Getting {name} agent ready\")\n\n    agent_data = self.get_agent(job, name)\n\n    agent_instance = create_agent(\n        name=agent_data[\"name\"],\n        model=self.get_llm(**agent_data[\"llm\"]),\n        system_prompt=agent_data[\"system_prompt\"],\n        tools=agent_data[\"tools\"],\n    )\n\n    job.event(f\"{name} agent thinking..\")\n\n    ret.result = agent_instance.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": instructions}]}\n    )\n\n    if verbose_result is False:\n        ret.result = ret.result[\"messages\"][-1].content\n\n    return ret\n</code></pre>","tags":["agent"]},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/","title":"Containerlab Worker","text":""},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/#norfab.workers.containerlab_worker.ContainerlabWorker","title":"<code>ContainerlabWorker(inventory: str, broker: str, worker_name: str, exit_event=None, init_done_event=None, log_level: str = None, log_queue: object = None)</code>","text":"<p>               Bases: <code>NFPWorker</code></p> <p>FastAPContainerlabWorker IWorker is a worker class that integrates with containerlab to run network topologies.</p> <p>Parameters:</p> Name Type Description Default <code>inventory</code> <code>str</code> <p>Inventory configuration for the worker.</p> required <code>broker</code> <code>str</code> <p>Broker URL to connect to.</p> required <code>worker_name</code> <code>str</code> <p>Name of this worker.</p> required <code>exit_event</code> <code>Event</code> <p>Event to signal worker to stop/exit.</p> <code>None</code> <code>init_done_event</code> <code>Event</code> <p>Event to signal when worker is done initializing.</p> <code>None</code> <code>log_level</code> <code>str</code> <p>Logging level for this worker.</p> <code>None</code> <code>log_queue</code> <code>object</code> <p>Queue for logging.</p> <code>None</code> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>def __init__(\n    self,\n    inventory: str,\n    broker: str,\n    worker_name: str,\n    exit_event=None,\n    init_done_event=None,\n    log_level: str = None,\n    log_queue: object = None,\n):\n    super().__init__(\n        inventory, broker, SERVICE, worker_name, exit_event, log_level, log_queue\n    )\n    self.init_done_event = init_done_event\n    self.exit_event = exit_event\n\n    # create directory to store lab topologies\n    self.topologies_dir = os.path.join(self.base_dir, \"topologies\")\n    os.makedirs(self.topologies_dir, exist_ok=True)\n\n    # merge local inventory with inventory from broker\n    merge_recursively(self.inventory[self.name], self.load_inventory())\n\n    self.clab_version = self.get_clab_version()\n\n    self.init_done_event.set()\n</code></pre>"},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/#norfab.workers.containerlab_worker.ContainerlabWorker.worker_exit","title":"<code>worker_exit()</code>","text":"<p>Terminates the current process by sending a SIGTERM signal to itself.</p> <p>This method retrieves the current process ID using <code>os.getpid()</code> and then sends a SIGTERM signal to terminate the process using <code>os.kill()</code>.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>def worker_exit(self):\n    \"\"\"\n    Terminates the current process by sending a SIGTERM signal to itself.\n\n    This method retrieves the current process ID using `os.getpid()` and then\n    sends a SIGTERM signal to terminate the process using `os.kill()`.\n    \"\"\"\n    os.kill(os.getpid(), signal.SIGTERM)\n</code></pre>"},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/#norfab.workers.containerlab_worker.ContainerlabWorker.get_version","title":"<code>get_version() -&gt; Result</code>","text":"<p>Produce a report of the versions of various Python packages.</p> <p>This method collects the versions of several specified Python packages and returns them in a dictionary.</p> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the task name and a dictionary with     the package names as keys and their respective versions as values.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_version(self) -&gt; Result:\n    \"\"\"\n    Produce a report of the versions of various Python packages.\n\n    This method collects the versions of several specified Python packages\n    and returns them in a dictionary.\n\n    Returns:\n        Result: An object containing the task name and a dictionary with\n                the package names as keys and their respective versions as values.\n    \"\"\"\n    libs = {\n        \"norfab\": \"\",\n        \"pydantic\": \"\",\n        \"python\": sys.version.split(\" \")[0],\n        \"platform\": sys.platform,\n        \"containerlab\": \".\".join([str(i) for i in self.clab_version]),\n    }\n    ret = Result(task=f\"{self.name}:get_version\", result=libs)\n\n    # get version of packages installed\n    for pkg in libs.keys():\n        try:\n            libs[pkg] = importlib.metadata.version(pkg)\n        except importlib.metadata.PackageNotFoundError:\n            pass\n\n    return ret\n</code></pre>"},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/#norfab.workers.containerlab_worker.ContainerlabWorker.get_inventory","title":"<code>get_inventory() -&gt; Result</code>","text":"<p>Retrieve the inventory of the Containerlab worker.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Result</code> <p>A dictionary containing the combined inventory of Containerlab.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_inventory(self) -&gt; Result:\n    \"\"\"\n    Retrieve the inventory of the Containerlab worker.\n\n    Returns:\n        Dict: A dictionary containing the combined inventory of Containerlab.\n    \"\"\"\n    return Result(\n        result=self.inventory[self.name],\n        task=f\"{self.name}:get_inventory\",\n    )\n</code></pre>"},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/#norfab.workers.containerlab_worker.ContainerlabWorker.get_containerlab_status","title":"<code>get_containerlab_status() -&gt; Result</code>","text":"<p>Retrieve the status of the Containerlab worker.</p> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>A result object containing the status of the Containerlab worker.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_containerlab_status(self) -&gt; Result:\n    \"\"\"\n    Retrieve the status of the Containerlab worker.\n\n    Returns:\n        Result: A result object containing the status of the Containerlab worker.\n    \"\"\"\n    status = \"OS NOT SUPPORTED\" if sys.platform.startswith(\"win\") else \"READY\"\n    return Result(\n        task=f\"{self.name}:get_containerlab_status\",\n        result={\"status\": status},\n    )\n</code></pre>"},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/#norfab.workers.containerlab_worker.ContainerlabWorker.get_running_labs","title":"<code>get_running_labs(job: Job, timeout: int = None) -&gt; Result</code>","text":"<p>Retrieve a list of running containerlab lab names.</p> <p>This method inspects the current state of containerlab and returns the names of labs that are currently running. The names are sorted and duplicates are removed.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>The timeout value in seconds for the inspection operation. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>A Result object containing the task name and a list of running</p> <code>Result</code> <p>lab names.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_running_labs(self, job: Job, timeout: int = None) -&gt; Result:\n    \"\"\"\n    Retrieve a list of running containerlab lab names.\n\n    This method inspects the current state of containerlab and returns\n    the names of labs that are currently running. The names are sorted\n    and duplicates are removed.\n\n    Args:\n        timeout (int, optional): The timeout value in seconds for the inspection\n            operation. Defaults to None.\n\n    Returns:\n        Result: A Result object containing the task name and a list of running\n        lab names.\n    \"\"\"\n    timeout = timeout or 600\n    ret = Result(task=f\"{self.name}:get_running_labs\", result=[])\n    inspect = self.inspect(job=job, timeout=timeout)\n\n    # form topologies list if any of them are running\n    if inspect.result:\n        ret.result = inspect.result.keys()\n        ret.result = list(sorted(set(ret.result)))\n\n    return ret\n</code></pre>"},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/#norfab.workers.containerlab_worker.ContainerlabWorker.run_containerlab_command","title":"<code>run_containerlab_command(job: Job, args: list, cwd: str = None, timeout: int = None, ret: Result = None, env: Union[None, dict] = None, expect_output: bool = True) -&gt; Tuple</code>","text":"<p>Executes a containerlab command using subprocess and processes its output.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>list</code> <p>The list of command-line arguments to execute.</p> required <code>cwd</code> <code>str</code> <p>The working directory to execute the command in. Defaults to None.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The timeout for the command execution in seconds. Defaults to None.</p> <code>None</code> <code>ret</code> <code>Result</code> <p>An optional Norfab result object to populate with the command's output. Defaults to None.</p> <code>None</code> <code>env</code> <code>(dict, Optional)</code> <p>OS Environment variables to use when running the process</p> <code>None</code> <code>expect_output</code> <code>(bool, Optional)</code> <p>whether to expect any output from command</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple</code> <p>If <code>ret</code> is None, returns a tuple containing: - output (str): The standard output of the command. - logs (list): A list of log messages from the command's standard error. - proc (subprocess.Popen): The subprocess object for the executed command.</p> <code>Result</code> <code>Tuple</code> <p>If <code>ret</code> is provided, returns the populated <code>Result</code> object with the following attributes: - result: The parsed JSON output or raw output of the command. - failed (bool): Indicates if the command execution failed. - errors (list): A list of error messages if the command failed. - messages (list): A list of log messages if the command succeeded.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the output cannot be parsed as JSON when <code>ret</code> is provided.</p> Notes <ul> <li>The method reads the command's standard error line by line and processes messages containing \"msg=\".</li> <li>If the command fails (non-zero return code), the <code>ret.failed</code> attribute is set to True, and errors are populated.</li> <li>If the command succeeds, the <code>ret.messages</code> attribute is populated with log messages.</li> </ul> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef run_containerlab_command(\n    self,\n    job: Job,\n    args: list,\n    cwd: str = None,\n    timeout: int = None,\n    ret: Result = None,\n    env: Union[None, dict] = None,\n    expect_output: bool = True,\n) -&gt; Tuple:\n    \"\"\"\n    Executes a containerlab command using subprocess and processes its output.\n\n    Args:\n        args (list): The list of command-line arguments to execute.\n        cwd (str, optional): The working directory to execute the command in. Defaults to None.\n        timeout (int, optional): The timeout for the command execution in seconds. Defaults to None.\n        ret (Result, optional): An optional Norfab result object to populate with the command's output. Defaults to None.\n        env (dict, Optional): OS Environment variables to use when running the process\n        expect_output (bool, Optional): whether to expect any output from command\n\n    Returns:\n        Tuple: If `ret` is None, returns a tuple containing:\n            - output (str): The standard output of the command.\n            - logs (list): A list of log messages from the command's standard error.\n            - proc (subprocess.Popen): The subprocess object for the executed command.\n        Result: If `ret` is provided, returns the populated `Result` object with the following attributes:\n            - result: The parsed JSON output or raw output of the command.\n            - failed (bool): Indicates if the command execution failed.\n            - errors (list): A list of error messages if the command failed.\n            - messages (list): A list of log messages if the command succeeded.\n\n    Raises:\n        Exception: If the output cannot be parsed as JSON when `ret` is provided.\n\n    Notes:\n        - The method reads the command's standard error line by line and processes messages containing \"msg=\".\n        - If the command fails (non-zero return code), the `ret.failed` attribute is set to True, and errors are populated.\n        - If the command succeeds, the `ret.messages` attribute is populated with log messages.\n    \"\"\"\n    timeout = timeout or 600\n    output, logs = \"\", []\n    begin = time.time()\n    timeout = timeout or 600\n    env = env or dict(os.environ)\n\n    with subprocess.Popen(\n        args,\n        cwd=cwd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n        env=env,\n    ) as proc:\n        while proc.poll() is None:\n            if time.time() - begin &gt; timeout:\n                raise TimeoutError(\n                    f\"Containerlab output collection {timeout}s timeout expired.\"\n                )\n            msg = proc.stderr.readline().strip()\n            if msg:\n                job.event(msg.split(\"msg=\")[-1].replace('\\\\\"', \"\").strip('\"'))\n                logs.append(msg)\n            time.sleep(0.01)\n        # read remaining messages\n        for msg in proc.stderr.readlines():\n            msg = msg.strip()\n            if msg:\n                job.event(msg.split(\"msg=\")[-1].replace('\\\\\"', \"\").strip('\"'))\n                logs.append(msg)\n            time.sleep(0.01)\n        # read process output\n        output = proc.stdout.read()\n\n    # populate Norfab result object\n    if ret is not None:\n        # check if command failed\n        if proc.returncode != 0:\n            ret.failed = True\n            ret.errors = [\"\\n\".join(logs)]\n        # check if got no output\n        elif not output.strip() and expect_output is True:\n            ret.failed = True\n            ret.errors = [\"\\n\".join(logs)]\n        else:\n            ret.messages = [\"\\n\".join(logs)]\n            try:\n                ret.result = json.loads(output)\n            except Exception:\n                # if failed, remove any beginning lines that are not part of json\n                try:\n                    line_split = output.splitlines()\n                    for index, line in enumerate(line_split):\n                        # find first json output line\n                        if \"{\" in line or \"[\" in line:\n                            ret.result = json.loads(\"\\n\".join(line_split[index:]))\n                            break\n                except Exception as e:\n                    ret.result = output\n                    log.error(\n                        f\"{self.name} - failed to load containerlab results into JSON, error: {e}, result: '{output}'\"\n                    )\n\n        return ret\n    # return command results as is\n    else:\n        return output, logs, proc\n</code></pre>"},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/#norfab.workers.containerlab_worker.ContainerlabWorker.deploy","title":"<code>deploy(job: Job, topology: str, reconfigure: bool = False, timeout: int = None, node_filter: Union[None, str] = None) -&gt; Result</code>","text":"<p>Deploys a containerlab topology.</p> <p>This method handles the deployment of a containerlab topology by downloading the topology file, organizing it into a specific folder structure, and executing the <code>containerlab deploy</code> command with the appropriate arguments.</p> <p>Parameters:</p> Name Type Description Default <code>topology</code> <code>str</code> <p>The path to the topology file to be deployed.</p> required <code>reconfigure</code> <code>bool</code> <p>If True, reconfigures an already deployed lab. Defaults to False.</p> <code>False</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for the deployment process. Defaults to None (no timeout).</p> <code>None</code> <code>node_filter</code> <code>str</code> <p>A filter to specify which nodes to deploy.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>deployment results with a list of nodes deployed</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the topology file cannot be fetched.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]}, input=DeployTask, output=DeployTaskResponse)\ndef deploy(\n    self,\n    job: Job,\n    topology: str,\n    reconfigure: bool = False,\n    timeout: int = None,\n    node_filter: Union[None, str] = None,\n) -&gt; Result:\n    \"\"\"\n    Deploys a containerlab topology.\n\n    This method handles the deployment of a containerlab topology by downloading\n    the topology file, organizing it into a specific folder structure, and executing\n    the `containerlab deploy` command with the appropriate arguments.\n\n    Args:\n        topology (str): The path to the topology file to be deployed.\n        reconfigure (bool, optional): If True, reconfigures an already deployed lab.\n            Defaults to False.\n        timeout (int, optional): The timeout in seconds for the deployment process.\n            Defaults to None (no timeout).\n        node_filter (str, optional): A filter to specify which nodes to deploy.\n\n    Returns:\n        Result: deployment results with a list of nodes deployed\n\n    Raises:\n        Exception: If the topology file cannot be fetched.\n    \"\"\"\n    timeout = timeout or 600\n    ret = Result(task=f\"{self.name}:deploy\")\n\n    # create folder to store topology\n    topology_folder = os.path.split(os.path.split(topology)[0])[-1]\n    topology_folder = os.path.join(self.topologies_dir, topology_folder)\n    os.makedirs(topology_folder, exist_ok=True)\n\n    # download topology file\n    topology_file = os.path.join(topology_folder, os.path.split(topology)[-1])\n    if self.is_url(topology):\n        downloaded_topology_file = self.fetch_file(\n            topology, raise_on_fail=True, read=False\n        )\n        os.rename(\n            downloaded_topology_file, topology_file\n        )  # move topology file under desired folder\n\n    # form command arguments\n    args = [\"containerlab\", \"deploy\", \"-f\", \"json\", \"-t\", topology_file]\n    if reconfigure is True:\n        args.append(\"--reconfigure\")\n        job.event(f\"Re-deploying lab {os.path.split(topology_file)[-1]}\")\n    else:\n        job.event(f\"Deploying lab {os.path.split(topology_file)[-1]}\")\n    if node_filter:\n        args.append(\"--node-filter\")\n        args.append(node_filter)\n\n    # add needed env variables\n    env = dict(os.environ)\n    env[\"CLAB_VERSION_CHECK\"] = \"disable\"\n\n    # run containerlab command\n    return self.run_containerlab_command(\n        args=args, cwd=topology_folder, timeout=timeout, ret=ret, env=env, job=job\n    )\n</code></pre>"},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/#norfab.workers.containerlab_worker.ContainerlabWorker.destroy_lab","title":"<code>destroy_lab(lab_name: str, job: Job, timeout: int = None) -&gt; Result</code>","text":"<p>Destroys a specified lab.</p> <p>Parameters:</p> Name Type Description Default <code>lab_name</code> <code>str</code> <p>The name of the lab to be destroyed.</p> required <code>timeout</code> <code>int</code> <p>The timeout value in seconds for the operation. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the status of the operation, errors (if any),     and the result indicating whether the lab was successfully destroyed.</p> Behavior <ul> <li>Retrieves the lab details using the <code>inspect</code> method.</li> <li>If the lab is not found, marks the operation as failed and returns an error.</li> <li>If the lab is found, retrieves the topology file and its folder.</li> <li>Executes the <code>containerlab destroy</code> command using the topology file.</li> <li>Updates the result to indicate success or failure of the destruction process.</li> </ul> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"DELETE\"]})\ndef destroy_lab(self, lab_name: str, job: Job, timeout: int = None) -&gt; Result:\n    \"\"\"\n    Destroys a specified lab.\n\n    Args:\n        lab_name (str): The name of the lab to be destroyed.\n        timeout (int, optional): The timeout value in seconds for the operation. Defaults to None.\n\n    Returns:\n        Result: An object containing the status of the operation, errors (if any),\n                and the result indicating whether the lab was successfully destroyed.\n\n    Behavior:\n        - Retrieves the lab details using the `inspect` method.\n        - If the lab is not found, marks the operation as failed and returns an error.\n        - If the lab is found, retrieves the topology file and its folder.\n        - Executes the `containerlab destroy` command using the topology file.\n        - Updates the result to indicate success or failure of the destruction process.\n    \"\"\"\n    timeout = timeout or 600\n    ret = Result(task=f\"{self.name}:destroy_lab\")\n\n    # get lab details\n    inspect = self.inspect(\n        job=job, timeout=timeout, lab_name=lab_name, details=True\n    )\n\n    if not inspect.result:\n        ret.failed = True\n        ret.errors = [f\"'{lab_name}' lab not found\"]\n        ret.result = {lab_name: False}\n    else:\n        topology_file = inspect.result[lab_name][0][\"Labels\"][\"clab-topo-file\"]\n        topology_folder = os.path.split(topology_file)[0]\n\n        # run destroy command\n        args = [\"containerlab\", \"destroy\", \"-t\", topology_file]\n        ret = self.run_containerlab_command(\n            args=args,\n            cwd=topology_folder,\n            timeout=timeout,\n            ret=ret,\n            job=job,\n            expect_output=False,\n        )\n\n        if not ret.failed:\n            ret.result = {lab_name: True}\n\n    return ret\n</code></pre>"},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/#norfab.workers.containerlab_worker.ContainerlabWorker.inspect","title":"<code>inspect(job: Job, lab_name: Union[None, str] = None, timeout: int = None, details: bool = False) -&gt; Result</code>","text":"<p>Inspect the container lab containers configuration and status.</p> <p>This method retrieves information about a specific container lab or all container labs, optionally including detailed information.</p> <p>Parameters:</p> Name Type Description Default <code>lab_name</code> <code>str</code> <p>The name of the container lab to inspect. If not provided, all container labs will be inspected.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The maximum time in seconds to wait for the inspection command to complete. Defaults to None.</p> <code>None</code> <code>details</code> <code>bool</code> <p>Whether to include detailed information in the inspection output. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the result of the inspection task.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef inspect(\n    self,\n    job: Job,\n    lab_name: Union[None, str] = None,\n    timeout: int = None,\n    details: bool = False,\n) -&gt; Result:\n    \"\"\"\n    Inspect the container lab containers configuration and status.\n\n    This method retrieves information about a specific container lab or all\n    container labs, optionally including detailed information.\n\n    Args:\n        lab_name (str, optional): The name of the container lab to inspect.\n            If not provided, all container labs will be inspected.\n        timeout (int, optional): The maximum time in seconds to wait for the\n            inspection command to complete. Defaults to None.\n        details (bool, optional): Whether to include detailed information in\n            the inspection output. Defaults to False.\n\n    Returns:\n        Result: An object containing the result of the inspection task.\n    \"\"\"\n    timeout = timeout or 600\n    ret = Result(task=f\"{self.name}:inspect\")\n\n    if lab_name:\n        args = [\"containerlab\", \"inspect\", \"-f\", \"json\", \"--name\", lab_name]\n    else:\n        args = [\"containerlab\", \"inspect\", \"-f\", \"json\", \"--all\"]\n    if details:\n        args.append(\"--details\")\n\n    ret = self.run_containerlab_command(\n        args=args, timeout=timeout, ret=ret, job=job\n    )\n\n    # check if lab name given and it is not in output\n    if lab_name and lab_name not in ret.result:\n        ret.failed = True\n        msg = f\"'{lab_name}' lab not found\"\n        ret.errors.append(msg)\n        log.error(msg)\n\n    return ret\n</code></pre>"},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/#norfab.workers.containerlab_worker.ContainerlabWorker.save","title":"<code>save(job: Job, lab_name: str, timeout: int = None) -&gt; Result</code>","text":"<p>Saves the config of a specified lab devices by invoking the <code>containerlab save</code> command.</p> <p>Parameters:</p> Name Type Description Default <code>lab_name</code> <code>str</code> <p>The name of the lab to save.</p> required <code>timeout</code> <code>int</code> <p>The maximum time in seconds to wait for the operation to complete. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the outcome of the save operation. If successful, <code>result</code> will contain a dictionary with the lab name as the key and <code>True</code> as the value. If unsuccessful, <code>failed</code> will be set to True, and <code>errors</code> will contain a list of error messages.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef save(self, job: Job, lab_name: str, timeout: int = None) -&gt; Result:\n    \"\"\"\n    Saves the config of a specified lab devices by invoking the `containerlab save` command.\n\n    Args:\n        lab_name (str): The name of the lab to save.\n        timeout (int, optional): The maximum time in seconds to wait for the operation\n            to complete. Defaults to None.\n\n    Returns:\n        Result: An object containing the outcome of the save operation. If successful,\n            `result` will contain a dictionary with the lab name as the key and `True`\n            as the value. If unsuccessful, `failed` will be set to True, and `errors`\n            will contain a list of error messages.\n    \"\"\"\n    timeout = timeout or 600\n    ret = Result(task=f\"{self.name}:save\")\n\n    # get lab details\n    inspect = self.inspect(\n        job=job, timeout=timeout, lab_name=lab_name, details=True\n    )\n\n    if not inspect.result:\n        ret.failed = True\n        ret.errors = [f\"'{lab_name}' lab not found\"]\n        ret.result = {lab_name: False}\n    else:\n        topology_file = inspect.result[lab_name][0][\"Labels\"][\"clab-topo-file\"]\n        topology_folder = os.path.split(topology_file)[0]\n\n        # run destroy command\n        args = [\"containerlab\", \"save\", \"-t\", topology_file]\n        ret = self.run_containerlab_command(\n            args=args,\n            cwd=topology_folder,\n            timeout=timeout,\n            ret=ret,\n            job=job,\n            expect_output=False,\n        )\n\n        if not ret.failed:\n            ret.result = {lab_name: True}\n\n    return ret\n</code></pre>"},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/#norfab.workers.containerlab_worker.ContainerlabWorker.restart_lab","title":"<code>restart_lab(job: Job, lab_name: str, timeout: int = None) -&gt; Result</code>","text":"<p>Restart a specified Containerlab lab.</p> <p>This method retrieves the lab details, destroys the existing lab, and redeploys it using the provided topology file.</p> <p>Parameters:</p> Name Type Description Default <code>lab_name</code> <code>str</code> <p>The name of the lab to restart.</p> required <code>timeout</code> <code>int</code> <p>The timeout value for the operation in seconds. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the status of the operation, any errors encountered,     and the result indicating whether the lab was successfully restarted.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef restart_lab(self, job: Job, lab_name: str, timeout: int = None) -&gt; Result:\n    \"\"\"\n    Restart a specified Containerlab lab.\n\n    This method retrieves the lab details, destroys the existing lab, and redeploys it\n    using the provided topology file.\n\n    Args:\n        lab_name (str): The name of the lab to restart.\n        timeout (int, optional): The timeout value for the operation in seconds. Defaults to None.\n\n    Returns:\n        Result: An object containing the status of the operation, any errors encountered,\n                and the result indicating whether the lab was successfully restarted.\n    \"\"\"\n    timeout = timeout or 600\n    ret = Result(task=f\"{self.name}:restart_lab\")\n\n    # get lab details\n    inspect = self.inspect(\n        job=job, timeout=timeout, lab_name=lab_name, details=True\n    )\n\n    if not inspect.result:\n        ret.failed = True\n        ret.errors = [f\"'{lab_name}' lab not found\"]\n        ret.result = {lab_name: False}\n    else:\n        topology_file = inspect.result[lab_name][0][\"Labels\"][\"clab-topo-file\"]\n        topology_folder = os.path.split(topology_file)[0]\n\n        # add needed env variables\n        env = dict(os.environ)\n        env[\"CLAB_VERSION_CHECK\"] = \"disable\"\n\n        # run destroy command\n        args = [\n            \"containerlab\",\n            \"deploy\",\n            \"-f\",\n            \"json\",\n            \"-t\",\n            topology_file,\n            \"--reconfigure\",\n        ]\n        ret = self.run_containerlab_command(\n            args=args,\n            cwd=topology_folder,\n            timeout=timeout,\n            ret=ret,\n            env=env,\n            job=job,\n        )\n\n        if not ret.failed:\n            ret.result = {lab_name: True}\n\n    return ret\n</code></pre>"},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/#norfab.workers.containerlab_worker.ContainerlabWorker.get_nornir_inventory","title":"<code>get_nornir_inventory(job: Job, lab_name: Union[None, str] = None, timeout: int = None, groups: Union[None, list] = None, use_default_credentials: bool = True) -&gt; Result</code>","text":"<p>Retrieves the Nornir inventory for a specified lab.</p> <p>This method inspects the container lab environment and generates a Nornir-compatible inventory of hosts based on the lab's configuration. It maps containerlab node kinds to Netmiko SSH platform types and extracts relevant connection details.</p> <p>Parameters:</p> Name Type Description Default <code>lab_name</code> <code>str</code> <p>The name of the container lab to inspect. If not given loads inventory for all labs.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The timeout value for the inspection operation. Defaults to None.</p> <code>None</code> <code>groups</code> <code>list</code> <p>A list of group names to assign to the hosts in the inventory. Defaults to None.</p> <code>None</code> <code>use_default_credentials</code> <code>bool</code> <p>Use Containerlab default credentials for hosts or not.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>A <code>Result</code> object containing the Nornir inventory. The <code>result</code> attribute</p> <code>Result</code> <p>includes a dictionary with host details. If the lab is not found or an error occurs,</p> <code>Result</code> <p>the <code>failed</code> attribute is set to True, and the <code>errors</code> attribute contains error messages.</p> Notes <ul> <li>The method uses a predefined mapping (<code>norfab.utils.platform_map</code>) to translate containerlab   node kinds to Netmiko platform types.</li> <li>If a container's SSH port cannot be determined, it is skipped, and an error is logged.</li> <li>The primary host IP address is determined dynamically using a UDP socket connection or   by checking the host IP address in the container's port configuration.</li> </ul> Example of returned inventory structure <p>{     \"hosts\": {         \"host_name\": {             \"hostname\": \"host_ip\",             \"platform\": \"netmiko_platform\",             \"groups\": [\"group1\", \"group2\"],         },         ...</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_nornir_inventory(\n    self,\n    job: Job,\n    lab_name: Union[None, str] = None,\n    timeout: int = None,\n    groups: Union[None, list] = None,\n    use_default_credentials: bool = True,\n) -&gt; Result:\n    \"\"\"\n    Retrieves the Nornir inventory for a specified lab.\n\n    This method inspects the container lab environment and generates a Nornir-compatible\n    inventory of hosts based on the lab's configuration. It maps containerlab node kinds\n    to Netmiko SSH platform types and extracts relevant connection details.\n\n    Args:\n        lab_name (str): The name of the container lab to inspect. If not given loads inventory for all labs.\n        timeout (int, optional): The timeout value for the inspection operation. Defaults to None.\n        groups (list, optional): A list of group names to assign to the hosts in the inventory. Defaults to None.\n        use_default_credentials (bool, optional): Use Containerlab default credentials for hosts or not.\n\n    Returns:\n        Result: A `Result` object containing the Nornir inventory. The `result` attribute\n        includes a dictionary with host details. If the lab is not found or an error occurs,\n        the `failed` attribute is set to True, and the `errors` attribute contains error messages.\n\n    Notes:\n        - The method uses a predefined mapping (`norfab.utils.platform_map`) to translate containerlab\n          node kinds to Netmiko platform types.\n        - If a container's SSH port cannot be determined, it is skipped, and an error is logged.\n        - The primary host IP address is determined dynamically using a UDP socket connection or\n          by checking the host IP address in the container's port configuration.\n\n    Example of returned inventory structure:\n        {\n            \"hosts\": {\n                \"host_name\": {\n                    \"hostname\": \"host_ip\",\n                    \"platform\": \"netmiko_platform\",\n                    \"groups\": [\"group1\", \"group2\"],\n                },\n                ...\n    \"\"\"\n    timeout = timeout or 600\n    groups = groups or []\n    ret = Result(task=f\"{self.name}:get_nornir_inventory\", result={\"hosts\": {}})\n\n    # get lab details\n    inspect = self.inspect(\n        job=job, lab_name=lab_name, timeout=timeout, details=True\n    )\n\n    # return empty result if lab not found\n    if not inspect.result:\n        ret.failed = True\n        ret.errors = [f\"'{lab_name}' lab not found\"]\n        return ret\n\n    # get host primary IP address\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    s.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n    s.connect((\"1.2.3.4\", 12345))\n    primary_host_ip = s.getsockname()[0]\n    log.debug(\n        f\"{self.name} - determined Containerlab host primary IP - '{primary_host_ip}'\"\n    )\n\n    # form hosts inventory\n    for lname, containers in inspect.result.items():\n        if lab_name and lname != lab_name:\n            continue\n        for container in containers:\n            host_name = container[\"Labels\"][\"clab-node-name\"]\n            host_port = None\n            host_ip = None\n\n            # get ssh port\n            for port in container[\"Ports\"]:\n                host_ip = primary_host_ip\n                if port[\"port\"] == 22 and port[\"protocol\"] == \"tcp\":\n                    host_port = port[\"host_port\"]\n                    # get host ip address\n                    if port[\"host_ip\"] not in [\n                        \"0.0.0.0\",\n                        \"127.0.0.1\",\n                        \"localhost\",\n                        \"::\",\n                    ]:\n                        host_ip = port[\"host_ip\"]\n                    break\n            else:\n                log.error(f\"{self.name} - {host_name} failed to map SSH port.\")\n                continue\n\n            # add host to Nornir inventory\n            ret.result[\"hosts\"][host_name] = {\n                \"hostname\": host_ip,\n                \"port\": host_port,\n                \"groups\": groups,\n            }\n\n            # get netmiko platform\n            clab_platform_name = container[\"Labels\"][\"clab-node-kind\"]\n            netmiko_platform = PlatformMap.convert(\n                \"containerlab\", \"netmiko\", clab_platform_name\n            )\n            if netmiko_platform:\n                ret.result[\"hosts\"][host_name][\"platform\"] = netmiko_platform[\n                    \"platform\"\n                ]\n            else:\n                log.warning(\n                    f\"{self.name} - {host_name} clab-node-kind '{clab_platform_name}' not mapped to Netmiko platform.\"\n                )\n                continue\n\n            # get default credentials\n            if use_default_credentials:\n                clab_platform = PlatformMap.get(\"containerlab\", clab_platform_name)\n                if not clab_platform:\n                    log.warning(\n                        f\"{self.name} - {host_name} clab-node-kind '{clab_platform_name}' not found.\"\n                    )\n                    continue\n                if clab_platform.get(\"username\"):\n                    ret.result[\"hosts\"][host_name][\"username\"] = clab_platform[\n                        \"username\"\n                    ]\n                if clab_platform.get(\"password\"):\n                    ret.result[\"hosts\"][host_name][\"password\"] = clab_platform[\n                        \"password\"\n                    ]\n\n    return ret\n</code></pre>"},{"location":"workers/containerlab/api_reference_workers_containerlab_worker/#norfab.workers.containerlab_worker.ContainerlabWorker.deploy_netbox","title":"<code>deploy_netbox(job: Job, lab_name: str = None, tenant: str = None, filters: Union[None, list] = None, devices: Union[None, list] = None, instance: str = None, image: str = None, ipv4_subnet: str = '172.100.100.0/24', ports: tuple = (12000, 15000), progress: bool = False, reconfigure: bool = False, timeout: int = 600, node_filter: Union[None, str] = None, dry_run: bool = False) -&gt; Result</code>","text":"<p>Deploys a containerlab topology using device data from the Netbox database.</p> <p>This method orchestrates the deployment of a containerlab topology by:</p> <ul> <li>Inspecting existing containers to determine subnets and ports in use.</li> <li>Allocating a management IPv4 subnet for the new lab, avoiding conflicts.</li> <li>Downloading inventory data from Netbox for the specified lab and filters.</li> <li>Saving the generated topology file to a dedicated folder.</li> <li>Executing the <code>containerlab deploy</code> command with appropriate arguments.</li> </ul> <p>To retrieve topology data from Netbox at least one of these arguments must be provided to identify a set of devices to include into Containerlab topology:</p> <ul> <li><code>tenant</code> - deploys lab using all devices and links that belong to this tenant</li> <li><code>devices</code> - lab deployed only using devices in the lists</li> <li><code>filters</code> - list of device filters to retrieve from Netbox</li> </ul> <p>If multiple of above arguments provided, resulting lab topology is a sum of all devices matched.</p> <p>Parameters:</p> Name Type Description Default <code>lab_name</code> <code>str</code> <p>The name to use for the lab to deploy.</p> <code>None</code> <code>tenant</code> <code>str</code> <p>Deploy lab for given tenant, lab name if not set becomes equal to tenant name.</p> <code>None</code> <code>filters</code> <code>list</code> <p>List of filters to apply when fetching devices from Netbox.</p> <code>None</code> <code>devices</code> <code>list</code> <p>List of specific devices to include in the topology.</p> <code>None</code> <code>instance</code> <code>str</code> <p>Netbox instance identifier.</p> <code>None</code> <code>image</code> <code>str</code> <p>Container image to use for devices.</p> <code>None</code> <code>ipv4_subnet</code> <code>str</code> <p>Management IPv4 subnet for the lab.</p> <code>'172.100.100.0/24'</code> <code>ports</code> <code>tuple</code> <p>Tuple specifying the range of ports to allocate.</p> <code>(12000, 15000)</code> <code>progress</code> <code>bool</code> <p>If True, emits progress events.</p> <code>False</code> <code>reconfigure</code> <code>bool</code> <p>If True, reconfigures an already deployed lab.</p> <code>False</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds for the deployment process.</p> <code>600</code> <code>node_filter</code> <code>str</code> <p>Comma-separated string of nodes to deploy.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, only generates and returns the topology inventory without deploying.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>deployment results with a list of nodes deployed</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the topology file cannot be fetched.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef deploy_netbox(\n    self,\n    job: Job,\n    lab_name: str = None,\n    tenant: str = None,\n    filters: Union[None, list] = None,\n    devices: Union[None, list] = None,\n    instance: str = None,\n    image: str = None,\n    ipv4_subnet: str = \"172.100.100.0/24\",\n    ports: tuple = (12000, 15000),\n    progress: bool = False,\n    reconfigure: bool = False,\n    timeout: int = 600,\n    node_filter: Union[None, str] = None,\n    dry_run: bool = False,\n) -&gt; Result:\n    \"\"\"\n    Deploys a containerlab topology using device data from the Netbox database.\n\n    This method orchestrates the deployment of a containerlab topology by:\n\n    - Inspecting existing containers to determine subnets and ports in use.\n    - Allocating a management IPv4 subnet for the new lab, avoiding conflicts.\n    - Downloading inventory data from Netbox for the specified lab and filters.\n    - Saving the generated topology file to a dedicated folder.\n    - Executing the `containerlab deploy` command with appropriate arguments.\n\n    To retrieve topology data from Netbox at least one of these arguments must be provided\n    to identify a set of devices to include into Containerlab topology:\n\n    - `tenant` - deploys lab using all devices and links that belong to this tenant\n    - `devices` - lab deployed only using devices in the lists\n    - `filters` - list of device filters to retrieve from Netbox\n\n    If multiple of above arguments provided, resulting lab topology is a sum of all\n    devices matched.\n\n    Args:\n        lab_name (str, optional): The name to use for the lab to deploy.\n        tenant (str, optional): Deploy lab for given tenant, lab name if not set\n            becomes equal to tenant name.\n        filters (list, optional): List of filters to apply when fetching devices from Netbox.\n        devices (list, optional): List of specific devices to include in the topology.\n        instance (str, optional): Netbox instance identifier.\n        image (str, optional): Container image to use for devices.\n        ipv4_subnet (str, optional): Management IPv4 subnet for the lab.\n        ports (tuple, optional): Tuple specifying the range of ports to allocate.\n        progress (bool, optional): If True, emits progress events.\n        reconfigure (bool, optional): If True, reconfigures an already deployed lab.\n        timeout (int, optional): Timeout in seconds for the deployment process.\n        node_filter (str, optional): Comma-separated string of nodes to deploy.\n        dry_run (bool, optional): If True, only generates and returns the topology\n            inventory without deploying.\n\n    Returns:\n        Result: deployment results with a list of nodes deployed\n\n    Raises:\n        Exception: If the topology file cannot be fetched.\n    \"\"\"\n    timeout = timeout or 600\n    ret = Result(task=f\"{self.name}:deploy_netbox\")\n    subnets_in_use = set()\n    ports_in_use = {}\n\n    # handle lab name and tenant name\n    if lab_name is None and tenant:\n        lab_name = tenant\n\n    # inspect existing containers\n    job.event(\"Checking existing containers\")\n    get_containers = self.inspect(job=job, details=True)\n    if get_containers.failed is True:\n        get_containers.task = f\"{self.name}:deploy_netbox\"\n        return get_containers\n\n    # collect TCP/UDP ports and subnets in use\n    job.event(\"Existing containers found, retrieving details\")\n    for lname, containers in get_containers.result.items():\n        for container in containers:\n            clab_name = container[\"Labels\"][\"containerlab\"]\n            clab_topo = container[\"Labels\"][\"clab-topo-file\"]\n            node_name = container[\"Labels\"][\"clab-node-name\"]\n            # collect ports that are in use\n            ports_in_use[node_name] = list(\n                set(\n                    [\n                        f\"{p['host_port']}:{p['port']}/{p['protocol']}\"\n                        for p in container[\"Ports\"]\n                        if \"host_port\" in p and \"port\" in p and \"protocol\" in p\n                    ]\n                )\n            )\n            # check existing subnets\n            if (\n                container[\"NetworkSettings\"][\"IPv4addr\"]\n                and container[\"NetworkSettings\"][\"IPv4pLen\"]\n            ):\n                ip = ipaddress.ip_interface(\n                    f\"{container['NetworkSettings']['IPv4addr']}/\"\n                    f\"{container['NetworkSettings']['IPv4pLen']}\"\n                )\n                subnet = str(ip.network.with_prefixlen)\n            else:\n                with open(clab_topo, encoding=\"utf-8\") as f:\n                    clab_topo_data = yaml.safe_load(f.read())\n                    if clab_topo_data.get(\"mgmt\", {}).get(\"ipv4-subnet\"):\n                        subnet = clab_topo_data[\"mgmt\"][\"ipv4-subnet\"]\n                    else:\n                        msg = f\"{clab_name} lab {node_name} node failed to determine mgmt subnet\"\n                        log.warning(msg)\n                        job.event(msg, severity=\"WARNING\")\n                        continue\n            subnets_in_use.add(subnet)\n            # reuse existing lab subnet\n            if clab_name == lab_name:\n                ipv4_subnet = subnet\n                job.event(\n                    f\"{ipv4_subnet} not in use by existing containers, using it\"\n                )\n            # allocate new subnet if its in use by other lab\n            elif clab_name != lab_name and ipv4_subnet == subnet:\n                msg = f\"{ipv4_subnet} already in use, allocating new subnet\"\n                log.info(msg)\n                job.event(msg)\n                ipv4_subnet = None\n\n    job.event(\"Collected TCP/UDP ports used by existing containers\")\n\n    # allocate new subnet\n    if ipv4_subnet is None:\n        pool = set(f\"172.100.{i}.0/24\" for i in range(100, 255))\n        ipv4_subnet = list(sorted(pool.difference(subnets_in_use)))[0]\n        job.event(f\"{lab_name} allocated new subnet {ipv4_subnet}\")\n\n    job.event(f\"{lab_name} fetching lab topology data from Netbox\")\n\n    # download inventory data from Netbox\n    netbox_reply = self.client.run_job(\n        service=\"netbox\",\n        task=\"get_containerlab_inventory\",\n        workers=\"any\",\n        timeout=timeout,\n        kwargs={\n            \"lab_name\": lab_name,\n            \"tenant\": tenant,\n            \"filters\": filters,\n            \"devices\": devices,\n            \"instance\": instance,\n            \"image\": image,\n            \"ipv4_subnet\": ipv4_subnet,\n            \"ports\": ports,\n            \"ports_map\": ports_in_use,\n            \"progress\": progress,\n        },\n    )\n\n    # use inventory from first worker that returned hosts data\n    for wname, wdata in netbox_reply.items():\n        if wdata[\"failed\"] is False and wdata[\"result\"]:\n            topology_inventory = wdata[\"result\"]\n            break\n    else:\n        msg = f\"{self.name} - Netbox returned no data for '{lab_name}' lab\"\n        log.error(msg)\n        raise RuntimeError(msg)\n\n    job.event(f\"{lab_name} topology data retrieved from Netbox\")\n\n    if dry_run is True:\n        ret.result = topology_inventory\n        return ret\n\n    # create folder to store topology\n    topology_folder = os.path.join(self.topologies_dir, lab_name)\n    os.makedirs(topology_folder, exist_ok=True)\n\n    # create topology file\n    topology_file = os.path.join(topology_folder, f\"{lab_name}.yaml\")\n    with open(topology_file, \"w\", encoding=\"utf-8\") as tf:\n        tf.write(yaml.dump(topology_inventory, default_flow_style=False))\n\n    job.event(f\"{lab_name} topology data saved to '{topology_file}'\")\n\n    return self.deploy(\n        job=job,\n        topology=topology_file,\n        reconfigure=reconfigure,\n        timeout=timeout,\n        node_filter=node_filter or \"\",\n    )\n</code></pre>"},{"location":"workers/containerlab/services_containerlab_service/","title":"Containerlab Service","text":"<p>Containerlab Service is based on Containerlab - an open-source tool for deploying and managing container-based network topologies. It provides a robust framework for automating network lab environments, enabling users to create, manage, and test network configurations efficiently.</p> <p></p> <p>Each NorFab Containerlab worker can manage multiple labs, offering scalability for complex network testing environments. </p> <p>By leveraging Containerlab, you can:</p> <ul> <li>Deploy network topologies using YAML-based configuration files.</li> <li>Source and deploy lab topologies from Netbox using Netbox Services.</li> <li>Inspect and monitor the status of running container labs.</li> <li>Save and restore lab configurations.</li> <li>Integrate with Nornir Service for automations testing.</li> </ul>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service/#supported-containerlab-versions","title":"Supported Containerlab Versions","text":"<ul> <li>0.67 - tested and supported by NorfAB before 0.11.0</li> <li>0.69 - tested and supported by NorfAB 0.11.0 onwards</li> </ul>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service/#containerlab-service-tasks","title":"Containerlab Service Tasks","text":"<p>Containerlab Service supports the following tasks:</p> Task Description Use Cases deploy Deploys a containerlab topology. Automating lab deployments, testing network configurations. deploy_netbox Deploys a containerlab topology using Netbox devices data. Automating lab deployments based on Netbox. destroy_lab Destroys a specified lab. Cleaning up resources, resetting lab environments. inspect Inspects the container lab containers configuration and status. Monitoring lab status, troubleshooting. save Saves the configuration of a specified lab. Backing up lab configurations. restart_lab Restarts a specified lab. Reinitializing lab environments. get_nornir_inventory Retrieves the Nornir inventory for a specified lab. Generating inventory for network automation tools.","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service/#containerlab-service-tasks-demo","title":"Containerlab Service Tasks Demo","text":"<p>Demos</p> DeployDeploy NetboxDestroyInspectSaveRestartNornir Inventory <p></p> <p></p> <p></p> <p></p> <p></p> <p> </p> <p> </p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service/#containerlab-service-show-commands","title":"Containerlab Service Show Commands","text":"<p>Containerlab service shell comes with a set of show commands to query service details:</p> <pre><code>nf#man tree show.containerlab\nroot\n\u2514\u2500\u2500 show:    NorFab show commands\n    \u2514\u2500\u2500 containerlab:    Show Containerlab service\n        \u251c\u2500\u2500 inventory:    show containerlab inventory data\n        \u251c\u2500\u2500 version:    show containerlab service version report\n        \u251c\u2500\u2500 status:    show containerlab status\n        \u251c\u2500\u2500 containers:    show containerlab containers\n        \u2502   \u251c\u2500\u2500 timeout:    Job timeout\n        \u2502   \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u2502   \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n        \u2502   \u251c\u2500\u2500 details:    Show container labs details\n        \u2502   \u2514\u2500\u2500 lab-name:    Show container for given lab only\n        \u2514\u2500\u2500 labs:    show containerlab running labs\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n            \u2514\u2500\u2500 verbose-result:    Control output details, default 'False'\nnf#\n</code></pre>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service/#nornir-service-integration","title":"Nornir Service Integration","text":"<p>NorFab Containerlab Service integrates seamlessly with the Nornir Service to enable network automation workflows. By using the <code>get_nornir_inventory</code> task, Containerlab Service can generate a Nornir-compatible inventory for a specified lab. This inventory can then be loaded into the Nornir Service, allowing it to manage the containerlab hosts as part of its runtime inventory. Refer to Nornir and Containerlab Services Integration</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service/#netbox-service-integration","title":"Netbox Service Integration","text":"<p>NorFab Containerlab Service integrates with Netbox to automate the deployment of network topologies using real device and link data. By leveraging the <code>deploy_netbox</code> task, users can:</p> <ul> <li>Fetch device and connection information directly from Netbox, using device lists, tenants, or advanced filters.</li> <li>Automatically generate Containerlab topology files based on Netbox inventory.</li> <li>Deploy, reconfigure, or update labs using up-to-date data from Netbox.</li> <li>Customize node parameters via Netbox device configuration context (e.g., image, kind, interface renaming).</li> <li>Seamlessly synchronize lab environments with the source of truth in Netbox.</li> </ul> <p>For more details and usage examples, see the deploy_netbox task documentation.</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service/#use-cases","title":"Use Cases","text":"<p>The Containerlab Service is ideal for the following scenarios:</p> <ol> <li> <p>Network Topology Testing:</p> <ul> <li>Deploy and test complex network topologies in a controlled environment.</li> <li>Validate network configurations and routing protocols before deploying to production.</li> </ul> </li> <li> <p>Network Automation Development:</p> <ul> <li>Use the <code>get_nornir_inventory</code> task to generate inventories for Nornir Service.</li> <li>Develop and test automation scripts against realistic network topologies.</li> </ul> </li> <li> <p>Training and Education:</p> <ul> <li>Create hands-on labs for network training programs.</li> <li>Simulate real-world network scenarios for educational purposes.</li> </ul> </li> <li> <p>Troubleshooting and Debugging:</p> <ul> <li>Recreate production network issues in a lab environment for debugging.</li> <li>Use the <code>inspect</code> task to monitor and analyze the state of network devices.</li> </ul> </li> <li> <p>Configuration Management:</p> <ul> <li>Save and restore network configurations using the <code>save</code> task.</li> <li>Reconfigure existing labs with updated topology files.</li> </ul> </li> <li> <p>Continuous Integration/Continuous Deployment (CI/CD):</p> <ul> <li>Integrate with CI/CD pipelines to validate network changes automatically.</li> <li>Use the <code>deploy</code> and <code>destroy_lab</code> tasks to spin up and tear down test environments dynamically.</li> </ul> </li> </ol> <p> Happy CLABing !! </p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_inventory/","title":"Containerlab Worker Inventory","text":"<p>Content of <code>inventory.yaml</code> need to be updated to include Containerlab worker details:</p> inventory.yaml<pre><code>broker: \n  endpoint: \"tcp://127.0.0.1:5555\" \n  shared_key: \"5z1:yW}]n?UXhGmz+5CeHN1&gt;:S9k!eCh6JyIhJqO\"\n\nworkers:\n  containerlab-worker-1: \n    - containerlab/containerlab-worker-1.yaml\n\ntopology: \n  workers: \n    - containerlab-worker-1\n</code></pre> <p>To obtain broker <code>shared_key</code> run this command on broker:</p> <pre><code>cd &lt;path/to/broker/inventory.yaml&gt;\nnfcli --show-broker-shared-key\n</code></pre> <p>Sample Containerlab worker inventory file content:</p> containerlab/containerlab-worker-1.yaml<pre><code>service: containerlab\n</code></pre>"},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy/","title":"Containerlab Service Deploy Task","text":"<p>task api name: <code>deploy</code></p> <p>The Containerlab service <code>deploy</code> task is designed to deploy network topologies using Containerlab. This task automates the deployment process by downloading the topology file, organizing it into a specific folder structure, and executing the <code>containerlab deploy</code> command with the appropriate arguments.</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy/#containerlab-deploy-task-overview","title":"Containerlab Deploy Task Overview","text":"<p>The <code>deploy</code> task provides the following features:</p> <ul> <li>Topology Deployment: Deploys a specified topology file to create a network lab.</li> <li>Reconfiguration: Supports reconfiguring an already deployed lab.</li> <li>Node Filtering: Allows deploying specific nodes using a filter.</li> <li>Timeouts: Configurable timeout for the deployment process.</li> </ul>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy/#containerlab-deploy-task-sample-usage","title":"Containerlab Deploy Task Sample Usage","text":"<p>Containerlab topology file content used in examples:</p> three-routers-lab.yaml<pre><code>name: three-routers-lab\n\ntopology:\n  nodes:\n    r1: \n      kind: ceos\n      image: ceosimage:4.30.0F\n      mgmt-ipv4: 172.100.101.12\n      ports:\n        - 12202:22\n        - 18802:80\n        - 18302:830\n        - 14402:443\n    r2: \n      kind: ceos\n      image: ceosimage:4.30.0F\n      mgmt-ipv4: 172.100.101.13\n      ports:\n        - 12203:22\n        - 18803:80\n        - 18303:830\n        - 14403:443\n    r3: \n      kind: ceos\n      image: ceosimage:4.30.0F\n      mgmt-ipv4: 172.100.101.14\n      ports:\n        - 12204:22\n        - 18804:80\n        - 18304:830\n        - 14404:443\n\n  links:\n    - endpoints: [\"r1:eth1\", \"r2:eth1\"]\n    - endpoints: [\"r2:eth2\", \"r3:eth2\"]\n    - endpoints: [\"r3:eth3\", \"r1:eth3\"]\n\nmgmt:\n  network: three_routers_lab    \n  ipv4-subnet: 172.100.101.0/24  \n</code></pre> <p>Below is an example of how to use the Containerlab deploy task to deploy a topology.</p> <p>Example</p> DemoCLIPython <p></p> <pre><code>nf# containerlab\nnf[containerlab]#deploy topology nf://containerlab/three-routers-topology.yaml\n--------------------------------------------- Job Events -----------------------------------------------\n05-May-2025 19:58:13.728 9e32d588fc27446db61200937974bb14 job started\n05-May-2025 19:58:13.753 INFO containerlab-worker-1 running containerlab.deploy  - Deploying lab three-routers-topology.yaml\n05-May-2025 19:58:13.770 INFO containerlab-worker-1 running containerlab.deploy  - 19:58:13 INFO Containerlab started version=0.67.0\n05-May-2025 19:58:13.782 INFO containerlab-worker-1 running containerlab.deploy  - 19:58:13 INFO Parsing &amp; checking topology file=three-routers-topology.yaml\n05-May-2025 19:58:13.793 INFO containerlab-worker-1 running containerlab.deploy  - 19:58:13 INFO Creating docker network name=three_routers_lab IPv4 subnet=172.100.101.0/24 IPv6 subnet=\"\" MTU=0\n05-May-2025 19:58:13.927 INFO containerlab-worker-1 running containerlab.deploy  - 19:58:13 INFO Creating container name=r3\n05-May-2025 19:58:13.939 INFO containerlab-worker-1 running containerlab.deploy  - 19:58:13 INFO Creating container name=r1\n05-May-2025 19:58:13.950 INFO containerlab-worker-1 running containerlab.deploy  - 19:58:13 INFO config file \n05-May-2025 19:58:13.961 INFO containerlab-worker-1 running containerlab.deploy  - 19:58:13 INFO Creating container name=r2\n05-May-2025 19:58:14.678 INFO containerlab-worker-1 running containerlab.deploy  - 19:58:14 INFO Running postdeploy actions for Arista cEOS 'r3' node\n05-May-2025 19:58:14.886 INFO containerlab-worker-1 running containerlab.deploy  - 19:58:14 INFO Created link: r2:eth2 \u25aa\u2504\u2504\u25aa r3:eth2\n05-May-2025 19:58:14.897 INFO containerlab-worker-1 running containerlab.deploy  - 19:58:14 INFO Running postdeploy actions for Arista cEOS 'r2' node\n05-May-2025 19:58:14.908 INFO containerlab-worker-1 running containerlab.deploy  - 19:58:14 INFO Created link: r1:eth1 \u25aa\u2504\u2504\u25aa r2:eth1\n05-May-2025 19:58:14.937 INFO containerlab-worker-1 running containerlab.deploy  - 19:58:14 INFO Created link: r3:eth3 \u25aa\u2504\u2504\u25aa r1:eth3\n05-May-2025 19:58:14.948 INFO containerlab-worker-1 running containerlab.deploy  - 19:58:14 INFO Running postdeploy actions for Arista cEOS 'r1' node\n05-May-2025 19:58:41.856 INFO containerlab-worker-1 running containerlab.deploy  - 19:58:41 INFO Adding host entries path=/etc/hosts\n05-May-2025 19:58:42.091 9e32d588fc27446db61200937974bb14 job completed in 28.363 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\ncontainerlab-worker-1:\n    ----------\n    containerlab-worker-1:\n        ----------\n        containers:\n            |_\n              ----------\n              lab_name:\n                  three-routers-lab\n              labPath:\n                  three-routers-topology.yaml\n              name:\n                  clab-three-routers-lab-r1\n              container_id:\n                  86205a93994c\n              image:\n                  ceosimage:4.30.0F\n              kind:\n                  ceos\n              state:\n                  running\n              ipv4_address:\n                  172.100.101.12/24\n              ipv6_address:\n                  N/A\n              owner:\n                  norfabuser\n            |_\n              ----------\n              lab_name:\n                  three-routers-lab\n              labPath:\n                  three-routers-topology.yaml\n              name:\n                  clab-three-routers-lab-r2\n              container_id:\n                  b5c74ff1f108\n              image:\n                  ceosimage:4.30.0F\n              kind:\n                  ceos\n              state:\n                  running\n              ipv4_address:\n                  172.100.101.13/24\n              ipv6_address:\n                  N/A\n              owner:\n                  norfabuser\n            |_\n              ----------\n              lab_name:\n                  three-routers-lab\n              labPath:\n                  three-routers-topology.yaml\n              name:\n                  clab-three-routers-lab-r3\n              container_id:\n                  1ddd29a0524d\n              image:\n                  ceosimage:4.30.0F\n              kind:\n                  ceos\n              state:\n                  running\n              ipv4_address:\n                  172.100.101.14/24\n              ipv6_address:\n                  N/A\n              owner:\n                  norfabuser\nnf[containerlab]# \n</code></pre> <p>In this example:</p> <ul> <li><code>nfcli</code> command starts the NorFab Interactive Shell</li> <li><code>containerlab</code> command switches to the Containerlab sub-shell</li> <li><code>deploy</code> command instruct Containerlab service to deploy a topology</li> <li><code>topology</code> specifies the path to the topology file stored on broker</li> </ul> <p>This code is complete and can run as is.</p> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"containerlab\",\n        task=\"deploy\",\n        kwargs={\n            \"topology\": \"nf://containerlab/three-routers-topology.yaml\",\n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy/#reconfiguring-an-existing-lab","title":"Reconfiguring an Existing Lab","text":"<p>The <code>deploy</code> task supports reconfiguring an already deployed lab by using the <code>reconfigure</code> argument. This allows you to update the lab configuration without destroying and redeploying it.</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy/#filtering-nodes-for-deployment","title":"Filtering Nodes for Deployment","text":"<p>The <code>deploy</code> task allows you to deploy specific nodes in a topology using the <code>node_filter</code> argument. This is useful for testing or updating specific parts of a lab without affecting the entire topology.</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy/#norfab-containerlab-cli-shell-reference","title":"NORFAB Containerlab CLI Shell Reference","text":"<p>Below are the commands supported by the <code>deploy</code> task:</p> <pre><code>nf#man tree containerlab.deploy\nroot\n\u2514\u2500\u2500 containerlab:    Containerlab service\n    \u2514\u2500\u2500 deploy:    Spins up a lab using provided topology\n        \u251c\u2500\u2500 timeout:    Job timeout\n        \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n        \u251c\u2500\u2500 *topology:    URL to topology file to deploy\n        \u251c\u2500\u2500 reconfigure:    Destroy the lab and then re-deploy it., default 'False'\n        \u251c\u2500\u2500 node-filter:    Comma-separated list of node names to deploy\n        \u2514\u2500\u2500 progress:    Display progress events, default 'True'\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy/#python-api-reference","title":"Python API Reference","text":"<p>Deploys a containerlab topology.</p> <p>This method handles the deployment of a containerlab topology by downloading the topology file, organizing it into a specific folder structure, and executing the <code>containerlab deploy</code> command with the appropriate arguments.</p> <p>Parameters:</p> Name Type Description Default <code>topology</code> <code>str</code> <p>The path to the topology file to be deployed.</p> required <code>reconfigure</code> <code>bool</code> <p>If True, reconfigures an already deployed lab. Defaults to False.</p> <code>False</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for the deployment process. Defaults to None (no timeout).</p> <code>None</code> <code>node_filter</code> <code>str</code> <p>A filter to specify which nodes to deploy.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>deployment results with a list of nodes deployed</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the topology file cannot be fetched.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]}, input=DeployTask, output=DeployTaskResponse)\ndef deploy(\n    self,\n    job: Job,\n    topology: str,\n    reconfigure: bool = False,\n    timeout: int = None,\n    node_filter: Union[None, str] = None,\n) -&gt; Result:\n    \"\"\"\n    Deploys a containerlab topology.\n\n    This method handles the deployment of a containerlab topology by downloading\n    the topology file, organizing it into a specific folder structure, and executing\n    the `containerlab deploy` command with the appropriate arguments.\n\n    Args:\n        topology (str): The path to the topology file to be deployed.\n        reconfigure (bool, optional): If True, reconfigures an already deployed lab.\n            Defaults to False.\n        timeout (int, optional): The timeout in seconds for the deployment process.\n            Defaults to None (no timeout).\n        node_filter (str, optional): A filter to specify which nodes to deploy.\n\n    Returns:\n        Result: deployment results with a list of nodes deployed\n\n    Raises:\n        Exception: If the topology file cannot be fetched.\n    \"\"\"\n    timeout = timeout or 600\n    ret = Result(task=f\"{self.name}:deploy\")\n\n    # create folder to store topology\n    topology_folder = os.path.split(os.path.split(topology)[0])[-1]\n    topology_folder = os.path.join(self.topologies_dir, topology_folder)\n    os.makedirs(topology_folder, exist_ok=True)\n\n    # download topology file\n    topology_file = os.path.join(topology_folder, os.path.split(topology)[-1])\n    if self.is_url(topology):\n        downloaded_topology_file = self.fetch_file(\n            topology, raise_on_fail=True, read=False\n        )\n        os.rename(\n            downloaded_topology_file, topology_file\n        )  # move topology file under desired folder\n\n    # form command arguments\n    args = [\"containerlab\", \"deploy\", \"-f\", \"json\", \"-t\", topology_file]\n    if reconfigure is True:\n        args.append(\"--reconfigure\")\n        job.event(f\"Re-deploying lab {os.path.split(topology_file)[-1]}\")\n    else:\n        job.event(f\"Deploying lab {os.path.split(topology_file)[-1]}\")\n    if node_filter:\n        args.append(\"--node-filter\")\n        args.append(node_filter)\n\n    # add needed env variables\n    env = dict(os.environ)\n    env[\"CLAB_VERSION_CHECK\"] = \"disable\"\n\n    # run containerlab command\n    return self.run_containerlab_command(\n        args=args, cwd=topology_folder, timeout=timeout, ret=ret, env=env, job=job\n    )\n</code></pre>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy_netbox/","title":"Containerlab Service Deploy Netbox Task","text":"<p>task api name: <code>deploy_netbox</code></p> <p>Containerlab service <code>deploy_netbox</code> task is designed to deploy network topologies using devices data retrieved from Netbox. This task automates the deployment process by fetching nodes and links data from Netbox and constructing the topology file, organizing it into a specific folder structure, and executing the <code>containerlab deploy</code> command with the appropriate arguments.</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy_netbox/#containerlab-deploy-netbox-task-overview","title":"Containerlab Deploy Netbox Task Overview","text":"<p>The <code>deploy_netbox</code> task provides the following features:</p> <ul> <li> <p>Automated Topology Deployment: Deploys a topology sourcing nodes and links data from Netbox using either or all of these:</p> <ul> <li>Netbox Devices List - use provided device names to construct topology and deploy lab</li> <li>Netbox Tenant - source devices for given tenant and deploy the lab</li> <li>Netbox Device Filters - fetch devices data from Netbox GraphQL API and deploy the lab</li> </ul> </li> <li> <p>Topology Links Sourcing - links formed using Netbox devices' connections and circuits data.</p> </li> <li>Reconfiguration: Supports reconfiguring an already deployed lab.</li> <li>Node Filtering: Allows deploying specific nodes using a filter.</li> </ul>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy_netbox/#how-it-works","title":"How It Works","text":"<p><code>deploy_netbox</code> task uses Netbox service get_containerlab_inventory task to fetch topology inventory data from Netbox.</p> <p></p> <ol> <li> <p>Client submits request to Containerlab service to deploy a lab</p> </li> <li> <p>Containerlab worker sends job request to Netbox service to retrieve topology data for requested devices</p> </li> <li> <p>Netbox service fetches data from the Netbox and constructs Containerlab inventory</p> </li> <li> <p>Netbox returns lab inventor data back to Containerlab worker</p> </li> <li> <p>Containerlab worker deploys lab using topology data provided by Netbox service</p> </li> </ol>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy_netbox/#device-config-context-containerlab-parameters","title":"Device Config Context Containerlab Parameters","text":"<p>Containerlab node details can be defined under device configuration context <code>norfab.containerlab</code> path, for example:</p> <pre><code>{\n    \"norfab\": {\n        \"containerlab\": {\n            \"kind\": \"ceos\",\n            \"image\": \"ceos:latest\",\n            \"mgmt-ipv4\": \"172.100.100.10/24\",\n            \"ports\": [\n                {10000: 22},\n                {10001: 830}\n            ],\n\n            ... any other containerlab node parameters ...\n\n            \"interfaces_rename\": [\n                {\n                    \"find\": \"Ethernet\",\n                    \"replace\": \"eth\",\n                    \"use_regex\": false\n                }\n            ]\n        }\n    }\n}\n</code></pre> <ul> <li><code>interfaces_rename</code>  - is a list of one or more interface renaming instructions, each item must have <code>find</code> and <code>replace</code> defined, optional <code>use_regex</code> flag specifies whether to use regex based pattern substitution.</li> <li><code>kind</code> - uses Netbox device <code>platform</code> field value by default</li> <li><code>image</code> - uses <code>image</code> value if provided, otherwise uses <code>{kind}:latest</code></li> </ul>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy_netbox/#containerlab-deploy-netbox-task-sample-usage","title":"Containerlab Deploy Netbox Task Sample Usage","text":"<p>Below is an example of how to use the Containerlab deploy Netbox task to deploy a topology.</p> <p>Examples</p> Demo - Devices ListCLIPython <p></p> <pre><code>nf#containerlab deploy-netbox lab-name foobar devices fceos4 fceos5\n--------------------------------------------- Job Events -----------------------------------------------\n31-May-2025 13:02:29.525 9e3b29210e1140f8b3a311e8c4669ca4 job started\n31-May-2025 13:02:29.533 INFO containerlab-worker-1 running containerlab.deploy_netbox  - Checking existing containers\n31-May-2025 13:02:29.573 INFO containerlab-worker-1 running containerlab.deploy_netbox  - Existing containers found, retrieving details\n31-May-2025 13:02:29.574 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 172.100.100.0/24 already in use, allocating new subnet\n31-May-2025 13:02:29.575 INFO containerlab-worker-1 running containerlab.deploy_netbox  - Collected TCP/UDP ports used by existing containers\n31-May-2025 13:02:29.576 INFO containerlab-worker-1 running containerlab.deploy_netbox  - foobar allocated new subnet 172.100.102.0/24\n31-May-2025 13:02:29.576 INFO containerlab-worker-1 running containerlab.deploy_netbox  - foobar fetching lab topology data from Netbox\n31-May-2025 13:02:31.090 INFO containerlab-worker-1 running containerlab.deploy_netbox  - foobar topology data retrieved from Netbox\n31-May-2025 13:02:31.094 INFO containerlab-worker-1 running containerlab.deploy_netbox  - foobar topology data saved to \n'/home/norfab/norfab/tests/nf_containerlab/__norfab__/files/worker/containerlab-worker-1/topologies/foobar/foobar.yaml'\n31-May-2025 13:02:31.095 INFO containerlab-worker-1 running containerlab.deploy_netbox  - foobar deploying lab using foobar.yaml topology file\n31-May-2025 13:02:31.123 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:31 INFO Containerlab started version=0.67.0\n31-May-2025 13:02:31.134 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:31 INFO Parsing &amp; checking topology file=foobar.yaml\n31-May-2025 13:02:31.145 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:31 INFO Creating docker network name=br-foobar IPv4 subnet=172.100.102.0/24 IPv6 subnet=\"\" MTU=0\n31-May-2025 13:02:31.257 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:31 INFO Creating lab directory \npath=/home/norfab/norfab/tests/nf_containerlab/__norfab__/files/worker/containerlab-worker-1/topologies/foobar/clab-foobar\n31-May-2025 13:02:31.268 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:31 INFO config file \n'/home/norfab/norfab/tests/nf_containerlab/__norfab__/files/worker/containerlab-worker-1/topologies/foobar/clab-foobar/fceos4/flash/startup-config' for node 'fceos4' already exists and will not be generated/reset\n31-May-2025 13:02:31.280 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:31 INFO Creating container name=fceos4\n31-May-2025 13:02:31.291 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:31 INFO config file \n'/home/norfab/norfab/tests/nf_containerlab/__norfab__/files/worker/containerlab-worker-1/topologies/foobar/clab-foobar/fceos5/flash/startup-config' for node 'fceos5' already exists and will not be generated/reset\n31-May-2025 13:02:31.302 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:31 INFO Creating container name=fceos5\n31-May-2025 13:02:32.065 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:32 INFO Created link: fceos5:eth1 \u25aa\u2504\u2504\u25aa fceos4:eth1\n31-May-2025 13:02:32.220 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:32 INFO Created link: fceos5:eth2 \u25aa\u2504\u2504\u25aa fceos4:eth2\n31-May-2025 13:02:32.592 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:32 INFO Created link: fceos5:eth3 \u25aa\u2504\u2504\u25aa fceos4:eth3\n31-May-2025 13:02:32.744 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:32 INFO Created link: fceos5:eth4 \u25aa\u2504\u2504\u25aa fceos4:eth4\n31-May-2025 13:02:32.844 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:32 INFO Created link: fceos5:eth6 \u25aa\u2504\u2504\u25aa fceos4:eth6\n31-May-2025 13:02:32.953 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:32 INFO Created link: fceos5:eth7 \u25aa\u2504\u2504\u25aa fceos4:eth7\n31-May-2025 13:02:32.964 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:32 INFO Running postdeploy actions for Arista cEOS 'fceos5' node\n31-May-2025 13:02:33.005 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:33 INFO Created link: fceos5:eth8 \u25aa\u2504\u2504\u25aa fceos4:eth101\n31-May-2025 13:02:33.053 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:33 INFO Created link: fceos5:eth11 \u25aa\u2504\u2504\u25aa fceos4:eth11\n31-May-2025 13:02:33.064 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:33 INFO Running postdeploy actions for Arista cEOS 'fceos4' node\n31-May-2025 13:02:54.730 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:54 INFO Adding host entries path=/etc/hosts\n31-May-2025 13:02:54.742 INFO containerlab-worker-1 running containerlab.deploy_netbox  - 13:02:54 INFO Adding SSH config for nodes path=/etc/ssh/ssh_config.d/clab-foobar.conf\n31-May-2025 13:02:54.859 9e3b29210e1140f8b3a311e8c4669ca4 job completed in 25.334 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\ncontainerlab-worker-1:\n    ----------\n    containers:\n        |_\n        ----------\n        lab_name:\n            foobar\n        labPath:\n            foobar.yaml\n        name:\n            clab-foobar-fceos4\n        container_id:\n            c24aa0089eca\n        image:\n            ceosimage:4.30.0F\n        kind:\n            ceos\n        state:\n            running\n        ipv4_address:\n            172.100.102.2/24\n            N/A\n        owner:\n            norfab\n        |_\n        lab_name:\n            foobar\n        labPath:\n            foobar.yaml\n        name:\n            clab-foobar-fceos5\n        container_id:\n            2098290d7a79\n        image:\n            ceosimage:4.30.0F\n        kind:\n            ceos\n        state:\n            running\n        ipv4_address:\n            172.100.102.3/24\n        ipv6_address:\n            N/A\n        owner:\n            norfab\nnf#show containerlab labs\ncontainerlab-worker-1:\n    - foobar\nnf#\n</code></pre> <p>In this example:</p> <ul> <li><code>nfcli</code> command starts the NorFab Interactive Shell</li> <li><code>containerlab</code> command switches to the Containerlab sub-shell</li> <li><code>deploy_netbox</code> command instruct Containerlab service to deploy a topology</li> <li><code>devices</code> specifies list of devices to fetch data and links for from netbox</li> </ul> <p>This code is complete and can run as is.</p> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"containerlab\",\n        task=\"deploy_netbox\",\n        kwargs={\n            \"devices\": [\"fceos4\", \"fceos5\"],\n            \"lab_name\": \"foobar\"\n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy_netbox/#reconfiguring-an-existing-lab","title":"Reconfiguring an Existing Lab","text":"<p>The <code>deploy_netbox</code> task supports reconfiguring an already deployed lab by using the <code>reconfigure</code> argument. This allows you to update the lab configuration without destroying and redeploying it.</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy_netbox/#filtering-nodes-for-deployment","title":"Filtering Nodes for Deployment","text":"<p>The <code>deploy_netbox</code> task allows you to deploy specific nodes in a topology using the <code>node_filter</code> argument. This is useful for testing or updating specific parts of a lab without affecting the entire topology.</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy_netbox/#norfab-containerlab-cli-shell-reference","title":"NORFAB Containerlab CLI Shell Reference","text":"<p>Below are the commands supported by the <code>deploy_netbox</code> task:</p> <pre><code>nf#man tree containerlab.deploy-netbox\n\u2514\u2500\u2500 containerlab:    Containerlab service\n    \u2514\u2500\u2500 deploy-netbox:    Spins up a lab using devices data from Netbox\n        \u251c\u2500\u2500 timeout:    Job timeout\n        \u251c\u2500\u2500 workers:    Filter worker to target, default 'any'\n        \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n        \u251c\u2500\u2500 lab-name:    Lab name to generate lab inventory for\n        \u251c\u2500\u2500 tenant:    Tenant name to generate lab inventory for\n        \u251c\u2500\u2500 filters:    Netbox device filters to generate lab inventory for\n        \u2502   \u251c\u2500\u2500 tenant:    Filter devices by tenants\n        \u2502   \u251c\u2500\u2500 device-name-contains:    Filter devices by name pattern\n        \u2502   \u251c\u2500\u2500 model:    Filter devices by models\n        \u2502   \u251c\u2500\u2500 platform:    Filter devices by platforms\n        \u2502   \u251c\u2500\u2500 region:    Filter devices by regions\n        \u2502   \u251c\u2500\u2500 role:    Filter devices by roles\n        \u2502   \u251c\u2500\u2500 site:    Filter devices by sites\n        \u2502   \u251c\u2500\u2500 status:    Filter devices by statuses\n        \u2502   \u2514\u2500\u2500 tag:    Filter devices by tags\n        \u251c\u2500\u2500 devices:    List of devices to generate lab inventory for\n        \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n        \u251c\u2500\u2500 netbox-instance:    Name of Netbox instance to pull inventory from\n        \u251c\u2500\u2500 ipv4-subnet:    IPv4 management subnet to use for lab, default '172.100.100.0/24'\n        \u251c\u2500\u2500 image:    Docker image to use for all nodes\n        \u251c\u2500\u2500 ports:    Range of TCP/UDP ports to use for nodes, default '[12000, 13000]'\n        \u251c\u2500\u2500 reconfigure:    Destroy the lab and then re-deploy it., default 'False'\n        \u2514\u2500\u2500 dry-run:    Do not deploy, only fetch inventory from Netbox\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_deploy_netbox/#python-api-reference","title":"Python API Reference","text":"<p>Deploys a containerlab topology using device data from the Netbox database.</p> <p>This method orchestrates the deployment of a containerlab topology by:</p> <ul> <li>Inspecting existing containers to determine subnets and ports in use.</li> <li>Allocating a management IPv4 subnet for the new lab, avoiding conflicts.</li> <li>Downloading inventory data from Netbox for the specified lab and filters.</li> <li>Saving the generated topology file to a dedicated folder.</li> <li>Executing the <code>containerlab deploy</code> command with appropriate arguments.</li> </ul> <p>To retrieve topology data from Netbox at least one of these arguments must be provided to identify a set of devices to include into Containerlab topology:</p> <ul> <li><code>tenant</code> - deploys lab using all devices and links that belong to this tenant</li> <li><code>devices</code> - lab deployed only using devices in the lists</li> <li><code>filters</code> - list of device filters to retrieve from Netbox</li> </ul> <p>If multiple of above arguments provided, resulting lab topology is a sum of all devices matched.</p> <p>Parameters:</p> Name Type Description Default <code>lab_name</code> <code>str</code> <p>The name to use for the lab to deploy.</p> <code>None</code> <code>tenant</code> <code>str</code> <p>Deploy lab for given tenant, lab name if not set becomes equal to tenant name.</p> <code>None</code> <code>filters</code> <code>list</code> <p>List of filters to apply when fetching devices from Netbox.</p> <code>None</code> <code>devices</code> <code>list</code> <p>List of specific devices to include in the topology.</p> <code>None</code> <code>instance</code> <code>str</code> <p>Netbox instance identifier.</p> <code>None</code> <code>image</code> <code>str</code> <p>Container image to use for devices.</p> <code>None</code> <code>ipv4_subnet</code> <code>str</code> <p>Management IPv4 subnet for the lab.</p> <code>'172.100.100.0/24'</code> <code>ports</code> <code>tuple</code> <p>Tuple specifying the range of ports to allocate.</p> <code>(12000, 15000)</code> <code>progress</code> <code>bool</code> <p>If True, emits progress events.</p> <code>False</code> <code>reconfigure</code> <code>bool</code> <p>If True, reconfigures an already deployed lab.</p> <code>False</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds for the deployment process.</p> <code>600</code> <code>node_filter</code> <code>str</code> <p>Comma-separated string of nodes to deploy.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, only generates and returns the topology inventory without deploying.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>deployment results with a list of nodes deployed</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the topology file cannot be fetched.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef deploy_netbox(\n    self,\n    job: Job,\n    lab_name: str = None,\n    tenant: str = None,\n    filters: Union[None, list] = None,\n    devices: Union[None, list] = None,\n    instance: str = None,\n    image: str = None,\n    ipv4_subnet: str = \"172.100.100.0/24\",\n    ports: tuple = (12000, 15000),\n    progress: bool = False,\n    reconfigure: bool = False,\n    timeout: int = 600,\n    node_filter: Union[None, str] = None,\n    dry_run: bool = False,\n) -&gt; Result:\n    \"\"\"\n    Deploys a containerlab topology using device data from the Netbox database.\n\n    This method orchestrates the deployment of a containerlab topology by:\n\n    - Inspecting existing containers to determine subnets and ports in use.\n    - Allocating a management IPv4 subnet for the new lab, avoiding conflicts.\n    - Downloading inventory data from Netbox for the specified lab and filters.\n    - Saving the generated topology file to a dedicated folder.\n    - Executing the `containerlab deploy` command with appropriate arguments.\n\n    To retrieve topology data from Netbox at least one of these arguments must be provided\n    to identify a set of devices to include into Containerlab topology:\n\n    - `tenant` - deploys lab using all devices and links that belong to this tenant\n    - `devices` - lab deployed only using devices in the lists\n    - `filters` - list of device filters to retrieve from Netbox\n\n    If multiple of above arguments provided, resulting lab topology is a sum of all\n    devices matched.\n\n    Args:\n        lab_name (str, optional): The name to use for the lab to deploy.\n        tenant (str, optional): Deploy lab for given tenant, lab name if not set\n            becomes equal to tenant name.\n        filters (list, optional): List of filters to apply when fetching devices from Netbox.\n        devices (list, optional): List of specific devices to include in the topology.\n        instance (str, optional): Netbox instance identifier.\n        image (str, optional): Container image to use for devices.\n        ipv4_subnet (str, optional): Management IPv4 subnet for the lab.\n        ports (tuple, optional): Tuple specifying the range of ports to allocate.\n        progress (bool, optional): If True, emits progress events.\n        reconfigure (bool, optional): If True, reconfigures an already deployed lab.\n        timeout (int, optional): Timeout in seconds for the deployment process.\n        node_filter (str, optional): Comma-separated string of nodes to deploy.\n        dry_run (bool, optional): If True, only generates and returns the topology\n            inventory without deploying.\n\n    Returns:\n        Result: deployment results with a list of nodes deployed\n\n    Raises:\n        Exception: If the topology file cannot be fetched.\n    \"\"\"\n    timeout = timeout or 600\n    ret = Result(task=f\"{self.name}:deploy_netbox\")\n    subnets_in_use = set()\n    ports_in_use = {}\n\n    # handle lab name and tenant name\n    if lab_name is None and tenant:\n        lab_name = tenant\n\n    # inspect existing containers\n    job.event(\"Checking existing containers\")\n    get_containers = self.inspect(job=job, details=True)\n    if get_containers.failed is True:\n        get_containers.task = f\"{self.name}:deploy_netbox\"\n        return get_containers\n\n    # collect TCP/UDP ports and subnets in use\n    job.event(\"Existing containers found, retrieving details\")\n    for lname, containers in get_containers.result.items():\n        for container in containers:\n            clab_name = container[\"Labels\"][\"containerlab\"]\n            clab_topo = container[\"Labels\"][\"clab-topo-file\"]\n            node_name = container[\"Labels\"][\"clab-node-name\"]\n            # collect ports that are in use\n            ports_in_use[node_name] = list(\n                set(\n                    [\n                        f\"{p['host_port']}:{p['port']}/{p['protocol']}\"\n                        for p in container[\"Ports\"]\n                        if \"host_port\" in p and \"port\" in p and \"protocol\" in p\n                    ]\n                )\n            )\n            # check existing subnets\n            if (\n                container[\"NetworkSettings\"][\"IPv4addr\"]\n                and container[\"NetworkSettings\"][\"IPv4pLen\"]\n            ):\n                ip = ipaddress.ip_interface(\n                    f\"{container['NetworkSettings']['IPv4addr']}/\"\n                    f\"{container['NetworkSettings']['IPv4pLen']}\"\n                )\n                subnet = str(ip.network.with_prefixlen)\n            else:\n                with open(clab_topo, encoding=\"utf-8\") as f:\n                    clab_topo_data = yaml.safe_load(f.read())\n                    if clab_topo_data.get(\"mgmt\", {}).get(\"ipv4-subnet\"):\n                        subnet = clab_topo_data[\"mgmt\"][\"ipv4-subnet\"]\n                    else:\n                        msg = f\"{clab_name} lab {node_name} node failed to determine mgmt subnet\"\n                        log.warning(msg)\n                        job.event(msg, severity=\"WARNING\")\n                        continue\n            subnets_in_use.add(subnet)\n            # reuse existing lab subnet\n            if clab_name == lab_name:\n                ipv4_subnet = subnet\n                job.event(\n                    f\"{ipv4_subnet} not in use by existing containers, using it\"\n                )\n            # allocate new subnet if its in use by other lab\n            elif clab_name != lab_name and ipv4_subnet == subnet:\n                msg = f\"{ipv4_subnet} already in use, allocating new subnet\"\n                log.info(msg)\n                job.event(msg)\n                ipv4_subnet = None\n\n    job.event(\"Collected TCP/UDP ports used by existing containers\")\n\n    # allocate new subnet\n    if ipv4_subnet is None:\n        pool = set(f\"172.100.{i}.0/24\" for i in range(100, 255))\n        ipv4_subnet = list(sorted(pool.difference(subnets_in_use)))[0]\n        job.event(f\"{lab_name} allocated new subnet {ipv4_subnet}\")\n\n    job.event(f\"{lab_name} fetching lab topology data from Netbox\")\n\n    # download inventory data from Netbox\n    netbox_reply = self.client.run_job(\n        service=\"netbox\",\n        task=\"get_containerlab_inventory\",\n        workers=\"any\",\n        timeout=timeout,\n        kwargs={\n            \"lab_name\": lab_name,\n            \"tenant\": tenant,\n            \"filters\": filters,\n            \"devices\": devices,\n            \"instance\": instance,\n            \"image\": image,\n            \"ipv4_subnet\": ipv4_subnet,\n            \"ports\": ports,\n            \"ports_map\": ports_in_use,\n            \"progress\": progress,\n        },\n    )\n\n    # use inventory from first worker that returned hosts data\n    for wname, wdata in netbox_reply.items():\n        if wdata[\"failed\"] is False and wdata[\"result\"]:\n            topology_inventory = wdata[\"result\"]\n            break\n    else:\n        msg = f\"{self.name} - Netbox returned no data for '{lab_name}' lab\"\n        log.error(msg)\n        raise RuntimeError(msg)\n\n    job.event(f\"{lab_name} topology data retrieved from Netbox\")\n\n    if dry_run is True:\n        ret.result = topology_inventory\n        return ret\n\n    # create folder to store topology\n    topology_folder = os.path.join(self.topologies_dir, lab_name)\n    os.makedirs(topology_folder, exist_ok=True)\n\n    # create topology file\n    topology_file = os.path.join(topology_folder, f\"{lab_name}.yaml\")\n    with open(topology_file, \"w\", encoding=\"utf-8\") as tf:\n        tf.write(yaml.dump(topology_inventory, default_flow_style=False))\n\n    job.event(f\"{lab_name} topology data saved to '{topology_file}'\")\n\n    return self.deploy(\n        job=job,\n        topology=topology_file,\n        reconfigure=reconfigure,\n        timeout=timeout,\n        node_filter=node_filter or \"\",\n    )\n</code></pre>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_destroy/","title":"Containerlab Service Destroy Task","text":"<p>task api name: <code>destroy_lab</code></p> <p>The Containerlab service <code>destroy_lab</code> task is designed to destroy a specified lab. This task ensures that all resources associated with the lab are cleaned up, including containers, networks, and other artifacts created during the lab deployment.</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_destroy/#containerlab-destroy-task-overview","title":"Containerlab Destroy Task Overview","text":"<p>The <code>destroy_lab</code> task provides the following features:</p> <ul> <li>Lab Cleanup: Removes all containers, networks, and other resources associated with the specified lab.</li> <li>Error Handling: Provides detailed error messages if the lab cannot be found or destroyed.</li> </ul>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_destroy/#containerlab-destroy-task-sample-usage","title":"Containerlab Destroy Task Sample Usage","text":"<p>Below is an example of how to use the Containerlab destroy task to clean up a lab.</p> <p>Example</p> DemoCLIPython <p></p> <pre><code>nf#containerlab destroy lab-name three-routers-lab\n--------------------------------------------- Job Events -----------------------------------------------\n05-May-2025 20:45:48.745 831d3d485489476c98159f8d4dbf7ec2 job started\n05-May-2025 20:45:48.805 INFO containerlab-worker-1 running containerlab.destroy_lab  - 20:45:48 INFO Parsing &amp; checking topology file=three-routers-topology.yaml\n05-May-2025 20:45:48.818 INFO containerlab-worker-1 running containerlab.destroy_lab  - 20:45:48 INFO Parsing &amp; checking topology file=three-routers-topology.yaml\n05-May-2025 20:45:48.831 INFO containerlab-worker-1 running containerlab.destroy_lab  - 20:45:48 INFO Destroying lab name=three-routers-lab\n05-May-2025 20:45:50.129 INFO containerlab-worker-1 running containerlab.destroy_lab  - 20:45:50 INFO Removed container name=clab-three-routers-lab-r2\n05-May-2025 20:45:50.348 INFO containerlab-worker-1 running containerlab.destroy_lab  - 20:45:50 INFO Removed container name=clab-three-routers-lab-r3\n05-May-2025 20:45:50.390 INFO containerlab-worker-1 running containerlab.destroy_lab  - 20:45:50 INFO Removed container name=clab-three-routers-lab-r1\n05-May-2025 20:45:50.401 INFO containerlab-worker-1 running containerlab.destroy_lab  - 20:45:50 INFO Removing host entries path=/etc/hosts\n05-May-2025 20:45:50.412 INFO containerlab-worker-1 running containerlab.destroy_lab  - 20:45:50 INFO Removing SSH config path=/etc/ssh/ssh_config.d/clab-three-routers-lab.conf\n05-May-2025 20:45:50.963 831d3d485489476c98159f8d4dbf7ec2 job completed in 2.218 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\ncontainerlab-worker-1:\n    ----------\n    three-routers-lab:\n        True\nnf#\n</code></pre> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"containerlab\",\n        task=\"destroy_lab\",\n        kwargs={\n            \"lab_name\": \"three-routers-lab\",\n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_destroy/#norfab-containerlab-cli-shell-reference","title":"NORFAB Containerlab CLI Shell Reference","text":"<p>Below are the commands supported by the <code>destroy_lab</code> task:</p> <pre><code>nf#man tree containerlab.destroy\nroot\n\u2514\u2500\u2500 containerlab:    Containerlab service\n    \u2514\u2500\u2500 destroy:    The destroy command destroys a lab referenced by its name\n        \u251c\u2500\u2500 timeout:    Job timeout\n        \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n        \u251c\u2500\u2500 lab-name:    Lab name to destroy\n        \u2514\u2500\u2500 progress:    Display progress events, default 'True'\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_destroy/#python-api-reference","title":"Python API Reference","text":"<p>Destroys a specified lab.</p> <p>Parameters:</p> Name Type Description Default <code>lab_name</code> <code>str</code> <p>The name of the lab to be destroyed.</p> required <code>timeout</code> <code>int</code> <p>The timeout value in seconds for the operation. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the status of the operation, errors (if any),     and the result indicating whether the lab was successfully destroyed.</p> Behavior <ul> <li>Retrieves the lab details using the <code>inspect</code> method.</li> <li>If the lab is not found, marks the operation as failed and returns an error.</li> <li>If the lab is found, retrieves the topology file and its folder.</li> <li>Executes the <code>containerlab destroy</code> command using the topology file.</li> <li>Updates the result to indicate success or failure of the destruction process.</li> </ul> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"DELETE\"]})\ndef destroy_lab(self, lab_name: str, job: Job, timeout: int = None) -&gt; Result:\n    \"\"\"\n    Destroys a specified lab.\n\n    Args:\n        lab_name (str): The name of the lab to be destroyed.\n        timeout (int, optional): The timeout value in seconds for the operation. Defaults to None.\n\n    Returns:\n        Result: An object containing the status of the operation, errors (if any),\n                and the result indicating whether the lab was successfully destroyed.\n\n    Behavior:\n        - Retrieves the lab details using the `inspect` method.\n        - If the lab is not found, marks the operation as failed and returns an error.\n        - If the lab is found, retrieves the topology file and its folder.\n        - Executes the `containerlab destroy` command using the topology file.\n        - Updates the result to indicate success or failure of the destruction process.\n    \"\"\"\n    timeout = timeout or 600\n    ret = Result(task=f\"{self.name}:destroy_lab\")\n\n    # get lab details\n    inspect = self.inspect(\n        job=job, timeout=timeout, lab_name=lab_name, details=True\n    )\n\n    if not inspect.result:\n        ret.failed = True\n        ret.errors = [f\"'{lab_name}' lab not found\"]\n        ret.result = {lab_name: False}\n    else:\n        topology_file = inspect.result[lab_name][0][\"Labels\"][\"clab-topo-file\"]\n        topology_folder = os.path.split(topology_file)[0]\n\n        # run destroy command\n        args = [\"containerlab\", \"destroy\", \"-t\", topology_file]\n        ret = self.run_containerlab_command(\n            args=args,\n            cwd=topology_folder,\n            timeout=timeout,\n            ret=ret,\n            job=job,\n            expect_output=False,\n        )\n\n        if not ret.failed:\n            ret.result = {lab_name: True}\n\n    return ret\n</code></pre>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_inspect/","title":"Containerlab Service Inspect Task","text":"<p>task api name: <code>inspect</code></p> <p>The Containerlab service <code>inspect</code> task is designed to inspect the configuration and status of container lab containers. This task provides detailed information about running labs, including their topology, container details, and status.</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_inspect/#containerlab-inspect-task-overview","title":"Containerlab Inspect Task Overview","text":"<p>The <code>inspect</code> task provides the following features:</p> <ul> <li>Lab Inspection: Retrieves information about a specific lab or all labs.</li> <li>Detailed View: Optionally includes detailed information about the lab's containers.</li> </ul>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_inspect/#containerlab-inspect-task-sample-usage","title":"Containerlab Inspect Task Sample Usage","text":"<p>Below is an example of how to use the Containerlab inspect task to inspect a lab.</p> <p>Example</p> DemoCLICLI with DetailsPython <p></p> <pre><code>nf#show containerlab containers lab-name three-routers-lab\ncontainerlab-worker-1:\n    ----------\n    containers:\n        |_\n          ----------\n          lab_name:\n              three-routers-lab\n          labPath:\n              __norfab__/files/worker/containerlab-worker-1/topologies/containerlab/three-routers-topology.yaml\n          name:\n              clab-three-routers-lab-r1\n          container_id:\n              4f7836a4d1ac\n          image:\n              ceosimage:4.30.0F\n          kind:\n              ceos\n          state:\n              running\n          ipv4_address:\n              172.100.101.12/24\n          ipv6_address:\n              N/A\n          owner:\n              norfabuser\n        |_\n          ----------\n          lab_name:\n              three-routers-lab\n          labPath:\n              __norfab__/files/worker/containerlab-worker-1/topologies/containerlab/three-routers-topology.yaml\n          name:\n              clab-three-routers-lab-r2\n          container_id:\n              6ea7120965b1\n          image:\n              ceosimage:4.30.0F\n          kind:\n              ceos\n          state:\n              running\n          ipv4_address:\n              172.100.101.13/24\n          ipv6_address:\n              N/A\n          owner:\n              norfabuser\n        |_\n          ----------\n          lab_name:\n              three-routers-lab\n              __norfab__/files/worker/containerlab-worker-1/topologies/containerlab/three-routers-topology.yaml\n          name:\n              clab-three-routers-lab-r3\n          container_id:\n              63ee900fde76\n          image:\n              ceosimage:4.30.0F\n          kind:\n              ceos\n          state:\n              running\n          ipv4_address:\n              172.100.101.14/24\n          ipv6_address:\n              N/A\n          owner:\n              norfabuser\nnf#\n</code></pre> <pre><code>nf#show containerlab containers lab-name three-routers-lab details\ncontainerlab-worker-1:\n    |_\n      ----------\n      Names:\n          - clab-three-routers-lab-r2\n      ID:\n          6ea7120965b142c397bab0c1a40550e00e967b8ae6031f7f66561f8decc0b45a\n      ShortID:\n          6ea7120965b1\n      Image:\n          ceosimage:4.30.0F\n      State:\n          running\n      Status:\n          Up 14 minutes\n      Labels:\n          ----------\n          clab-mgmt-net-bridge:\n              br-f71d180c51e5\n          clab-node-group:\n          clab-node-kind:\n              ceos\n          clab-node-lab-dir:\n              /home/norfabuser/norfab/tests/nf_containerlab/__norfab__/files/worker/containerlab-worker-1/topologies/containerlab/clab-three-routers-lab/r2\n          clab-node-longname:\n              clab-three-routers-lab-r2\n          clab-node-name:\n              r2\n          clab-node-type:\n          clab-owner:\n              norfabuser\n          clab-topo-file:\n              /home/norfabuser/norfab/tests/nf_containerlab/__norfab__/files/worker/containerlab-worker-1/topologies/containerlab/three-routers-topology.yaml\n          containerlab:\n              three-routers-lab\n      Pid:\n          7083\n      NetworkSettings:\n          ----------\n          IPv4addr:\n              172.100.101.13\n          IPv4pLen:\n              24\n          IPv4Gw:\n              172.100.101.1\n          IPv6addr:\n          IPv6pLen:\n              0\n          IPv6Gw:\n      Mounts:\n          |_\n            ----------\n            Source:\n                /home/norfabuser/norfab/tests/nf_containerlab/__norfab__/files/worker/containerlab-worker-1/topologies/containerlab/clab-three-routers-lab/r2/flash\n            Destination:\n                /mnt/flash\n      Ports:\n          |_\n            ----------\n            host_ip:\n                0.0.0.0\n            host_port:\n                12203\n            port:\n                22\n            protocol:\n                tcp\n          |_\n            ----------\n            host_ip:\n                ::\n            host_port:\n                12203\n            port:\n                22\n            protocol:\n                tcp\n          |_\n            ----------\n            host_ip:\n                0.0.0.0\n            host_port:\n                14403\n            port:\n                443\n            protocol:\n                tcp\n          |_\n            ----------\n            host_ip:\n                ::\n            host_port:\n                14403\n            port:\n                443\n            protocol:\n                tcp\n          |_\n            ----------\n            host_ip:\n                0.0.0.0\n            host_port:\n                18803\n            port:\n                80\n            protocol:\n                tcp\n          |_\n            ----------\n            host_ip:\n                ::\n            host_port:\n                18803\n            port:\n                80\n            protocol:\n                tcp\n          |_\n            ----------\n            host_ip:\n                0.0.0.0\n            host_port:\n                18303\n            port:\n                830\n            protocol:\n                tcp\n          |_\n            ----------\n            host_ip:\n                ::\n            host_port:\n                18303\n            port:\n                830\n            protocol:\n                tcp\n\n&lt;...omitted for brevity...&gt;\n</code></pre> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"containerlab\",\n        task=\"inspect\",\n        kwargs={\n            \"lab_name\": \"three-routers-lab\",\n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_inspect/#norfab-containerlab-cli-shell-reference","title":"NORFAB Containerlab CLI Shell Reference","text":"<p>Below are the commands supported by the <code>inspect</code> task:</p> <pre><code>nf#man tree show.containerlab.containers\nroot\n\u2514\u2500\u2500 show:    NorFab show commands\n    \u2514\u2500\u2500 containerlab:    Show Containerlab service\n        \u2514\u2500\u2500 containers:    show containerlab containers\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n            \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n            \u251c\u2500\u2500 details:    Show container labs details\n            \u2514\u2500\u2500 lab-name:    Show container for given lab only\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_inspect/#python-api-reference","title":"Python API Reference","text":"<p>Inspect the container lab containers configuration and status.</p> <p>This method retrieves information about a specific container lab or all container labs, optionally including detailed information.</p> <p>Parameters:</p> Name Type Description Default <code>lab_name</code> <code>str</code> <p>The name of the container lab to inspect. If not provided, all container labs will be inspected.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The maximum time in seconds to wait for the inspection command to complete. Defaults to None.</p> <code>None</code> <code>details</code> <code>bool</code> <p>Whether to include detailed information in the inspection output. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the result of the inspection task.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef inspect(\n    self,\n    job: Job,\n    lab_name: Union[None, str] = None,\n    timeout: int = None,\n    details: bool = False,\n) -&gt; Result:\n    \"\"\"\n    Inspect the container lab containers configuration and status.\n\n    This method retrieves information about a specific container lab or all\n    container labs, optionally including detailed information.\n\n    Args:\n        lab_name (str, optional): The name of the container lab to inspect.\n            If not provided, all container labs will be inspected.\n        timeout (int, optional): The maximum time in seconds to wait for the\n            inspection command to complete. Defaults to None.\n        details (bool, optional): Whether to include detailed information in\n            the inspection output. Defaults to False.\n\n    Returns:\n        Result: An object containing the result of the inspection task.\n    \"\"\"\n    timeout = timeout or 600\n    ret = Result(task=f\"{self.name}:inspect\")\n\n    if lab_name:\n        args = [\"containerlab\", \"inspect\", \"-f\", \"json\", \"--name\", lab_name]\n    else:\n        args = [\"containerlab\", \"inspect\", \"-f\", \"json\", \"--all\"]\n    if details:\n        args.append(\"--details\")\n\n    ret = self.run_containerlab_command(\n        args=args, timeout=timeout, ret=ret, job=job\n    )\n\n    # check if lab name given and it is not in output\n    if lab_name and lab_name not in ret.result:\n        ret.failed = True\n        msg = f\"'{lab_name}' lab not found\"\n        ret.errors.append(msg)\n        log.error(msg)\n\n    return ret\n</code></pre>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_nornir_inventory/","title":"Containerlab Service Nornir Inventory Task","text":"<p>task api name: <code>get_nornir_inventory</code></p> <p>The Containerlab service <code>get_nornir_inventory</code> task is designed to generate a Nornir-compatible inventory for a specified lab. This task inspects the container lab environment and maps container details to Nornir inventory format, enabling seamless integration with network automation workflows.</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_nornir_inventory/#containerlab-nornir-inventory-task-overview","title":"Containerlab Nornir Inventory Task Overview","text":"<p>The <code>get_nornir_inventory</code> task provides the following features:</p> <ul> <li>Inventory Generation: Creates a Nornir-compatible inventory for a specified lab or all labs.</li> <li>Platform Mapping: Maps containerlab node kinds to Netmiko SSH platform types.</li> <li>Default Credentials: Optionally includes default credentials for containerlab nodes.</li> </ul>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_nornir_inventory/#containerlab-nornir-inventory-task-sample-usage","title":"Containerlab Nornir Inventory Task Sample Usage","text":"<p>Below is an example of how to use the Containerlab <code>get_nornir_inventory</code> task to generate an inventory.</p> <p>Example</p> DemoCLIPython <p></p> <pre><code>nf#containerlab get-nornir-inventory lab-name three-routers-lab\n--------------------------------------------- Job Events -----------------------------------------------\n05-May-2025 21:14:09.594 fc13d3b11ad54c2fb2330af20a7ceacd job started\n05-May-2025 21:14:09.926 fc13d3b11ad54c2fb2330af20a7ceacd job completed in 0.332 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\ncontainerlab-worker-1:\n    ----------\n    hosts:\n        ----------\n        r2:\n            ----------\n            hostname:\n                192.168.1.130\n            port:\n                12203\n            groups:\n            platform:\n                arista_eos\n            username:\n                admin\n            password:\n                admin\n        r3:\n            ----------\n            hostname:\n                192.168.1.130\n            port:\n                12204\n            groups:\n            platform:\n            username:\n                admin\n            password:\n                admin\n        r1:\n            ----------\n            hostname:\n                192.168.1.130\n            port:\n                12202\n            groups:\n            platform:\n                arista_eos\n            username:\n                admin\n            password:\n                admin\nnf#\n</code></pre> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"containerlab\",\n        task=\"get_nornir_inventory\",\n        kwargs={\n            \"lab_name\": \"three-routers-lab\",\n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_nornir_inventory/#norfab-containerlab-cli-shell-reference","title":"NORFAB Containerlab CLI Shell Reference","text":"<p>Below are the commands supported by the <code>get_nornir_inventory</code> task:</p> <pre><code>nf#man tree containerlab.get-nornir-inventory\nroot\n\u2514\u2500\u2500 containerlab:    Containerlab service\n    \u2514\u2500\u2500 get-nornir-inventory:    Get nornir inventory for a given lab\n        \u251c\u2500\u2500 timeout:    Job timeout\n        \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n        \u251c\u2500\u2500 lab-name:    Lab name to get Nornir inventory for\n        \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n        \u2514\u2500\u2500 groups:    List of groups to include in host's inventory\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_nornir_inventory/#python-api-reference","title":"Python API Reference","text":"<p>Retrieves the Nornir inventory for a specified lab.</p> <p>This method inspects the container lab environment and generates a Nornir-compatible inventory of hosts based on the lab's configuration. It maps containerlab node kinds to Netmiko SSH platform types and extracts relevant connection details.</p> <p>Parameters:</p> Name Type Description Default <code>lab_name</code> <code>str</code> <p>The name of the container lab to inspect. If not given loads inventory for all labs.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The timeout value for the inspection operation. Defaults to None.</p> <code>None</code> <code>groups</code> <code>list</code> <p>A list of group names to assign to the hosts in the inventory. Defaults to None.</p> <code>None</code> <code>use_default_credentials</code> <code>bool</code> <p>Use Containerlab default credentials for hosts or not.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>A <code>Result</code> object containing the Nornir inventory. The <code>result</code> attribute</p> <code>Result</code> <p>includes a dictionary with host details. If the lab is not found or an error occurs,</p> <code>Result</code> <p>the <code>failed</code> attribute is set to True, and the <code>errors</code> attribute contains error messages.</p> Notes <ul> <li>The method uses a predefined mapping (<code>norfab.utils.platform_map</code>) to translate containerlab   node kinds to Netmiko platform types.</li> <li>If a container's SSH port cannot be determined, it is skipped, and an error is logged.</li> <li>The primary host IP address is determined dynamically using a UDP socket connection or   by checking the host IP address in the container's port configuration.</li> </ul> Example of returned inventory structure <p>{     \"hosts\": {         \"host_name\": {             \"hostname\": \"host_ip\",             \"platform\": \"netmiko_platform\",             \"groups\": [\"group1\", \"group2\"],         },         ...</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_nornir_inventory(\n    self,\n    job: Job,\n    lab_name: Union[None, str] = None,\n    timeout: int = None,\n    groups: Union[None, list] = None,\n    use_default_credentials: bool = True,\n) -&gt; Result:\n    \"\"\"\n    Retrieves the Nornir inventory for a specified lab.\n\n    This method inspects the container lab environment and generates a Nornir-compatible\n    inventory of hosts based on the lab's configuration. It maps containerlab node kinds\n    to Netmiko SSH platform types and extracts relevant connection details.\n\n    Args:\n        lab_name (str): The name of the container lab to inspect. If not given loads inventory for all labs.\n        timeout (int, optional): The timeout value for the inspection operation. Defaults to None.\n        groups (list, optional): A list of group names to assign to the hosts in the inventory. Defaults to None.\n        use_default_credentials (bool, optional): Use Containerlab default credentials for hosts or not.\n\n    Returns:\n        Result: A `Result` object containing the Nornir inventory. The `result` attribute\n        includes a dictionary with host details. If the lab is not found or an error occurs,\n        the `failed` attribute is set to True, and the `errors` attribute contains error messages.\n\n    Notes:\n        - The method uses a predefined mapping (`norfab.utils.platform_map`) to translate containerlab\n          node kinds to Netmiko platform types.\n        - If a container's SSH port cannot be determined, it is skipped, and an error is logged.\n        - The primary host IP address is determined dynamically using a UDP socket connection or\n          by checking the host IP address in the container's port configuration.\n\n    Example of returned inventory structure:\n        {\n            \"hosts\": {\n                \"host_name\": {\n                    \"hostname\": \"host_ip\",\n                    \"platform\": \"netmiko_platform\",\n                    \"groups\": [\"group1\", \"group2\"],\n                },\n                ...\n    \"\"\"\n    timeout = timeout or 600\n    groups = groups or []\n    ret = Result(task=f\"{self.name}:get_nornir_inventory\", result={\"hosts\": {}})\n\n    # get lab details\n    inspect = self.inspect(\n        job=job, lab_name=lab_name, timeout=timeout, details=True\n    )\n\n    # return empty result if lab not found\n    if not inspect.result:\n        ret.failed = True\n        ret.errors = [f\"'{lab_name}' lab not found\"]\n        return ret\n\n    # get host primary IP address\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    s.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n    s.connect((\"1.2.3.4\", 12345))\n    primary_host_ip = s.getsockname()[0]\n    log.debug(\n        f\"{self.name} - determined Containerlab host primary IP - '{primary_host_ip}'\"\n    )\n\n    # form hosts inventory\n    for lname, containers in inspect.result.items():\n        if lab_name and lname != lab_name:\n            continue\n        for container in containers:\n            host_name = container[\"Labels\"][\"clab-node-name\"]\n            host_port = None\n            host_ip = None\n\n            # get ssh port\n            for port in container[\"Ports\"]:\n                host_ip = primary_host_ip\n                if port[\"port\"] == 22 and port[\"protocol\"] == \"tcp\":\n                    host_port = port[\"host_port\"]\n                    # get host ip address\n                    if port[\"host_ip\"] not in [\n                        \"0.0.0.0\",\n                        \"127.0.0.1\",\n                        \"localhost\",\n                        \"::\",\n                    ]:\n                        host_ip = port[\"host_ip\"]\n                    break\n            else:\n                log.error(f\"{self.name} - {host_name} failed to map SSH port.\")\n                continue\n\n            # add host to Nornir inventory\n            ret.result[\"hosts\"][host_name] = {\n                \"hostname\": host_ip,\n                \"port\": host_port,\n                \"groups\": groups,\n            }\n\n            # get netmiko platform\n            clab_platform_name = container[\"Labels\"][\"clab-node-kind\"]\n            netmiko_platform = PlatformMap.convert(\n                \"containerlab\", \"netmiko\", clab_platform_name\n            )\n            if netmiko_platform:\n                ret.result[\"hosts\"][host_name][\"platform\"] = netmiko_platform[\n                    \"platform\"\n                ]\n            else:\n                log.warning(\n                    f\"{self.name} - {host_name} clab-node-kind '{clab_platform_name}' not mapped to Netmiko platform.\"\n                )\n                continue\n\n            # get default credentials\n            if use_default_credentials:\n                clab_platform = PlatformMap.get(\"containerlab\", clab_platform_name)\n                if not clab_platform:\n                    log.warning(\n                        f\"{self.name} - {host_name} clab-node-kind '{clab_platform_name}' not found.\"\n                    )\n                    continue\n                if clab_platform.get(\"username\"):\n                    ret.result[\"hosts\"][host_name][\"username\"] = clab_platform[\n                        \"username\"\n                    ]\n                if clab_platform.get(\"password\"):\n                    ret.result[\"hosts\"][host_name][\"password\"] = clab_platform[\n                        \"password\"\n                    ]\n\n    return ret\n</code></pre>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_restart/","title":"Containerlab Service Restart Task","text":"<p>task api name: <code>restart_lab</code></p> <p>The Containerlab service <code>restart_lab</code> task is designed to restart a specified lab. This task destroys the existing lab and redeploys it using the same topology file, ensuring a clean and consistent lab environment.</p> <p>Warning</p> <p>Restart task calls Containerlab deploy command with <code>--reconfigure</code> flag. Any non saved state will be lost. Use save task to save configuration as needed.</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_restart/#containerlab-restart-task-overview","title":"Containerlab Restart Task Overview","text":"<p>The <code>restart_lab</code> task provides the following features:</p> <ul> <li>Lab Restart: Destroys the current lab and redeploys it using the original topology file.</li> <li>Error Handling: Provides detailed error messages if the lab cannot be found or restarted.</li> </ul>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_restart/#containerlab-restart-task-sample-usage","title":"Containerlab Restart Task Sample Usage","text":"<p>Below is an example of how to use the Containerlab restart task to restart a lab.</p> <p>Example</p> DemoCLIPython <p></p> <pre><code>nf#containerlab restart lab-name three-routers-lab\n--------------------------------------------- Job Events -----------------------------------------------\n05-May-2025 20:51:33.947 ee23b3ec4bfb474fac0a7e87e910862b job started\n05-May-2025 20:51:34.011 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:34 INFO Containerlab started version=0.67.0\n05-May-2025 20:51:34.022 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:34 INFO Parsing &amp; checking topology file=three-routers-topology.yaml\n05-May-2025 20:51:34.034 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:34 INFO Destroying lab name=three-routers-lab\n05-May-2025 20:51:35.527 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:35 INFO Removed container name=clab-three-routers-lab-r2\n05-May-2025 20:51:35.614 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:35 INFO Removed container name=clab-three-routers-lab-r3\n05-May-2025 20:51:35.659 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:35 INFO Removed container name=clab-three-routers-lab-r1\n05-May-2025 20:51:35.670 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:35 INFO Removing host entries path=/etc/hosts\n05-May-2025 20:51:35.681 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:35 INFO Removing SSH config path=/etc/ssh/ssh_config.d/clab-three-routers-lab.conf\n05-May-2025 20:51:35.790 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:35 INFO Creating container name=r1\n05-May-2025 20:51:35.813 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:35 INFO Creating container name=r3\n05-May-2025 20:51:36.402 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:36 INFO Running postdeploy actions for Arista cEOS 'r3' node\n05-May-2025 20:51:36.658 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:36 INFO Created link: r3:eth3 \u25aa\u2504\u2504\u25aa r1:eth3\n05-May-2025 20:51:36.669 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:36 INFO Running postdeploy actions for Arista cEOS 'r1' node\n05-May-2025 20:51:36.779 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:36 INFO Created link: r1:eth1 \u25aa\u2504\u2504\u25aa r2:eth1\n05-May-2025 20:51:36.821 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:36 INFO Created link: r2:eth2 \u25aa\u2504\u2504\u25aa r3:eth2\n05-May-2025 20:51:36.832 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:51:36 INFO Running postdeploy actions for Arista cEOS 'r2' node\n05-May-2025 20:52:03.895 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:52:03 INFO Adding host entries path=/etc/hosts\n05-May-2025 20:52:03.906 INFO containerlab-worker-1 running containerlab.restart_lab  - 20:52:03 INFO Adding SSH config for nodes path=/etc/ssh/ssh_config.d/clab-three-routers-lab.conf\n05-May-2025 20:52:04.142 ee23b3ec4bfb474fac0a7e87e910862b job completed in 30.195 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\ncontainerlab-worker-1:\n    ----------\n    three-routers-lab:\n        True\nnf#\n</code></pre> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"containerlab\",\n        task=\"restart_lab\",\n        kwargs={\n            \"lab_name\": \"three-routers-lab\",\n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_restart/#norfab-containerlab-cli-shell-reference","title":"NORFAB Containerlab CLI Shell Reference","text":"<p>Below are the commands supported by the <code>restart_lab</code> task:</p> <pre><code>nf#man tree containerlab.restart\nroot\n\u2514\u2500\u2500 containerlab:    Containerlab service\n    \u2514\u2500\u2500 restart:    Restart lab devices\n        \u251c\u2500\u2500 timeout:    Job timeout\n        \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n        \u251c\u2500\u2500 lab-name:    Lab name to restart\n        \u2514\u2500\u2500 progress:    Display progress events, default 'True'\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_restart/#python-api-reference","title":"Python API Reference","text":"<p>Restart a specified Containerlab lab.</p> <p>This method retrieves the lab details, destroys the existing lab, and redeploys it using the provided topology file.</p> <p>Parameters:</p> Name Type Description Default <code>lab_name</code> <code>str</code> <p>The name of the lab to restart.</p> required <code>timeout</code> <code>int</code> <p>The timeout value for the operation in seconds. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the status of the operation, any errors encountered,     and the result indicating whether the lab was successfully restarted.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef restart_lab(self, job: Job, lab_name: str, timeout: int = None) -&gt; Result:\n    \"\"\"\n    Restart a specified Containerlab lab.\n\n    This method retrieves the lab details, destroys the existing lab, and redeploys it\n    using the provided topology file.\n\n    Args:\n        lab_name (str): The name of the lab to restart.\n        timeout (int, optional): The timeout value for the operation in seconds. Defaults to None.\n\n    Returns:\n        Result: An object containing the status of the operation, any errors encountered,\n                and the result indicating whether the lab was successfully restarted.\n    \"\"\"\n    timeout = timeout or 600\n    ret = Result(task=f\"{self.name}:restart_lab\")\n\n    # get lab details\n    inspect = self.inspect(\n        job=job, timeout=timeout, lab_name=lab_name, details=True\n    )\n\n    if not inspect.result:\n        ret.failed = True\n        ret.errors = [f\"'{lab_name}' lab not found\"]\n        ret.result = {lab_name: False}\n    else:\n        topology_file = inspect.result[lab_name][0][\"Labels\"][\"clab-topo-file\"]\n        topology_folder = os.path.split(topology_file)[0]\n\n        # add needed env variables\n        env = dict(os.environ)\n        env[\"CLAB_VERSION_CHECK\"] = \"disable\"\n\n        # run destroy command\n        args = [\n            \"containerlab\",\n            \"deploy\",\n            \"-f\",\n            \"json\",\n            \"-t\",\n            topology_file,\n            \"--reconfigure\",\n        ]\n        ret = self.run_containerlab_command(\n            args=args,\n            cwd=topology_folder,\n            timeout=timeout,\n            ret=ret,\n            env=env,\n            job=job,\n        )\n\n        if not ret.failed:\n            ret.result = {lab_name: True}\n\n    return ret\n</code></pre>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_save/","title":"Containerlab Service Save Task","text":"<p>task api name: <code>save</code></p> <p>The Containerlab service <code>save</code> task is used to save the configuration of devices in a specified lab. This task ensures that the current state of the lab is preserved, allowing users to back up configurations for future use or restoration.</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_save/#containerlab-save-task-overview","title":"Containerlab Save Task Overview","text":"<p>The <code>save</code> task provides the following features:</p> <ul> <li>Configuration Backup: Saves the current configuration of all devices in the lab.</li> <li>Lab-Specific: Operates on a specified lab, ensuring targeted configuration management.</li> </ul>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_save/#containerlab-save-task-sample-usage","title":"Containerlab Save Task Sample Usage","text":"<p>Below is an example of how to use the Containerlab save task to back up a lab's configuration.</p> <p>Example</p> DemoCLIPython <p></p> <pre><code>nf#containerlab save lab-name three-routers-lab\n--------------------------------------------- Job Events -----------------------------------------------\n05-May-2025 20:48:25.322 7ffd783a18ee4db7b92d1643ef8b3390 job started\n05-May-2025 20:48:25.391 INFO containerlab-worker-1 running containerlab.save  - 20:48:25 INFO Parsing &amp; checking topology file=three-routers-topology.yaml\n05-May-2025 20:48:25.551 INFO containerlab-worker-1 running containerlab.save  - 20:48:25 INFO saved cEOS configuration from r2 node to\n/home/norfabuser/norfab/tests/nf_containerlab/__norfab__/files/worker/containerlab-worker-1/topologies/containerlab/clab-three-routers-lab/r2/flash/startup-config\n05-May-2025 20:48:25.905 INFO containerlab-worker-1 running containerlab.save  - 20:48:25 INFO saved cEOS configuration from r3 node to\n/home/norfabuser/norfab/tests/nf_containerlab/__norfab__/files/worker/containerlab-worker-1/topologies/containerlab/clab-three-routers-lab/r3/flash/startup-config\n05-May-2025 20:48:26.285 INFO containerlab-worker-1 running containerlab.save  - 20:48:26 INFO saved cEOS configuration from r1 node to\n/home/norfabuser/norfab/tests/nf_containerlab/__norfab__/files/worker/containerlab-worker-1/topologies/containerlab/clab-three-routers-lab/r1/flash/startup-config\n05-May-2025 20:48:26.510 7ffd783a18ee4db7b92d1643ef8b3390 job completed in 1.188 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\ncontainerlab-worker-1:\n    ----------\n    three-routers-lab:\n        True\nnf#\n</code></pre> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"containerlab\",\n        task=\"save\",\n        kwargs={\n            \"lab_name\": \"three-routers-lab\",\n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_save/#norfab-containerlab-cli-shell-reference","title":"NORFAB Containerlab CLI Shell Reference","text":"<p>Below are the commands supported by the <code>save</code> task:</p> <pre><code>nf# man tree containerlab.save\nroot\n\u2514\u2500\u2500 containerlab:    Containerlab service\n    \u2514\u2500\u2500 save:    Perform configuration save for all containers running in a lab\n        \u251c\u2500\u2500 timeout:    Job timeout\n        \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n        \u251c\u2500\u2500 lab-name:    Lab name to save configurations for\n        \u2514\u2500\u2500 progress:    Display progress events, default 'True'\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["containerlab"]},{"location":"workers/containerlab/services_containerlab_service_tasks_save/#python-api-reference","title":"Python API Reference","text":"<p>Saves the config of a specified lab devices by invoking the <code>containerlab save</code> command.</p> <p>Parameters:</p> Name Type Description Default <code>lab_name</code> <code>str</code> <p>The name of the lab to save.</p> required <code>timeout</code> <code>int</code> <p>The maximum time in seconds to wait for the operation to complete. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the outcome of the save operation. If successful, <code>result</code> will contain a dictionary with the lab name as the key and <code>True</code> as the value. If unsuccessful, <code>failed</code> will be set to True, and <code>errors</code> will contain a list of error messages.</p> Source code in <code>norfab\\workers\\containerlab_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef save(self, job: Job, lab_name: str, timeout: int = None) -&gt; Result:\n    \"\"\"\n    Saves the config of a specified lab devices by invoking the `containerlab save` command.\n\n    Args:\n        lab_name (str): The name of the lab to save.\n        timeout (int, optional): The maximum time in seconds to wait for the operation\n            to complete. Defaults to None.\n\n    Returns:\n        Result: An object containing the outcome of the save operation. If successful,\n            `result` will contain a dictionary with the lab name as the key and `True`\n            as the value. If unsuccessful, `failed` will be set to True, and `errors`\n            will contain a list of error messages.\n    \"\"\"\n    timeout = timeout or 600\n    ret = Result(task=f\"{self.name}:save\")\n\n    # get lab details\n    inspect = self.inspect(\n        job=job, timeout=timeout, lab_name=lab_name, details=True\n    )\n\n    if not inspect.result:\n        ret.failed = True\n        ret.errors = [f\"'{lab_name}' lab not found\"]\n        ret.result = {lab_name: False}\n    else:\n        topology_file = inspect.result[lab_name][0][\"Labels\"][\"clab-topo-file\"]\n        topology_folder = os.path.split(topology_file)[0]\n\n        # run destroy command\n        args = [\"containerlab\", \"save\", \"-t\", topology_file]\n        ret = self.run_containerlab_command(\n            args=args,\n            cwd=topology_folder,\n            timeout=timeout,\n            ret=ret,\n            job=job,\n            expect_output=False,\n        )\n\n        if not ret.failed:\n            ret.result = {lab_name: True}\n\n    return ret\n</code></pre>","tags":["containerlab"]},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/","title":"FastAPI Worker","text":""},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.FastAPIWorker","title":"<code>FastAPIWorker(inventory: str, broker: str, worker_name: str, exit_event=None, init_done_event=None, log_level: str = None, log_queue: object = None)</code>","text":"<p>               Bases: <code>NFPWorker</code></p> <p>FastAPIWorker is a worker class that integrates with FastAPI and Uvicorn to serve a FastAPI application. It handles initialization, starting the server, and managing bearer tokens.</p> <p>Parameters:</p> Name Type Description Default <code>inventory</code> <code>str</code> <p>Inventory configuration for the worker.</p> required <code>broker</code> <code>str</code> <p>Broker URL to connect to.</p> required <code>worker_name</code> <code>str</code> <p>Name of this worker.</p> required <code>exit_event</code> <code>Event</code> <p>Event to signal worker to stop/exit.</p> <code>None</code> <code>init_done_event</code> <code>Event</code> <p>Event to signal when worker is done initializing.</p> <code>None</code> <code>log_level</code> <code>str</code> <p>Logging level for this worker.</p> <code>None</code> <code>log_queue</code> <code>object</code> <p>Queue for logging.</p> <code>None</code> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>def __init__(\n    self,\n    inventory: str,\n    broker: str,\n    worker_name: str,\n    exit_event=None,\n    init_done_event=None,\n    log_level: str = None,\n    log_queue: object = None,\n):\n    super().__init__(\n        inventory, broker, SERVICE, worker_name, exit_event, log_level, log_queue\n    )\n    self.init_done_event = init_done_event\n    self.exit_event = exit_event\n    self.api_prefix = \"/api\"\n\n    # get inventory from broker\n    self.fastapi_inventory = self.load_inventory()\n    self.uvicorn_inventory = {\n        \"host\": \"0.0.0.0\",\n        \"port\": 8000,\n        **self.fastapi_inventory.pop(\"uvicorn\", {}),\n    }\n\n    # instantiate cache\n    self.cache_dir = os.path.join(self.base_dir, \"cache\")\n    os.makedirs(self.cache_dir, exist_ok=True)\n    self.cache = self._get_diskcache()\n    self.cache.expire()\n\n    # start FastAPI server\n    self.fastapi_start()\n\n    self.service_tasks_api_discovery_thread = threading.Thread(\n        target=service_tasks_api_discovery, args=(self,)\n    )\n    self.service_tasks_api_discovery_thread.start()\n\n    self.init_done_event.set()\n</code></pre>"},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.FastAPIWorker.fastapi_start","title":"<code>fastapi_start()</code>","text":"<p>Starts the FastAPI server.</p> <p>This method initializes the FastAPI application using the provided configuration, starts the Uvicorn server in a separate thread, and waits for the server to be fully started before logging the server's URL.</p> <p>Steps:</p> <ol> <li>Create the FastAPI application using <code>make_fast_api_app</code>.</li> <li>Configure the Uvicorn server with the application and settings.</li> <li>Start the Uvicorn server in a new thread.</li> <li>Wait for the server to start.</li> <li>Log the server's URL.'</li> </ol> <p>Attributes:</p> Name Type Description <code>self.app</code> <code>FastAPI</code> <p>The FastAPI application instance.</p> <code>self.uvicorn_server</code> <code>Server</code> <p>The Uvicorn server instance.</p> <code>self.uvicorn_server_thread</code> <code>Thread</code> <p>The thread running the Uvicorn server.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the server fails to start.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>def fastapi_start(self):\n    \"\"\"\n    Starts the FastAPI server.\n\n    This method initializes the FastAPI application using the provided\n    configuration, starts the Uvicorn server in a separate thread, and waits\n    for the server to be fully started before logging the server's URL.\n\n    Steps:\n\n    1. Create the FastAPI application using `make_fast_api_app`.\n    2. Configure the Uvicorn server with the application and settings.\n    3. Start the Uvicorn server in a new thread.\n    4. Wait for the server to start.\n    5. Log the server's URL.'\n\n    Attributes:\n        self.app (FastAPI): The FastAPI application instance.\n        self.uvicorn_server (uvicorn.Server): The Uvicorn server instance.\n        self.uvicorn_server_thread (threading.Thread): The thread running the Uvicorn server.\n\n    Raises:\n        Exception: If the server fails to start.\n    \"\"\"\n    self.app = make_fast_api_app(\n        worker=self, config=self.fastapi_inventory.get(\"fastapi\", {})\n    )\n\n    # start uvicorn server in a thread\n    config = uvicorn.Config(app=self.app, **self.uvicorn_inventory)\n    self.uvicorn_server = uvicorn.Server(config=config)\n\n    self.uvicorn_server_thread = threading.Thread(target=self.uvicorn_server.run)\n    self.uvicorn_server_thread.start()\n\n    # wait for server to start\n    while not self.uvicorn_server.started:\n        time.sleep(0.001)\n\n    log.info(\n        f\"{self.name} - Uvicorn server started, serving FastAPI app at \"\n        f\"http://{self.uvicorn_inventory['host']}:{self.uvicorn_inventory['port']}\"\n    )\n</code></pre>"},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.FastAPIWorker.worker_exit","title":"<code>worker_exit()</code>","text":"<p>Terminates the current process by sending a SIGTERM signal to itself.</p> <p>This method retrieves the current process ID using <code>os.getpid()</code> and then sends a SIGTERM signal to terminate the process using <code>os.kill()</code>.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>def worker_exit(self):\n    \"\"\"\n    Terminates the current process by sending a SIGTERM signal to itself.\n\n    This method retrieves the current process ID using `os.getpid()` and then\n    sends a SIGTERM signal to terminate the process using `os.kill()`.\n    \"\"\"\n    os.kill(os.getpid(), signal.SIGTERM)\n</code></pre>"},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.FastAPIWorker.get_version","title":"<code>get_version() -&gt; Result</code>","text":"<p>Produce a report of the versions of various Python packages.</p> <p>This method collects the versions of several specified Python packages and returns them in a dictionary.</p> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the task name and a dictionary with     the package names as keys and their respective versions as values.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_version(self) -&gt; Result:\n    \"\"\"\n    Produce a report of the versions of various Python packages.\n\n    This method collects the versions of several specified Python packages\n    and returns them in a dictionary.\n\n    Returns:\n        Result: An object containing the task name and a dictionary with\n                the package names as keys and their respective versions as values.\n    \"\"\"\n    libs = {\n        \"norfab\": \"\",\n        \"fastapi\": \"\",\n        \"uvicorn\": \"\",\n        \"pydantic\": \"\",\n        \"python-multipart\": \"\",\n        \"python\": sys.version.split(\" \")[0],\n        \"platform\": sys.platform,\n    }\n    # get version of packages installed\n    for pkg in libs.keys():\n        try:\n            libs[pkg] = importlib.metadata.version(pkg)\n        except importlib.metadata.PackageNotFoundError:\n            pass\n\n    return Result(task=f\"{self.name}:get_version\", result=libs)\n</code></pre>"},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.FastAPIWorker.get_inventory","title":"<code>get_inventory() -&gt; Result</code>","text":"<p>Retrieve the inventory of the FastAPI worker.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Result</code> <p>A dictionary containing the combined inventory of FastAPI and Uvicorn.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_inventory(self) -&gt; Result:\n    \"\"\"\n    Retrieve the inventory of the FastAPI worker.\n\n    Returns:\n        Dict: A dictionary containing the combined inventory of FastAPI and Uvicorn.\n    \"\"\"\n    return Result(\n        result={**self.fastapi_inventory, \"uvicorn\": self.uvicorn_inventory},\n        task=f\"{self.name}:get_inventory\",\n    )\n</code></pre>"},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.FastAPIWorker.get_openapi_schema","title":"<code>get_openapi_schema(paths: bool = False) -&gt; Result</code>","text":"<p>Generates and returns the OpenAPI schema for the FastAPI application.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>bool</code> <p>If True, returns a list of available API endpoint paths. If False, returns the full OpenAPI schema. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing either the list of endpoint paths or the full OpenAPI schema</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_openapi_schema(self, paths: bool = False) -&gt; Result:\n    \"\"\"\n    Generates and returns the OpenAPI schema for the FastAPI application.\n\n    Args:\n        paths (bool, optional): If True, returns a list of available API endpoint paths.\n            If False, returns the full OpenAPI schema. Defaults to False.\n\n    Returns:\n        Result: An object containing either the list of endpoint paths or the full OpenAPI schema\n    \"\"\"\n    schema = make_openapi_schema(self.app)\n    if paths is True:\n        return Result(\n            result=list(schema[\"paths\"].keys()),\n            task=f\"{self.name}:get_openapi_schema\",\n        )\n    else:\n        return Result(\n            result=schema,\n            task=f\"{self.name}:get_openapi_schema\",\n        )\n</code></pre>"},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.FastAPIWorker.bearer_token_store","title":"<code>bearer_token_store(job: Job, username: str, token: str, expire: int = None) -&gt; Result</code>","text":"<p>Method to store a bearer token in the database.</p> <p>This method stores a bearer token associated with a username in the cache.</p> <p>If an expiration time is not provided, it retrieves the default token TTL from the FastAPI inventory configuration.</p> <p>Parameters:</p> Name Type Description Default <code>username</code> <code>str</code> <p>str - The name of the user to store the token for.</p> required <code>token</code> <code>str</code> <p>str - The token string to store.</p> required <code>expire</code> <code>int</code> <p>int, optional - The number of seconds before the token expires.</p> <code>None</code> <p>Returns:</p> Type Description <code>Result</code> <p>bool - Returns True if the token is successfully stored.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>@Task(fastapi=False, mcp=False)\ndef bearer_token_store(\n    self, job: Job, username: str, token: str, expire: int = None\n) -&gt; Result:\n    \"\"\"\n    Method to store a bearer token in the database.\n\n    This method stores a bearer token associated with a username in the cache.\n\n    If an expiration time is not provided, it retrieves the default token TTL\n    from the FastAPI inventory configuration.\n\n    Args:\n        username: str - The name of the user to store the token for.\n        token: str - The token string to store.\n        expire: int, optional - The number of seconds before the token expires.\n\n    Returns:\n        bool - Returns True if the token is successfully stored.\n    \"\"\"\n    expire = expire or self.fastapi_inventory.get(\"auth_bearer\", {}).get(\n        \"token_ttl\", expire\n    )\n    self.cache.expire()\n    cache_key = f\"bearer_token::{token}\"\n    if cache_key in self.cache:\n        user_token = self.cache.get(cache_key)\n    else:\n        user_token = {\n            \"token\": token,\n            \"username\": username,\n            \"created\": str(datetime.now()),\n        }\n    self.cache.set(cache_key, user_token, expire=expire, tag=username)\n\n    return Result(task=f\"{self.name}:bearer_token_store\", result=True)\n</code></pre>"},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.FastAPIWorker.bearer_token_delete","title":"<code>bearer_token_delete(job: Job, username: str = None, token: str = None) -&gt; Result</code>","text":"<p>Deletes a bearer token from the cache. This method removes a bearer token from the cache based on either the token itself or the associated username.</p> <p>If a token is provided, it will be removed directly. If a username is provided, all tokens associated with that username will be evicted from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>username</code> <code>str</code> <p>The username associated with the token(s) to be removed. Defaults to None.</p> <code>None</code> <code>token</code> <code>str</code> <p>The bearer token to be removed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>Result</code> <p>True if the operation was successful, otherwise raises an exception.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the token removal from the cache fails.</p> <code>Exception</code> <p>If neither username nor token is provided.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>@Task(fastapi=False, mcp=False)\ndef bearer_token_delete(\n    self, job: Job, username: str = None, token: str = None\n) -&gt; Result:\n    \"\"\"\n    Deletes a bearer token from the cache.\n    This method removes a bearer token from the cache based on either\n    the token itself or the associated username.\n\n    If a token is provided, it will be removed directly. If a username\n    is provided, all tokens associated with that username will be evicted\n    from the cache.\n\n    Args:\n        username (str, optional): The username associated with the token(s) to be removed. Defaults to None.\n        token (str, optional): The bearer token to be removed. Defaults to None.\n\n    Returns:\n        bool: True if the operation was successful, otherwise raises an exception.\n\n    Raises:\n        RuntimeError: If the token removal from the cache fails.\n        Exception: If neither username nor token is provided.\n    \"\"\"\n    self.cache.expire()\n    token_removed_count = 0\n    if token:\n        cache_key = f\"bearer_token::{token}\"\n        if cache_key in self.cache:\n            if self.cache.delete(cache_key, retry=True):\n                token_removed_count = 1\n            else:\n                raise RuntimeError(f\"Failed to remove {username} token from cache\")\n    elif username:\n        token_removed_count = self.cache.evict(tag=username, retry=True)\n    else:\n        raise Exception(\"Cannot delete, either username or token must be provided\")\n\n    log.info(\n        f\"{self.name} removed {token_removed_count} token(s) for user {username}\"\n    )\n\n    return Result(task=f\"{self.name}:bearer_token_delete\", result=True)\n</code></pre>"},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.FastAPIWorker.bearer_token_list","title":"<code>bearer_token_list(job: Job, username: str = None) -&gt; Result</code>","text":"<p>Retrieves a list of bearer tokens from the cache, optionally filtered by username.</p> <p>Parameters:</p> Name Type Description Default <code>username</code> <code>str</code> <p>The username to filter tokens by. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>Result</code> <p>A list of dictionaries containing token information. Each dictionary contains:</p> <ul> <li>\"username\" (str): The username associated with the token.</li> <li>\"token\" (str): The bearer token.</li> <li>\"age\" (str): The age of the token.</li> <li>\"creation\" (str): The creation time of the token.</li> <li>\"expires\" (str): The expiration time of the token, if available.</li> </ul> <p>If no tokens are found, a list with a single dictionary containing empty strings for all fields is returned.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>@Task(fastapi=False, mcp=False)\ndef bearer_token_list(self, job: Job, username: str = None) -&gt; Result:\n    \"\"\"\n    Retrieves a list of bearer tokens from the cache, optionally filtered by username.\n\n    Args:\n        username (str, optional): The username to filter tokens by. Defaults to None.\n\n    Returns:\n        list: A list of dictionaries containing token information. Each dictionary contains:\n\n            - \"username\" (str): The username associated with the token.\n            - \"token\" (str): The bearer token.\n            - \"age\" (str): The age of the token.\n            - \"creation\" (str): The creation time of the token.\n            - \"expires\" (str): The expiration time of the token, if available.\n\n    If no tokens are found, a list with a single dictionary containing\n    empty strings for all fields is returned.\n    \"\"\"\n\n    self.cache.expire()\n    ret = Result(task=f\"{self.name}:bearer_token_list\", result=[])\n\n    for cache_key in self.cache:\n        token_data, expires, tag = self.cache.get(\n            cache_key, expire_time=True, tag=True\n        )\n        if username and tag != username:\n            continue\n        if expires is not None:\n            expires = datetime.fromtimestamp(expires)\n        creation = datetime.fromisoformat(token_data[\"created\"])\n        age = datetime.now() - creation\n        ret.result.append(\n            {\n                \"username\": token_data[\"username\"],\n                \"token\": token_data[\"token\"],\n                \"age\": str(age),\n                \"creation\": str(creation),\n                \"expires\": str(expires),\n            }\n        )\n\n    # return empty result if no tokens found\n    if not ret.result:\n        ret.result = [\n            {\n                \"username\": \"\",\n                \"token\": \"\",\n                \"age\": \"\",\n                \"creation\": \"\",\n                \"expires\": \"\",\n            }\n        ]\n\n    return ret\n</code></pre>"},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.FastAPIWorker.bearer_token_check","title":"<code>bearer_token_check(token: str, job: Job) -&gt; Result</code>","text":"<p>Checks if the provided bearer token is present in the cache and still active.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The bearer token to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>Result</code> <p>True if the token is found in the cache, False otherwise.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>@Task(fastapi=False, mcp=False)\ndef bearer_token_check(self, token: str, job: Job) -&gt; Result:\n    \"\"\"\n    Checks if the provided bearer token is present in the cache and still active.\n\n    Args:\n        token (str): The bearer token to check.\n\n    Returns:\n        bool: True if the token is found in the cache, False otherwise.\n    \"\"\"\n    self.cache.expire()\n    cache_key = f\"bearer_token::{token}\"\n    return Result(\n        task=f\"{self.name}:bearer_token_check\", result=cache_key in self.cache\n    )\n</code></pre>"},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.FastAPIWorker.discover","title":"<code>discover(job, service: str = 'all', progress: bool = True) -&gt; Result</code>","text":"<p>Discovers available services tasks and auto-generate API endpoints for them.</p> <p>Parameters:</p> Name Type Description Default <code>service</code> <code>str</code> <p>The name of the service to discover. Defaults to \"all\".</p> <code>'all'</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the discovery results for the specified service.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef discover(self, job, service: str = \"all\", progress: bool = True) -&gt; Result:\n    \"\"\"\n    Discovers available services tasks and auto-generate API endpoints for them.\n\n    Args:\n        service (str, optional): The name of the service to discover. Defaults to \"all\".\n\n    Returns:\n        Result: An object containing the discovery results for the specified service.\n    \"\"\"\n    job.event(\"Discovering NorFab services tasks\")\n    ret = Result(task=f\"{self.name}:discover\")\n    ret.result = service_tasks_api_discovery(\n        self, cycles=1, discover_service=service\n    )\n\n    return ret\n</code></pre>"},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.create_api_endpoint","title":"<code>create_api_endpoint(service: str, task_name: str, schema: dict, worker: object) -&gt; callable</code>","text":"<p>Creates an asynchronous FastAPI endpoint function for a given service task.</p> <p>Parameters:</p> Name Type Description Default <code>service</code> <code>str</code> <p>Service name.</p> required <code>task_name</code> <code>str</code> <p>Task name.</p> required <code>schema</code> <code>dict</code> <p>Input schema for the task.</p> required <code>worker</code> <code>object</code> <p>Worker instance.</p> required <p>Returns:</p> Name Type Description <code>function</code> <code>callable</code> <p>An asynchronous endpoint function</p> <p>The generated endpoint expects a JSON body containing arguments for the job and returns the result of the job execution.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>def create_api_endpoint(\n    service: str, task_name: str, schema: dict, worker: object\n) -&gt; callable:\n    \"\"\"\n    Creates an asynchronous FastAPI endpoint function for a given service task.\n\n    Args:\n        service (str): Service name.\n        task_name (str): Task name.\n        schema (dict): Input schema for the task.\n        worker (object): Worker instance.\n\n    Returns:\n        function: An asynchronous endpoint function\n\n    The generated endpoint expects a JSON body containing arguments for\n    the job and returns the result of the job execution.\n    \"\"\"\n    # We will handle a missing token ourselves\n    get_bearer_token = HTTPBearer(auto_error=False)\n    default_workers = schema[\"properties\"].get(\"workers\", {}).get(\"default\", \"all\")\n\n    def get_token(\n        auth: Optional[HTTPAuthorizationCredentials] = Depends(get_bearer_token),\n    ) -&gt; str:\n        # check token exists in database\n        if (\n            auth is None\n            or worker.bearer_token_check(auth.credentials, Job()).result is False\n        ):\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=UnauthorizedMessage().detail,\n            )\n        return auth.credentials\n\n    async def endpoint(\n        request: Request,\n        token: str = Depends(get_token),\n    ) -&gt; Dict[Annotated[str, Body(description=\"Worker Name\")], Result]:\n        kwargs = await request.json()\n        workers = kwargs.pop(\"workers\", default_workers)\n\n        log.debug(\n            f\"FastAPI running '{service}:{task_name}' task, on '{workers}' workers, job data: '{kwargs}'\"\n        )\n\n        res = worker.client.run_job(\n            service=service,\n            task=task_name,\n            kwargs=kwargs,\n            workers=workers,\n        )\n        return res\n\n    return endpoint\n</code></pre>"},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.make_openapi_schema","title":"<code>make_openapi_schema(app, regenerate: bool = False, json_refs: Optional[dict] = None) -&gt; Dict</code>","text":"<p>Generates and returns the OpenAPI schema for a FastAPI application.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>FastAPI</code> <p>The FastAPI application instance.</p> required <code>regenerate</code> <code>bool</code> <p>If True, forces regeneration of the OpenAPI schema.</p> <code>False</code> <code>json_refs</code> <code>dict</code> <p>A dictionary of JSON references to include in the schema under the \"$defs\" key.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict</code> <p>The generated OpenAPI schema as a dictionary.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>def make_openapi_schema(\n    app, regenerate: bool = False, json_refs: Optional[dict] = None\n) -&gt; Dict:\n    \"\"\"\n    Generates and returns the OpenAPI schema for a FastAPI application.\n\n    Args:\n        app (FastAPI): The FastAPI application instance.\n        regenerate (bool, optional): If True, forces regeneration of the OpenAPI schema.\n        json_refs (dict, optional): A dictionary of JSON references to include in\n            the schema under the \"$defs\" key.\n\n    Returns:\n        dict: The generated OpenAPI schema as a dictionary.\n    \"\"\"\n    # make app to re-generate openapi schema\n    if regenerate is True:\n        app.openapi_schema = None\n        app.setup()\n\n    openapi_schema = get_openapi(title=API_TITLE, version=\"1\", routes=app.routes)\n\n    # add json references\n    if json_refs:\n        openapi_schema[\"$defs\"] = json_refs\n\n    return openapi_schema\n</code></pre>"},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.service_tasks_api_discovery","title":"<code>service_tasks_api_discovery(worker, cycles: int = 30, discover_service: str = 'all') -&gt; Dict</code>","text":"<p>Periodically discovers available service tasks and dynamically registers FastAPI endpoints for them.</p> <p>This function performs the following steps in a loop:</p> <ol> <li>Retrieves a list of available services from the worker's client.</li> <li>For each service, fetches its available tasks.</li> <li>For each task, checks if it should be exposed via FastAPI (i.e.,     <code>task[\"fastapi\"]</code> is not False).</li> <li>If the corresponding API endpoint does not already exist, registers a new     FastAPI route for the task, using its input schema and metadata.</li> <li>Forces regeneration of the OpenAPI schema after new endpoints are added.</li> </ol> <p>The loop runs on fastapi service startup up to 30 cycles or until the worker's exit event is set, with a 10-second delay between cycles.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>def service_tasks_api_discovery(\n    worker, cycles: int = 30, discover_service: str = \"all\"\n) -&gt; Dict:\n    \"\"\"\n    Periodically discovers available service tasks and dynamically registers\n    FastAPI endpoints for them.\n\n    This function performs the following steps in a loop:\n\n    1. Retrieves a list of available services from the worker's client.\n    2. For each service, fetches its available tasks.\n    3. For each task, checks if it should be exposed via FastAPI (i.e.,\n        `task[\"fastapi\"]` is not False).\n    4. If the corresponding API endpoint does not already exist, registers a new\n        FastAPI route for the task, using its input schema and metadata.\n    5. Forces regeneration of the OpenAPI schema after new endpoints are added.\n\n    The loop runs on fastapi service startup up to 30 cycles or until the worker's\n    exit event is set, with a 10-second delay between cycles.\n    \"\"\"\n    result = {}\n    json_refs = {}  # dictionary to store JSON references\n    while not worker.exit_event.is_set() and cycles &gt; 0:\n        tasks = []\n        services = []\n        try:\n            # get a list of workers and construct a list of services\n            services = worker.client.get(\"mmi.service.broker\", \"show_workers\")\n            services = [\n                s[\"service\"]\n                for s in services[\"results\"]\n                if discover_service == \"all\" or s[\"service\"] == discover_service\n            ]\n\n            # retrieve NorFab services and their tasks\n            for service in services:\n                # skip already discovered services\n                if service in result:\n                    continue\n                service_tasks = worker.client.run_job(\n                    service=service,\n                    task=\"list_tasks\",\n                    workers=\"any\",\n                    timeout=3,\n                )\n                # skip if client request timed out\n                if service_tasks is None:\n                    continue\n                for wres in service_tasks.values():\n                    for t in wres[\"result\"]:\n                        t[\"service\"] = service\n                    tasks.extend(wres[\"result\"])\n\n            for task in tasks:\n                # skip task endpoint creation if set to false\n                if task[\"fastapi\"] is False:\n                    continue\n                # save service to results\n                result.setdefault(task[\"service\"], [])\n                # continue with creating API endpoint for task\n                path = f\"{worker.api_prefix}/{task['service']}/{task['name']}/\"\n                for route in worker.app.routes:\n                    if isinstance(route, Route) and route.path == path:\n                        break  # do no re-create existing endpoints\n                else:\n                    # form OpenAPI schema for API endpoint\n                    schema = task[\"inputSchema\"]\n                    fastapi_schema = task[\"fastapi\"].pop(\"schema\", {\"properties\": {}})\n                    schema[\"properties\"] = {\n                        **fastapi_schema[\"properties\"],\n                        **schema[\"properties\"],\n                    }\n                    _ = schema[\"properties\"].pop(\"job\", None)\n                    # extract json references\n                    if \"$defs\" in schema:\n                        json_refs.update(schema.pop(\"$defs\"))\n                    # form add_api_route arguments\n                    task[\"fastapi\"].setdefault(\"methods\", [\"POST\"])\n                    task[\"fastapi\"].setdefault(\"path\", path)\n                    task[\"fastapi\"].setdefault(\"description\", task[\"description\"])\n                    task[\"fastapi\"].setdefault(\"name\", task[\"name\"])\n                    # register API endpoint\n                    log.debug(\n                        f\"Registering API endpoint {task['fastapi']['path']}, schema: {schema}\"\n                    )\n                    worker.app.add_api_route(\n                        endpoint=create_api_endpoint(\n                            service=task[\"service\"],\n                            task_name=task[\"name\"],\n                            schema=schema,\n                            worker=worker,\n                        ),\n                        responses={\n                            status.HTTP_401_UNAUTHORIZED: dict(\n                                model=UnauthorizedMessage\n                            )\n                        },\n                        openapi_extra={\n                            \"requestBody\": {\n                                \"required\": True,\n                                \"content\": {\"application/json\": {\"schema\": schema}},\n                            }\n                        },\n                        tags=[f\"NORFAB {task['service'].upper()}\"],\n                        **task[\"fastapi\"],\n                    )\n                    worker.app.openapi_schema = make_openapi_schema(\n                        app=worker.app, regenerate=True, json_refs=json_refs\n                    )\n                    # save discovered task to results\n                    result[task[\"service\"]].append(task[\"fastapi\"][\"name\"])\n        except Exception as e:\n            log.exception(f\"Failed to discover services tasks, error: {e}\")\n\n        cycles -= 1\n        time.sleep(10)\n\n    return result\n</code></pre>"},{"location":"workers/fastapi/api_reference_workers_fastapi_worker/#norfab.workers.fastapi_worker.make_fast_api_app","title":"<code>make_fast_api_app(worker: object, config: dict) -&gt; FastAPI</code>","text":"<p>Create a FastAPI application with endpoints for posting, getting, and running jobs.</p> <p>This function sets up a FastAPI application with three endpoints:</p> <ul> <li>POST /job: To post a job to the NorFab service.</li> <li>GET /job: To get job results from the NorFab service.</li> <li>POST /job/run: To run a job and return job results synchronously.</li> </ul> <p>Each endpoint requires a bearer token for authentication, which is validated against the worker's token database.</p> <p>Parameters:</p> Name Type Description Default <code>worker</code> <code>object</code> <p>An object representing the worker that will handle the job requests.</p> required <code>config</code> <code>dict</code> <p>A dictionary of configuration options for the FastAPI application.</p> required <p>Returns:</p> Name Type Description <code>FastAPI</code> <code>FastAPI</code> <p>A FastAPI application instance.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>def make_fast_api_app(worker: object, config: dict) -&gt; FastAPI:\n    \"\"\"\n    Create a FastAPI application with endpoints for posting, getting, and running jobs.\n\n    This function sets up a FastAPI application with three endpoints:\n\n    - POST /job: To post a job to the NorFab service.\n    - GET /job: To get job results from the NorFab service.\n    - POST /job/run: To run a job and return job results synchronously.\n\n    Each endpoint requires a bearer token for authentication, which is validated\n    against the worker's token database.\n\n    Args:\n        worker (object): An object representing the worker that will handle the job requests.\n        config (dict): A dictionary of configuration options for the FastAPI application.\n\n    Returns:\n        FastAPI: A FastAPI application instance.\n    \"\"\"\n    config = {\n        \"title\": API_TITLE,\n        \"summary\": \"NorFab Services Tasks FastAPI application with endpoints for posting, getting, and running jobs\",\n        **config,\n    }\n    app = FastAPI(**config)\n\n    # We will handle a missing token ourselves\n    get_bearer_token = HTTPBearer(auto_error=False)\n\n    def get_token(\n        auth: Optional[HTTPAuthorizationCredentials] = Depends(get_bearer_token),\n    ) -&gt; str:\n        # check token exists in database\n        if (\n            auth is None\n            or worker.bearer_token_check(auth.credentials, Job()).result is False\n        ):\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=UnauthorizedMessage().detail,\n            )\n        return auth.credentials\n\n    @app.post(\n        f\"{worker.api_prefix}/job\",\n        responses={status.HTTP_401_UNAUTHORIZED: dict(model=UnauthorizedMessage)},\n        tags=[\"NORFAB\"],\n    )\n    def post_job(\n        service: Annotated[\n            str, Body(description=\"The name of the service to post the job to\")\n        ],\n        task: Annotated[\n            str, Body(description=\"The task to be executed by the service\")\n        ],\n        args: Annotated[\n            List[Any], Body(description=\"A list of positional arguments for the task\")\n        ] = None,\n        kwargs: Annotated[\n            Dict[str, Any],\n            Body(description=\"A dictionary of keyword arguments for the task\"),\n        ] = None,\n        workers: Annotated[\n            Union[str, List[str]], Body(description=\"The workers to dispatch the task\")\n        ] = \"all\",\n        uuid: Annotated[\n            str, Body(description=\"Optional a unique identifier to use for the job\")\n        ] = None,\n        timeout: Annotated[\n            int, Body(description=\"The timeout for the job in seconds\")\n        ] = 600,\n        token: str = Depends(get_token),\n    ) -&gt; ClientPostJobResponse:\n        \"\"\"\n        Method to post the job to NorFab.\n\n        Args:\n            service: The name of the service to post the job to.\n            task: The task to be executed by the service.\n            args: A list of positional arguments for the task. Defaults to None.\n            kwargs: A dictionary of keyword arguments for the task. Defaults to None.\n            workers: The workers to dispatch the task. Defaults to \"all\".\n            uuid: Optional a unique identifier to use for the job. Defaults to None.\n            timeout: The timeout for the job in seconds. Defaults to 600.\n\n        Returns:\n            The response from the NorFab service.\n        \"\"\"\n        log.debug(\n            f\"{worker.name} - received job post request, service {service}, task {task}, args {args}, kwargs {kwargs}\"\n        )\n        res = worker.client.post(\n            service=service,\n            task=task,\n            args=args,\n            kwargs=kwargs,\n            workers=workers,\n            timeout=timeout,\n            uuid=uuid,\n        )\n        return res\n\n    @app.get(\n        f\"{worker.api_prefix}/job\",\n        responses={status.HTTP_401_UNAUTHORIZED: dict(model=UnauthorizedMessage)},\n        tags=[\"NORFAB\"],\n    )\n    def get_job(\n        service: Annotated[\n            str, Body(description=\"The name of the service to get the job from\")\n        ],\n        uuid: Annotated[str, Body(description=\"A unique identifier for the job\")],\n        workers: Annotated[\n            Union[str, List[str]],\n            Body(description=\"The workers to dispatch the get request to\"),\n        ] = \"all\",\n        timeout: Annotated[\n            int, Body(description=\"The timeout for the job in seconds\")\n        ] = 600,\n        token: str = Depends(get_token),\n    ) -&gt; ClientGetJobResponse:\n        \"\"\"\n        Method to get job results from NorFab.\n\n        Args:\n            service: The name of the service to get the job from.\n            workers: The workers to dispatch the get request to. Defaults to \"all\".\n            uuid: A unique identifier for the job.\n            timeout: The timeout for the job get requests in seconds. Defaults to 600.\n\n        Returns:\n            The response from the NorFab service.\n        \"\"\"\n        log.debug(\n            f\"{worker.name} - received job get request, service {service}, uuid {uuid}\"\n        )\n        res = worker.client.get(\n            service=service,\n            uuid=uuid,\n            workers=workers,\n            timeout=timeout,\n        )\n        return res\n\n    @app.post(\n        f\"{worker.api_prefix}/job/run\",\n        responses={status.HTTP_401_UNAUTHORIZED: dict(model=UnauthorizedMessage)},\n        tags=[\"NORFAB\"],\n    )\n    def run_job(\n        service: Annotated[\n            str, Body(description=\"The name of the service to post the job to\")\n        ],\n        task: Annotated[\n            str, Body(description=\"The task to be executed by the service\")\n        ],\n        args: Annotated[\n            List[Any], Body(description=\"A list of positional arguments for the task\")\n        ] = None,\n        kwargs: Annotated[\n            Dict[str, Any],\n            Body(description=\"A dictionary of keyword arguments for the task\"),\n        ] = None,\n        workers: Annotated[\n            Union[str, List[str]], Body(description=\"The workers to dispatch the task\")\n        ] = \"all\",\n        uuid: Annotated[\n            str, Body(description=\"Optional a unique identifier to use for the job\")\n        ] = None,\n        timeout: Annotated[\n            int, Body(description=\"The timeout for the job in seconds\")\n        ] = 600,\n        token: str = Depends(get_token),\n    ) -&gt; Dict[str, Result]:\n        \"\"\"\n        Method to run job and return job results synchronously. This function\n        is blocking, internally it uses post/get methods to submit job request\n        and waits for job results to come through for all workers request was\n        dispatched to, exiting either once timeout expires or after all workers\n        reported job result back to the client.\n\n        Args:\n            service: The name of the service to post the job to.\n            task: The task to be executed by the service.\n            args: A list of positional arguments for the task. Defaults to None.\n            kwargs: A dictionary of keyword arguments for the task. Defaults to None.\n            workers: The workers to dispatch the task. Defaults to \"all\".\n            uuid: A unique identifier for the job. Defaults to None.\n            timeout: The timeout for the job in seconds. Defaults to 600.\n\n        Returns:\n            The response from the NorFab service.\n        \"\"\"\n        log.debug(\n            f\"{worker.name} - received run job request, service {service}, task {task}, args {args}, kwargs {kwargs}\"\n        )\n        res = worker.client.run_job(\n            service=service,\n            task=task,\n            uuid=uuid,\n            args=args,\n            kwargs=kwargs,\n            workers=workers,\n            timeout=timeout,\n        )\n        return res\n\n    return app\n</code></pre>"},{"location":"workers/fastapi/services_fastapi_service/","title":"FastAPI REST API Service","text":"<p>The FastAPI Service created to serve a set of REST API endpoints to interact with NorFab to start, run, list and retrieve jobs result.</p> <p>NorFab FastAPI Service build using FastAPI library, as the name implies, a well adopted open-source library for building RESTful APIs.</p> <p></p> <p>FastAPI Service acts as a thin wrapper for NorFab Python API, accepting northbound REST API requests and translating them into NorFab client API calls, returning job execution results in response.</p>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service/#overview","title":"Overview","text":"<p>The FastAPI Service in Norfab provides a robust and efficient way to REST API into NorFab environment for network automation and management tasks. FastAPI is known for its high performance, ease of use, and automatic generation of interactive API documentation. By leveraging FastAPI, Norfab enables developers to utilize scalable and maintainable APIs that can handle a wide range of network automation operations.</p>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service/#norfab-fastapi-service-key-features","title":"NorFab FastAPI Service Key Features","text":"<ul> <li> <p>High Performance: FastAPI is built on top of Starlette for the web parts and Pydantic for the data parts, ensuring high performance and fast response times.</p> </li> <li> <p>Automatic Documentation: NorFab FastAPI service comes with automatically generated interactive API documentation using Swagger UI and ReDoc, making it easy to explore and test the API endpoints.</p> </li> <li> <p>Data Validation: FastAPI uses Pydantic for data validation, ensuring that the input and output data is correctly formatted and adheres to the specified schema.</p> </li> <li> <p>Security: NorFab FastAPI service includes Bearer token API authentication, ensuring that the APIs are secure and protected.</p> </li> <li> <p>API Endpoints Auto-Generation: API Endpoints auto-generated using Services' tasks functions definitions</p> </li> </ul>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service/#use-cases","title":"Use Cases","text":"<ul> <li> <p>Network Device Management: Use the NorFab FastAPI Service for managing network devices, including configuration changes, state retrieval, firmware updates etc.</p> </li> <li> <p>Inventory Management: Use REST API to automate the process of updating and maintaining network inventory, ensuring that the inventory data is always accurate and up-to-date.</p> </li> <li> <p>Configuration Compliance: Utilize REST API to automate configuration compliance checks and audits, ensuring that network devices adhere to predefined standards and policies.</p> </li> <li> <p>Automation Workflows: Use the NorFab FastAPI Service APIs to orchestrate complex automation workflows, integrating with other services and tools to streamline network operations.</p> </li> </ul>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service/#getting-started","title":"Getting Started","text":"<p>To get started with the FastAPI Service, you need to define the necessary parameters in your NorFab inventory. Refer to the FastAPI Inventory section for detailed instructions on setting up your inventory and running FastAPI REST API Service with NorFab.</p>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service/#api-endpoints","title":"API Endpoints","text":"<ul> <li><code>http://&lt;IP Address or FQDN&gt;/api/xyz</code> NorFab REST API endpoints</li> <li><code>http://&lt;IP Address or FQDN&gt;/docs</code> url for Swagger UI documentation</li> <li><code>http://&lt;IP Address or FQDN&gt;/redoc</code> url for ReDoc UI documentation</li> <li><code>http://&lt;IP Address or FQDN&gt;/openapi.json</code> NorFab REST API OpenAPI schema file</li> </ul>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service/#authentication","title":"Authentication","text":"<p>FastAPI NorFab service supports bearer token authentication using authorization header:</p> <pre><code>headers = {\"Authorization\": \"Bearer f343ff34r3fg4g5g34gf34g34g3g34g4\"}\n</code></pre> <p>Where token is an arbitrary string with no constraints imposed on length or complexity of the token. By default FastAPI Service does not have any bearer tokens stored, instead, every user willing to access REST API need to have token explicitly recorded in FastAPI Service database using nfcli auth commands.</p> <p>Note: Proper token management is critical to ensure secure access to the FastAPI Service. Always store tokens securely and revoke unused or compromised tokens promptly.</p> <p>To create token run this command from nfcli shell:</p> <pre><code>nf#fastapi auth create-token username foo token 4565t4yjn56h534gh35h543h5h45h4h4 expire 3600\n{\n    \"fastapi-worker-1\": true\n}\nnf#\n</code></pre> <p><code>expire</code> argument is optional and indicates token expiration time in seconds, if no <code>expire</code> argument provided token does not expire. Multiple tokens can be stored for any given user.</p> <p>Refer to auth tasks documentation for further details on how to work with FastAPI tokens.</p>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service/#api-documentation","title":"API Documentation","text":"<p>Once FastAPI worker started it serves REST API documentation under  <code>http://&lt;IP Address or FQDN&gt;/docs</code> url for Swagger UI documentation and <code>http://&lt;IP Address or FQDN&gt;/redoc</code> for ReDoc UI documentation. Either of these UIs can be used to try and execute API requests and to familiarize yourself with API endpoints, requests payload and expected responses.</p>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service/#https-support","title":"HTTPS Support","text":"<p>NorFab FastAPI Service does not support HTTPS natively. To support HTTPS need to use reverse proxy (Nginx, Caddy etc.) or application loadbalancer that will perform TLS termination. Refer to FastAPI documentation for further details on how to serve FastAPI applications via HTTPS.</p>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service/#conclusion","title":"Conclusion","text":"<p>The FastAPI Service in Norfab provides a powerful and efficient RESTful APIs for network automation and management. With its high performance, automatic documentation, and robust feature set, FastAPI enables API integration that can handle a wide range of network operations. By using NorFab FastAPI REST API service, you can enhance your network automation capabilities and streamline your network management processes.</p>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service_inventory/","title":"FasAPI Worker Inventory","text":"<p>Content of <code>inventory.yaml</code> need to be updated to include FastAPI worker details:</p> inventory.yaml<pre><code>broker: \n  endpoint: \"tcp://127.0.0.1:5555\" \n  shared_key: \"5z1:yW}]n?UXhGmz+5CeHN1&gt;:S9k!eCh6JyIhJqO\"\n\nworkers:\n  fastapi-worker-1: \n    - fastapi/fastapi-worker-1.yaml\n\ntopology: \n  workers: \n    - fastapi-worker-1\n</code></pre> <p>To obtain broker <code>shared_key</code> run this command on broker:</p> <pre><code>cd &lt;path/to/broker/inventory.yaml&gt;\nnfcli --show-broker-shared-key\n</code></pre> <p>Sample FasAPI worker inventory definition</p> fastapi/fastapi-worker-1.yaml<pre><code>service: fastapi\nauth_bearer:\n  token_ttl: None\n\n# below parameters passed onto app = FastAPI(**fastapi_inventory) \n# https://fastapi.tiangolo.com/reference/fastapi/#fastapi.FastAPI\nfastapi:\n  title: FastAPI\n  docs_url: \"/docs\"\n  redoc_url: \"/redoc\"\n  # ..etc\n\n# below parameters passed onto config = uvicorn.Config(app, **uvicorn_inventory)\n# https://www.uvicorn.org/#config-and-server-instances\nuvicorn:\n  host: 0.0.0.0\n  port: 8000\n  # ..etc\n</code></pre>"},{"location":"workers/fastapi/services_fastapi_service_task_auth/","title":"FastAPI Auth Tasks","text":"<p>FastAPI service supports bearer token REST API authentication. To handle tokens lifecycle a number of FastAPI Service methods created allowing to store, delete, list and check API tokens.</p>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service_task_auth/#auth-tasks-sample-usage","title":"Auth Tasks Sample Usage","text":"<p>To store authentication token in FastAPI service database:</p> <pre><code>nf#fastapi auth create-token username foobar token f343ff34r3fg4g5g34gf34g34g3g34g4 expire 3600\n{\n    \"fastapi-worker-1\": true\n}\nnf#\n</code></pre> <p><code>expire</code> argument is optional and indicates token expiration time in seconds, if no <code>expire</code> argument provided token does not expire. Multiple tokens can be stored for any given user.</p> <p>To list stored tokens for specific user:</p> <pre><code>nf#fastapi auth list-tokens username foobar\n worker            username  token                             age             creation                    expires\n fastapi-worker-1  foobar    f343ff34r3fg4g5g34gf34g34g3g34g4  0:01:29.688340  2025-02-16 20:08:51.914919  2025-02-16 21:08:51.914919\n fastapi-worker-1  foobar    888945f96b824bf1b4358de790c452b6  8:08:51.548662  2025-02-16 12:01:30.054597  None\n fastapi-worker-1  foobar    04d685d203274a089c1a7df1395ed7e1  7:58:23.498734  2025-02-16 12:11:58.104525  None\n fastapi-worker-1  foobar    dfea48a8e412451cb2918fd526ab6c99  7:58:22.485560  2025-02-16 12:11:59.117699  None\nnf#\n</code></pre> <p>To list all stored tokens:</p> <pre><code>nf#fastapi auth list-tokens\n worker            username  token                             age             creation                    expires\n fastapi-worker-1  pytest    11111111111111111111111111111111  1:26:18.492274  2025-02-16 18:44:18.124019  None\n fastapi-worker-1  foobar    f343ff34r3fg4g5g34gf34g34g3g34g4  0:01:44.701374  2025-02-16 20:08:51.914919  2025-02-16 21:08:51.914919\n fastapi-worker-1  foobar    888945f96b824bf1b4358de790c452b6  8:09:06.561696  2025-02-16 12:01:30.054597  None\n fastapi-worker-1  foobar    04d685d203274a089c1a7df1395ed7e1  7:58:38.511768  2025-02-16 12:11:58.104525  None\n fastapi-worker-1  foobar    dfea48a8e412451cb2918fd526ab6c99  7:58:37.498594  2025-02-16 12:11:59.117699  None\n fastapi-worker-1  foo       4565t4yjn56h534gh35h543h5h45h4h4  0:00:27.641482  2025-02-16 20:10:08.974811  2025-02-16 21:10:08.974812\nnf#\n</code></pre> <p>To delete specific token:</p> <pre><code>nf#fastapi auth delete-token token 888945f96b824bf1b4358de790c452b6\n{\n    \"fastapi-worker-1\": true\n}\nnf#\n</code></pre> <p>To delete all tokens for given user:</p> <pre><code>nf#fastapi auth delete-token username foo\n{\n    \"fastapi-worker-1\": true\n}\nnf#\n</code></pre> <p>To check if token is valid:</p> <pre><code>nf#fastapi auth check-token token 888945f96b824bf1b4358de790c452b6\n{\n    \"fastapi-worker-1\": false\n}\nnf#fastapi auth check-token token f343ff34r3fg4g5g34gf34g34g3g34g4\n{\n    \"fastapi-worker-1\": true\n}\nnf#\n</code></pre>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service_task_auth/#norfab-fastapi-service-auth-tasks-command-shell-reference","title":"NORFAB FastAPI Service Auth Tasks Command Shell Reference","text":"<p>NorFab shell supports these command options for FastAPI <code>auth</code> tasks:</p> <pre><code>nf#man tree fastapi.auth\nroot\n\u2514\u2500\u2500 fastapi:    FastAPI service\n    \u2514\u2500\u2500 auth:    Manage auth tokens\n        \u251c\u2500\u2500 create-token:    Create authentication token\n        \u2502   \u251c\u2500\u2500 timeout:    Job timeout\n        \u2502   \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u2502   \u251c\u2500\u2500 token:    Token string to store, autogenerate if not given\n        \u2502   \u251c\u2500\u2500 *username:    Name of the user to store token for\n        \u2502   \u2514\u2500\u2500 expire:    Seconds before token expire\n        \u251c\u2500\u2500 list-tokens:    Retrieve authentication tokens\n        \u2502   \u251c\u2500\u2500 timeout:    Job timeout\n        \u2502   \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u2502   \u2514\u2500\u2500 username:    Name of the user to list tokens for\n        \u251c\u2500\u2500 delete-token:    Delete existing authentication token\n        \u2502   \u251c\u2500\u2500 timeout:    Job timeout\n        \u2502   \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u2502   \u251c\u2500\u2500 username:    Name of the user to delete tokens for\n        \u2502   \u2514\u2500\u2500 token:    Token string to delete\n        \u2514\u2500\u2500 check-token:    Check if given token valid\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n            \u2514\u2500\u2500 *token:    Token string to check\nnf#\n</code></pre>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service_task_auth/#python-api-reference","title":"Python API Reference","text":"","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service_task_auth/#bearer_token_store","title":"bearer_token_store","text":"<p>Method to store a bearer token in the database.</p> <p>This method stores a bearer token associated with a username in the cache.</p> <p>If an expiration time is not provided, it retrieves the default token TTL from the FastAPI inventory configuration.</p> <p>Parameters:</p> Name Type Description Default <code>username</code> <code>str</code> <p>str - The name of the user to store the token for.</p> required <code>token</code> <code>str</code> <p>str - The token string to store.</p> required <code>expire</code> <code>int</code> <p>int, optional - The number of seconds before the token expires.</p> <code>None</code> <p>Returns:</p> Type Description <code>Result</code> <p>bool - Returns True if the token is successfully stored.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>@Task(fastapi=False, mcp=False)\ndef bearer_token_store(\n    self, job: Job, username: str, token: str, expire: int = None\n) -&gt; Result:\n    \"\"\"\n    Method to store a bearer token in the database.\n\n    This method stores a bearer token associated with a username in the cache.\n\n    If an expiration time is not provided, it retrieves the default token TTL\n    from the FastAPI inventory configuration.\n\n    Args:\n        username: str - The name of the user to store the token for.\n        token: str - The token string to store.\n        expire: int, optional - The number of seconds before the token expires.\n\n    Returns:\n        bool - Returns True if the token is successfully stored.\n    \"\"\"\n    expire = expire or self.fastapi_inventory.get(\"auth_bearer\", {}).get(\n        \"token_ttl\", expire\n    )\n    self.cache.expire()\n    cache_key = f\"bearer_token::{token}\"\n    if cache_key in self.cache:\n        user_token = self.cache.get(cache_key)\n    else:\n        user_token = {\n            \"token\": token,\n            \"username\": username,\n            \"created\": str(datetime.now()),\n        }\n    self.cache.set(cache_key, user_token, expire=expire, tag=username)\n\n    return Result(task=f\"{self.name}:bearer_token_store\", result=True)\n</code></pre>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service_task_auth/#bearer_token_delete","title":"bearer_token_delete","text":"<p>Deletes a bearer token from the cache. This method removes a bearer token from the cache based on either the token itself or the associated username.</p> <p>If a token is provided, it will be removed directly. If a username is provided, all tokens associated with that username will be evicted from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>username</code> <code>str</code> <p>The username associated with the token(s) to be removed. Defaults to None.</p> <code>None</code> <code>token</code> <code>str</code> <p>The bearer token to be removed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>Result</code> <p>True if the operation was successful, otherwise raises an exception.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the token removal from the cache fails.</p> <code>Exception</code> <p>If neither username nor token is provided.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>@Task(fastapi=False, mcp=False)\ndef bearer_token_delete(\n    self, job: Job, username: str = None, token: str = None\n) -&gt; Result:\n    \"\"\"\n    Deletes a bearer token from the cache.\n    This method removes a bearer token from the cache based on either\n    the token itself or the associated username.\n\n    If a token is provided, it will be removed directly. If a username\n    is provided, all tokens associated with that username will be evicted\n    from the cache.\n\n    Args:\n        username (str, optional): The username associated with the token(s) to be removed. Defaults to None.\n        token (str, optional): The bearer token to be removed. Defaults to None.\n\n    Returns:\n        bool: True if the operation was successful, otherwise raises an exception.\n\n    Raises:\n        RuntimeError: If the token removal from the cache fails.\n        Exception: If neither username nor token is provided.\n    \"\"\"\n    self.cache.expire()\n    token_removed_count = 0\n    if token:\n        cache_key = f\"bearer_token::{token}\"\n        if cache_key in self.cache:\n            if self.cache.delete(cache_key, retry=True):\n                token_removed_count = 1\n            else:\n                raise RuntimeError(f\"Failed to remove {username} token from cache\")\n    elif username:\n        token_removed_count = self.cache.evict(tag=username, retry=True)\n    else:\n        raise Exception(\"Cannot delete, either username or token must be provided\")\n\n    log.info(\n        f\"{self.name} removed {token_removed_count} token(s) for user {username}\"\n    )\n\n    return Result(task=f\"{self.name}:bearer_token_delete\", result=True)\n</code></pre>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service_task_auth/#bearer_token_list","title":"bearer_token_list","text":"<p>Retrieves a list of bearer tokens from the cache, optionally filtered by username.</p> <p>Parameters:</p> Name Type Description Default <code>username</code> <code>str</code> <p>The username to filter tokens by. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>Result</code> <p>A list of dictionaries containing token information. Each dictionary contains:</p> <ul> <li>\"username\" (str): The username associated with the token.</li> <li>\"token\" (str): The bearer token.</li> <li>\"age\" (str): The age of the token.</li> <li>\"creation\" (str): The creation time of the token.</li> <li>\"expires\" (str): The expiration time of the token, if available.</li> </ul> <p>If no tokens are found, a list with a single dictionary containing empty strings for all fields is returned.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>@Task(fastapi=False, mcp=False)\ndef bearer_token_list(self, job: Job, username: str = None) -&gt; Result:\n    \"\"\"\n    Retrieves a list of bearer tokens from the cache, optionally filtered by username.\n\n    Args:\n        username (str, optional): The username to filter tokens by. Defaults to None.\n\n    Returns:\n        list: A list of dictionaries containing token information. Each dictionary contains:\n\n            - \"username\" (str): The username associated with the token.\n            - \"token\" (str): The bearer token.\n            - \"age\" (str): The age of the token.\n            - \"creation\" (str): The creation time of the token.\n            - \"expires\" (str): The expiration time of the token, if available.\n\n    If no tokens are found, a list with a single dictionary containing\n    empty strings for all fields is returned.\n    \"\"\"\n\n    self.cache.expire()\n    ret = Result(task=f\"{self.name}:bearer_token_list\", result=[])\n\n    for cache_key in self.cache:\n        token_data, expires, tag = self.cache.get(\n            cache_key, expire_time=True, tag=True\n        )\n        if username and tag != username:\n            continue\n        if expires is not None:\n            expires = datetime.fromtimestamp(expires)\n        creation = datetime.fromisoformat(token_data[\"created\"])\n        age = datetime.now() - creation\n        ret.result.append(\n            {\n                \"username\": token_data[\"username\"],\n                \"token\": token_data[\"token\"],\n                \"age\": str(age),\n                \"creation\": str(creation),\n                \"expires\": str(expires),\n            }\n        )\n\n    # return empty result if no tokens found\n    if not ret.result:\n        ret.result = [\n            {\n                \"username\": \"\",\n                \"token\": \"\",\n                \"age\": \"\",\n                \"creation\": \"\",\n                \"expires\": \"\",\n            }\n        ]\n\n    return ret\n</code></pre>","tags":["FastAPI"]},{"location":"workers/fastapi/services_fastapi_service_task_auth/#bearer_token_check","title":"bearer_token_check","text":"<p>Checks if the provided bearer token is present in the cache and still active.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The bearer token to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>Result</code> <p>True if the token is found in the cache, False otherwise.</p> Source code in <code>norfab\\workers\\fastapi_worker.py</code> <pre><code>@Task(fastapi=False, mcp=False)\ndef bearer_token_check(self, token: str, job: Job) -&gt; Result:\n    \"\"\"\n    Checks if the provided bearer token is present in the cache and still active.\n\n    Args:\n        token (str): The bearer token to check.\n\n    Returns:\n        bool: True if the token is found in the cache, False otherwise.\n    \"\"\"\n    self.cache.expire()\n    cache_key = f\"bearer_token::{token}\"\n    return Result(\n        task=f\"{self.name}:bearer_token_check\", result=cache_key in self.cache\n    )\n</code></pre>","tags":["FastAPI"]},{"location":"workers/fastmcp/api_reference_workers_fastmcp_worker/","title":"FastMCP Worker","text":""},{"location":"workers/fastmcp/api_reference_workers_fastmcp_worker/#norfab.workers.fastmcp_worker.FastMCPWorker","title":"<code>FastMCPWorker(inventory: str, broker: str, worker_name: str, exit_event=None, init_done_event=None, log_level: str = None, log_queue: object = None)</code>","text":"<p>               Bases: <code>NFPWorker</code></p> Source code in <code>norfab\\workers\\fastmcp_worker.py</code> <pre><code>def __init__(\n    self,\n    inventory: str,\n    broker: str,\n    worker_name: str,\n    exit_event=None,\n    init_done_event=None,\n    log_level: str = None,\n    log_queue: object = None,\n):\n    super().__init__(\n        inventory, broker, SERVICE, worker_name, exit_event, log_level, log_queue\n    )\n    self.init_done_event = init_done_event\n    self.exit_event = exit_event\n    self.norfab_services_tasks = {}\n    self.mcp_server_name = \"NorFab MCP Server\"\n\n    # get inventory from broker\n    self.fastmcp_inventory = self.load_inventory()\n    self.fastmcp_inventory.setdefault(\"host\", \"0.0.0.0\")\n    self.fastmcp_inventory.setdefault(\"port\", 8001)\n\n    # instantiate cache\n    self.cache_dir = os.path.join(self.base_dir, \"cache\")\n    os.makedirs(self.cache_dir, exist_ok=True)\n    self.cache = self.get_diskcache()\n    self.cache.expire()\n\n    # start FastMCP server\n    self.fastmcp_start()\n\n    self.service_tasks_discovery_thread = threading.Thread(\n        target=service_tasks_discovery, args=(self,)\n    )\n    self.service_tasks_discovery_thread.start()\n\n    self.init_done_event.set()\n</code></pre>"},{"location":"workers/fastmcp/api_reference_workers_fastmcp_worker/#norfab.workers.fastmcp_worker.FastMCPWorker.get_diskcache","title":"<code>get_diskcache() -&gt; FanoutCache</code>","text":"<p>Initializes and returns a FanoutCache object.</p> <p>The FanoutCache is configured with the following parameters:</p> <ul> <li>directory: The directory where the cache will be stored.</li> <li>shards: Number of shards to use for the cache.</li> <li>timeout: Timeout for cache operations in seconds.</li> <li>size_limit: Maximum size of the cache in bytes.</li> </ul> <p>Returns:</p> Name Type Description <code>FanoutCache</code> <code>FanoutCache</code> <p>An instance of FanoutCache configured with the specified parameters.</p> Source code in <code>norfab\\workers\\fastmcp_worker.py</code> <pre><code>def get_diskcache(self) -&gt; FanoutCache:\n    \"\"\"\n    Initializes and returns a FanoutCache object.\n\n    The FanoutCache is configured with the following parameters:\n\n    - directory: The directory where the cache will be stored.\n    - shards: Number of shards to use for the cache.\n    - timeout: Timeout for cache operations in seconds.\n    - size_limit: Maximum size of the cache in bytes.\n\n    Returns:\n        FanoutCache: An instance of FanoutCache configured with the specified parameters.\n    \"\"\"\n    return FanoutCache(\n        directory=self.cache_dir,\n        shards=4,\n        timeout=1,  # 1 second\n        size_limit=1073741824,  #  1 GigaByte\n    )\n</code></pre>"},{"location":"workers/fastmcp/api_reference_workers_fastmcp_worker/#norfab.workers.fastmcp_worker.FastMCPWorker.get_version","title":"<code>get_version() -&gt; Result</code>","text":"<p>Retrieves version information for key libraries and the current Python environment.</p> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing a dictionary with the version numbers of 'norfab', 'mcp', 'uvicorn', 'pydantic', the Python version, and the platform. If a package is not found, its version will be an empty string.</p> Source code in <code>norfab\\workers\\fastmcp_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_version(self) -&gt; Result:\n    \"\"\"\n    Retrieves version information for key libraries and the current Python environment.\n\n    Returns:\n        Result: An object containing a dictionary with the version numbers of\n            'norfab', 'mcp', 'uvicorn', 'pydantic', the Python version, and the platform.\n            If a package is not found, its version will be an empty string.\n    \"\"\"\n\n    libs = {\n        \"norfab\": \"\",\n        \"uvicorn\": \"\",\n        \"pydantic\": \"\",\n        \"mcp\": \"\",\n        \"python\": sys.version.split(\" \")[0],\n        \"platform\": sys.platform,\n    }\n    # get version of packages installed\n    for pkg in libs.keys():\n        try:\n            libs[pkg] = importlib.metadata.version(pkg)\n        except importlib.metadata.PackageNotFoundError:\n            pass\n\n    return Result(task=f\"{self.name}:get_version\", result=libs)\n</code></pre>"},{"location":"workers/fastmcp/api_reference_workers_fastmcp_worker/#norfab.workers.fastmcp_worker.FastMCPWorker.get_inventory","title":"<code>get_inventory() -&gt; Result</code>","text":"<p>Retrieves the current inventory from the FastMCP worker.</p> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing a copy of the worker's inventory and the task name.</p> Source code in <code>norfab\\workers\\fastmcp_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_inventory(self) -&gt; Result:\n    \"\"\"\n    Retrieves the current inventory from the FastMCP worker.\n\n    Returns:\n        Result: An object containing a copy of the worker's inventory and the task name.\n    \"\"\"\n    return Result(\n        result={**self.fastmcp_inventory},\n        task=f\"{self.name}:get_inventory\",\n    )\n</code></pre>"},{"location":"workers/fastmcp/api_reference_workers_fastmcp_worker/#norfab.workers.fastmcp_worker.FastMCPWorker.get_tools","title":"<code>get_tools(brief: bool = False, service: str = 'all', name: str = '*') -&gt; Result</code>","text":"<p>Retrieve tools from the available norfab services, optionally filtered by service name and tool name pattern.</p> <p>Parameters:</p> Name Type Description Default <code>brief</code> <code>bool</code> <p>If True, returns a list of tool names. If False, returns a dictionary with tool details.</p> <code>False</code> <code>service</code> <code>str</code> <p>The name of the service to filter tools by. Use \"all\" to include all services.</p> <code>'all'</code> <code>name</code> <code>str</code> <p>A glob pattern to match tool names.</p> <code>'*'</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the filtered tools. If brief is True, result is a list of tool names. Otherwise, result is a dictionary mapping tool names to their details.</p> Source code in <code>norfab\\workers\\fastmcp_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_tools(\n    self, brief: bool = False, service: str = \"all\", name: str = \"*\"\n) -&gt; Result:\n    \"\"\"\n    Retrieve tools from the available norfab services, optionally filtered by service\n    name and tool name pattern.\n\n    Args:\n        brief (bool, optional): If True, returns a list of tool names. If False,\n            returns a dictionary with tool details.\n        service (str, optional): The name of the service to filter tools by.\n            Use \"all\" to include all services.\n        name (str, optional): A glob pattern to match tool names.\n\n    Returns:\n        Result: An object containing the filtered tools. If brief is True, result\n            is a list of tool names. Otherwise, result is a dictionary mapping tool\n            names to their details.\n    \"\"\"\n    ret = Result(\n        result={},\n        task=f\"{self.name}:get_tools\",\n    )\n    if brief:\n        ret.result = []\n        for service_name, tasks in self.norfab_services_tasks.items():\n            if service == \"all\" or service_name == service:\n                for tool_name, tool_data in tasks.items():\n                    if fnmatch(tool_name, name):\n                        ret.result.append(tool_name)\n    else:\n        for service_name, tasks in self.norfab_services_tasks.items():\n            if service == \"all\" or service_name == service:\n                for tool_name, tool_data in tasks.items():\n                    if fnmatch(tool_name, name):\n                        ret.result[tool_name] = tool_data[\"tool\"].model_dump()\n\n    return ret\n</code></pre>"},{"location":"workers/fastmcp/api_reference_workers_fastmcp_worker/#norfab.workers.fastmcp_worker.FastMCPWorker.discover","title":"<code>discover(job, service: str = 'all', progress: bool = True) -&gt; Result</code>","text":"<p>Discovers available services tasks and auto-generate tools for them.</p> <p>Parameters:</p> Name Type Description Default <code>service</code> <code>str</code> <p>The name of the service to discover. Defaults to \"all\".</p> <code>'all'</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the discovery results for the specified service.</p> Source code in <code>norfab\\workers\\fastmcp_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef discover(self, job, service: str = \"all\", progress: bool = True) -&gt; Result:\n    \"\"\"\n    Discovers available services tasks and auto-generate tools for them.\n\n    Args:\n        service (str, optional): The name of the service to discover. Defaults to \"all\".\n\n    Returns:\n        Result: An object containing the discovery results for the specified service.\n    \"\"\"\n    job.event(\"Discovering NorFab services tasks\")\n    ret = Result(task=f\"{self.name}:discover\")\n    ret.result = service_tasks_discovery(self, cycles=1, discover_service=service)\n\n    return ret\n</code></pre>"},{"location":"workers/fastmcp/api_reference_workers_fastmcp_worker/#norfab.workers.fastmcp_worker.FastMCPWorker.get_status","title":"<code>get_status() -&gt; Result</code>","text":"<p>Retrieves the current status of the application, including its name, URL, and the count of available tools.</p> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing a dictionary with the application's name, URL, and the number of tools, as well as the task identifier.</p> Source code in <code>norfab\\workers\\fastmcp_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_status(self) -&gt; Result:\n    \"\"\"\n    Retrieves the current status of the application, including its name,\n    URL, and the count of available tools.\n\n    Returns:\n        Result: An object containing a dictionary with the application's name,\n            URL, and the number of tools, as well as the task identifier.\n    \"\"\"\n    tools = self.get_tools(brief=True).result\n\n    return Result(\n        result={\n            \"name\": self.app.name,\n            \"url\": f\"http://{self.fastmcp_inventory['host']}:{self.fastmcp_inventory['port']}/mcp/\",\n            \"tools_count\": len(tools),\n        },\n        task=f\"{self.name}:get_status\",\n    )\n</code></pre>"},{"location":"workers/fastmcp/api_reference_workers_fastmcp_worker/#norfab.workers.fastmcp_worker.FastMCPWorker.fastmcp_start","title":"<code>fastmcp_start()</code>","text":"<p>Starts the FastMCP server for the NorFab MCP application.</p> <p>This method initializes a FastMCP application instance with the specified host and port from <code>self.fastmcp_inventory</code>.</p> <p>It registers two MCP server endpoints:</p> <ul> <li><code>list_tools</code>: Asynchronously returns a list of available     tools by aggregating all tools from <code>self.norfab_services_tasks</code>.</li> <li><code>call_tool</code>: Asynchronously handles tool invocation requests by     parsing the tool name, extracting the corresponding service and     task, and running the job using <code>self.client.run_job</code>.</li> </ul> <p>The FastMCP server is started in a separate thread using the \"streamable-http\" transport.</p> Source code in <code>norfab\\workers\\fastmcp_worker.py</code> <pre><code>def fastmcp_start(self):\n    \"\"\"\n    Starts the FastMCP server for the NorFab MCP application.\n\n    This method initializes a FastMCP application instance with\n    the specified host and port from `self.fastmcp_inventory`.\n\n    It registers two MCP server endpoints:\n\n      - `list_tools`: Asynchronously returns a list of available\n        tools by aggregating all tools from `self.norfab_services_tasks`.\n      - `call_tool`: Asynchronously handles tool invocation requests by\n        parsing the tool name, extracting the corresponding service and\n        task, and running the job using `self.client.run_job`.\n\n    The FastMCP server is started in a separate thread using the\n    \"streamable-http\" transport.\n    \"\"\"\n    self.app = FastMCP(\n        self.mcp_server_name,\n        port=self.fastmcp_inventory[\"port\"],\n        host=self.fastmcp_inventory[\"host\"],\n    )\n\n    @self.app._mcp_server.list_tools()\n    async def list_tools() -&gt; list[types.Tool]:\n        ret = []\n        for service, tasks in self.norfab_services_tasks.items():\n            for tool_name, tool_data in tasks.items():\n                ret.append(tool_data[\"tool\"])  # types.Tool object\n        return ret\n\n    @self.app._mcp_server.call_tool()\n    async def call_tool(name: str, arguments: dict[str, Any]) -&gt; dict[str, Any]:\n        log.error(f\"Calling tool '{name}' with arguments: '{arguments}'\")\n\n        # form NorFab service and task names\n        service, tool_name = name.split(\"__\")\n        service = service[8:]\n        tool_name = tool_name[5:]\n        task_name = self.norfab_services_tasks[service][name][\"task\"][\"name\"]\n\n        log.error(\n            f\"Calling NorFab service '{service}' task '{task_name}' with arguments: '{arguments}'\"\n        )\n\n        res = self.client.run_job(\n            service=service,\n            task=task_name,\n            kwargs=arguments,\n            workers=\"all\",\n        )\n\n        return {\"result\": res}\n\n    self.app_server_thread = threading.Thread(\n        target=self.app.run, kwargs={\"transport\": \"streamable-http\"}\n    )\n    self.app_server_thread.start()\n\n    log.info(\n        f\"{self.name} - MCP server started, serving FastMCP app at \"\n        f\"http://{self.fastmcp_inventory['host']}:{self.fastmcp_inventory['port']}/mcp/\"\n    )\n</code></pre>"},{"location":"workers/fastmcp/api_reference_workers_fastmcp_worker/#norfab.workers.fastmcp_worker.service_tasks_discovery","title":"<code>service_tasks_discovery(worker: Any, cycles: int = 5, discover_service: str = 'all') -&gt; None</code>","text":"<p>Discovers available tasks from NorFab services and registers them as tools for the worker. This function periodically queries the broker for available services and their tasks, and registers each discovered task as a tool in the worker's <code>norfab_services_tasks</code> dictionary. It continues this process for a specified number of cycles or until the worker's exit event is set.</p> <p>Parameters:</p> Name Type Description Default <code>worker</code> <code>Any</code> <p>The worker instance responsible for managing service tasks and tools.</p> required <code>cycles</code> <code>int</code> <p>The number of discovery cycles to perform.</p> <code>5</code> <code>discover_service</code> <code>str</code> <p>The name of a specific service to discover tasks from. If set to \"all\", tasks from all services are discovered. Defaults to \"all\".</p> <code>'all'</code> Source code in <code>norfab\\workers\\fastmcp_worker.py</code> <pre><code>def service_tasks_discovery(\n    worker: Any, cycles: int = 5, discover_service: str = \"all\"\n) -&gt; None:\n    \"\"\"\n    Discovers available tasks from NorFab services and registers them\n    as tools for the worker. This function periodically queries the\n    broker for available services and their tasks, and registers each\n    discovered task as a tool in the worker's `norfab_services_tasks`\n    dictionary. It continues this process for a specified number of\n    cycles or until the worker's exit event is set.\n\n    Args:\n        worker: The worker instance responsible for managing service\n            tasks and tools.\n        cycles (int, optional): The number of discovery cycles to perform.\n        discover_service (str, optional): The name of a specific service\n            to discover tasks from. If set to \"all\", tasks from all services\n            are discovered. Defaults to \"all\".\n    \"\"\"\n    result = {}\n    while not worker.exit_event.is_set() and cycles &gt; 0:\n        tasks = []\n        services = []\n        try:\n            # get a list of workers and construct a list of services\n            services = worker.client.get(\"mmi.service.broker\", \"show_workers\")\n            services = [\n                s[\"service\"]\n                for s in services[\"results\"]\n                if discover_service == \"all\" or s[\"service\"] == discover_service\n            ]\n\n            # retrieve NorFab services and their tasks\n            for service in services:\n                # skip already discovered services\n                if service in result:\n                    continue\n                service_tasks = worker.client.run_job(\n                    service=service,\n                    task=\"list_tasks\",\n                    workers=\"any\",\n                    timeout=3,\n                )\n                # skip if client request timed out\n                if service_tasks is None:\n                    continue\n                for wres in service_tasks.values():\n                    for t in wres[\"result\"]:\n                        t[\"service\"] = service\n                    tasks.extend(wres[\"result\"])\n\n            # create tools for discovered tasks\n            for task in tasks:\n                # skip task tool creation if set to false\n                if task[\"mcp\"] is False:\n                    continue\n                # save service to results\n                result.setdefault(task[\"service\"], {})\n                # continue with creating tool for task\n                task_tool = {\n                    \"name\": task[\"name\"],\n                    \"description\": task[\"description\"],\n                    \"inputSchema\": task[\"inputSchema\"],\n                    \"outputSchema\": task[\"outputSchema\"],\n                    **task[\"mcp\"],\n                }\n                task_tool[\"name\"] = (\n                    f\"service_{task['service']}__task_{task_tool['name']}\"\n                )\n                # skip already discovered tasks\n                if task_tool[\"name\"] in result[task[\"service\"]]:\n                    continue\n                # save discovered task to return results\n                result[task[\"service\"]][task_tool[\"name\"]] = {\n                    \"tool\": types.Tool(**task_tool),\n                    \"task\": task,\n                }\n            # save tools to worker tasks dictionary\n            worker.norfab_services_tasks.update(result)\n        except Exception as e:\n            log.exception(f\"Failed to discover services tasks, error: {e}\")\n\n        cycles -= 1\n        time.sleep(5)\n\n    return result\n</code></pre>"},{"location":"workers/filesharing/api_reference_workers_filesharing_worker/","title":"Filesharing Worker","text":""},{"location":"workers/filesharing/api_reference_workers_filesharing_worker/#norfab.workers.filesharing_worker.filesharing_worker.FileSharingWorker","title":"<code>FileSharingWorker(inventory: Any, broker: str, worker_name: str, exit_event: Any = None, init_done_event: Any = None, log_level: str = 'WARNING', log_queue: object = None)</code>","text":"<p>               Bases: <code>NFPWorker</code></p> Source code in <code>norfab\\workers\\filesharing_worker\\filesharing_worker.py</code> <pre><code>def __init__(\n    self,\n    inventory: Any,\n    broker: str,\n    worker_name: str,\n    exit_event: Any = None,\n    init_done_event: Any = None,\n    log_level: str = \"WARNING\",\n    log_queue: object = None,\n):\n    super().__init__(\n        inventory, broker, SERVICE, worker_name, exit_event, log_level, log_queue\n    )\n    self.init_done_event = init_done_event\n\n    # get inventory from broker\n    self.filesharing_inventory = self.load_inventory()\n    self.base_dir = self.filesharing_inventory.get(\"base_dir\")\n\n    self.init_done_event.set()\n    log.debug(f\"{self.name} - Started, {self.filesharing_inventory}\")\n</code></pre>"},{"location":"workers/filesharing/api_reference_workers_filesharing_worker/#norfab.workers.filesharing_worker.filesharing_worker.FileSharingWorker.list_files","title":"<code>list_files(url: str) -&gt; Result</code>","text":"<p>List files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL path starting with 'nf://' to list files from</p> required <p>Returns:</p> Type Description <code>Result</code> <p>Result containing list of files or error message</p> Source code in <code>norfab\\workers\\filesharing_worker\\filesharing_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef list_files(self, url: str) -&gt; Result:\n    \"\"\"\n    List files in a directory.\n\n    Args:\n        url: URL path starting with 'nf://' to list files from\n\n    Returns:\n        Result containing list of files or error message\n    \"\"\"\n    ret = Result(result=None)\n    try:\n        full_path = self._safe_path(url)\n    except ValueError as e:\n        ret.failed = True\n        ret.errors = [str(e)]\n        return ret\n\n    if os.path.exists(full_path) and os.path.isdir(full_path):\n        ret.result = os.listdir(full_path)\n    else:\n        ret.errors = [\"Directory Not Found\"]\n        ret.failed = True\n    return ret\n</code></pre>"},{"location":"workers/filesharing/api_reference_workers_filesharing_worker/#norfab.workers.filesharing_worker.filesharing_worker.FileSharingWorker.file_details","title":"<code>file_details(url: str) -&gt; Result</code>","text":"<p>Get file details including md5 hash, size, and existence.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL path starting with 'nf://' to get file details</p> required <p>Returns:</p> Type Description <code>Result</code> <p>Result containing md5hash, size_bytes, and exists fields</p> Source code in <code>norfab\\workers\\filesharing_worker\\filesharing_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef file_details(self, url: str) -&gt; Result:\n    \"\"\"\n    Get file details including md5 hash, size, and existence.\n\n    Args:\n        url: URL path starting with 'nf://' to get file details\n\n    Returns:\n        Result containing md5hash, size_bytes, and exists fields\n    \"\"\"\n    ret = Result(result={\"md5hash\": None, \"size_bytes\": None, \"exists\": False})\n    try:\n        full_path = self._safe_path(url)\n    except ValueError as e:\n        ret.failed = True\n        ret.errors = [str(e)]\n        return ret\n    exists = os.path.exists(full_path) and os.path.isfile(full_path)\n\n    # calculate md5 hash\n    md5hash = None\n    if exists:\n        with open(full_path, \"rb\") as f:\n            file_hash = hashlib.md5()\n            chunk = f.read(8192)\n            while chunk:\n                file_hash.update(chunk)\n                chunk = f.read(8192)\n        md5hash = file_hash.hexdigest()\n        size = os.path.getsize(full_path)\n        ret.result = {\n            \"md5hash\": md5hash,\n            \"size_bytes\": size,\n            \"exists\": exists,\n        }\n    else:\n        ret.failed = True\n        ret.errors = [f\"'{url}' file not found\"]\n\n    return ret\n</code></pre>"},{"location":"workers/filesharing/api_reference_workers_filesharing_worker/#norfab.workers.filesharing_worker.filesharing_worker.FileSharingWorker.walk","title":"<code>walk(url: str) -&gt; Result</code>","text":"<p>Recursively list all files from all subdirectories.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL path starting with 'nf://' to walk directories</p> required <p>Returns:</p> Type Description <code>Result</code> <p>Result containing list of all file paths or error message</p> Source code in <code>norfab\\workers\\filesharing_worker\\filesharing_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef walk(self, url: str) -&gt; Result:\n    \"\"\"\n    Recursively list all files from all subdirectories.\n\n    Args:\n        url: URL path starting with 'nf://' to walk directories\n\n    Returns:\n        Result containing list of all file paths or error message\n    \"\"\"\n    ret = Result(result=None)\n    try:\n        full_path = self._safe_path(url)\n    except ValueError as e:\n        ret.failed = True\n        ret.errors = [str(e)]\n        return ret\n\n    if os.path.exists(full_path) and os.path.isdir(full_path):\n        files_list = []\n        for root, dirs, files in os.walk(full_path):\n            # skip path containing folders like __folders__\n            if root.count(\"__\") &gt;= 2:\n                continue\n            root = root.replace(self.base_dir, \"\")\n            root = root.lstrip(\"\\\\\")\n            root = root.replace(\"\\\\\", \"/\")\n            for file in files:\n                # skip hidden/system files\n                if file.startswith(\".\"):\n                    continue\n                if root:\n                    files_list.append(f\"nf://{root}/{file}\")\n                else:\n                    files_list.append(f\"nf://{file}\")\n        ret.result = files_list\n    else:\n        ret.failed = True\n        ret.errors = [\"Directory Not Found\"]\n    return ret\n</code></pre>"},{"location":"workers/filesharing/api_reference_workers_filesharing_worker/#norfab.workers.filesharing_worker.filesharing_worker.FileSharingWorker.fetch_file","title":"<code>fetch_file(job: Job, url: str, chunk_size: int = 256000, offset: int = 0, chunk_timeout: int = 5) -&gt; Result</code>","text":"<p>Fetch a file in chunks with offset support.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL path starting with 'nf://' to fetch file from</p> required <code>chunk_size</code> <code>int</code> <p>Size of chunk to read in bytes (default: 256KB)</p> <code>256000</code> <code>offset</code> <code>int</code> <p>Number of bytes to offset (default: 0)</p> <code>0</code> <p>Returns:</p> Type Description <code>Result</code> <p>Result containing file chunk bytes or error message</p> Source code in <code>norfab\\workers\\filesharing_worker\\filesharing_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef fetch_file(\n    self,\n    job: Job,\n    url: str,\n    chunk_size: int = 256000,\n    offset: int = 0,\n    chunk_timeout: int = 5,\n) -&gt; Result:\n    \"\"\"\n    Fetch a file in chunks with offset support.\n\n    Args:\n        url: URL path starting with 'nf://' to fetch file from\n        chunk_size: Size of chunk to read in bytes (default: 256KB)\n        offset: Number of bytes to offset (default: 0)\n\n    Returns:\n        Result containing file chunk bytes or error message\n    \"\"\"\n    ret = Result(result=None)\n    try:\n        full_path = self._safe_path(url)\n    except ValueError as e:\n        ret.failed = True\n        ret.errors = [str(e)]\n        return ret\n\n    if os.path.exists(full_path):\n        size = os.path.getsize(full_path)\n        with open(full_path, \"rb\") as f:\n            while True:\n                f.seek(offset, os.SEEK_SET)\n                chunk = f.read(chunk_size)\n                if chunk:\n                    job.stream(chunk)\n                if f.tell() &gt;= size:\n                    break\n                client_response = job.wait_client_input(timeout=chunk_timeout)\n                if not client_response:\n                    raise RuntimeError(\n                        f\"{self.name}:fetch_file - {chunk_timeout}s chunk timeout reached before received next chunk request from client\"\n                    )\n                offset = client_response[\"offset\"]\n\n        ret.result = True\n    else:\n        ret.failed = True\n        ret.errors = [f\"'{url}' file not found\"]\n\n    return ret\n</code></pre>"},{"location":"workers/filesharing/services_filesharing_service/","title":"NORFAB File Service (File Sharing)","text":"<p>NORFAB includes a built-in File Sharing service (<code>service=\"filesharing\"</code>) that lets clients and workers access files by an <code>nf://...</code> URL.</p> <p>Common uses:</p> <ul> <li>Store templates, playbooks, golden configs, and other \"assets\" next to your inventory.</li> <li>Let workers download an input file (for example: Nornir <code>file_copy</code> can accept <code>nf://...</code> sources).</li> <li>Browse what files are available and fetch them locally.</li> </ul> <p>For protocol-level streaming details, see development/file_streaming_fetch_file.md.</p>","tags":["services","nfcli"]},{"location":"workers/filesharing/services_filesharing_service/#url-format-and-path-rules","title":"URL format and path rules","text":"<p>File Sharing uses URLs in the form:</p> <ul> <li><code>nf://&lt;path&gt;</code></li> </ul> <p>Where <code>&lt;path&gt;</code> is resolved relative to the File Sharing worker <code>base_dir</code>.</p> <p>Examples (assuming <code>base_dir</code> is your inventory folder):</p> <ul> <li><code>nf://filesharing/test_file_1.txt</code> \u2192 <code>&lt;base_dir&gt;/filesharing/test_file_1.txt</code></li> <li><code>nf://cli/commands.txt</code> \u2192 <code>&lt;base_dir&gt;/cli/commands.txt</code></li> </ul> <p>The service rejects:</p> <ul> <li>Non-<code>nf://</code> URLs</li> <li>Absolute paths</li> <li>Directory traversal (paths that would escape <code>base_dir</code>)</li> </ul>","tags":["services","nfcli"]},{"location":"workers/filesharing/services_filesharing_service/#what-the-service-provides","title":"What the service provides","text":"<p>The File Sharing worker exposes these tasks:</p> <ul> <li>list_files \u2014 list directory entries (non-recursive)</li> <li>walk \u2014 recursively list files under a directory (returns a list of <code>nf://...</code> file URLs)</li> <li>file_details \u2014 returns file metadata including existence, size in bytes, and MD5 hash</li> <li>fetch_file \u2014 streams the file to the client with chunking and offset support</li> </ul> <p>For detailed information about each task, see the individual task documentation pages linked above.</p>","tags":["services","nfcli"]},{"location":"workers/filesharing/services_filesharing_service_inventory/","title":"Filesharing Worker Inventory","text":"<p>When you start NORFAB via <code>NorFab</code> (NFAPI) with the broker enabled, NFAPI injects a filesharing worker:</p> <ul> <li>Worker name: <code>filesharing-worker-1</code></li> <li>Service: <code>filesharing</code></li> <li>Base directory: <code>base_dir = &lt;inventory.base_dir&gt;</code></li> </ul> <p>That means you usually do not need to add a File Sharing worker manually.</p> <p>If you do want to define it explicitly (or point it at a different directory), add it to inventory:</p> inventory.yaml<pre><code>workers:\n  filesharing-worker-1:\n    - service: filesharing\n      base_dir: ./\n\ntopology:\n  broker: true\n  workers:\n    - filesharing-worker-1\n</code></pre>"},{"location":"workers/filesharing/services_filesharing_service_tasks_fetch_file/","title":"Filesharing Service Fetch File Task","text":"<p>task api name: <code>fetch_file</code></p> <p>The <code>fetch_file</code> task streams a file from the File Sharing worker to the client in chunks with offset support. This enables efficient downloading of large files with resume capability. The task is typically called through the client helper method <code>NFPClient.fetch_file()</code> which handles the streaming protocol, caching, and local file management automatically.</p>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_fetch_file/#using-it-from-python","title":"Using it from Python","text":"<pre><code>from norfab.core.nfapi import NorFab\n\nwith NorFab(inventory=\"./inventory.yaml\") as nf:\n    client = nf.make_client()\n\n    # Download file (recommended way - uses client helper)\n    ret = client.fetch_file(url=\"nf://filesharing/test_file_1.txt\")\n    local_path = ret[\"content\"]\n    print(local_path)\n\n    # Download and read content as text\n    ret = client.fetch_file(url=\"nf://filesharing/test_file_1.txt\", read=True)\n    print(ret[\"content\"])  # text content\n\n    # Direct task invocation (not recommended - use client helper instead)\n    reply = client.run_job(\n        service=\"filesharing\",\n        task=\"fetch_file\",\n        workers=\"any\",\n        kwargs={\n            \"url\": \"nf://filesharing/test_file_1.txt\",\n            \"chunk_size\": 256000,\n            \"offset\": 0,\n        },\n    )\n    print(reply)\n</code></pre>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_fetch_file/#using-it-from-nfcli","title":"Using it from <code>nfcli</code>","text":"<p>NORFAB CLI exposes File Sharing under the <code>file</code> command group.</p> <pre><code>nf#man tree file.copy\nroot\n\u2514\u2500\u2500 file:    File sharing service\n    \u2514\u2500\u2500 copy:    Copy files\n        \u251c\u2500\u2500 url:    File location, default 'nf://'\n        \u251c\u2500\u2500 destination:    File location to save downloaded content\n        \u2514\u2500\u2500 read:    Print file content, default 'False'\n\nnf#\n</code></pre>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_fetch_file/#api-reference","title":"API Reference","text":"<p>Fetch a file in chunks with offset support.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL path starting with 'nf://' to fetch file from</p> required <code>chunk_size</code> <code>int</code> <p>Size of chunk to read in bytes (default: 256KB)</p> <code>256000</code> <code>offset</code> <code>int</code> <p>Number of bytes to offset (default: 0)</p> <code>0</code> <p>Returns:</p> Type Description <code>Result</code> <p>Result containing file chunk bytes or error message</p> Source code in <code>norfab\\workers\\filesharing_worker\\filesharing_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef fetch_file(\n    self,\n    job: Job,\n    url: str,\n    chunk_size: int = 256000,\n    offset: int = 0,\n    chunk_timeout: int = 5,\n) -&gt; Result:\n    \"\"\"\n    Fetch a file in chunks with offset support.\n\n    Args:\n        url: URL path starting with 'nf://' to fetch file from\n        chunk_size: Size of chunk to read in bytes (default: 256KB)\n        offset: Number of bytes to offset (default: 0)\n\n    Returns:\n        Result containing file chunk bytes or error message\n    \"\"\"\n    ret = Result(result=None)\n    try:\n        full_path = self._safe_path(url)\n    except ValueError as e:\n        ret.failed = True\n        ret.errors = [str(e)]\n        return ret\n\n    if os.path.exists(full_path):\n        size = os.path.getsize(full_path)\n        with open(full_path, \"rb\") as f:\n            while True:\n                f.seek(offset, os.SEEK_SET)\n                chunk = f.read(chunk_size)\n                if chunk:\n                    job.stream(chunk)\n                if f.tell() &gt;= size:\n                    break\n                client_response = job.wait_client_input(timeout=chunk_timeout)\n                if not client_response:\n                    raise RuntimeError(\n                        f\"{self.name}:fetch_file - {chunk_timeout}s chunk timeout reached before received next chunk request from client\"\n                    )\n                offset = client_response[\"offset\"]\n\n        ret.result = True\n    else:\n        ret.failed = True\n        ret.errors = [f\"'{url}' file not found\"]\n\n    return ret\n</code></pre>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_file_details/","title":"Filesharing Service File Details Task","text":"<p>task api name: <code>file_details</code></p> <p>The <code>file_details</code> task returns metadata about a file including its existence status, size in bytes, and MD5 hash. This is useful for verifying file integrity, checking if a file exists before downloading, or comparing local and remote file versions without transferring the entire file.</p>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_file_details/#using-it-from-python","title":"Using it from Python","text":"<pre><code>from norfab.core.nfapi import NorFab\n\nwith NorFab(inventory=\"./inventory.yaml\") as nf:\n    client = nf.make_client()\n\n    reply = client.run_job(\n        service=\"filesharing\",\n        task=\"file_details\",\n        workers=\"any\",\n        kwargs={\"url\": \"nf://filesharing/test_file_1.txt\"},\n    )\n    print(reply)\n</code></pre>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_file_details/#using-it-from-nfcli","title":"Using it from <code>nfcli</code>","text":"<p>NORFAB CLI exposes File Sharing under the <code>file</code> command group.</p> <pre><code>nf#man tree file.details\nroot\n\u2514\u2500\u2500 file:    File sharing service\n    \u2514\u2500\u2500 details:    Show file details\n        \u2514\u2500\u2500 url:    File location, default 'nf://'\n\nnf#\n</code></pre>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_file_details/#api-reference","title":"API Reference","text":"<p>Get file details including md5 hash, size, and existence.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL path starting with 'nf://' to get file details</p> required <p>Returns:</p> Type Description <code>Result</code> <p>Result containing md5hash, size_bytes, and exists fields</p> Source code in <code>norfab\\workers\\filesharing_worker\\filesharing_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef file_details(self, url: str) -&gt; Result:\n    \"\"\"\n    Get file details including md5 hash, size, and existence.\n\n    Args:\n        url: URL path starting with 'nf://' to get file details\n\n    Returns:\n        Result containing md5hash, size_bytes, and exists fields\n    \"\"\"\n    ret = Result(result={\"md5hash\": None, \"size_bytes\": None, \"exists\": False})\n    try:\n        full_path = self._safe_path(url)\n    except ValueError as e:\n        ret.failed = True\n        ret.errors = [str(e)]\n        return ret\n    exists = os.path.exists(full_path) and os.path.isfile(full_path)\n\n    # calculate md5 hash\n    md5hash = None\n    if exists:\n        with open(full_path, \"rb\") as f:\n            file_hash = hashlib.md5()\n            chunk = f.read(8192)\n            while chunk:\n                file_hash.update(chunk)\n                chunk = f.read(8192)\n        md5hash = file_hash.hexdigest()\n        size = os.path.getsize(full_path)\n        ret.result = {\n            \"md5hash\": md5hash,\n            \"size_bytes\": size,\n            \"exists\": exists,\n        }\n    else:\n        ret.failed = True\n        ret.errors = [f\"'{url}' file not found\"]\n\n    return ret\n</code></pre>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_list_files/","title":"Filesharing Service List FIles Task","text":"<p>task api name: <code>list_files</code></p> <p>The <code>list_files</code> task lists the contents of a directory at the specified URL path in a non-recursive manner. This task returns only the immediate files and subdirectories within the given path, without descending into nested directories. It's useful for browsing the structure of your file sharing repository one level at a time.</p>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_list_files/#using-it-from-python","title":"Using it from Python","text":"<pre><code>from norfab.core.nfapi import NorFab\n\nwith NorFab(inventory=\"./inventory.yaml\") as nf:\n    client = nf.make_client()\n\n    reply = client.run_job(\n        service=\"filesharing\",\n        task=\"list_files\",\n        workers=\"any\",\n        kwargs={\"url\": \"nf://\"},\n    )\n    print(reply)\n</code></pre>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_list_files/#using-it-from-nfcli","title":"Using it from <code>nfcli</code>","text":"<p>NORFAB CLI exposes File Sharing under the <code>file</code> command group.</p> <pre><code>nf#man tree file.list\nroot\n\u2514\u2500\u2500 file:    File sharing service\n    \u2514\u2500\u2500 list:    List files\n        \u2514\u2500\u2500 url:    Directory to list content for, default 'nf://'\n\nnf#\n</code></pre>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_list_files/#api-reference","title":"API Reference","text":"<p>List files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL path starting with 'nf://' to list files from</p> required <p>Returns:</p> Type Description <code>Result</code> <p>Result containing list of files or error message</p> Source code in <code>norfab\\workers\\filesharing_worker\\filesharing_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef list_files(self, url: str) -&gt; Result:\n    \"\"\"\n    List files in a directory.\n\n    Args:\n        url: URL path starting with 'nf://' to list files from\n\n    Returns:\n        Result containing list of files or error message\n    \"\"\"\n    ret = Result(result=None)\n    try:\n        full_path = self._safe_path(url)\n    except ValueError as e:\n        ret.failed = True\n        ret.errors = [str(e)]\n        return ret\n\n    if os.path.exists(full_path) and os.path.isdir(full_path):\n        ret.result = os.listdir(full_path)\n    else:\n        ret.errors = [\"Directory Not Found\"]\n        ret.failed = True\n    return ret\n</code></pre>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_walk/","title":"Filesharing Service Walk Task","text":"<p>task api name: <code>walk</code></p> <p>The <code>walk</code> task recursively lists all files from all subdirectories under the specified URL path. This is useful when you need to discover all files available in a directory tree, returning a list of complete <code>nf://...</code> URLs for each file found. The task skips hidden files (starting with <code>.</code>) and special directories (containing <code>__</code>).</p>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_walk/#using-it-from-python","title":"Using it from Python","text":"<pre><code>from norfab.core.nfapi import NorFab\n\nwith NorFab(inventory=\"./inventory.yaml\") as nf:\n    client = nf.make_client()\n\n    reply = client.run_job(\n        service=\"filesharing\",\n        task=\"walk\",\n        workers=\"any\",\n        kwargs={\"url\": \"nf://\"},\n    )\n    print(reply)\n</code></pre>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_walk/#using-it-from-nfcli","title":"Using it from <code>nfcli</code>","text":"<p>NORFAB CLI exposes File Sharing under the <code>file</code> command group.</p> <pre><code>nf#man tree file\nroot\n\u2514\u2500\u2500 file:    File sharing service\n    \u2514\u2500\u2500 walk:    Walk directory tree recursively\n\nnf#\n</code></pre>","tags":["filesharing"]},{"location":"workers/filesharing/services_filesharing_service_tasks_walk/#api-reference","title":"API Reference","text":"<p>Recursively list all files from all subdirectories.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL path starting with 'nf://' to walk directories</p> required <p>Returns:</p> Type Description <code>Result</code> <p>Result containing list of all file paths or error message</p> Source code in <code>norfab\\workers\\filesharing_worker\\filesharing_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef walk(self, url: str) -&gt; Result:\n    \"\"\"\n    Recursively list all files from all subdirectories.\n\n    Args:\n        url: URL path starting with 'nf://' to walk directories\n\n    Returns:\n        Result containing list of all file paths or error message\n    \"\"\"\n    ret = Result(result=None)\n    try:\n        full_path = self._safe_path(url)\n    except ValueError as e:\n        ret.failed = True\n        ret.errors = [str(e)]\n        return ret\n\n    if os.path.exists(full_path) and os.path.isdir(full_path):\n        files_list = []\n        for root, dirs, files in os.walk(full_path):\n            # skip path containing folders like __folders__\n            if root.count(\"__\") &gt;= 2:\n                continue\n            root = root.replace(self.base_dir, \"\")\n            root = root.lstrip(\"\\\\\")\n            root = root.replace(\"\\\\\", \"/\")\n            for file in files:\n                # skip hidden/system files\n                if file.startswith(\".\"):\n                    continue\n                if root:\n                    files_list.append(f\"nf://{root}/{file}\")\n                else:\n                    files_list.append(f\"nf://{file}\")\n        ret.result = files_list\n    else:\n        ret.failed = True\n        ret.errors = [\"Directory Not Found\"]\n    return ret\n</code></pre>","tags":["filesharing"]},{"location":"workers/netbox/api_reference_workers_netbox_worker/","title":"Netbox Worker","text":""},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxAllocationError","title":"<code>NetboxAllocationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when there is an error in allocating resource in Netbox</p>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.UnsupportedNetboxVersion","title":"<code>UnsupportedNetboxVersion</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when there is an error in allocating resource in Netbox</p>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker","title":"<code>NetboxWorker(inventory, broker, worker_name, exit_event=None, init_done_event=None, log_level=None, log_queue: object = None)</code>","text":"<p>               Bases: <code>NFPWorker</code></p> <p>NetboxWorker class for interacting with Netbox API and managing inventory.</p> <p>Parameters:</p> Name Type Description Default <code>inventory</code> <code>dict</code> <p>The inventory data.</p> required <code>broker</code> <code>object</code> <p>The broker instance.</p> required <code>worker_name</code> <code>str</code> <p>The name of the worker.</p> required <code>exit_event</code> <code>Event</code> <p>Event to signal exit.</p> <code>None</code> <code>init_done_event</code> <code>Event</code> <p>Event to signal initialization completion.</p> <code>None</code> <code>log_level</code> <code>int</code> <p>Logging level.</p> <code>None</code> <code>log_queue</code> <code>object</code> <p>Queue for logging.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the inventory has no Netbox instances.</p> <p>Attributes:</p> Name Type Description <code>default_instance</code> <code>str</code> <p>Default Netbox instance name.</p> <code>inventory</code> <code>dict</code> <p>Inventory data.</p> <code>nb_version</code> <code>tuple</code> <p>Netbox version.</p> <code>compatible_ge_v4</code> <code>tuple</code> <p>Minimum supported Netbox v4 version (4.4.0+).</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>def __init__(\n    self,\n    inventory,\n    broker,\n    worker_name,\n    exit_event=None,\n    init_done_event=None,\n    log_level=None,\n    log_queue: object = None,\n):\n    super().__init__(\n        inventory, broker, SERVICE, worker_name, exit_event, log_level, log_queue\n    )\n    self.init_done_event = init_done_event\n    self.cache = None\n\n    # get inventory from broker\n    self.netbox_inventory = self.load_inventory()\n    if not self.netbox_inventory:\n        log.critical(\n            f\"{self.name} - Broker {self.broker} returned no inventory for {self.name}, killing myself...\"\n        )\n        self.destroy()\n\n    assert self.netbox_inventory.get(\n        \"instances\"\n    ), f\"{self.name} - inventory has no Netbox instances\"\n\n    # extract parameters from imvemtory\n    self.netbox_connect_timeout = self.netbox_inventory.get(\n        \"netbox_connect_timeout\", 10\n    )\n    self.netbox_read_timeout = self.netbox_inventory.get(\"netbox_read_timeout\", 300)\n    self.cache_use = self.netbox_inventory.get(\"cache_use\", True)\n    self.cache_ttl = self.netbox_inventory.get(\"cache_ttl\", 31557600)  # 1 Year\n    self.branch_create_timeout = self.netbox_inventory.get(\n        \"branch_create_timeout\", 120\n    )\n\n    # find default instance\n    for name, params in self.netbox_inventory[\"instances\"].items():\n        if params.get(\"default\") is True:\n            self.default_instance = name\n            break\n    else:\n        self.default_instance = name\n\n    # check Netbox compatibility\n    self._verify_compatibility()\n\n    # instantiate cache\n    self.cache_dir = os.path.join(self.base_dir, \"cache\")\n    os.makedirs(self.cache_dir, exist_ok=True)\n    self.cache = self._get_diskcache()\n\n    self.init_done_event.set()\n    log.info(f\"{self.name} - Started\")\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.worker_exit","title":"<code>worker_exit() -&gt; None</code>","text":"<p>Worker exist sanity checks. Closes the cache if it exists.</p> <p>This method checks if the cache attribute is present and not None. If the cache exists, it closes the cache to release any resources associated with it.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>def worker_exit(self) -&gt; None:\n    \"\"\"\n    Worker exist sanity checks. Closes the cache if it exists.\n\n    This method checks if the cache attribute is present and not None.\n    If the cache exists, it closes the cache to release any resources\n    associated with it.\n    \"\"\"\n    if self.cache:\n        self.cache.close()\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.get_inventory","title":"<code>get_inventory() -&gt; Result</code>","text":"<p>NorFab Task to return running inventory for NetBox worker.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the NetBox inventory.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_inventory(self) -&gt; Result:\n    \"\"\"\n    NorFab Task to return running inventory for NetBox worker.\n\n    Returns:\n        dict: A dictionary containing the NetBox inventory.\n    \"\"\"\n    return Result(\n        task=f\"{self.name}:get_inventory\", result=dict(self.netbox_inventory)\n    )\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.get_version","title":"<code>get_version(**kwargs: Any) -&gt; Result</code>","text":"<p>Retrieves the version information of Netbox instances.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the version information of the Netbox</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_version(self, **kwargs: Any) -&gt; Result:\n    \"\"\"\n    Retrieves the version information of Netbox instances.\n\n    Returns:\n        dict: A dictionary containing the version information of the Netbox\n    \"\"\"\n    libs = {\n        \"norfab\": \"\",\n        \"pynetbox\": \"\",\n        \"requests\": \"\",\n        \"python\": sys.version.split(\" \")[0],\n        \"platform\": sys.platform,\n        \"diskcache\": \"\",\n        \"netbox_version\": self.nb_version,\n    }\n    # get version of packages installed\n    for pkg in libs.keys():\n        try:\n            libs[pkg] = importlib.metadata.version(pkg)\n        except importlib.metadata.PackageNotFoundError:\n            pass\n\n    return Result(task=f\"{self.name}:get_version\", result=libs)\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.get_netbox_status","title":"<code>get_netbox_status(instance: Union[None, str] = None) -&gt; Result</code>","text":"<p>Retrieve the status of NetBox instances.</p> <p>This method queries the status of a specific NetBox instance if the <code>instance</code> parameter is provided. If no instance is specified, it queries the status of all instances in the NetBox inventory.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>The name of the specific NetBox instance to query.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the status of the requested NetBox   instance(s).</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_netbox_status(self, instance: Union[None, str] = None) -&gt; Result:\n    \"\"\"\n    Retrieve the status of NetBox instances.\n\n    This method queries the status of a specific NetBox instance if the\n    `instance` parameter is provided. If no instance is specified, it\n    queries the status of all instances in the NetBox inventory.\n\n    Args:\n        instance (str, optional): The name of the specific NetBox instance to query.\n\n    Returns:\n        dict: A dictionary containing the status of the requested NetBox\n              instance(s).\n    \"\"\"\n    ret = Result(result={}, task=f\"{self.name}:get_netbox_status\")\n    if instance:\n        ret.result[instance] = self._query_netbox_status(instance)\n    else:\n        for name in self.netbox_inventory[\"instances\"].keys():\n            ret.result[name] = self._query_netbox_status(name)\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.get_compatibility","title":"<code>get_compatibility(job: Job) -&gt; Result</code>","text":"<p>Checks the compatibility of Netbox instances based on their version.</p> <p>This method retrieves the status and version of Netbox instances and determines if they are compatible with the required versions. It logs a warning if any instance is not reachable.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary where the keys are the instance names and the values are   booleans indicating compatibility (True/False) or None if the instance   is not reachable.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_compatibility(self, job: Job) -&gt; Result:\n    \"\"\"\n    Checks the compatibility of Netbox instances based on their version.\n\n    This method retrieves the status and version of Netbox instances and determines\n    if they are compatible with the required versions. It logs a warning if any\n    instance is not reachable.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n\n    Returns:\n        dict: A dictionary where the keys are the instance names and the values are\n              booleans indicating compatibility (True/False) or None if the instance\n              is not reachable.\n    \"\"\"\n    ret = Result(task=f\"{self.name}:get_compatibility\", result={})\n    netbox_status = self.get_netbox_status(job=job)\n    for instance, params in netbox_status.result.items():\n        if params[\"status\"] is not True:\n            log.warning(f\"{self.name} - {instance} Netbox instance not reachable\")\n            ret.result[instance] = None\n        else:\n            if \"-docker-\" in params[\"netbox-version\"].lower():\n                self.nb_version[instance] = tuple(\n                    [\n                        int(i)\n                        for i in params[\"netbox-version\"]\n                        .lower()\n                        .split(\"-docker-\")[0]\n                        .split(\".\")\n                    ]\n                )\n            else:\n                self.nb_version[instance] = tuple(\n                    [int(i) for i in params[\"netbox-version\"].split(\".\")]\n                )\n            # check Netbox 4.4+ compatibility\n            if self.nb_version[instance] &gt;= self.compatible_ge_v4:\n                ret.result[instance] = True\n            else:\n                ret.result[instance] = False\n                log.error(\n                    f\"{self.name} - {instance} Netbox version {self.nb_version[instance]} is not supported, \"\n                    f\"minimum required version is {self.compatible_ge_v4}\"\n                )\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.has_plugin","title":"<code>has_plugin(plugin_name: str, instance: str, strict: bool = False) -&gt; bool</code>","text":"<p>Check if a specified plugin is installed in a given NetBox instance.</p> <p>Parameters:</p> Name Type Description Default <code>plugin_name</code> <code>str</code> <p>The name of the plugin to check for.</p> required <code>instance</code> <code>str</code> <p>The identifier or address of the NetBox instance.</p> required <code>strict</code> <code>bool</code> <p>If True, raises a RuntimeError when the plugin is not found.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the plugin is installed, False otherwise.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>def has_plugin(self, plugin_name: str, instance: str, strict: bool = False) -&gt; bool:\n    \"\"\"\n    Check if a specified plugin is installed in a given NetBox instance.\n\n    Args:\n        plugin_name (str): The name of the plugin to check for.\n        instance (str): The identifier or address of the NetBox instance.\n        strict (bool, optional): If True, raises a RuntimeError when the plugin is not found.\n\n    Returns:\n        bool: True if the plugin is installed, False otherwise.\n    \"\"\"\n    nb_status = self._query_netbox_status(instance)\n\n    if plugin_name in nb_status[\"plugins\"]:\n        return True\n    elif strict is True:\n        raise RuntimeError(\n            f\"'{instance}' Netbox instance has no '{plugin_name}' plugin installed\"\n        )\n\n    return False\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.cache_list","title":"<code>cache_list(keys: str = '*', details: bool = False) -&gt; Result</code>","text":"<p>Retrieve a list of cache keys, optionally with details about each key.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>str</code> <p>A pattern to match cache keys against. Defaults to \"*\".</p> <code>'*'</code> <code>details</code> <code>bool</code> <p>If True, include detailed information about each cache key. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <code>Result</code> <p>A list of cache keys or a list of dictionaries with detailed information if <code>details</code> is True.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef cache_list(self, keys: str = \"*\", details: bool = False) -&gt; Result:\n    \"\"\"\n    Retrieve a list of cache keys, optionally with details about each key.\n\n    Args:\n        keys (str): A pattern to match cache keys against. Defaults to \"*\".\n        details (bool): If True, include detailed information about each cache key. Defaults to False.\n\n    Returns:\n        list: A list of cache keys or a list of dictionaries with detailed information if `details` is True.\n    \"\"\"\n    self.cache.expire()\n    ret = Result(task=f\"{self.name}:cache_list\", result=[])\n    for cache_key in self.cache:\n        if fnmatchcase(cache_key, keys):\n            if details:\n                _, expires = self.cache.get(cache_key, expire_time=True)\n                expires = datetime.fromtimestamp(expires)\n                creation = expires - timedelta(seconds=self.cache_ttl)\n                age = datetime.now() - creation\n                ret.result.append(\n                    {\n                        \"key\": cache_key,\n                        \"age\": str(age),\n                        \"creation\": str(creation),\n                        \"expires\": str(expires),\n                    }\n                )\n            else:\n                ret.result.append(cache_key)\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.cache_clear","title":"<code>cache_clear(job: Job, key: str = None, keys: str = None) -&gt; Result</code>","text":"<p>Clears specified keys from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>key</code> <code>str</code> <p>A specific key to remove from the cache.</p> <code>None</code> <code>keys</code> <code>str</code> <p>A glob pattern to match multiple keys to remove from the cache.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>Result</code> <p>A list of keys that were successfully removed from the cache.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a specified key or a key matching the glob pattern could not be removed from the cache.</p> <p>Notes:</p> <ul> <li>If neither <code>key</code> nor <code>keys</code> is provided, the function will return a message indicating that there is nothing to clear.</li> <li>If <code>key</code> is provided, it will attempt to remove that specific key from the cache.</li> <li>If <code>keys</code> is provided, it will attempt to remove all keys matching the glob pattern from the cache.</li> </ul> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"DELETE\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef cache_clear(self, job: Job, key: str = None, keys: str = None) -&gt; Result:\n    \"\"\"\n    Clears specified keys from the cache.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        key (str, optional): A specific key to remove from the cache.\n        keys (str, optional): A glob pattern to match multiple keys to remove from the cache.\n\n    Returns:\n        list: A list of keys that were successfully removed from the cache.\n\n    Raises:\n        RuntimeError: If a specified key or a key matching the glob pattern could not be removed from the cache.\n\n    Notes:\n\n    - If neither `key` nor `keys` is provided, the function will return a message indicating that there is nothing to clear.\n    - If `key` is provided, it will attempt to remove that specific key from the cache.\n    - If `keys` is provided, it will attempt to remove all keys matching the glob pattern from the cache.\n    \"\"\"\n    ret = Result(task=f\"{self.name}:cache_clear\", result=[])\n    # check if has keys to clear\n    if key == keys == None:  # noqa\n        ret.result = \"Noting to clear, specify key or keys\"\n        return ret\n    # remove specific key from cache\n    if key:\n        if key in self.cache:\n            if self.cache.delete(key, retry=True):\n                ret.result.append(key)\n            else:\n                raise RuntimeError(f\"Failed to remove {key} from cache\")\n        else:\n            ret.messages.append(f\"Key {key} not in cache.\")\n    # remove all keys matching glob pattern\n    if keys:\n        for cache_key in self.cache:\n            if fnmatchcase(cache_key, keys):\n                if self.cache.delete(cache_key, retry=True):\n                    ret.result.append(cache_key)\n                else:\n                    raise RuntimeError(f\"Failed to remove {key} from cache\")\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.cache_get","title":"<code>cache_get(job: Job, key: str = None, keys: str = None, raise_missing: bool = False) -&gt; Result</code>","text":"<p>Retrieve values from the cache based on a specific key or a pattern of keys.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>key</code> <code>str</code> <p>A specific key to retrieve from the cache.</p> <code>None</code> <code>keys</code> <code>str</code> <p>A glob pattern to match multiple keys in the cache.</p> <code>None</code> <code>raise_missing</code> <code>bool</code> <p>If True, raises a KeyError if the specific key is not found in the cache. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the results of the cache retrieval. The keys are the cache keys and the values are the corresponding cache values.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If raise_missing is True and the specific key is not found in the cache.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef cache_get(\n    self, job: Job, key: str = None, keys: str = None, raise_missing: bool = False\n) -&gt; Result:\n    \"\"\"\n    Retrieve values from the cache based on a specific key or a pattern of keys.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        key (str, optional): A specific key to retrieve from the cache.\n        keys (str, optional): A glob pattern to match multiple keys in the cache.\n        raise_missing (bool, optional): If True, raises a KeyError if the specific\n            key is not found in the cache. Defaults to False.\n\n    Returns:\n        dict: A dictionary containing the results of the cache retrieval. The keys are\n            the cache keys and the values are the corresponding cache values.\n\n    Raises:\n        KeyError: If raise_missing is True and the specific key is not found in the cache.\n    \"\"\"\n    ret = Result(task=f\"{self.name}:cache_clear\", result={})\n    # get specific key from cache\n    if key:\n        if key in self.cache:\n            ret.result[key] = self.cache[key]\n        elif raise_missing:\n            raise KeyError(f\"Key {key} not in cache.\")\n    # get all keys matching glob pattern\n    if keys:\n        for cache_key in self.cache:\n            if fnmatchcase(cache_key, keys):\n                ret.result[cache_key] = self.cache[cache_key]\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.graphql","title":"<code>graphql(job: Job, instance: Union[None, str] = None, dry_run: bool = False, obj: Union[str, dict] = None, filters: Union[None, dict, str] = None, fields: Union[None, list] = None, queries: Union[None, dict] = None, query_string: str = None) -&gt; Result</code>","text":"<p>Function to query Netbox v3 or Netbox v4 GraphQL API.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>instance</code> <code>Union[None, str]</code> <p>Netbox instance name</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>only return query content, do not run it</p> <code>False</code> <code>obj</code> <code>Union[str, dict]</code> <p>Object to query</p> <code>None</code> <code>filters</code> <code>Union[None, dict, str]</code> <p>Filters to apply to the query</p> <code>None</code> <code>fields</code> <code>Union[None, list]</code> <p>Fields to retrieve in the query</p> <code>None</code> <code>queries</code> <code>Union[None, dict]</code> <p>Dictionary of queries to execute</p> <code>None</code> <code>query_string</code> <code>str</code> <p>Raw query string to execute</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>GraphQL request data returned by Netbox</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If required arguments are not provided</p> <code>Exception</code> <p>If GraphQL query fails</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"POST\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef graphql(\n    self,\n    job: Job,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    obj: Union[str, dict] = None,\n    filters: Union[None, dict, str] = None,\n    fields: Union[None, list] = None,\n    queries: Union[None, dict] = None,\n    query_string: str = None,\n) -&gt; Result:\n    \"\"\"\n    Function to query Netbox v3 or Netbox v4 GraphQL API.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        instance: Netbox instance name\n        dry_run: only return query content, do not run it\n        obj: Object to query\n        filters: Filters to apply to the query\n        fields: Fields to retrieve in the query\n        queries: Dictionary of queries to execute\n        query_string: Raw query string to execute\n\n    Returns:\n        dict: GraphQL request data returned by Netbox\n\n    Raises:\n        RuntimeError: If required arguments are not provided\n        Exception: If GraphQL query fails\n    \"\"\"\n    nb_params = self._get_instance_params(instance)\n    instance = instance or self.default_instance\n    ret = Result(task=f\"{self.name}:graphql\", resources=[instance])\n\n    # form graphql query(ies) payload\n    if queries:\n        queries_list = []\n        for alias, query_data in queries.items():\n            query_data[\"alias\"] = alias\n            if self.nb_version[instance] &gt;= (4, 4, 0):\n                queries_list.append(_form_query_v4(**query_data))\n            else:\n                raise UnsupportedNetboxVersion(\n                    f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n                    f\"minimum required version is {self.compatible_ge_v4}\"\n                )\n        queries_strings = \"    \".join(queries_list)\n        query = f\"query {{{queries_strings}}}\"\n    elif obj and filters and fields:\n        if self.nb_version[instance] &gt;= (4, 4, 0):\n            query = _form_query_v4(obj, filters, fields)\n        else:\n            raise UnsupportedNetboxVersion(\n                f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n                f\"minimum required version is {self.compatible_ge_v4}\"\n            )\n        query = f\"query {{{query}}}\"\n    elif query_string:\n        query = query_string\n    else:\n        raise RuntimeError(\n            f\"{self.name} - graphql method expects queries argument or obj, filters, \"\n            f\"fields arguments or query_string argument provided\"\n        )\n    payload = json.dumps({\"query\": query})\n\n    # form and return dry run response\n    if dry_run:\n        ret.result = {\n            \"url\": f\"{nb_params['url']}/graphql/\",\n            \"data\": payload,\n            \"verify\": nb_params.get(\"ssl_verify\", True),\n            \"headers\": {\n                \"Content-Type\": \"application/json\",\n                \"Accept\": \"application/json\",\n                \"Authorization\": f\"Token ...{nb_params['token'][-6:]}\",\n            },\n        }\n        return ret\n\n    # send request to Netbox GraphQL API\n    log.debug(\n        f\"{self.name} - sending GraphQL query '{payload}' to URL '{nb_params['url']}/graphql/'\"\n    )\n    req = requests.post(\n        url=f\"{nb_params['url']}/graphql/\",\n        headers={\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\",\n            \"Authorization\": f\"Token {nb_params['token']}\",\n        },\n        data=payload,\n        verify=nb_params.get(\"ssl_verify\", True),\n        timeout=(self.netbox_connect_timeout, self.netbox_read_timeout),\n    )\n    try:\n        req.raise_for_status()\n    except Exception:\n        raise Exception(\n            f\"{self.name} -  Netbox GraphQL query failed, query '{query}', \"\n            f\"URL '{req.url}', status-code '{req.status_code}', reason '{req.reason}', \"\n            f\"response content '{req.text}'\"\n        )\n\n    # return results\n    reply = req.json()\n    if reply.get(\"errors\"):\n        msg = f\"{self.name} - GrapQL query error '{reply['errors']}', query '{payload}'\"\n        log.error(msg)\n        ret.errors.append(msg)\n        if reply.get(\"data\"):\n            ret.result = reply[\"data\"]  # at least return some data\n    elif queries or query_string:\n        ret.result = reply[\"data\"]\n    else:\n        ret.result = reply[\"data\"][obj]\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.rest","title":"<code>rest(job: Job, instance: Union[None, str] = None, method: str = 'get', api: str = '', **kwargs: Any) -&gt; Result</code>","text":"<p>Sends a request to the Netbox REST API.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>The Netbox instance name to get parameters for.</p> <code>None</code> <code>method</code> <code>str</code> <p>The HTTP method to use for the request (e.g., 'get', 'post'). Defaults to \"get\".</p> <code>'get'</code> <code>api</code> <code>str</code> <p>The API endpoint to send the request to. Defaults to \"\".</p> <code>''</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the request (e.g., params, data, json).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Result</code> <p>Union[dict, list]: The JSON response from the API, parsed into a dictionary or list.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the HTTP request returned an unsuccessful status code.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"POST\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef rest(\n    self,\n    job: Job,\n    instance: Union[None, str] = None,\n    method: str = \"get\",\n    api: str = \"\",\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Sends a request to the Netbox REST API.\n\n    Args:\n        instance (str, optional): The Netbox instance name to get parameters for.\n        method (str, optional): The HTTP method to use for the request (e.g., 'get', 'post'). Defaults to \"get\".\n        api (str, optional): The API endpoint to send the request to. Defaults to \"\".\n        **kwargs: Additional arguments to pass to the request (e.g., params, data, json).\n\n    Returns:\n        Union[dict, list]: The JSON response from the API, parsed into a dictionary or list.\n\n    Raises:\n        requests.exceptions.HTTPError: If the HTTP request returned an unsuccessful status code.\n    \"\"\"\n    ret = Result(task=f\"{self.name}:rest\", result={})\n    nb_params = self._get_instance_params(instance)\n\n    # send request to Netbox REST API\n    response = getattr(requests, method)(\n        url=f\"{nb_params['url']}/api/{api}/\",\n        headers={\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\",\n            \"Authorization\": f\"Token {nb_params['token']}\",\n        },\n        verify=nb_params.get(\"ssl_verify\", True),\n        **kwargs,\n    )\n\n    response.raise_for_status()\n    try:\n        ret.result = response.json()\n    except Exception as e:\n        log.debug(f\"Failed to decode json, error: {e}\")\n        ret.result = response.text if response.text else response.status_code\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.get_devices","title":"<code>get_devices(job: Job, filters: Union[None, list] = None, instance: Union[None, str] = None, dry_run: bool = False, devices: Union[None, list] = None, cache: Union[bool, str] = None) -&gt; Result</code>","text":"<p>Retrieves device data from Netbox using the GraphQL API.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>filters</code> <code>list</code> <p>A list of filter dictionaries to filter devices.</p> <code>None</code> <code>instance</code> <code>str</code> <p>The Netbox instance name.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, only returns the query content without executing it. Defaults to False.</p> <code>False</code> <code>devices</code> <code>list</code> <p>A list of device names to query data for.</p> <code>None</code> <code>cache</code> <code>Union[bool, str]</code> <p>Cache usage options:</p> <ul> <li>True: Use data stored in cache if it is up to date, refresh it otherwise.</li> <li>False: Do not use cache and do not update cache.</li> <li>\"refresh\": Ignore data in cache and replace it with data fetched from Netbox.</li> <li>\"force\": Use data in cache without checking if it is up to date.</li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary keyed by device name with device data.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the GraphQL query fails or if there are errors in the query result.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_devices(\n    self,\n    job: Job,\n    filters: Union[None, list] = None,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    devices: Union[None, list] = None,\n    cache: Union[bool, str] = None,\n) -&gt; Result:\n    \"\"\"\n    Retrieves device data from Netbox using the GraphQL API.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        filters (list, optional): A list of filter dictionaries to filter devices.\n        instance (str, optional): The Netbox instance name.\n        dry_run (bool, optional): If True, only returns the query content without executing it. Defaults to False.\n        devices (list, optional): A list of device names to query data for.\n        cache (Union[bool, str], optional): Cache usage options:\n\n            - True: Use data stored in cache if it is up to date, refresh it otherwise.\n            - False: Do not use cache and do not update cache.\n            - \"refresh\": Ignore data in cache and replace it with data fetched from Netbox.\n            - \"force\": Use data in cache without checking if it is up to date.\n\n    Returns:\n        dict: A dictionary keyed by device name with device data.\n\n    Raises:\n        Exception: If the GraphQL query fails or if there are errors in the query result.\n    \"\"\"\n    instance = instance or self.default_instance\n    ret = Result(task=f\"{self.name}:get_devices\", result={}, resources=[instance])\n    cache = self.cache_use if cache is None else cache\n    filters = filters or []\n    devices = devices or []\n    queries = {}  # devices queries\n    device_fields = [\n        \"name\",\n        \"last_updated\",\n        \"custom_field_data\",\n        \"tags {name}\",\n        \"device_type {model}\",\n        \"role {name}\",\n        \"config_context\",\n        \"tenant {name}\",\n        \"platform {name}\",\n        \"serial\",\n        \"asset_tag\",\n        \"site {name slug tags{name} }\",\n        \"location {name}\",\n        \"rack {name}\",\n        \"status\",\n        \"primary_ip4 {address}\",\n        \"primary_ip6 {address}\",\n        \"airflow\",\n        \"position\",\n        \"id\",\n    ]\n\n    if cache == True or cache == \"force\":\n        # retrieve last updated data from Netbox for devices\n        last_updated_query = {\n            f\"devices_by_filter_{index}\": {\n                \"obj\": \"device_list\",\n                \"filters\": filter_item,\n                \"fields\": [\"name\", \"last_updated\"],\n            }\n            for index, filter_item in enumerate(filters)\n        }\n        if devices:\n            # use cache data without checking if it is up to date for cached devices\n            if cache == \"force\":\n                for device_name in list(devices):\n                    device_cache_key = f\"get_devices::{device_name}\"\n                    if device_cache_key in self.cache:\n                        devices.remove(device_name)\n                        ret.result[device_name] = self.cache[device_cache_key]\n            # query netbox last updated data for devices\n            if self.nb_version[instance] &gt;= (4, 4, 0):\n                dlist = '[\"{dl}\"]'.format(dl='\", \"'.join(devices))\n                filters_dict = {\"name\": f\"{{in_list: {dlist}}}\"}\n            else:\n                raise UnsupportedNetboxVersion(\n                    f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n                    f\"minimum required version is {self.compatible_ge_v4}\"\n                )\n            last_updated_query[\"devices_by_devices_list\"] = {\n                \"obj\": \"device_list\",\n                \"filters\": filters_dict,\n                \"fields\": [\"name\", \"last_updated\"],\n            }\n        last_updated = self.graphql(\n            job=job,\n            queries=last_updated_query,\n            instance=instance,\n            dry_run=dry_run,\n        )\n        last_updated.raise_for_status(f\"{self.name} - get devices query failed\")\n\n        # return dry run result\n        if dry_run:\n            ret.result[\"get_devices_dry_run\"] = last_updated.result\n            return ret\n\n        # try to retrieve device data from cache\n        self.cache.expire()  # remove expired items from cache\n        for devices_list in last_updated.result.values():\n            for device in devices_list:\n                device_cache_key = f\"get_devices::{device['name']}\"\n                # check if cache is up to date and use it if so\n                if device_cache_key in self.cache and (\n                    self.cache[device_cache_key].get(\"last_updated\")\n                    == device[\"last_updated\"]\n                    or cache == \"force\"\n                ):\n                    ret.result[device[\"name\"]] = self.cache[device_cache_key]\n                    # remove device from list of devices to retrieve\n                    if device[\"name\"] in devices:\n                        devices.remove(device[\"name\"])\n                # cache old or no cache, fetch device data\n                elif device[\"name\"] not in devices:\n                    devices.append(device[\"name\"])\n    # ignore cache data, fetch data from netbox\n    elif cache == False or cache == \"refresh\":\n        queries = {\n            f\"devices_by_filter_{index}\": {\n                \"obj\": \"device_list\",\n                \"filters\": filter_item,\n                \"fields\": device_fields,\n            }\n            for index, filter_item in enumerate(filters)\n        }\n\n    # fetch devices data from Netbox\n    if devices or queries:\n        if devices:\n            if self.nb_version[instance] &gt;= (4, 4, 0):\n                dlist = '[\"{dl}\"]'.format(dl='\", \"'.join(devices))\n                filters_dict = {\"name\": f\"{{in_list: {dlist}}}\"}\n            else:\n                raise UnsupportedNetboxVersion(\n                    f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n                    f\"minimum required version is {self.compatible_ge_v4}\"\n                )\n            queries[\"devices_by_devices_list\"] = {\n                \"obj\": \"device_list\",\n                \"filters\": filters_dict,\n                \"fields\": device_fields,\n            }\n\n        # send queries\n        query_result = self.graphql(\n            job=job, queries=queries, instance=instance, dry_run=dry_run\n        )\n\n        # check for errors\n        if query_result.errors:\n            msg = f\"{self.name} - get devices query failed with errors:\\n{query_result.errors}\"\n            raise Exception(msg)\n\n        # return dry run result\n        if dry_run:\n            ret.result[\"get_devices_dry_run\"] = query_result.result\n            return ret\n\n        # process devices data\n        devices_data = query_result.result\n        for devices_list in devices_data.values():\n            for device in devices_list:\n                if device[\"name\"] not in ret.result:\n                    device_name = device.pop(\"name\")\n                    # cache device data\n                    if cache != False:\n                        cache_key = f\"get_devices::{device_name}\"\n                        self.cache.set(cache_key, device, expire=self.cache_ttl)\n                    # add device data to return result\n                    ret.result[device_name] = device\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.get_interfaces","title":"<code>get_interfaces(job: Job, instance: Union[None, str] = None, devices: Union[None, list] = None, interface_regex: Union[None, str] = None, ip_addresses: bool = False, inventory_items: bool = False, dry_run: bool = False, cache: Union[bool, str] = None) -&gt; Result</code>","text":"<p>Retrieve device interfaces from Netbox using GraphQL API.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>instance</code> <code>str</code> <p>Netbox instance name.</p> <code>None</code> <code>devices</code> <code>list</code> <p>List of devices to retrieve interfaces for.</p> <code>None</code> <code>interface_regex</code> <code>str</code> <p>Regex pattern to match interfaces by name, case insensitive.</p> <code>None</code> <code>ip_addresses</code> <code>bool</code> <p>If True, retrieves interface IPs. Defaults to False.</p> <code>False</code> <code>inventory_items</code> <code>bool</code> <p>If True, retrieves interface inventory items. Defaults to False.</p> <code>False</code> <code>dry_run</code> <code>bool</code> <p>If True, only return query content, do not run it. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>Dictionary keyed by device name with interface details.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no interfaces data is returned for the specified devices.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_interfaces(\n    self,\n    job: Job,\n    instance: Union[None, str] = None,\n    devices: Union[None, list] = None,\n    interface_regex: Union[None, str] = None,\n    ip_addresses: bool = False,\n    inventory_items: bool = False,\n    dry_run: bool = False,\n    cache: Union[bool, str] = None,\n) -&gt; Result:\n    \"\"\"\n    Retrieve device interfaces from Netbox using GraphQL API.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        instance (str, optional): Netbox instance name.\n        devices (list, optional): List of devices to retrieve interfaces for.\n        interface_regex (str, optional): Regex pattern to match interfaces by name, case insensitive.\n        ip_addresses (bool, optional): If True, retrieves interface IPs. Defaults to False.\n        inventory_items (bool, optional): If True, retrieves interface inventory items. Defaults to False.\n        dry_run (bool, optional): If True, only return query content, do not run it. Defaults to False.\n\n    Returns:\n        dict: Dictionary keyed by device name with interface details.\n\n    Raises:\n        Exception: If no interfaces data is returned for the specified devices.\n    \"\"\"\n    instance = instance or self.default_instance\n    devices = devices or []\n    ret = Result(\n        task=f\"{self.name}:get_interfaces\",\n        result={d: {} for d in devices},\n        resources=[instance],\n    )\n\n    intf_fields = [\n        \"name\",\n        \"enabled\",\n        \"description\",\n        \"mtu\",\n        \"parent {name}\",\n        \"mode\",\n        \"untagged_vlan {vid name}\",\n        \"vrf {name}\",\n        \"tagged_vlans {vid name}\",\n        \"tags {name}\",\n        \"custom_fields\",\n        \"last_updated\",\n        \"bridge {name}\",\n        \"child_interfaces {name}\",\n        \"bridge_interfaces {name}\",\n        \"member_interfaces {name}\",\n        \"wwn\",\n        \"duplex\",\n        \"speed\",\n        \"id\",\n        \"device {name}\",\n        \"label\",\n        \"mark_connected\",\n    ]\n    intf_fields.append(\"mac_addresses {mac_address}\")\n\n    # add IP addresses to interfaces fields\n    if ip_addresses:\n        intf_fields.append(\n            \"ip_addresses {address status role dns_name description custom_fields last_updated tenant {name} tags {name}}\"\n        )\n\n    # form interfaces query dictionary\n    dlist = str(devices).replace(\"'\", '\"')  # swap quotes\n    if self.nb_version[instance] &gt;= (4, 4, 0):\n        # add interface name regex filter\n        if interface_regex:\n            filters = (\n                \"{device: {name: {in_list: \"\n                + dlist\n                + \"}}\"\n                + \", name: {i_regex: \"\n                + f'\"{interface_regex}\"'\n                + \"}}\"\n            )\n        else:\n            filters = \"{device: {name: {in_list: \" + dlist + \"}}}\"\n    else:\n        raise UnsupportedNetboxVersion(\n            f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n            f\"minimum required version is {self.compatible_ge_v4}\"\n        )\n\n    queries = {\n        \"interfaces\": {\n            \"obj\": \"interface_list\",\n            \"filters\": filters,\n            \"fields\": intf_fields,\n        }\n    }\n\n    # add query to retrieve inventory items\n    if inventory_items:\n        if self.nb_version[instance] &gt;= (4, 4, 0):\n            dlist = str(devices).replace(\"'\", '\"')  # swap quotes\n            inv_filters = (\n                \"{device: {name: {in_list: \"\n                + dlist\n                + '}}, component_type: {app_label: {exact: \"dcim\"}}}'\n            )\n        else:\n            raise UnsupportedNetboxVersion(\n                f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n                f\"minimum required version is {self.compatible_ge_v4}\"\n            )\n        inv_fields = [\n            \"name\",\n            \"component {... on InterfaceType {id}}\",\n            \"role {name}\",\n            \"manufacturer {name}\",\n            \"custom_fields\",\n            \"label\",\n            \"description\",\n            \"tags {name}\",\n            \"asset_tag\",\n            \"serial\",\n            \"part_id\",\n        ]\n        queries[\"inventor_items\"] = {\n            \"obj\": \"inventory_item_list\",\n            \"filters\": inv_filters,\n            \"fields\": inv_fields,\n        }\n\n    query_result = self.graphql(\n        job=job, instance=instance, queries=queries, dry_run=dry_run\n    )\n\n    # return dry run result\n    if dry_run:\n        return query_result\n\n    interfaces_data = query_result.result\n\n    # exit if no Interfaces returned\n    if interfaces_data is None or not interfaces_data.get(\"interfaces\"):\n        raise Exception(\n            f\"{self.name} - no interfaces data in '{interfaces_data}' returned by '{instance}' \"\n            f\"for devices {', '.join(devices)}\"\n        )\n\n    # process query results\n    interfaces = interfaces_data.pop(\"interfaces\")\n\n    # process inventory items\n    if inventory_items:\n        inventory_items_list = interfaces_data.pop(\"inventor_items\")\n        # transform inventory items list to a dictionary keyed by intf_id\n        inventory_items_dict = {}\n        while inventory_items_list:\n            inv_item = inventory_items_list.pop()\n            # skip inventory items that does not assigned to components\n            if inv_item.get(\"component\") is None:\n                continue\n            intf_id = str(inv_item.pop(\"component\").pop(\"id\"))\n            inventory_items_dict.setdefault(intf_id, [])\n            inventory_items_dict[intf_id].append(inv_item)\n        # iterate over interfaces and add inventory items\n        for intf in interfaces:\n            intf[\"inventory_items\"] = inventory_items_dict.pop(intf[\"id\"], [])\n\n    # transform interfaces list to dictionary keyed by device and interfaces names\n    while interfaces:\n        intf = interfaces.pop()\n        device_name = intf.pop(\"device\").pop(\"name\")\n        intf_name = intf.pop(\"name\")\n        if device_name in ret.result:  # Netbox issue #16299\n            ret.result[device_name][intf_name] = intf\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.get_connections","title":"<code>get_connections(job: Job, devices: list[str], instance: Union[None, str] = None, dry_run: bool = False, cables: bool = False, cache: Union[bool, str] = None, include_virtual: bool = True, interface_regex: Union[None, str] = None) -&gt; Result</code>","text":"<p>Retrieve interface connection details for specified devices from Netbox.</p> <p>This task retrieves these connections:</p> <ul> <li>Physical interfaces connections</li> <li>Child/virtual interfaces connections using parent interface connections details</li> <li>Lag interfaces connections using member ports connections details</li> <li>Lag child interfaces connections using member ports connections details</li> <li>Console port and console server ports connections</li> <li>Connections to provider networks for physical, child/virtual and lag interfaces</li> </ul> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>devices</code> <code>list</code> <p>List of device names to retrieve connections for.</p> required <code>instance</code> <code>str</code> <p>Netbox instance name for the GraphQL query.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, perform a dry run without making actual changes.</p> <code>False</code> <code>cables</code> <code>bool</code> <p>if True includes interfaces' directly attached cables details</p> <code>False</code> <code>include_virtual</code> <code>bool</code> <p>if True include connections for virtual and LAG interfaces</p> <code>True</code> <code>interface_regex</code> <code>str</code> <p>Regex pattern to match interfaces, console ports and console server ports by name, case insensitive.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing connection details for each device:</p> <pre><code>{\n    \"netbox-worker-1.2\": {\n        \"r1\": {\n            \"Console\": {\n                \"breakout\": false,\n                \"remote_device\": \"termserv1\",\n                \"remote_device_status\": \"active\",\n                \"remote_interface\": \"ConsoleServerPort1\",\n                \"remote_termination_type\": \"consoleserverport\",\n                \"termination_type\": \"consoleport\"\n            },\n            \"eth1\": {\n                \"breakout\": false,\n                \"remote_device\": \"r2\",\n                \"remote_device_status\": \"active\",\n                \"remote_interface\": \"eth8\",\n                \"remote_termination_type\": \"interface\",\n                \"termination_type\": \"interface\"\n            }\n        }\n    }\n}\n</code></pre> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error in the GraphQL query or data retrieval process.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_connections(\n    self,\n    job: Job,\n    devices: list[str],\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    cables: bool = False,\n    cache: Union[bool, str] = None,\n    include_virtual: bool = True,\n    interface_regex: Union[None, str] = None,\n) -&gt; Result:\n    \"\"\"\n    Retrieve interface connection details for specified devices from Netbox.\n\n    This task retrieves these connections:\n\n    - Physical interfaces connections\n    - Child/virtual interfaces connections using parent interface connections details\n    - Lag interfaces connections using member ports connections details\n    - Lag child interfaces connections using member ports connections details\n    - Console port and console server ports connections\n    - Connections to provider networks for physical, child/virtual and lag interfaces\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        devices (list): List of device names to retrieve connections for.\n        instance (str, optional): Netbox instance name for the GraphQL query.\n        dry_run (bool, optional): If True, perform a dry run without making actual changes.\n        cables (bool, optional): if True includes interfaces' directly attached cables details\n        include_virtual (bool, optional): if True include connections for virtual and LAG interfaces\n        interface_regex (str, optional): Regex pattern to match interfaces, console ports and\n            console server ports by name, case insensitive.\n\n    Returns:\n        dict: A dictionary containing connection details for each device:\n\n            ```\n            {\n                \"netbox-worker-1.2\": {\n                    \"r1\": {\n                        \"Console\": {\n                            \"breakout\": false,\n                            \"remote_device\": \"termserv1\",\n                            \"remote_device_status\": \"active\",\n                            \"remote_interface\": \"ConsoleServerPort1\",\n                            \"remote_termination_type\": \"consoleserverport\",\n                            \"termination_type\": \"consoleport\"\n                        },\n                        \"eth1\": {\n                            \"breakout\": false,\n                            \"remote_device\": \"r2\",\n                            \"remote_device_status\": \"active\",\n                            \"remote_interface\": \"eth8\",\n                            \"remote_termination_type\": \"interface\",\n                            \"termination_type\": \"interface\"\n                        }\n                    }\n                }\n            }\n            ```\n\n    Raises:\n        Exception: If there is an error in the GraphQL query or data retrieval process.\n    \"\"\"\n    instance = instance or self.default_instance\n    ret = Result(\n        task=f\"{self.name}:get_connections\",\n        result={d: {} for d in devices},\n        resources=[instance],\n    )\n\n    # form lists of fields to request from netbox\n    cable_fields = \"\"\"\n        cable {\n            type\n            status\n            tenant {name}\n            label\n            tags {name}\n            length\n            length_unit\n            custom_fields\n        }\n    \"\"\"\n    interfaces_fields = [\n        \"name\",\n        \"type\",\n        \"device {name, status}\",\n        \"\"\"\n        member_interfaces {\n          name\n          connected_endpoints {\n            __typename\n            ... on ProviderNetworkType {name}\n            ... on InterfaceType {name, device {name, status}, child_interfaces {name}, lag {name child_interfaces {name}}}\n          }\n        }\n        \"\"\",\n        \"\"\"\n        parent {\n          name\n          type\n          member_interfaces {\n            name\n            connected_endpoints {\n              __typename\n              ... on ProviderNetworkType {name}\n              ... on InterfaceType {name, device {name, status}, child_interfaces {name}, lag {name child_interfaces {name}}}\n            }\n          }\n          connected_endpoints {\n            __typename\n            ... on ProviderNetworkType {name}\n            ... on InterfaceType {name, device {name, status}, child_interfaces {name}, lag {name child_interfaces {name}}}\n          }\n        }\n        \"\"\",\n        \"\"\"\n        connected_endpoints {\n            __typename \n            ... on ProviderNetworkType {name}\n            ... on InterfaceType {name, device {name, status}, child_interfaces {name}, lag {name child_interfaces {name}}}\n        }\n        \"\"\",\n    ]\n    interfaces_fields.append(\n        \"\"\"\n        link_peers {\n            __typename\n            ... on InterfaceType {name device {name, status}}\n            ... on FrontPortType {name device {name, status}}\n            ... on RearPortType {name device {name, status}}\n        }\n    \"\"\"\n    )\n    console_ports_fields = [\n        \"name\",\n        \"device {name, status}\",\n        \"type\",\n        \"\"\"connected_endpoints {\n          __typename \n          ... on ConsoleServerPortType {name device {name, status}}\n        }\"\"\",\n        \"\"\"link_peers {\n          __typename\n          ... on ConsoleServerPortType {name device {name, status}}\n          ... on FrontPortType {name device {name, status}}\n          ... on RearPortType {name device {name, status}}\n        }\"\"\",\n    ]\n    console_server_ports_fields = [\n        \"name\",\n        \"device {name, status}\",\n        \"type\",\n        \"\"\"connected_endpoints {\n          __typename \n          ... on ConsolePortType {name device {name, status}}\n        }\"\"\",\n        \"\"\"link_peers {\n          __typename\n          ... on ConsolePortType {name device {name, status}}\n          ... on FrontPortType {name device {name, status}}\n          ... on RearPortType {name device {name, status}}\n        }\"\"\",\n    ]\n    power_outlet_fields = [\n        \"name\",\n        \"device {name, status}\",\n        \"type\",\n        \"\"\"connected_endpoints {\n          __typename \n          ... on PowerPortType {name device {name, status}}\n        }\"\"\",\n        \"\"\"link_peers {\n          __typename\n          ... on PowerPortType {name device {name, status}}\n        }\"\"\",\n    ]\n\n    # check if need to include cables info\n    if cables is True:\n        interfaces_fields.append(cable_fields)\n        console_ports_fields.append(cable_fields)\n        console_server_ports_fields.append(cable_fields)\n        power_outlet_fields.append(cable_fields)\n\n    # form query dictionary with aliases to get data from Netbox\n    dlist = str(devices).replace(\"'\", '\"')  # swap quotes\n    if self.nb_version[instance] &gt;= (4, 4, 0):\n        if interface_regex:\n            filters = (\n                \"{device: {name: {in_list: \"\n                + dlist\n                + \"}}, \"\n                + \"name: {i_regex: \"\n                + f'\"{interface_regex}\"'\n                + \"}}\"\n            )\n        else:\n            filters = \"{device: {name: {in_list: \" + dlist + \"}}}\"\n    else:\n        raise UnsupportedNetboxVersion(\n            f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n            f\"minimum required version is {self.compatible_ge_v4}\"\n        )\n\n    queries = {\n        \"interface\": {\n            \"obj\": \"interface_list\",\n            \"filters\": filters,\n            \"fields\": interfaces_fields,\n        },\n        \"consoleport\": {\n            \"obj\": \"console_port_list\",\n            \"filters\": filters,\n            \"fields\": console_ports_fields,\n        },\n        \"consoleserverport\": {\n            \"obj\": \"console_server_port_list\",\n            \"filters\": filters,\n            \"fields\": console_server_ports_fields,\n        },\n        \"poweroutlet\": {\n            \"obj\": \"power_outlet_list\",\n            \"filters\": filters,\n            \"fields\": power_outlet_fields,\n        },\n    }\n\n    # retrieve full list of devices interface with all cables\n    query_result = self.graphql(\n        job=job, queries=queries, instance=instance, dry_run=dry_run\n    )\n\n    # return dry run result\n    if dry_run:\n        return query_result\n\n    all_ports = query_result.result\n    if not all_ports:\n        return ret\n\n    # extract physical interfaces connections\n    for port_type, ports in all_ports.items():\n        for port in ports:\n            # skip ports that have no remote device connected\n            endpoints = port[\"connected_endpoints\"]\n            if not endpoints or not all(i for i in endpoints):\n                continue\n\n            # extract required parameters\n            cable = port.get(\"cable\", {})\n            device_name = port[\"device\"][\"name\"]\n            port_name = port[\"name\"]\n            link_peers = port[\"link_peers\"]\n            remote_termination_type = endpoints[0][\"__typename\"].lower()\n            remote_termination_type = remote_termination_type.replace(\"type\", \"\")\n\n            # form initial connection dictionary\n            connection = {\n                \"breakout\": len(endpoints) &gt; 1,\n                \"remote_termination_type\": remote_termination_type,\n                \"termination_type\": port_type,\n            }\n\n            # add remote connection details\n            if remote_termination_type == \"providernetwork\":\n                connection[\"remote_device\"] = None\n                connection[\"remote_device_status\"] = None\n                connection[\"remote_interface\"] = None\n                connection[\"provider\"] = endpoints[0][\"name\"]\n            else:\n                remote_interface = endpoints[0][\"name\"]\n                if len(endpoints) &gt; 1:\n                    remote_interface = list(sorted([i[\"name\"] for i in endpoints]))\n                connection[\"remote_interface\"] = remote_interface\n                connection[\"remote_device\"] = endpoints[0][\"device\"][\"name\"]\n                connection[\"remote_device_status\"] = endpoints[0][\"device\"][\n                    \"status\"\n                ]\n\n            # add cable and its peer details\n            if cables:\n                peer_termination_type = link_peers[0][\"__typename\"].lower()\n                peer_termination_type = peer_termination_type.replace(\"type\", \"\")\n                cable[\"peer_termination_type\"] = peer_termination_type\n                cable[\"peer_device\"] = link_peers[0].get(\"device\", {}).get(\"name\")\n                cable[\"peer_interface\"] = link_peers[0].get(\"name\")\n                if len(link_peers) &gt; 1:  # handle breakout cable\n                    cable[\"peer_interface\"] = [i[\"name\"] for i in link_peers]\n                connection[\"cable\"] = cable\n\n            # add physical connection to the results\n            ret.result[device_name][port_name] = connection\n\n    # extract virtual interfaces connections\n    for port_type, ports in all_ports.items():\n        for port in ports:\n            # add child virtual interfaces connections\n            if (\n                not include_virtual\n                or port[\"type\"] != \"virtual\"\n                or not port[\"parent\"]\n            ):\n                continue\n            device_name = port[\"device\"][\"name\"]\n            interface_name = port[\"name\"]\n            parent = port[\"parent\"]\n            connection = {\n                \"remote_device\": None,\n                \"remote_device_status\": None,\n                \"remote_interface\": None,\n                \"remote_termination_type\": \"virtual\",\n                \"termination_type\": \"virtual\",\n            }\n            # find connection endpoint\n            if parent[\"type\"] == \"lag\":\n                try:\n                    endpoint = parent[\"member_interfaces\"][0][\n                        \"connected_endpoints\"\n                    ][0]\n                except:\n                    continue\n            elif parent[\"connected_endpoints\"]:\n                try:\n                    endpoint = parent[\"connected_endpoints\"][0]\n                except:\n                    continue\n            connection[\"remote_device\"] = endpoint[\"device\"][\"name\"]\n            connection[\"remote_device_status\"] = endpoint[\"device\"][\"status\"]\n            remote_termination_type = endpoint[\"__typename\"].lower()\n            remote_termination_type = remote_termination_type.replace(\"type\", \"\")\n            # collect virtual interfaces facing provider\n            if remote_termination_type == \"providernetwork\":\n                connection[\"provider\"] = endpoint[\"name\"]\n            # find matching remote virtual interface for LAG subif\n            elif \".\" in interface_name and parent[\"type\"] == \"lag\":\n                subif_id = interface_name.split(\".\")[1]\n                for remote_child in endpoint[\"lag\"][\"child_interfaces\"]:\n                    if remote_child[\"name\"].endswith(f\".{subif_id}\"):\n                        connection[\"remote_interface\"] = remote_child[\"name\"]\n                        break\n                # no matching subinterface found, associate child interface with remote interface\n                else:\n                    connection[\"remote_interface\"] = endpoint[\"lag\"][\"name\"]\n                    connection[\"remote_termination_type\"] = \"lag\"\n            # find matching remote virtual interface for physical interface subif\n            elif \".\" in interface_name:\n                subif_id = interface_name.split(\".\")[1]\n                for remote_child in endpoint[\"child_interfaces\"]:\n                    if remote_child[\"name\"].endswith(f\".{subif_id}\"):\n                        connection[\"remote_interface\"] = remote_child[\"name\"]\n                        break\n                # no matching subinterface found, associate child interface with remote interface\n                else:\n                    connection[\"remote_interface\"] = endpoint[\"name\"]\n                    connection[\"remote_termination_type\"] = remote_termination_type\n            # add virtual interface connection to results\n            ret.result[device_name][interface_name] = connection\n\n    # extract LAG interfaces connections\n    for port_type, ports in all_ports.items():\n        for port in ports:\n            if not include_virtual or port[\"type\"] != \"lag\":\n                continue\n            device_name = port[\"device\"][\"name\"]\n            interface_name = port[\"name\"]\n            connection = {\n                \"remote_device\": None,\n                \"remote_device_status\": None,\n                \"remote_interface\": None,\n                \"remote_termination_type\": \"lag\",\n                \"termination_type\": \"lag\",\n            }\n            try:\n                endpoint = port[\"member_interfaces\"][0][\"connected_endpoints\"][0]\n            except:\n                continue\n            remote_termination_type = endpoint[\"__typename\"].lower()\n            remote_termination_type = remote_termination_type.replace(\"type\", \"\")\n            # collect lag interfaces facing provider\n            if remote_termination_type == \"providernetwork\":\n                connection[\"provider\"] = endpoint[\"name\"]\n            # find remote lag interface\n            elif endpoint[\"lag\"]:\n                connection[\"remote_interface\"] = endpoint[\"lag\"][\"name\"]\n                connection[\"remote_device\"] = endpoint[\"device\"][\"name\"]\n                connection[\"remote_device_status\"] = endpoint[\"device\"][\"status\"]\n            # add lag interface connection to results\n            ret.result[device_name][interface_name] = connection\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.get_circuits","title":"<code>get_circuits(job: Job, devices: list, cid: Union[None, list] = None, instance: Union[None, str] = None, dry_run: bool = False, cache: Union[bool, str] = True) -&gt; Result</code>","text":"<p>Retrieve circuit information for specified devices from Netbox.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>devices</code> <code>list</code> <p>List of device names to retrieve circuits for.</p> required <code>cid</code> <code>list</code> <p>List of circuit IDs to filter by.</p> <code>None</code> <code>instance</code> <code>str</code> <p>Netbox instance to query.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, perform a dry run without making changes. Defaults to False.</p> <code>False</code> <code>cache</code> <code>Union[bool, str]</code> <p>Cache usage options:</p> <ul> <li>True: Use data stored in cache if it is up to date, refresh it otherwise.</li> <li>False: Do not use cache and do not update cache.</li> <li>\"refresh\": Ignore data in cache and replace it with data fetched from Netbox.</li> <li>\"force\": Use data in cache without checking if it is up to date.</li> </ul> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>dictionary keyed by device names with circuits data.</p> <p>Task to retrieve device's circuits data from Netbox.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_circuits(\n    self,\n    job: Job,\n    devices: list,\n    cid: Union[None, list] = None,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    cache: Union[bool, str] = True,\n) -&gt; Result:\n    \"\"\"\n    Retrieve circuit information for specified devices from Netbox.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        devices (list): List of device names to retrieve circuits for.\n        cid (list, optional): List of circuit IDs to filter by.\n        instance (str, optional): Netbox instance to query.\n        dry_run (bool, optional): If True, perform a dry run without making changes. Defaults to False.\n        cache (Union[bool, str], optional): Cache usage options:\n\n            - True: Use data stored in cache if it is up to date, refresh it otherwise.\n            - False: Do not use cache and do not update cache.\n            - \"refresh\": Ignore data in cache and replace it with data fetched from Netbox.\n            - \"force\": Use data in cache without checking if it is up to date.\n\n    Returns:\n        dict: dictionary keyed by device names with circuits data.\n\n    Task to retrieve device's circuits data from Netbox.\n    \"\"\"\n    cid = cid or []\n    log.info(\n        f\"{self.name}:get_circuits - {instance or self.default_instance} Netbox, \"\n        f\"devices {', '.join(devices)}, cid {cid}\"\n    )\n    instance = instance or self.default_instance\n\n    # form final result object\n    ret = Result(\n        task=f\"{self.name}:get_circuits\",\n        result={d: {} for d in devices},\n        resources=[instance],\n    )\n    cache = self.cache_use if cache is None else cache\n    cid = cid or []\n    circuit_fields = [\n        \"cid\",\n        \"tags {name}\",\n        \"provider {name}\",\n        \"commit_rate\",\n        \"description\",\n        \"status\",\n        \"type {name}\",\n        \"provider_account {name}\",\n        \"tenant {name}\",\n        \"termination_a {id last_updated}\",\n        \"termination_z {id last_updated}\",\n        \"custom_fields\",\n        \"comments\",\n        \"last_updated\",\n    ]\n\n    # form initial circuits filters based on devices' sites and cid list\n    circuits_filters = {}\n    device_data = self.get_devices(\n        job=job, devices=copy.deepcopy(devices), instance=instance, cache=cache\n    )\n    sites = list(set([i[\"site\"][\"slug\"] for i in device_data.result.values()]))\n    if self.nb_version[instance] &gt;= (4, 4, 0):\n        slist = str(sites).replace(\"'\", '\"')  # swap quotes\n        if cid:\n            clist = str(cid).replace(\"'\", '\"')  # swap quotes\n            circuits_filters = \"{terminations: {site: {slug: {in_list: slist}}}, cid: {in_list: clist}}\"\n            circuits_filters = circuits_filters.replace(\"slist\", slist).replace(\n                \"clist\", clist\n            )\n        else:\n            circuits_filters = \"{terminations: {site: {slug: {in_list: slist }}}}\"\n            circuits_filters = circuits_filters.replace(\"slist\", slist)\n    else:\n        raise UnsupportedNetboxVersion(\n            f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n            f\"minimum required version is {self.compatible_ge_v4}\"\n        )\n\n    log.info(\n        f\"{self.name}:get_circuits - constructed circuits filters: '{circuits_filters}'\"\n    )\n\n    if cache == True or cache == \"force\":\n        log.info(f\"{self.name}:get_circuits - retrieving circuits data from cache\")\n        cid_list = []  #  new cid list for follow up query\n        # retrieve last updated data from Netbox for circuits and their terminations\n        last_updated = self.graphql(\n            job=job,\n            obj=\"circuit_list\",\n            filters=circuits_filters,\n            fields=[\n                \"cid\",\n                \"last_updated\",\n                \"termination_a {id last_updated}\",\n                \"termination_z {id last_updated}\",\n            ],\n            dry_run=dry_run,\n            instance=instance,\n        )\n        last_updated.raise_for_status(f\"{self.name} - get circuits query failed\")\n\n        # return dry run result\n        if dry_run:\n            ret.result[\"get_circuits_dry_run\"] = last_updated.result\n            return ret\n\n        # retrieve circuits data from cache\n        self.cache.expire()  # remove expired items from cache\n        for device in devices:\n            for circuit in last_updated.result:\n                circuit_cache_key = f\"get_circuits::{circuit['cid']}\"\n                log.info(\n                    f\"{self.name}:get_circuits - searching cache for key {circuit_cache_key}\"\n                )\n                # check if cache is up to date and use it if so\n                if circuit_cache_key in self.cache:\n                    cache_ckt = self.cache[circuit_cache_key]\n                    # check if device uses this circuit\n                    if device not in cache_ckt:\n                        continue\n                    # use cache forcefully\n                    if cache == \"force\":\n                        ret.result[device][circuit[\"cid\"]] = cache_ckt[device]\n                    # check circuit cache is up to date\n                    if cache_ckt[device][\"last_updated\"] != circuit[\"last_updated\"]:\n                        continue\n                    if (\n                        cache_ckt[device][\"termination_a\"]\n                        and circuit[\"termination_a\"]\n                        and cache_ckt[device][\"termination_a\"][\"last_updated\"]\n                        != circuit[\"termination_a\"][\"last_updated\"]\n                    ):\n                        continue\n                    if (\n                        cache_ckt[device][\"termination_z\"]\n                        and circuit[\"termination_z\"]\n                        and cache_ckt[device][\"termination_z\"][\"last_updated\"]\n                        != circuit[\"termination_z\"][\"last_updated\"]\n                    ):\n                        continue\n                    ret.result[device][circuit[\"cid\"]] = cache_ckt[device]\n                    log.info(\n                        f\"{self.name}:get_circuits - {circuit['cid']} retrieved data from cache\"\n                    )\n                elif circuit[\"cid\"] not in cid_list:\n                    cid_list.append(circuit[\"cid\"])\n                    log.info(\n                        f\"{self.name}:get_circuits - {circuit['cid']} no cache data found, fetching from Netbox\"\n                    )\n        # form new filters dictionary to fetch remaining circuits data\n        circuits_filters = {}\n        if cid_list:\n            cid_list = str(cid_list).replace(\"'\", '\"')  # swap quotes\n            if self.nb_version[instance] &gt;= (4, 4, 0):\n                circuits_filters = \"{cid: {in_list: cid_list}}\"\n                circuits_filters = circuits_filters.replace(\"cid_list\", cid_list)\n            else:\n                raise UnsupportedNetboxVersion(\n                    f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n                    f\"minimum required version is {self.compatible_ge_v4}\"\n                )\n    # ignore cache data, fetch circuits from netbox\n    elif cache == False or cache == \"refresh\":\n        pass\n\n    if circuits_filters:\n        query_result = self.graphql(\n            job=job,\n            obj=\"circuit_list\",\n            filters=circuits_filters,\n            fields=circuit_fields,\n            dry_run=dry_run,\n            instance=instance,\n        )\n        query_result.raise_for_status(f\"{self.name} - get circuits query failed\")\n\n        # return dry run result\n        if dry_run is True:\n            return query_result\n\n        all_circuits = query_result.result\n\n        # iterate over circuits and map them to devices\n        log.info(\n            f\"{self.name}:get_circuits - retrieved data for {len(all_circuits)} \"\n            f\"circuits from netbox, mapping circuits to devices\"\n        )\n        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n            results = [\n                executor.submit(\n                    self._map_circuit, job, circuit, ret, instance, devices, cache\n                )\n                for circuit in all_circuits\n            ]\n            for _ in concurrent.futures.as_completed(results):\n                continue\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.get_nornir_inventory","title":"<code>get_nornir_inventory(job: Job, filters: Union[None, list] = None, devices: Union[None, list] = None, instance: Union[None, str] = None, interfaces: Union[dict, bool] = False, connections: Union[dict, bool] = False, circuits: Union[dict, bool] = False, nbdata: bool = True, bgp_peerings: Union[dict, bool] = False, primary_ip: str = 'ip4') -&gt; Result</code>","text":"<p>Retrieve and construct Nornir inventory from NetBox data.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>filters</code> <code>list</code> <p>List of filters to apply when retrieving devices from NetBox.</p> <code>None</code> <code>devices</code> <code>list</code> <p>List of specific devices to retrieve from NetBox.</p> <code>None</code> <code>instance</code> <code>str</code> <p>NetBox instance to use.</p> <code>None</code> <code>interfaces</code> <code>Union[dict, bool]</code> <p>If True, include interfaces data     in the inventory. If a dict, use it as arguments for the get_interfaces method.</p> <code>False</code> <code>connections</code> <code>Union[dict, bool]</code> <p>If True, include connections data     in the inventory. If a dict, use it as arguments for the get_connections method.</p> <code>False</code> <code>circuits</code> <code>Union[dict, bool]</code> <p>If True, include circuits data in the     inventory. If a dict, use it as arguments for the get_circuits method.</p> <code>False</code> <code>nbdata</code> <code>bool</code> <p>If True, include a copy of NetBox device's data in the host's data.</p> <code>True</code> <code>primary_ip</code> <code>str</code> <p>Specify whether to use 'ip4' or 'ip6' for the primary     IP address. Defaults to 'ip4'.</p> <code>'ip4'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>Nornir inventory dictionary containing hosts and their respective data.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_nornir_inventory(\n    self,\n    job: Job,\n    filters: Union[None, list] = None,\n    devices: Union[None, list] = None,\n    instance: Union[None, str] = None,\n    interfaces: Union[dict, bool] = False,\n    connections: Union[dict, bool] = False,\n    circuits: Union[dict, bool] = False,\n    nbdata: bool = True,\n    bgp_peerings: Union[dict, bool] = False,\n    primary_ip: str = \"ip4\",\n) -&gt; Result:\n    \"\"\"\n    Retrieve and construct Nornir inventory from NetBox data.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        filters (list, optional): List of filters to apply when retrieving devices from NetBox.\n        devices (list, optional): List of specific devices to retrieve from NetBox.\n        instance (str, optional): NetBox instance to use.\n        interfaces (Union[dict, bool], optional): If True, include interfaces data\n                in the inventory. If a dict, use it as arguments for the get_interfaces method.\n        connections (Union[dict, bool], optional): If True, include connections data\n                in the inventory. If a dict, use it as arguments for the get_connections method.\n        circuits (Union[dict, bool], optional): If True, include circuits data in the\n                inventory. If a dict, use it as arguments for the get_circuits method.\n        nbdata (bool, optional): If True, include a copy of NetBox device's data in the host's data.\n        primary_ip (str, optional): Specify whether to use 'ip4' or 'ip6' for the primary\n                IP address. Defaults to 'ip4'.\n\n    Returns:\n        dict: Nornir inventory dictionary containing hosts and their respective data.\n    \"\"\"\n    hosts = {}\n    filters = filters or []\n    devices = devices or []\n    inventory = {\"hosts\": hosts}\n    ret = Result(task=f\"{self.name}:get_nornir_inventory\", result=inventory)\n\n    # check Netbox status\n    netbox_status = self.get_netbox_status(job=job, instance=instance)\n    if netbox_status.result[instance or self.default_instance][\"status\"] is False:\n        return ret\n\n    # retrieve devices data\n    nb_devices = self.get_devices(\n        job=job, filters=filters, devices=devices, instance=instance\n    )\n\n    # form Nornir hosts inventory\n    for device_name, device in nb_devices.result.items():\n        host = device[\"config_context\"].pop(\"nornir\", {})\n        host.setdefault(\"data\", {})\n        name = host.pop(\"name\", device_name)\n        hosts[name] = host\n        # add platform if not provided in device config context\n        if not host.get(\"platform\"):\n            if device[\"platform\"]:\n                host[\"platform\"] = device[\"platform\"][\"name\"]\n            else:\n                log.warning(f\"{self.name} - no platform found for '{name}' device\")\n        # add hostname if not provided in config context\n        if not host.get(\"hostname\"):\n            if device[\"primary_ip4\"] and primary_ip in [\"ip4\", \"ipv4\"]:\n                host[\"hostname\"] = device[\"primary_ip4\"][\"address\"].split(\"/\")[0]\n            elif device[\"primary_ip6\"] and primary_ip in [\"ip6\", \"ipv6\"]:\n                host[\"hostname\"] = device[\"primary_ip6\"][\"address\"].split(\"/\")[0]\n            else:\n                host[\"hostname\"] = name\n        # add netbox data to host's data\n        if nbdata is True:\n            host[\"data\"].update(device)\n\n    # return if no hosts found for provided parameters\n    if not hosts:\n        log.warning(f\"{self.name} - no viable hosts returned by Netbox\")\n        return ret\n\n    # add interfaces data\n    if interfaces:\n        # decide on get_interfaces arguments\n        kwargs = interfaces if isinstance(interfaces, dict) else {}\n        # add 'interfaces' key to all hosts' data\n        for host in hosts.values():\n            host[\"data\"].setdefault(\"interfaces\", {})\n        # query interfaces data from netbox\n        nb_interfaces = self.get_interfaces(\n            job=job, devices=list(hosts), instance=instance, **kwargs\n        )\n        # save interfaces data to hosts' inventory\n        while nb_interfaces.result:\n            device, device_interfaces = nb_interfaces.result.popitem()\n            hosts[device][\"data\"][\"interfaces\"] = device_interfaces\n\n    # add connections data\n    if connections:\n        # decide on get_interfaces arguments\n        kwargs = connections if isinstance(connections, dict) else {}\n        # add 'connections' key to all hosts' data\n        for host in hosts.values():\n            host[\"data\"].setdefault(\"connections\", {})\n        # query connections data from netbox\n        nb_connections = self.get_connections(\n            job=job, devices=list(hosts), instance=instance, **kwargs\n        )\n        # save connections data to hosts' inventory\n        while nb_connections.result:\n            device, device_connections = nb_connections.result.popitem()\n            hosts[device][\"data\"][\"connections\"] = device_connections\n\n    # add circuits data\n    if circuits:\n        # decide on get_interfaces arguments\n        kwargs = circuits if isinstance(circuits, dict) else {}\n        # add 'circuits' key to all hosts' data\n        for host in hosts.values():\n            host[\"data\"].setdefault(\"circuits\", {})\n        # query circuits data from netbox\n        nb_circuits = self.get_circuits(\n            job=job, devices=list(hosts), instance=instance, **kwargs\n        )\n        # save circuits data to hosts' inventory\n        while nb_circuits.result:\n            device, device_circuits = nb_circuits.result.popitem()\n            hosts[device][\"data\"][\"circuits\"] = device_circuits\n\n    # add bgp peerings data\n    if bgp_peerings:\n        # decide on get_interfaces arguments\n        kwargs = bgp_peerings if isinstance(bgp_peerings, dict) else {}\n        # add 'bgp_peerings' key to all hosts' data\n        for host in hosts.values():\n            host[\"data\"].setdefault(\"bgp_peerings\", {})\n        # query bgp_peerings data from netbox\n        nb_bgp_peerings = self.get_bgp_peerings(\n            job=job, devices=list(hosts), instance=instance, **kwargs\n        )\n        # save circuits data to hosts' inventory\n        while nb_bgp_peerings.result:\n            device, device_bgp_peerings = nb_bgp_peerings.result.popitem()\n            hosts[device][\"data\"][\"bgp_peerings\"] = device_bgp_peerings\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.get_nornir_hosts","title":"<code>get_nornir_hosts(kwargs: dict, timeout: int) -&gt; List[str]</code>","text":"<p>Retrieves a list of unique Nornir hosts from Nornir service based on provided filter criteria.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>Dictionary of keyword arguments, where keys starting with 'F' are used as filters.</p> required <code>timeout</code> <code>int</code> <p>Timeout value (in seconds) for the job execution.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>Sorted list of unique Nornir host names that match the filter criteria.</p> Notes <ul> <li>Only filters with keys starting with 'F' are considered.</li> <li>Hosts are collected from all workers where the job did not fail.</li> </ul> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>def get_nornir_hosts(self, kwargs: dict, timeout: int) -&gt; List[str]:\n    \"\"\"\n    Retrieves a list of unique Nornir hosts from Nornir service based on provided filter criteria.\n\n    Args:\n        kwargs (dict): Dictionary of keyword arguments, where keys starting with 'F' are used as filters.\n        timeout (int): Timeout value (in seconds) for the job execution.\n\n    Returns:\n        list: Sorted list of unique Nornir host names that match the filter criteria.\n\n    Notes:\n        - Only filters with keys starting with 'F' are considered.\n        - Hosts are collected from all workers where the job did not fail.\n    \"\"\"\n    ret = []\n    filters = {k: v for k, v in kwargs.items() if k.startswith(\"F\")}\n    if filters:\n        nornir_hosts = self.client.run_job(\n            \"nornir\",\n            \"get_nornir_hosts\",\n            kwargs=filters,\n            workers=\"all\",\n            timeout=timeout,\n        )\n        for w, r in nornir_hosts.items():\n            if r[\"failed\"] is False and isinstance(r[\"result\"], list):\n                ret.extend(r[\"result\"])\n\n    return list(sorted(set(ret)))\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.sync_device_facts","title":"<code>sync_device_facts(job: Job, instance: Union[None, str] = None, dry_run: bool = False, datasource: str = 'nornir', timeout: int = 60, devices: Union[None, list] = None, batch_size: int = 10, branch: str = None, **kwargs: Any) -&gt; Result</code>","text":"<p>Updates device facts in NetBox, this task updates this device attributes:</p> <ul> <li>serial number</li> </ul> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>instance</code> <code>str</code> <p>The NetBox instance to use.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, no changes will be made to NetBox.</p> <code>False</code> <code>datasource</code> <code>str</code> <p>The data source to use. Supported datasources:</p> <ul> <li>nornir - uses Nornir Service parse task to retrieve devices' data     using NAPALM <code>get_facts</code> getter</li> </ul> <code>'nornir'</code> <code>timeout</code> <code>int</code> <p>The timeout for the job execution. Defaults to 60.</p> <code>60</code> <code>devices</code> <code>list</code> <p>The list of devices to update.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The number of devices to process in each batch.</p> <code>10</code> <code>branch</code> <code>str</code> <p>Branch name to use, need to have branching plugin installed, automatically creates branch if it does not exist in Netbox.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the datasource job.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the results of the update operation.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If a device does not exist in NetBox.</p> <code>UnsupportedServiceError</code> <p>If the specified datasource is not supported.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"PATCH\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef sync_device_facts(\n    self,\n    job: Job,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    datasource: str = \"nornir\",\n    timeout: int = 60,\n    devices: Union[None, list] = None,\n    batch_size: int = 10,\n    branch: str = None,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Updates device facts in NetBox, this task updates this device attributes:\n\n    - serial number\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        instance (str, optional): The NetBox instance to use.\n        dry_run (bool, optional): If True, no changes will be made to NetBox.\n        datasource (str, optional): The data source to use. Supported datasources:\n\n            - **nornir** - uses Nornir Service parse task to retrieve devices' data\n                using NAPALM `get_facts` getter\n\n        timeout (int, optional): The timeout for the job execution. Defaults to 60.\n        devices (list, optional): The list of devices to update.\n        batch_size (int, optional): The number of devices to process in each batch.\n        branch (str, optional): Branch name to use, need to have branching plugin installed,\n            automatically creates branch if it does not exist in Netbox.\n        **kwargs: Additional keyword arguments to pass to the datasource job.\n\n    Returns:\n        dict: A dictionary containing the results of the update operation.\n\n    Raises:\n        Exception: If a device does not exist in NetBox.\n        UnsupportedServiceError: If the specified datasource is not supported.\n    \"\"\"\n    devices = devices or []\n    instance = instance or self.default_instance\n    ret = Result(\n        task=f\"{self.name}:sync_device_facts\",\n        resources=[instance],\n        dry_run=dry_run,\n        diff={},\n        result={},\n    )\n    nb = self._get_pynetbox(instance, branch=branch)\n    kwargs[\"add_details\"] = True\n\n    if datasource == \"nornir\":\n        # source hosts list from Nornir\n        if kwargs:\n            devices.extend(self.get_nornir_hosts(kwargs, timeout))\n            devices = list(set(devices))\n            job.event(f\"Syncing {len(devices)} devices\")\n        # fetch devices data from Netbox\n        nb_devices = self.get_devices(\n            job=job,\n            instance=instance,\n            devices=copy.copy(devices),\n            cache=\"refresh\",\n        ).result\n        # remove devices that does not exist in Netbox\n        for d in list(devices):\n            if d not in nb_devices:\n                msg = f\"'{d}' device does not exist in Netbox\"\n                ret.errors.append(msg)\n                log.error(msg)\n                devices.remove(d)\n        # iterate over devices in batches\n        for i in range(0, len(devices), batch_size):\n            kwargs[\"FL\"] = devices[i : i + batch_size]\n            kwargs[\"getters\"] = \"get_facts\"\n            job.event(f\"retrieving facts for devices {', '.join(kwargs['FL'])}\")\n            data = self.client.run_job(\n                \"nornir\",\n                \"parse\",\n                kwargs=kwargs,\n                workers=\"all\",\n                timeout=timeout,\n            )\n\n            # Collect devices to update in bulk\n            devices_to_update = []\n\n            for worker, results in data.items():\n                if results[\"failed\"]:\n                    msg = f\"{worker} get_facts failed, errors: {'; '.join(results['errors'])}\"\n                    ret.errors.append(msg)\n                    log.error(msg)\n                    continue\n                for host, host_data in results[\"result\"].items():\n                    if host_data[\"napalm_get\"][\"failed\"]:\n                        msg = f\"{host} facts update failed: '{host_data['napalm_get']['exception']}'\"\n                        ret.errors.append(msg)\n                        log.error(msg)\n                        continue\n\n                    nb_device = nb_devices[host]\n\n                    facts = host_data[\"napalm_get\"][\"result\"][\"get_facts\"]\n                    desired_state = {\n                        \"serial\": facts[\"serial_number\"],\n                    }\n                    current_state = {\n                        \"serial\": nb_device[\"serial\"],\n                    }\n\n                    # Compare and get fields that need updating\n                    updates, diff = compare_netbox_object_state(\n                        desired_state=desired_state,\n                        current_state=current_state,\n                    )\n\n                    # Only update if there are changes\n                    if updates:\n                        updates[\"id\"] = int(nb_device[\"id\"])\n                        devices_to_update.append(updates)\n                        ret.diff[host] = diff\n\n                    ret.result[host] = {\n                        (\n                            \"sync_device_facts_dry_run\"\n                            if dry_run\n                            else \"sync_device_facts\"\n                        ): (updates if updates else \"Device facts in sync\")\n                    }\n                    if branch is not None:\n                        ret.result[host][\"branch\"] = branch\n\n            # Perform bulk update\n            if devices_to_update and not dry_run:\n                try:\n                    nb.dcim.devices.update(devices_to_update)\n                except Exception as e:\n                    ret.errors.append(f\"Bulk update failed: {e}\")\n    else:\n        raise UnsupportedServiceError(\n            f\"'{datasource}' datasource service not supported\"\n        )\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.sync_device_interfaces","title":"<code>sync_device_interfaces(job: Job, instance: Union[None, str] = None, dry_run: bool = False, datasource: str = 'nornir', timeout: int = 60, devices: Union[None, list] = None, create: bool = True, batch_size: int = 10, branch: str = None, **kwargs: Any) -&gt; Result</code>","text":"<p>Update or create device interfaces in Netbox using devices interfaces data sourced via Nornir service <code>parse</code> task using NAPALM getter.</p> <p>Interface parameters updated:</p> <ul> <li>interface name</li> <li>interface description</li> <li>mtu</li> <li>mac address</li> <li>admin status</li> <li>speed</li> </ul> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata.</p> required <code>instance</code> <code>str</code> <p>The Netbox instance name to use.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, no changes will be made to Netbox.</p> <code>False</code> <code>datasource</code> <code>str</code> <p>The data source to use. Supported datasources:</p> <ul> <li>nornir - uses Nornir Service parse task to retrieve devices' data     using NAPALM get_interfaces getter</li> </ul> <code>'nornir'</code> <code>timeout</code> <code>int</code> <p>The timeout for the job.</p> <code>60</code> <code>devices</code> <code>list</code> <p>List of devices to update.</p> <code>None</code> <code>create</code> <code>bool</code> <p>If True, new interfaces will be created if they do not exist.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>The number of devices to process in each batch.</p> <code>10</code> <code>branch</code> <code>str</code> <p>Branch name to use, need to have branching plugin installed, automatically creates branch if it does not exist in Netbox.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the datasource job.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the results of the update operation.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If a device does not exist in Netbox.</p> <code>UnsupportedServiceError</code> <p>If the specified datasource is not supported.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"PATCH\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef sync_device_interfaces(\n    self,\n    job: Job,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    datasource: str = \"nornir\",\n    timeout: int = 60,\n    devices: Union[None, list] = None,\n    create: bool = True,\n    batch_size: int = 10,\n    branch: str = None,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Update or create device interfaces in Netbox using devices interfaces\n    data sourced via Nornir service `parse` task using NAPALM getter.\n\n    Interface parameters updated:\n\n    - interface name\n    - interface description\n    - mtu\n    - mac address\n    - admin status\n    - speed\n\n    Args:\n        job: NorFab Job object containing relevant metadata.\n        instance (str, optional): The Netbox instance name to use.\n        dry_run (bool, optional): If True, no changes will be made to Netbox.\n        datasource (str, optional): The data source to use. Supported datasources:\n\n            - **nornir** - uses Nornir Service parse task to retrieve devices' data\n                using NAPALM get_interfaces getter\n\n        timeout (int, optional): The timeout for the job.\n        devices (list, optional): List of devices to update.\n        create (bool, optional): If True, new interfaces will be created if they do not exist.\n        batch_size (int, optional): The number of devices to process in each batch.\n        branch (str, optional): Branch name to use, need to have branching plugin installed,\n            automatically creates branch if it does not exist in Netbox.\n        **kwargs: Additional keyword arguments to pass to the datasource job.\n\n    Returns:\n        dict: A dictionary containing the results of the update operation.\n\n    Raises:\n        Exception: If a device does not exist in Netbox.\n        UnsupportedServiceError: If the specified datasource is not supported.\n    \"\"\"\n    devices = devices or []\n    instance = instance or self.default_instance\n    ret = Result(\n        task=f\"{self.name}:sync_device_interfaces\",\n        result={},\n        resources=[instance],\n        dry_run=dry_run,\n        diff={},\n    )\n    nb = self._get_pynetbox(instance, branch=branch)\n    kwargs[\"add_details\"] = True\n\n    if datasource == \"nornir\":\n        # source hosts list from Nornir\n        if kwargs:\n            devices.extend(self.get_nornir_hosts(kwargs, timeout))\n            devices = list(set(devices))\n            job.event(f\"syncing {len(devices)} devices\")\n\n        # fetch devices interfaces data from Netbox\n        nb_interfaces_data = self.get_interfaces(\n            job=job,\n            instance=instance,\n            devices=copy.copy(devices),\n            cache=\"refresh\",\n        ).result\n\n        # fetch devices data from Netbox\n        nb_devices_data = self.get_devices(\n            job=job,\n            instance=instance,\n            devices=copy.copy(devices),\n        ).result\n\n        # iterate over devices in batches\n        for i in range(0, len(devices), batch_size):\n            kwargs[\"FL\"] = devices[i : i + batch_size]\n            kwargs[\"getters\"] = \"get_interfaces\"\n            job.event(\n                f\"retrieving interfaces for devices {', '.join(kwargs['FL'])}\"\n            )\n            data = self.client.run_job(\n                \"nornir\",\n                \"parse\",\n                kwargs=kwargs,\n                workers=\"all\",\n                timeout=timeout,\n            )\n\n            # Collect interfaces to update and create in bulk\n            interfaces_to_update = []\n            interfaces_to_create = []\n            mac_addresses_to_create = []\n\n            for worker, results in data.items():\n                if results[\"failed\"]:\n                    msg = f\"{worker} get_interfaces failed, errors: {'; '.join(results['errors'])}\"\n                    ret.errors.append(msg)\n                    log.error(msg)\n                    continue\n\n                for host, host_data in results[\"result\"].items():\n                    if host_data[\"napalm_get\"][\"failed\"]:\n                        msg = f\"{host} interfaces update failed: '{host_data['napalm_get']['exception']}'\"\n                        ret.errors.append(msg)\n                        log.error(msg)\n                        continue\n\n                    nb_interfaces = nb_interfaces_data.get(host, {})\n                    if not nb_interfaces:\n                        msg = f\"'{host}' has no interfaces in Netbox, skipping\"\n                        ret.errors.append(msg)\n                        log.warning(msg)\n                        continue\n\n                    # Get device ID for creating new interfaces\n                    nb_device = nb_devices_data.get(host)\n                    if not nb_device:\n                        msg = f\"'{host}' does not exist in Netbox\"\n                        ret.errors.append(msg)\n                        log.error(msg)\n                        continue\n\n                    interfaces = host_data[\"napalm_get\"][\"result\"][\"get_interfaces\"]\n\n                    sync_key = \"sync_device_interfaces\"\n                    create_key = \"created_device_interfaces\"\n                    if dry_run:\n                        sync_key = \"sync_device_interfaces_dry_run\"\n                        create_key = \"created_device_interfaces_dry_run\"\n                    ret.result[host] = {\n                        sync_key: {},\n                        create_key: {},\n                    }\n                    if branch is not None:\n                        ret.result[host][\"branch\"] = branch\n\n                    # Process network device interfaces\n                    for intf_name, interface_data in interfaces.items():\n                        if intf_name in nb_interfaces:\n                            # Interface exists - prepare update\n                            nb_intf = nb_interfaces[intf_name]\n\n                            # Build desired state\n                            desired_state = {\n                                \"description\": interface_data.get(\n                                    \"description\", \"\"\n                                ),\n                                \"enabled\": interface_data.get(\"is_enabled\", True),\n                            }\n                            if 10000 &gt; interface_data.get(\"mtu\", 0) &gt; 0:\n                                desired_state[\"mtu\"] = interface_data[\"mtu\"]\n                            if interface_data.get(\"speed\", 0) &gt; 0:\n                                desired_state[\"speed\"] = (\n                                    interface_data[\"speed\"] * 1000\n                                )\n\n                            # Build current state\n                            current_state = {\n                                \"description\": nb_intf.get(\"description\", \"\"),\n                                \"enabled\": nb_intf.get(\"enabled\", True),\n                            }\n                            if nb_intf.get(\"mtu\"):\n                                current_state[\"mtu\"] = nb_intf[\"mtu\"]\n                            if nb_intf.get(\"speed\"):\n                                current_state[\"speed\"] = nb_intf[\"speed\"]\n\n                            # Compare and get fields that need updating\n                            updates, diff = compare_netbox_object_state(\n                                desired_state=desired_state,\n                                current_state=current_state,\n                            )\n\n                            # Only update if there are changes\n                            if updates:\n                                updates[\"id\"] = int(nb_intf[\"id\"])\n                                interfaces_to_update.append(updates)\n                                ret.diff.setdefault(host, {})[intf_name] = diff\n\n                            ret.result[host][sync_key][intf_name] = (\n                                updates if updates else \"Interface in sync\"\n                            )\n\n                            mac_address = (\n                                interface_data.get(\"mac_address\", \"\")\n                                .strip()\n                                .lower()\n                            )\n                            if mac_address and mac_address not in [\"none\", \"\"]:\n                                # Check if MAC already exists\n                                for nb_mac in nb_intf.get(\"mac_addresses\") or []:\n                                    if (\n                                        nb_mac.get(\"mac_address\", \"\").lower()\n                                        == mac_address\n                                    ):\n                                        break\n                                else:\n                                    # Prepare MAC address for creation\n                                    mac_addresses_to_create.append(\n                                        {\n                                            \"mac_address\": mac_address,\n                                            \"assigned_object_type\": \"dcim.interface\",\n                                            \"assigned_object_id\": int(\n                                                nb_intf[\"id\"]\n                                            ),\n                                        }\n                                    )\n                        elif create:\n                            # Interface doesn't exist - prepare creation\n                            new_intf = {\n                                \"name\": intf_name,\n                                \"device\": int(nb_device[\"id\"]),\n                                \"type\": \"other\",\n                                \"description\": interface_data.get(\n                                    \"description\", \"\"\n                                ),\n                                \"enabled\": interface_data.get(\"is_enabled\", True),\n                            }\n                            if 10000 &gt; interface_data.get(\"mtu\", 0) &gt; 0:\n                                new_intf[\"mtu\"] = interface_data[\"mtu\"]\n                            if interface_data.get(\"speed\", 0) &gt; 0:\n                                new_intf[\"speed\"] = interface_data[\"speed\"] * 1000\n\n                            mac_address = (\n                                interface_data.get(\"mac_address\", \"\")\n                                .strip()\n                                .lower()\n                            )\n                            if mac_address and mac_address not in [\"none\", \"\"]:\n                                mac_addresses_to_create.append(\n                                    {\n                                        \"mac_address\": mac_address,\n                                        \"assigned_object_type\": \"dcim.interface\",\n                                        \"assigned_object_id\": int(nb_intf[\"id\"]),\n                                    }\n                                )\n\n                            interfaces_to_create.append(new_intf)\n                            ret.result[host][create_key][intf_name] = new_intf\n\n            # Perform bulk updates and creations\n            if interfaces_to_update and not dry_run:\n                try:\n                    nb.dcim.interfaces.update(interfaces_to_update)\n                    job.event(\n                        f\"Bulk updated {len(interfaces_to_update)} interfaces\"\n                    )\n                except Exception as e:\n                    msg = f\"Bulk interface update failed: {e}\"\n                    ret.errors.append(msg)\n                    log.error(msg)\n\n            if interfaces_to_create and not dry_run:\n                try:\n                    _ = nb.dcim.interfaces.create(interfaces_to_create)\n                    job.event(\n                        f\"Bulk created {len(interfaces_to_create)} interfaces\"\n                    )\n                except Exception as e:\n                    msg = f\"Bulk interface creation failed: {e}\"\n                    ret.errors.append(msg)\n                    log.error(msg)\n\n            # Bulk create MAC addresses\n            if mac_addresses_to_create and not dry_run:\n                try:\n                    nb.dcim.mac_addresses.create(mac_addresses_to_create)\n                    job.event(\n                        f\"Bulk created {len(mac_addresses_to_create)} MAC addresses\"\n                    )\n                except Exception as e:\n                    msg = f\"Bulk MAC address creation failed: {e}\"\n                    ret.errors.append(msg)\n                    log.error(msg)\n\n    else:\n        raise UnsupportedServiceError(\n            f\"'{datasource}' datasource service not supported\"\n        )\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.update_interfaces_description","title":"<code>update_interfaces_description(job: Job, devices: list, description_template: str = None, descriptions: dict = None, interfaces: Union[None, list] = None, interface_regex: Union[None, str] = None, instance: Union[None, str] = None, dry_run: bool = False, timeout: int = 60, branch: str = None) -&gt; Result</code>","text":"<p>Updates the description of interfaces for specified devices in NetBox.</p> <p>This method retrieves interface connections for the given devices, renders new descriptions using a Jinja2 template, and updates the interface descriptions in NetBox accordingly.</p> <p>Only interfaces, console ports and console server ports supported.</p> <p>Jinja2 environment receives these context variables for description template rendering:</p> <ul> <li>device - pynetbox <code>dcim.device</code> object</li> <li>interface - pynetbox object - <code>dcim/interface</code>, <code>dcip.consoleport</code>,     <code>dcim.consoleserverport</code> - depending on what kind of interface is that.</li> <li>remote_device - string</li> <li>remote_interface - string</li> <li>termination_type - string</li> <li>cable - dictionary of directly attached cable attributes:<ul> <li>type</li> <li>status</li> <li>tenant - dictionary of <code>{name: tenant_name}</code></li> <li>label</li> <li>tags - list of <code>{name: tag_name}</code> dictionaries</li> <li>custom_fields - dictionary with custom fields data</li> <li>peer_termination_type</li> <li>peer_device</li> <li>peer_interface</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job context for logging and event handling.</p> required <code>devices</code> <code>list</code> <p>List of device names to update interfaces for.</p> required <code>description_template</code> <code>str</code> <p>Jinja2 template string for the interface description. Can reference remote template using <code>nf://path/to/template.txt</code>.</p> <code>None</code> <code>descriptions</code> <code>dict</code> <p>Dictionary keyed by interface names with values being interface description strings</p> <code>None</code> <code>interfaces</code> <code>Union[None, list]</code> <p>Specific interfaces to update.</p> <code>None</code> <code>interface_regex</code> <code>Union[None, str]</code> <p>Regex pattern to filter interfaces.</p> <code>None</code> <code>instance</code> <code>Union[None, str]</code> <p>NetBox instance identifier.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, performs a dry run without saving changes.</p> <code>False</code> <code>timeout</code> <code>int</code> <p>Timeout for NetBox API requests.</p> <code>60</code> <code>branch</code> <code>str</code> <p>Branch name for NetBox instance.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the outcome of the update operation, including before and after descriptions.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"PATCH\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef update_interfaces_description(\n    self,\n    job: Job,\n    devices: list,\n    description_template: str = None,\n    descriptions: dict = None,\n    interfaces: Union[None, list] = None,\n    interface_regex: Union[None, str] = None,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    timeout: int = 60,\n    branch: str = None,\n) -&gt; Result:\n    \"\"\"\n    Updates the description of interfaces for specified devices in NetBox.\n\n    This method retrieves interface connections for the given devices, renders\n    new descriptions using a Jinja2 template, and updates the interface descriptions\n    in NetBox accordingly.\n\n    Only interfaces, console ports and console server ports supported.\n\n    Jinja2 environment receives these context variables for description template rendering:\n\n    - device - pynetbox `dcim.device` object\n    - interface - pynetbox object - `dcim/interface`, `dcip.consoleport`,\n        `dcim.consoleserverport` - depending on what kind of interface is that.\n    - remote_device - string\n    - remote_interface - string\n    - termination_type - string\n    - cable - dictionary of directly attached cable attributes:\n        - type\n        - status\n        - tenant - dictionary of `{name: tenant_name}`\n        - label\n        - tags - list of `{name: tag_name}` dictionaries\n        - custom_fields - dictionary with custom fields data\n        - peer_termination_type\n        - peer_device\n        - peer_interface\n\n    Args:\n        job (Job): The job context for logging and event handling.\n        devices (list): List of device names to update interfaces for.\n        description_template (str): Jinja2 template string for the interface description.\n            Can reference remote template using `nf://path/to/template.txt`.\n        descriptions (dict): Dictionary keyed by interface names with values being interface\n            description strings\n        interfaces (Union[None, list], optional): Specific interfaces to update.\n        interface_regex (Union[None, str], optional): Regex pattern to filter interfaces.\n        instance (Union[None, str], optional): NetBox instance identifier.\n        dry_run (bool, optional): If True, performs a dry run without saving changes.\n        timeout (int, optional): Timeout for NetBox API requests.\n        branch (str, optional): Branch name for NetBox instance.\n\n    Returns:\n        Result: An object containing the outcome of the update operation, including\n            before and after descriptions.\n    \"\"\"\n    result = {}\n    instance = instance or self.default_instance\n    ret = Result(\n        task=f\"{self.name}:update_interfaces_description\",\n        result=result,\n        resources=[instance],\n    )\n    nb = self._get_pynetbox(instance, branch=branch)\n\n    if description_template:\n        # get list of all interfaces connections\n        nb_connections = self.get_connections(\n            job=job,\n            devices=devices,\n            interface_regex=interface_regex,\n            instance=instance,\n            include_virtual=True,\n            cables=True,\n        )\n        # produce interfaces description and update it\n        while nb_connections.result:\n            device, device_connections = nb_connections.result.popitem()\n            ret.result.setdefault(device, {})\n            for interface, connection in device_connections.items():\n                job.event(f\"{device}:{interface} updating description\")\n                if connection[\"termination_type\"] == \"consoleport\":\n                    nb_interface = nb.dcim.console_ports.get(\n                        device=device, name=interface\n                    )\n                elif connection[\"termination_type\"] == \"consoleserverport\":\n                    nb_interface = nb.dcim.console_server_ports.get(\n                        device=device, name=interface\n                    )\n                elif connection[\"termination_type\"] == \"powerport\":\n                    nb_interface = nb.dcim.power_ports.get(\n                        device=device, name=interface\n                    )\n                elif connection[\"termination_type\"] == \"poweroutlet\":\n                    nb_interface = nb.dcim.power_outlets.get(\n                        device=device, name=interface\n                    )\n                else:\n                    nb_interface = nb.dcim.interfaces.get(\n                        device=device, name=interface\n                    )\n                nb_device = nb.dcim.devices.get(name=device)\n                rendered_description = self.jinja2_render_templates(\n                    templates=[description_template],\n                    context={\n                        \"device\": nb_device,\n                        \"interface\": nb_interface,\n                        **connection,\n                    },\n                )\n                rendered_description = str(rendered_description).strip()\n                ret.result[device][interface] = {\n                    \"-\": str(nb_interface.description),\n                    \"+\": rendered_description,\n                }\n                nb_interface.description = rendered_description\n                if dry_run is False:\n                    nb_interface.save()\n    if descriptions:\n        for device in devices:\n            ret.result.setdefault(device, {})\n            for interface, description in descriptions.items():\n                nb_interface = nb.dcim.interfaces.get(name=interface, device=device)\n                if nb_interface:\n                    ret.result[device][interface] = {\n                        \"-\": str(nb_interface.description),\n                        \"+\": description,\n                    }\n                    nb_interface.description = description\n                    if dry_run is False:\n                        nb_interface.save()\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.sync_device_ip","title":"<code>sync_device_ip(job: Job, instance: Union[None, str] = None, dry_run: bool = False, datasource: str = 'nornir', timeout: int = 60, devices: Union[None, list] = None, create: bool = True, batch_size: int = 10, branch: str = None, **kwargs: Any) -&gt; Result</code>","text":"<p>Update the IP addresses of devices in Netbox.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>instance</code> <code>str</code> <p>The Netbox instance name to use.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, no changes will be made.</p> <code>False</code> <code>datasource</code> <code>str</code> <p>The data source to use. Supported datasources:</p> <ul> <li>nornir - uses Nornir Service parse task to retrieve devices' data     using NAPALM get_interfaces_ip getter</li> </ul> <code>'nornir'</code> <code>timeout</code> <code>int</code> <p>The timeout for the operation.</p> <code>60</code> <code>devices</code> <code>list</code> <p>The list of devices to update.</p> <code>None</code> <code>create</code> <code>bool</code> <p>If True, new IP addresses will be created if they do not exist.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>The number of devices to process in each batch.</p> <code>10</code> <code>branch</code> <code>str</code> <p>Branch name to use, need to have branching plugin installed, automatically creates branch if it does not exist in Netbox.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the results of the update operation.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If a device does not exist in Netbox.</p> <code>UnsupportedServiceError</code> <p>If the specified datasource is not supported.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"PATCH\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef sync_device_ip(\n    self,\n    job: Job,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    datasource: str = \"nornir\",\n    timeout: int = 60,\n    devices: Union[None, list] = None,\n    create: bool = True,\n    batch_size: int = 10,\n    branch: str = None,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Update the IP addresses of devices in Netbox.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        instance (str, optional): The Netbox instance name to use.\n        dry_run (bool, optional): If True, no changes will be made.\n        datasource (str, optional): The data source to use. Supported datasources:\n\n            - **nornir** - uses Nornir Service parse task to retrieve devices' data\n                using NAPALM get_interfaces_ip getter\n\n        timeout (int, optional): The timeout for the operation.\n        devices (list, optional): The list of devices to update.\n        create (bool, optional): If True, new IP addresses will be created if they do not exist.\n        batch_size (int, optional): The number of devices to process in each batch.\n        branch (str, optional): Branch name to use, need to have branching plugin installed,\n            automatically creates branch if it does not exist in Netbox.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the results of the update operation.\n\n    Raises:\n        Exception: If a device does not exist in Netbox.\n        UnsupportedServiceError: If the specified datasource is not supported.\n    \"\"\"\n    result = {}\n    devices = devices or []\n    instance = instance or self.default_instance\n    ret = Result(\n        task=f\"{self.name}:sync_device_ip\", result=result, resources=[instance]\n    )\n    nb = self._get_pynetbox(instance, branch=branch)\n\n    if datasource == \"nornir\":\n        # source hosts list from Nornir\n        if kwargs:\n            devices.extend(self.get_nornir_hosts(kwargs, timeout))\n        # iterate over devices in batches\n        for i in range(0, len(devices), batch_size):\n            kwargs[\"FL\"] = devices[i : i + batch_size]\n            kwargs[\"getters\"] = \"get_interfaces_ip\"\n            data = self.client.run_job(\n                \"nornir\",\n                \"parse\",\n                kwargs=kwargs,\n                workers=\"all\",\n                timeout=timeout,\n            )\n            for worker, results in data.items():\n                if results[\"failed\"]:\n                    log.error(\n                        f\"{worker} get_interfaces_ip failed, errors: {'; '.join(results['errors'])}\"\n                    )\n                    continue\n                for host, host_data in results[\"result\"].items():\n                    updated, created = {}, {}\n                    result[host] = {\n                        \"sync_ip_dry_run\" if dry_run else \"sync_ip\": updated,\n                        \"created_ip_dry_run\" if dry_run else \"created_ip\": created,\n                    }\n                    if branch is not None:\n                        result[host][\"branch\"] = branch\n                    interfaces = host_data[\"napalm_get\"][\"get_interfaces_ip\"]\n                    nb_device = nb.dcim.devices.get(name=host)\n                    if not nb_device:\n                        raise Exception(f\"'{host}' does not exist in Netbox\")\n                    nb_interfaces = nb.dcim.interfaces.filter(\n                        device_id=nb_device.id\n                    )\n                    # update interface IP addresses\n                    for nb_interface in nb_interfaces:\n                        if nb_interface.name not in interfaces:\n                            continue\n                        interface = interfaces.pop(nb_interface.name)\n                        # merge v6 into v4 addresses to save code repetition\n                        ips = {\n                            **interface.get(\"ipv4\", {}),\n                            **interface.get(\"ipv6\", {}),\n                        }\n                        # update/create IP addresses\n                        for ip, ip_data in ips.items():\n                            prefix_length = ip_data[\"prefix_length\"]\n                            # get IP address info from Netbox\n                            nb_ip = nb.ipam.ip_addresses.filter(\n                                address=f\"{ip}/{prefix_length}\"\n                            )\n                            if len(nb_ip) &gt; 1:\n                                log.warning(\n                                    f\"{host} got multiple {ip}/{prefix_length} IP addresses from Netbox, \"\n                                    f\"NorFab Netbox Service only supports handling of non-duplicate IPs.\"\n                                )\n                                continue\n                            # decide what to do\n                            if not nb_ip and create is False:\n                                continue\n                            elif not nb_ip and create is True:\n                                if dry_run is not True:\n                                    try:\n                                        nb_ip = nb.ipam.ip_addresses.create(\n                                            address=f\"{ip}/{prefix_length}\"\n                                        )\n                                    except Exception as e:\n                                        msg = f\"{host} failed to create {ip}/{prefix_length}, error: {e}\"\n                                        log.error(msg)\n                                        job.event(msg, resource=instance)\n                                        continue\n                                    nb_ip.assigned_object_type = \"dcim.interface\"\n                                    nb_ip.assigned_object_id = nb_interface.id\n                                    nb_ip.status = \"active\"\n                                    nb_ip.save()\n                                created[f\"{ip}/{prefix_length}\"] = nb_interface.name\n                                job.event(\n                                    f\"{host} created IP address {ip}/{prefix_length} for {nb_interface.name} interface\",\n                                    resource=instance,\n                                )\n                            elif nb_ip:\n                                nb_ip = list(nb_ip)[0]\n                                if dry_run is not True:\n                                    nb_ip.assigned_object_type = \"dcim.interface\"\n                                    nb_ip.assigned_object_id = nb_interface.id\n                                    nb_ip.status = \"active\"\n                                    nb_ip.save()\n                                updated[nb_ip.address] = nb_interface.name\n                                job.event(\n                                    f\"{host} updated IP address {ip}/{prefix_length} for {nb_interface.name} interface\",\n                                    resource=instance,\n                                )\n\n    else:\n        raise UnsupportedServiceError(\n            f\"'{datasource}' datasource service not supported\"\n        )\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.create_ip","title":"<code>create_ip(job: Job, prefix: Union[str, dict], device: Union[None, str] = None, interface: Union[None, str] = None, description: Union[None, str] = None, vrf: Union[None, str] = None, tags: Union[None, list] = None, dns_name: Union[None, str] = None, tenant: Union[None, str] = None, comments: Union[None, str] = None, role: Union[None, str] = None, status: Union[None, str] = None, is_primary: Union[None, bool] = None, instance: Union[None, str] = None, dry_run: Union[None, bool] = False, branch: Union[None, str] = None, mask_len: Union[None, int] = None, create_peer_ip: Union[None, bool] = True) -&gt; Result</code>","text":"<p>Allocate the next available IP address from a given subnet.</p> <p>This task finds or creates an IP address in NetBox, updates its metadata, optionally links it to a device/interface, and supports a dry run mode for previewing changes.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The prefix from which to allocate the IP address, could be:</p> <ul> <li>IPv4 prefix string e.g. 10.0.0.0/24</li> <li>IPv6 prefix string e.g. 2001::/64</li> <li>Prefix description string to filter by</li> <li>Dictionary with prefix filters to feed <code>pynetbox</code> get method     e.g. <code>{\"prefix\": \"10.0.0.0/24\", \"site__name\": \"foo\"}</code></li> </ul> required <code>description</code> <code>str</code> <p>A description for the allocated IP address.</p> <code>None</code> <code>device</code> <code>str</code> <p>The device associated with the IP address.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface associated with the IP address.</p> <code>None</code> <code>vrf</code> <code>str</code> <p>The VRF (Virtual Routing and Forwarding) instance.</p> <code>None</code> <code>tags</code> <code>list</code> <p>A list of tags to associate with the IP address.</p> <code>None</code> <code>dns_name</code> <code>str</code> <p>The DNS name for the IP address.</p> <code>None</code> <code>tenant</code> <code>str</code> <p>The tenant associated with the IP address.</p> <code>None</code> <code>comments</code> <code>str</code> <p>Additional comments for the IP address.</p> <code>None</code> <code>instance</code> <code>str</code> <p>The NetBox instance to use.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, do not actually allocate the IP address.</p> <code>False</code> <code>branch</code> <code>str</code> <p>Branch name to use, need to have branching plugin installed, automatically creates branch if it does not exist in Netbox.</p> <code>None</code> <code>mask_len</code> <code>int</code> <p>mask length to use for IP address on creation or to update existing IP address. On new IP address creation will create child subnet of <code>mask_len</code> within parent <code>prefix</code>, new subnet not created for existing IP addresses. <code>mask_len</code> argument ignored on dry run and ip allocated from parent prefix directly.</p> <code>None</code> <code>create_peer_ip</code> <code>bool</code> <p>If True creates IP address for link peer - remote device interface connected to requested device and interface</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the result of the IP allocation.</p> <p>Tasks execution follow these steps:</p> <ol> <li> <p>Tries to find an existing IP in NetBox matching the device/interface/description.     If found, uses it; otherwise, proceeds to create a new IP.</p> </li> <li> <p>If prefix is a string, determines if it\u2019s an IP network or a description.     Builds a filter dictionary for NetBox queries, optionally including VRF.</p> </li> <li> <p>Queries NetBox for the prefix using the constructed filter.</p> </li> <li> <p>If dry_run is True, fetches the next available IP but doesn\u2019t create it.</p> </li> <li> <p>If not a dry run, creates the next available IP in the prefix.</p> </li> <li> <p>Updates IP attributes (description, VRF, tenant, DNS name, comments, role, tags)     if provided and different from current values. Handles interface assignment and     can set the IP as primary for the device.</p> </li> <li> <p>If changes were made and not a dry run, saves the IP and device updates to NetBox.</p> </li> </ol> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"POST\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef create_ip(\n    self,\n    job: Job,\n    prefix: Union[str, dict],\n    device: Union[None, str] = None,\n    interface: Union[None, str] = None,\n    description: Union[None, str] = None,\n    vrf: Union[None, str] = None,\n    tags: Union[None, list] = None,\n    dns_name: Union[None, str] = None,\n    tenant: Union[None, str] = None,\n    comments: Union[None, str] = None,\n    role: Union[None, str] = None,\n    status: Union[None, str] = None,\n    is_primary: Union[None, bool] = None,\n    instance: Union[None, str] = None,\n    dry_run: Union[None, bool] = False,\n    branch: Union[None, str] = None,\n    mask_len: Union[None, int] = None,\n    create_peer_ip: Union[None, bool] = True,\n) -&gt; Result:\n    \"\"\"\n    Allocate the next available IP address from a given subnet.\n\n    This task finds or creates an IP address in NetBox, updates its metadata,\n    optionally links it to a device/interface, and supports a dry run mode for\n    previewing changes.\n\n    Args:\n        prefix (str): The prefix from which to allocate the IP address, could be:\n\n            - IPv4 prefix string e.g. 10.0.0.0/24\n            - IPv6 prefix string e.g. 2001::/64\n            - Prefix description string to filter by\n            - Dictionary with prefix filters to feed `pynetbox` get method\n                e.g. `{\"prefix\": \"10.0.0.0/24\", \"site__name\": \"foo\"}`\n\n        description (str, optional): A description for the allocated IP address.\n        device (str, optional): The device associated with the IP address.\n        interface (str, optional): The interface associated with the IP address.\n        vrf (str, optional): The VRF (Virtual Routing and Forwarding) instance.\n        tags (list, optional): A list of tags to associate with the IP address.\n        dns_name (str, optional): The DNS name for the IP address.\n        tenant (str, optional): The tenant associated with the IP address.\n        comments (str, optional): Additional comments for the IP address.\n        instance (str, optional): The NetBox instance to use.\n        dry_run (bool, optional): If True, do not actually allocate the IP address.\n        branch (str, optional): Branch name to use, need to have branching plugin\n            installed, automatically creates branch if it does not exist in Netbox.\n        mask_len (int, optional): mask length to use for IP address on creation or to\n            update existing IP address. On new IP address creation will create child\n            subnet of `mask_len` within parent `prefix`, new subnet not created for\n            existing IP addresses. `mask_len` argument ignored on dry run and ip allocated\n            from parent prefix directly.\n        create_peer_ip (bool, optional): If True creates IP address for link peer -\n            remote device interface connected to requested device and interface\n\n    Returns:\n        dict: A dictionary containing the result of the IP allocation.\n\n    Tasks execution follow these steps:\n\n    1. Tries to find an existing IP in NetBox matching the device/interface/description.\n        If found, uses it; otherwise, proceeds to create a new IP.\n\n    2. If prefix is a string, determines if it\u2019s an IP network or a description.\n        Builds a filter dictionary for NetBox queries, optionally including VRF.\n\n    3. Queries NetBox for the prefix using the constructed filter.\n\n    4. If dry_run is True, fetches the next available IP but doesn\u2019t create it.\n\n    5. If not a dry run, creates the next available IP in the prefix.\n\n    6. Updates IP attributes (description, VRF, tenant, DNS name, comments, role, tags)\n        if provided and different from current values. Handles interface assignment and\n        can set the IP as primary for the device.\n\n    7. If changes were made and not a dry run, saves the IP and device updates to NetBox.\n    \"\"\"\n    instance = instance or self.default_instance\n    ret = Result(task=f\"{self.name}:create_ip\", result={}, resources=[instance])\n    tags = tags or []\n    has_changes = False\n    nb_ip = None\n    nb_device = None\n    create_peer_ip_data = {}\n    nb = self._get_pynetbox(instance, branch=branch)\n\n    # source parent prefix from Netbox\n    if isinstance(prefix, str):\n        # try converting prefix to network, if fails prefix is not an IP network\n        try:\n            _ = ipaddress.ip_network(prefix)\n            is_network = True\n        except:\n            is_network = False\n        if is_network is True and vrf:\n            prefix = {\"prefix\": prefix, \"vrf__name\": vrf}\n        elif is_network is True:\n            prefix = {\"prefix\": prefix}\n        elif is_network is False and vrf:\n            prefix = {\"description\": prefix, \"vrf__name\": vrf}\n        elif is_network is False:\n            prefix = {\"description\": prefix}\n    nb_prefix = nb.ipam.prefixes.get(**prefix)\n    if not nb_prefix:\n        raise NetboxAllocationError(\n            f\"Unable to source parent prefix from Netbox - {prefix}\"\n        )\n    parent_prefix_len = int(str(nb_prefix).split(\"/\")[1])\n\n    # try to source existing IP from netbox\n    if device and interface and description:\n        nb_ip = nb.ipam.ip_addresses.get(\n            device=device,\n            interface=interface,\n            description=description,\n            parent=str(nb_prefix),\n        )\n    elif device and interface:\n        nb_ip = nb.ipam.ip_addresses.get(\n            device=device, interface=interface, parent=str(nb_prefix)\n        )\n    elif description:\n        nb_ip = nb.ipam.ip_addresses.get(\n            description=description, parent=str(nb_prefix)\n        )\n\n    # create new IP address\n    if not nb_ip:\n        # check if interface has link peer that has IP within parent prefix\n        if device and interface:\n            connection = self.get_connections(\n                job=job,\n                devices=[device],\n                interface_regex=interface,\n                instance=instance,\n                include_virtual=True,\n            )\n            if interface in connection.result[device]:\n                peer = connection.result[device][interface]\n                # do not process breakout cables\n                if isinstance(peer[\"remote_interface\"], list):\n                    peer[\"remote_interface\"] = None\n                # try to source peer ip subnet\n                nb_peer_ip = None\n                if peer[\"remote_device\"] and peer[\"remote_interface\"]:\n                    nb_peer_ip = nb.ipam.ip_addresses.get(\n                        device=peer[\"remote_device\"],\n                        interface=peer[\"remote_interface\"],\n                        parent=str(nb_prefix),\n                    )\n                # try to source peer ip subnet\n                nb_peer_prefix = None\n                if nb_peer_ip:\n                    peer_ip = ipaddress.ip_interface(nb_peer_ip.address)\n                    nb_peer_prefix = nb.ipam.prefixes.get(\n                        prefix=str(peer_ip.network),\n                        vrf__name=vrf,\n                    )\n                elif create_peer_ip and peer[\"remote_interface\"]:\n                    create_peer_ip_data = {\n                        \"device\": peer[\"remote_device\"],\n                        \"interface\": peer[\"remote_interface\"],\n                        \"vrf\": vrf,\n                        \"branch\": branch,\n                        \"tenant\": tenant,\n                        \"dry_run\": dry_run,\n                        \"tags\": tags,\n                        \"status\": status,\n                        \"create_peer_ip\": False,\n                        \"instance\": instance,\n                    }\n                # use peer subnet to create IP address\n                if nb_peer_prefix:\n                    nb_prefix = nb_peer_prefix\n                    mask_len = None  # cancel subnet creation\n                    job.event(\n                        f\"Using link peer '{peer['remote_device']}:{peer['remote_interface']}' \"\n                        f\"prefix '{nb_peer_prefix}' to create IP address\"\n                    )\n        # if mask_len provided create new subnet\n        if mask_len and not dry_run and mask_len != parent_prefix_len:\n            if mask_len &lt; parent_prefix_len:\n                raise ValueError(\n                    f\"Mask length '{mask_len}' must be longer then '{parent_prefix_len}' prefix length\"\n                )\n            prefix_status = status\n            if prefix_status not in [\"active\", \"reserved\", \"deprecated\"]:\n                prefix_status = None\n            child_subnet = self.create_prefix(\n                job=job,\n                parent=str(nb_prefix),\n                prefixlen=mask_len,\n                vrf=vrf,\n                tags=tags,\n                tenant=tenant,\n                status=prefix_status,\n                instance=instance,\n                branch=branch,\n            )\n            prefix = {\"prefix\": child_subnet.result[\"prefix\"]}\n            if vrf:\n                prefix[\"vrf__name\"] = vrf\n            nb_prefix = nb.ipam.prefixes.get(**prefix)\n\n            if not nb_prefix:\n                raise NetboxAllocationError(\n                    f\"Unable to source child prefix of mask length \"\n                    f\"'{mask_len}' from '{prefix}' parent prefix\"\n                )\n        # execute dry run on new IP\n        if dry_run is True:\n            nb_ip = nb_prefix.available_ips.list()[0]\n            ret.status = \"unchanged\"\n            ret.dry_run = True\n            ret.result = {\n                \"address\": str(nb_ip),\n                \"description\": description,\n                \"vrf\": vrf,\n                \"device\": device,\n                \"interface\": interface,\n            }\n            # add branch to results\n            if branch is not None:\n                ret.result[\"branch\"] = branch\n            return ret\n        # create new IP\n        else:\n            nb_ip = nb_prefix.available_ips.create()\n            job.event(\n                f\"Created '{nb_ip}' IP address for '{device}:{interface}' within '{nb_prefix}' prefix\"\n            )\n        ret.status = \"created\"\n    else:\n        job.event(f\"Using existing IP address {nb_ip}\")\n        ret.status = \"updated\"\n\n    # update IP address parameters\n    if description and description != nb_ip.description:\n        nb_ip.description = description\n        has_changes = True\n    if vrf and vrf != nb_ip.vrf:\n        nb_ip.vrf = {\"name\": vrf}\n        has_changes = True\n    if tenant and tenant != nb_ip.tenant:\n        nb_ip.tenant = {\"name\": tenant}\n        has_changes = True\n    if dns_name and dns_name != nb_ip.dns_name:\n        nb_ip.dns_name = dns_name\n        has_changes = True\n    if comments and comments != nb_ip.comments:\n        nb_ip.comments = comments\n        has_changes = True\n    if role and role != nb_ip.role:\n        nb_ip.role = role\n        has_changes = True\n    if tags and not any(t in nb_ip.tags for t in tags):\n        for t in tags:\n            if t not in nb_ip.tags:\n                nb_ip.tags.append({\"name\": t})\n                has_changes = True\n    if device and interface:\n        nb_interface = nb.dcim.interfaces.get(device=device, name=interface)\n        if not nb_interface:\n            raise NetboxAllocationError(\n                f\"Unable to source '{device}:{interface}' interface from Netbox\"\n            )\n        if (\n            hasattr(nb_ip, \"assigned_object\")\n            and nb_ip.assigned_object != nb_interface.id\n        ):\n            nb_ip.assigned_object_id = nb_interface.id\n            nb_ip.assigned_object_type = \"dcim.interface\"\n            if is_primary is not None:\n                nb_device = nb.dcim.devices.get(name=device)\n                nb_device.primary_ip4 = nb_ip.id\n            has_changes = True\n    if mask_len and not str(nb_ip).endswith(f\"/{mask_len}\"):\n        address = str(nb_ip).split(\"/\")[0]\n        nb_ip.address = f\"{address}/{mask_len}\"\n        has_changes = True\n\n    # save IP address into Netbox\n    if dry_run:\n        ret.status = \"unchanged\"\n        ret.dry_run = True\n    elif has_changes:\n        nb_ip.save()\n        job.event(f\"Updated '{str(nb_ip)}' IP address parameters\")\n        # make IP primary for device\n        if is_primary is True and nb_device:\n            nb_device.save()\n    else:\n        ret.status = \"unchanged\"\n\n    # form and return results\n    ret.result = {\n        \"address\": str(nb_ip),\n        \"description\": str(nb_ip.description),\n        \"vrf\": str(nb_ip.vrf) if not vrf else nb_ip.vrf[\"name\"],\n        \"device\": device,\n        \"interface\": interface,\n    }\n    # add branch to results\n    if branch is not None:\n        ret.result[\"branch\"] = branch\n\n    # create IP address for peer\n    if create_peer_ip and create_peer_ip_data:\n        job.event(\n            f\"Creating IP address for link peer '{create_peer_ip_data['device']}:{create_peer_ip_data['interface']}'\"\n        )\n        peer_ip = self.create_ip(\n            **create_peer_ip_data, prefix=str(nb_prefix), job=job\n        )\n        if peer_ip.failed == False:\n            ret.result[\"peer\"] = peer_ip.result\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.create_prefix","title":"<code>create_prefix(job: Job, parent: Union[str, dict], description: str = None, prefixlen: int = 30, vrf: str = None, tags: Union[None, list] = None, tenant: str = None, comments: str = None, role: str = None, site: str = None, status: str = None, instance: Union[None, str] = None, dry_run: bool = False, branch: str = None) -&gt; Result</code>","text":"<p>Creates a new IP prefix in NetBox or updates an existing one.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>Union[str, dict]</code> <p>Parent prefix to allocate new prefix from, could be:</p> <ul> <li>IPv4 prefix string e.g. 10.0.0.0/24</li> <li>IPv6 prefix string e.g. 2001::/64</li> <li>Prefix description string to filter by</li> <li>Dictionary with prefix filters for <code>pynetbox</code> prefixes.get method     e.g. <code>{\"prefix\": \"10.0.0.0/24\", \"site__name\": \"foo\"}</code></li> </ul> required <code>description</code> <code>str</code> <p>Description for the new prefix, prefix description used for deduplication to source existing prefixes.</p> <code>None</code> <code>prefixlen</code> <code>int</code> <p>The prefix length of the new prefix to create, by default allocates next available /30 point-to-point prefix.</p> <code>30</code> <code>vrf</code> <code>str</code> <p>Name of the VRF to associate with the prefix.</p> <code>None</code> <code>tags</code> <code>Union[None, list]</code> <p>List of tags to assign to the prefix.</p> <code>None</code> <code>tenant</code> <code>str</code> <p>Name of the tenant to associate with the prefix.</p> <code>None</code> <code>comments</code> <code>str</code> <p>Comments for the prefix.</p> <code>None</code> <code>role</code> <code>str</code> <p>Role to assign to the prefix.</p> <code>None</code> <code>site</code> <code>str</code> <p>Name of the site to associate with the prefix.</p> <code>None</code> <code>status</code> <code>str</code> <p>Status of the prefix.</p> <code>None</code> <code>instance</code> <code>Union[None, str]</code> <p>NetBox instance identifier.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, simulates the creation without making changes.</p> <code>False</code> <code>branch</code> <code>str</code> <p>Branch name to use, need to have branching plugin installed, automatically creates branch if it does not exist in Netbox.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the outcome, including status, details of the prefix, and resources used.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    input=CreatePrefixInput,\n    fastapi={\"methods\": [\"POST\"], \"schema\": NetboxFastApiArgs.model_json_schema()},\n)\ndef create_prefix(\n    self,\n    job: Job,\n    parent: Union[str, dict],\n    description: str = None,\n    prefixlen: int = 30,\n    vrf: str = None,\n    tags: Union[None, list] = None,\n    tenant: str = None,\n    comments: str = None,\n    role: str = None,\n    site: str = None,\n    status: str = None,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    branch: str = None,\n) -&gt; Result:\n    \"\"\"\n    Creates a new IP prefix in NetBox or updates an existing one.\n\n    Args:\n        parent (Union[str, dict]): Parent prefix to allocate new prefix from, could be:\n\n            - IPv4 prefix string e.g. 10.0.0.0/24\n            - IPv6 prefix string e.g. 2001::/64\n            - Prefix description string to filter by\n            - Dictionary with prefix filters for `pynetbox` prefixes.get method\n                e.g. `{\"prefix\": \"10.0.0.0/24\", \"site__name\": \"foo\"}`\n\n        description (str): Description for the new prefix, prefix description used for\n            deduplication to source existing prefixes.\n        prefixlen (int, optional): The prefix length of the new prefix to create, by default\n            allocates next available /30 point-to-point prefix.\n        vrf (str, optional): Name of the VRF to associate with the prefix.\n        tags (Union[None, list], optional): List of tags to assign to the prefix.\n        tenant (str, optional): Name of the tenant to associate with the prefix.\n        comments (str, optional): Comments for the prefix.\n        role (str, optional): Role to assign to the prefix.\n        site (str, optional): Name of the site to associate with the prefix.\n        status (str, optional): Status of the prefix.\n        instance (Union[None, str], optional): NetBox instance identifier.\n        dry_run (bool, optional): If True, simulates the creation without making changes.\n        branch (str, optional): Branch name to use, need to have branching plugin installed,\n            automatically creates branch if it does not exist in Netbox.\n\n    Returns:\n        Result: An object containing the outcome, including status, details of the prefix, and resources used.\n    \"\"\"\n    instance = instance or self.default_instance\n    changed = {}\n    ret = Result(\n        task=f\"{self.name}:create_prefix\",\n        result={},\n        resources=[instance],\n        diff=changed,\n    )\n    tags = tags or []\n    nb_prefix = None\n    nb = self._get_pynetbox(instance, branch=branch)\n\n    job.event(\n        f\"Processing prefix create request within '{parent}' for '/{prefixlen}' subnet\"\n    )\n\n    # source parent prefix from Netbox\n    if isinstance(parent, str):\n        # check if parent prefix is IP network or description\n        try:\n            _ = ipaddress.ip_network(parent)\n            is_network = True\n        except:\n            is_network = False\n        if is_network is True and vrf:\n            parent_filters = {\"prefix\": parent, \"vrf__name\": vrf}\n        elif is_network is True:\n            parent_filters = {\"prefix\": parent}\n        elif is_network is False and vrf:\n            parent_filters = {\"description\": parent, \"vrf__name\": vrf}\n        elif is_network is False:\n            parent_filters = {\"description\": parent}\n    nb_parent_prefix = nb.ipam.prefixes.get(**parent_filters)\n    if not nb_parent_prefix:\n        raise NetboxAllocationError(\n            f\"Unable to source parent prefix from Netbox - {parent}\"\n        )\n\n    # check that parent vrf and new prefix vrf are same\n    if vrf and str(nb_parent_prefix.vrf) != vrf:\n        raise NetboxAllocationError(\n            f\"Parent prefix vrf '{nb_parent_prefix.vrf}' not same as requested child prefix vrf '{vrf}'\"\n        )\n\n    # try to source existing prefix from netbox\n    prefix_filters = {}\n    if vrf:\n        prefix_filters[\"vrf__name\"] = vrf\n    if site:\n        prefix_filters[\"site__name\"] = site\n    if description:\n        prefix_filters[\"description\"] = description\n    try:\n        if prefix_filters:\n            nb_prefix = nb.ipam.prefixes.get(\n                within=nb_parent_prefix.prefix, **prefix_filters\n            )\n    except Exception as e:\n        raise NetboxAllocationError(\n            f\"Failed to source existing prefix from Netbox using filters '{prefix_filters}', error: {e}\"\n        )\n\n    # create new prefix\n    if not nb_prefix:\n        job.event(f\"Creating new '/{prefixlen}' prefix within '{parent}' prefix\")\n        # execute dry run on new prefix\n        if dry_run is True:\n            nb_prefixes = nb_parent_prefix.available_prefixes.list()\n            if not nb_prefixes:\n                raise NetboxAllocationError(\n                    f\"Parent prefix '{parent}' has no child prefixes available\"\n                )\n            for pfx in nb_prefixes:\n                # parent prefix empty, can use first subnet as a child prefix\n                if pfx.prefix == nb_parent_prefix.prefix:\n                    nb_prefix = (\n                        nb_parent_prefix.prefix.split(\"/\")[0] + f\"/{prefixlen}\"\n                    )\n                    break\n                # find child prefix by prefixlenght\n                elif str(pfx).endswith(f\"/{prefixlen}\"):\n                    nb_prefix = str(pfx)\n                    break\n            else:\n                raise NetboxAllocationError(\n                    f\"Parent prefix '{parent}' has no child prefixes available with '/{prefixlen}' prefix length\"\n                )\n            ret.status = \"unchanged\"\n            ret.dry_run = True\n            ret.result = {\n                \"prefix\": nb_prefix,\n                \"description\": description,\n                \"parent\": nb_parent_prefix.prefix,\n                \"vrf\": vrf,\n                \"site\": site,\n            }\n            # add branch to results\n            if branch is not None:\n                ret.result[\"branch\"] = branch\n            return ret\n        # create new prefix\n        else:\n            try:\n                nb_prefix = nb_parent_prefix.available_prefixes.create(\n                    {\"prefix_length\": prefixlen}\n                )\n            except Exception as e:\n                raise NetboxAllocationError(\n                    f\"Failed creating child prefix of '/{prefixlen}' prefix length \"\n                    f\"within parent prefix '{str(nb_parent_prefix)}', error: {e}\"\n                )\n        job.event(f\"Created new '{nb_prefix}' prefix within '{parent}' prefix\")\n        ret.status = \"created\"\n    else:\n        # check existing prefix length matching requested length\n        if not nb_prefix.prefix.endswith(f\"/{prefixlen}\"):\n            raise NetboxAllocationError(\n                f\"Found existing child prefix '{nb_prefix.prefix}' with mismatch \"\n                f\"requested prefix length '/{prefixlen}'\"\n            )\n        job.event(f\"Using existing prefix {nb_prefix}\")\n\n    # update prefix parameters\n    if description and description != nb_prefix.description:\n        changed[\"description\"] = {\"-\": str(nb_prefix.description), \"+\": description}\n        nb_prefix.description = description\n    if vrf and vrf != str(nb_prefix.vrf):\n        changed[\"vrf\"] = {\"-\": str(nb_prefix.vrf), \"+\": vrf}\n        nb_prefix.vrf = {\"name\": vrf}\n    if tenant and tenant != str(nb_prefix.tenant):\n        changed[\"tenant\"] = {\n            \"-\": str(nb_prefix.tenant) if nb_prefix.tenant else None,\n            \"+\": tenant,\n        }\n        nb_prefix.tenant = {\"name\": tenant}\n    if site and str(nb_prefix.scope) != site:\n        nb_site = nb.dcim.sites.get(name=site)\n        if not nb_site:\n            raise NetboxAllocationError(f\"Failed to get '{site}' site from Netbox\")\n        changed[\"site\"] = {\n            \"-\": str(nb_prefix.scope) if nb_prefix.scope else None,\n            \"+\": nb_site.name,\n        }\n        nb_prefix.scope_type = \"dcim.site\"\n        nb_prefix.scope_id = nb_site.id\n    if status and status.lower() != nb_prefix.status:\n        changed[\"status\"] = {\"-\": str(nb_prefix.status), \"+\": status.title()}\n        nb_prefix.status = status.lower()\n    if comments and comments != nb_prefix.comments:\n        changed[\"comments\"] = {\"-\": str(nb_prefix.comments), \"+\": comments}\n        nb_prefix.comments = comments\n    if role and role != nb_prefix.role:\n        changed[\"role\"] = {\"-\": str(nb_prefix.role), \"+\": role}\n        nb_prefix.role = {\"name\": role}\n    existing_tags = [str(t) for t in nb_prefix.tags]\n    if tags and not any(t in existing_tags for t in tags):\n        changed[\"tags\"] = {\n            \"-\": existing_tags,\n            \"+\": [t for t in tags if t not in existing_tags] + existing_tags,\n        }\n        for t in tags:\n            if t not in existing_tags:\n                nb_prefix.tags.append({\"name\": t})\n\n    # save prefix into Netbox\n    if dry_run:\n        ret.status = \"unchanged\"\n        ret.dry_run = True\n        ret.diff = changed\n    elif changed:\n        ret.diff = changed\n        nb_prefix.save()\n        if ret.status != \"created\":\n            ret.status = \"updated\"\n    else:\n        ret.status = \"unchanged\"\n\n    # source vrf name\n    vrf_name = None\n    if nb_prefix.vrf:\n        if isinstance(nb_prefix.vrf, dict):\n            vrf_name = nb_prefix.vrf[\"name\"]\n        else:\n            vrf_name = nb_prefix.vrf.name\n\n    # form and return results\n    ret.result = {\n        \"prefix\": nb_prefix.prefix,\n        \"description\": nb_prefix.description,\n        \"vrf\": vrf_name,\n        \"site\": str(nb_prefix.scope) if nb_prefix.scope else site,\n        \"parent\": nb_parent_prefix.prefix,\n    }\n    # add branch to results\n    if branch is not None:\n        ret.result[\"branch\"] = branch\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.get_containerlab_inventory","title":"<code>get_containerlab_inventory(job: Job, lab_name: str = None, tenant: Union[None, str] = None, filters: Union[None, list] = None, devices: Union[None, list] = None, instance: Union[None, str] = None, image: Union[None, str] = None, ipv4_subnet: str = '172.100.100.0/24', ports: tuple = (12000, 15000), ports_map: Union[None, dict] = None, cache: Union[bool, str] = False) -&gt; Result</code>","text":"<p>Retrieve and construct Containerlab inventory from NetBox data.</p> <p>Containerlab node details must be defined under device configuration context <code>norfab.containerlab</code> path, for example:</p> <pre><code>{\n    \"norfab\": {\n        \"containerlab\": {\n            \"kind\": \"ceos\",\n            \"image\": \"ceos:latest\",\n            \"mgmt-ipv4\": \"172.100.100.10/24\",\n            \"ports\": [\n                {10000: 22},\n                {10001: 830}\n            ],\n\n            ... any other node parameters ...\n\n            \"interfaces_rename\": [\n                {\n                    \"find\": \"eth\",\n                    \"replace\": \"Eth\",\n                    \"use_regex\": false\n                }\n            ]\n        }\n    }\n}\n</code></pre> <p>For complete list of parameters refer to Containerlab nodes definition.</p> <p>Special handling given to these parameters:</p> <ul> <li><code>lab_name</code> - if not provided uses <code>tenant</code> argument value as a lab name</li> <li><code>kind</code> - uses device platform field value by default</li> <li><code>image</code> - uses <code>image</code> value if provided, otherwise uses <code>{kind}:latest</code></li> <li><code>interfaces_rename</code> - a list of one or more interface renaming instructions,     each item must have <code>find</code> and <code>replace</code> defined, optional <code>use_regex</code>     flag specifies whether to use regex based pattern substitution.</li> </ul> <p>To retrieve topology data from Netbox at least one of these arguments must be provided to identify a set of devices to include into Containerlab topology:</p> <ul> <li><code>tenant</code> - topology constructed using all devices and links that belong to this tenant</li> <li><code>devices</code> - creates topology only using devices in the lists</li> <li><code>filters</code> - list of device filters to retrieve from Netbox and add to topology</li> </ul> <p>If multiple of above arguments provided, resulting lab topology is a sum of all devices matched.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>lab_name</code> <code>(str, Mandatory)</code> <p>Name of containerlab to construct inventory for.</p> <code>None</code> <code>tenant</code> <code>str</code> <p>Construct topology using given tenant's devices</p> <code>None</code> <code>filters</code> <code>list</code> <p>List of filters to apply when retrieving devices from NetBox.</p> <code>None</code> <code>devices</code> <code>list</code> <p>List of specific devices to retrieve from NetBox.</p> <code>None</code> <code>instance</code> <code>str</code> <p>NetBox instance to use.</p> <code>None</code> <code>image</code> <code>str</code> <p>Default containerlab image to use,</p> <code>None</code> <code>ipv4_subnet</code> <code>(str, Optional)</code> <p>Management subnet to use to IP number nodes starting with 2nd IP in the subnet, in assumption that 1st IP is a default gateway.</p> <code>'172.100.100.0/24'</code> <code>ports</code> <code>(tuple, Optional)</code> <p>Ports range to use for nodes.</p> <code>(12000, 15000)</code> <code>ports_map</code> <code>(dict, Optional)</code> <p>dictionary keyed by node name with list of ports maps to use,</p> <code>None</code> <code>cache</code> <code>Union[bool, str]</code> <p>Cache usage options:</p> <ul> <li>True: Use data stored in cache if it is up to date, refresh it otherwise.</li> <li>False: Do not use cache and do not update cache.</li> <li>\"refresh\": Ignore data in cache and replace it with data fetched from Netbox.</li> <li>\"force\": Use data in cache without checking if it is up to date.</li> </ul> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>Containerlab inventory dictionary containing lab topology data</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_containerlab_inventory(\n    self,\n    job: Job,\n    lab_name: str = None,\n    tenant: Union[None, str] = None,\n    filters: Union[None, list] = None,\n    devices: Union[None, list] = None,\n    instance: Union[None, str] = None,\n    image: Union[None, str] = None,\n    ipv4_subnet: str = \"172.100.100.0/24\",\n    ports: tuple = (12000, 15000),\n    ports_map: Union[None, dict] = None,\n    cache: Union[bool, str] = False,\n) -&gt; Result:\n    \"\"\"\n    Retrieve and construct Containerlab inventory from NetBox data.\n\n    Containerlab node details must be defined under device configuration\n    context `norfab.containerlab` path, for example:\n\n    ```\n    {\n        \"norfab\": {\n            \"containerlab\": {\n                \"kind\": \"ceos\",\n                \"image\": \"ceos:latest\",\n                \"mgmt-ipv4\": \"172.100.100.10/24\",\n                \"ports\": [\n                    {10000: 22},\n                    {10001: 830}\n                ],\n\n                ... any other node parameters ...\n\n                \"interfaces_rename\": [\n                    {\n                        \"find\": \"eth\",\n                        \"replace\": \"Eth\",\n                        \"use_regex\": false\n                    }\n                ]\n            }\n        }\n    }\n    ```\n\n    For complete list of parameters refer to\n    [Containerlab nodes definition](https://containerlab.dev/manual/nodes/).\n\n    Special handling given to these parameters:\n\n    - `lab_name` - if not provided uses `tenant` argument value as a lab name\n    - `kind` - uses device platform field value by default\n    - `image` - uses `image` value if provided, otherwise uses `{kind}:latest`\n    - `interfaces_rename` - a list of one or more interface renaming instructions,\n        each item must have `find` and `replace` defined, optional `use_regex`\n        flag specifies whether to use regex based pattern substitution.\n\n    To retrieve topology data from Netbox at least one of these arguments must be provided\n    to identify a set of devices to include into Containerlab topology:\n\n    - `tenant` - topology constructed using all devices and links that belong to this tenant\n    - `devices` - creates topology only using devices in the lists\n    - `filters` - list of device filters to retrieve from Netbox and add to topology\n\n    If multiple of above arguments provided, resulting lab topology is a sum of all\n    devices matched.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        lab_name (str, Mandatory): Name of containerlab to construct inventory for.\n        tenant (str, optional): Construct topology using given tenant's devices\n        filters (list, optional): List of filters to apply when retrieving devices from NetBox.\n        devices (list, optional): List of specific devices to retrieve from NetBox.\n        instance (str, optional): NetBox instance to use.\n        image (str, optional): Default containerlab image to use,\n        ipv4_subnet (str, Optional): Management subnet to use to IP number nodes\n            starting with 2nd IP in the subnet, in assumption that 1st IP is a default gateway.\n        ports (tuple, Optional): Ports range to use for nodes.\n        ports_map (dict, Optional): dictionary keyed by node name with list of ports maps to use,\n        cache (Union[bool, str], optional): Cache usage options:\n\n            - True: Use data stored in cache if it is up to date, refresh it otherwise.\n            - False: Do not use cache and do not update cache.\n            - \"refresh\": Ignore data in cache and replace it with data fetched from Netbox.\n            - \"force\": Use data in cache without checking if it is up to date.\n\n    Returns:\n        dict: Containerlab inventory dictionary containing lab topology data\n    \"\"\"\n    devices = devices or []\n    filters = filters or []\n    nodes, links = {}, []\n    ports_map = ports_map or {}\n    endpts_done = []  # to deduplicate links\n    instance = instance or self.default_instance\n    # handle lab name and tenant name with filters\n    if lab_name is None and tenant:\n        lab_name = tenant\n    # add tenant filters\n    if tenant:\n        filters = filters or [{}]\n        for filter in filters:\n            if self.nb_version[instance] &gt;= (4, 4, 0):\n                filter[\"tenant\"] = f'{{name: {{exact: \"{tenant}\"}}}}'\n            else:\n                raise UnsupportedNetboxVersion(\n                    f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n                    f\"minimum required version is {self.compatible_ge_v4}\"\n                )\n\n    # construct inventory\n    inventory = {\n        \"name\": lab_name,\n        \"topology\": {\"nodes\": nodes, \"links\": links},\n        \"mgmt\": {\"ipv4-subnet\": ipv4_subnet, \"network\": f\"br-{lab_name}\"},\n    }\n    ret = Result(\n        task=f\"{self.name}:get_containerlab_inventory\",\n        result=inventory,\n        resources=[instance],\n    )\n    mgmt_net = ipaddress.ip_network(ipv4_subnet)\n    available_ips = list(mgmt_net.hosts())[1:]\n\n    # run checks\n    if not available_ips:\n        raise ValueError(f\"Need IPs to allocate, but '{ipv4_subnet}' given\")\n    if ports:\n        available_ports = list(range(ports[0], ports[1]))\n    else:\n        raise ValueError(f\"Need ports to allocate, but '{ports}' given\")\n\n    # check Netbox status\n    netbox_status = self.get_netbox_status(job=job, instance=instance)\n    if netbox_status.result[instance][\"status\"] is False:\n        ret.failed = True\n        ret.messages = [f\"Netbox status is no good: {netbox_status}\"]\n        return ret\n\n    # retrieve devices data\n    log.debug(\n        f\"Fetching devices from {instance} Netbox instance, devices '{devices}', filters '{filters}'\"\n    )\n    job.event(\"Fetching devices data from Netbox\")\n    nb_devices = self.get_devices(\n        job=job,\n        filters=filters,\n        devices=devices,\n        instance=instance,\n        cache=cache,\n    )\n\n    # form Containerlab nodes inventory\n    for device_name, device in nb_devices.result.items():\n        node = device[\"config_context\"].get(\"norfab\", {}).get(\"containerlab\", {})\n        # populate node parameters\n        if not node.get(\"kind\"):\n            if device[\"platform\"]:\n                node[\"kind\"] = device[\"platform\"][\"name\"]\n            else:\n                msg = (\n                    f\"{device_name} - has no 'kind' of 'platform' defined, skipping\"\n                )\n                log.warning(msg)\n                job.event(msg, severity=\"WARNING\")\n                continue\n        if not node.get(\"image\"):\n            if image:\n                node[\"image\"] = image\n            else:\n                node[\"image\"] = f\"{node['kind']}:latest\"\n        if not node.get(\"mgmt-ipv4\"):\n            if available_ips:\n                node[\"mgmt-ipv4\"] = f\"{available_ips.pop(0)}\"\n            else:\n                raise RuntimeError(\"Run out of IP addresses to allocate\")\n        if not node.get(\"ports\"):\n            node[\"ports\"] = []\n            # use ports map\n            if ports_map.get(device_name):\n                node[\"ports\"] = ports_map[device_name]\n            # allocate next-available ports\n            else:\n                for port in [\n                    \"22/tcp\",\n                    \"23/tcp\",\n                    \"80/tcp\",\n                    \"161/udp\",\n                    \"443/tcp\",\n                    \"830/tcp\",\n                    \"8080/tcp\",\n                ]:\n                    if available_ports:\n                        node[\"ports\"].append(f\"{available_ports.pop(0)}:{port}\")\n                    else:\n                        raise RuntimeError(\n                            \"Run out of TCP / UDP ports to allocate.\"\n                        )\n\n        # save node content\n        nodes[device_name] = node\n        job.event(f\"Node added {device_name}\")\n\n    # return if no nodes found for provided parameters\n    if not nodes:\n        msg = f\"{self.name} - no devices found in Netbox\"\n        log.error(msg)\n        ret.failed = True\n        ret.messages = [\n            f\"{self.name} - no devices found in Netbox, \"\n            f\"devices - '{devices}', filters - '{filters}'\"\n        ]\n        ret.errors = [msg]\n        return ret\n\n    job.event(\"Fetching connections data from Netbox\")\n\n    # query interface connections data from netbox\n    nb_connections = self.get_connections(\n        job=job, devices=list(nodes), instance=instance, cache=cache\n    )\n    # save connections data to links inventory\n    while nb_connections.result:\n        device, device_connections = nb_connections.result.popitem()\n        for interface, connection in device_connections.items():\n            # skip non ethernet links\n            if connection.get(\"termination_type\") != \"interface\":\n                continue\n            # skip orphaned links\n            if not connection.get(\"remote_interface\"):\n                continue\n            # skip connections to devices that are not part of lab\n            if connection[\"remote_device\"] not in nodes:\n                continue\n            endpoints = []\n            link = {\n                \"type\": \"veth\",\n                \"endpoints\": endpoints,\n            }\n            # add A node\n            endpoints.append(\n                {\n                    \"node\": device,\n                    \"interface\": interface,\n                }\n            )\n            # add B node\n            endpoints.append({\"node\": connection[\"remote_device\"]})\n            if connection.get(\"breakout\") is True:\n                endpoints[-1][\"interface\"] = connection[\"remote_interface\"][0]\n            else:\n                endpoints[-1][\"interface\"] = connection[\"remote_interface\"]\n            # save the link\n            a_end = (\n                endpoints[0][\"node\"],\n                endpoints[0][\"interface\"],\n            )\n            b_end = (\n                endpoints[1][\"node\"],\n                endpoints[1][\"interface\"],\n            )\n            if a_end not in endpts_done and b_end not in endpts_done:\n                endpts_done.append(a_end)\n                endpts_done.append(b_end)\n                links.append(link)\n                job.event(\n                    f\"Link added {endpoints[0]['node']}:{endpoints[0]['interface']}\"\n                    f\" - {endpoints[1]['node']}:{endpoints[1]['interface']}\"\n                )\n\n    # query circuits connections data from netbox\n    nb_circuits = self.get_circuits(\n        job=job, devices=list(nodes), instance=instance, cache=cache\n    )\n    # save circuits data to hosts' inventory\n    while nb_circuits.result:\n        device, device_circuits = nb_circuits.result.popitem()\n        for cid, circuit in device_circuits.items():\n            # skip circuits not connected to devices\n            if not circuit.get(\"remote_interface\"):\n                continue\n            # skip circuits to devices that are not part of lab\n            if circuit[\"remote_device\"] not in nodes:\n                continue\n            endpoints = []\n            link = {\n                \"type\": \"veth\",\n                \"endpoints\": endpoints,\n            }\n            # add A node\n            endpoints.append(\n                {\n                    \"node\": device,\n                    \"interface\": circuit[\"interface\"],\n                }\n            )\n            # add B node\n            endpoints.append(\n                {\n                    \"node\": circuit[\"remote_device\"],\n                    \"interface\": circuit[\"remote_interface\"],\n                }\n            )\n            # save the link\n            a_end = (\n                endpoints[0][\"node\"],\n                endpoints[0][\"interface\"],\n            )\n            b_end = (\n                endpoints[1][\"node\"],\n                endpoints[1][\"interface\"],\n            )\n            if a_end not in endpts_done and b_end not in endpts_done:\n                endpts_done.append(a_end)\n                endpts_done.append(b_end)\n                links.append(link)\n                job.event(\n                    f\"Link added {endpoints[0]['node']}:{endpoints[0]['interface']}\"\n                    f\" - {endpoints[1]['node']}:{endpoints[1]['interface']}\"\n                )\n\n    # rename links' interfaces\n    for node_name, node_data in nodes.items():\n        interfaces_rename = node_data.pop(\"interfaces_rename\", [])\n        if interfaces_rename:\n            job.event(f\"Renaming {node_name} interfaces\")\n        for item in interfaces_rename:\n            if not item.get(\"find\") or not item.get(\"replace\"):\n                log.error(\n                    f\"{self.name} - interface rename need to have\"\n                    f\" 'find' and 'replace' defined, skipping: {item}\"\n                )\n                continue\n            pattern = item[\"find\"]\n            replace = item[\"replace\"]\n            use_regex = item.get(\"use_regex\", False)\n            # go over links one by one and rename interfaces\n            for link in links:\n                for endpoint in link[\"endpoints\"]:\n                    if endpoint[\"node\"] != node_name:\n                        continue\n                    if use_regex:\n                        renamed = re.sub(\n                            pattern,\n                            replace,\n                            endpoint[\"interface\"],\n                        )\n                    else:\n                        renamed = endpoint[\"interface\"].replace(pattern, replace)\n                    if endpoint[\"interface\"] != renamed:\n                        msg = f\"{node_name} interface {endpoint['interface']} renamed to {renamed}\"\n                        log.debug(msg)\n                        job.event(msg)\n                        endpoint[\"interface\"] = renamed\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.delete_branch","title":"<code>delete_branch(job: Job, branch: str = None, instance: str = None) -&gt; Result</code>","text":"<p>Deletes a branch with the specified name from the NetBox instance.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job context for the operation.</p> required <code>branch</code> <code>str</code> <p>The name of the branch to delete.</p> <code>None</code> <code>instance</code> <code>str</code> <p>The NetBox instance name.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the outcome of the deletion operation, including whether the branch was found and deleted.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"DELETE\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef delete_branch(\n    self,\n    job: Job,\n    branch: str = None,\n    instance: str = None,\n) -&gt; Result:\n    \"\"\"\n    Deletes a branch with the specified name from the NetBox instance.\n\n    Args:\n        job (Job): The job context for the operation.\n        branch (str, optional): The name of the branch to delete.\n        instance (str, optional): The NetBox instance name.\n\n    Returns:\n        Result: An object containing the outcome of the deletion operation,\n            including whether the branch was found and deleted.\n    \"\"\"\n    instance = instance or self.default_instance\n    ret = Result(\n        task=f\"{self.name}:delete_branch\",\n        result=None,\n        resources=[instance],\n    )\n    nb = self._get_pynetbox(instance)\n\n    job.event(f\"Deleting branch '{branch}', Netbo instance '{instance}'\")\n\n    nb_branch = nb.plugins.branching.branches.get(name=branch)\n\n    if nb_branch:\n        nb_branch.delete()\n        ret.result = True\n        job.event(f\"'{branch}' deleted from '{instance}' Netbox instance\")\n    else:\n        msg = f\"'{branch}' branch does not exist in '{instance}' Netbox instance\"\n        ret.result = None\n        ret.messages.append(msg)\n        job.event(msg)\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.expand_alphanumeric_range","title":"<code>expand_alphanumeric_range(range_pattern: str) -&gt; list</code>","text":"<p>Expand alphanumeric ranges.</p> <p>Examples:</p> <ul> <li>Ethernet[1-3] -&gt; ['Ethernet1', 'Ethernet2', 'Ethernet3']</li> <li>[ge,xe]-0/0/[0-9] -&gt; ['ge-0/0/0', 'ge-0/0/1', ..., 'xe-0/0/9']</li> </ul> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>def expand_alphanumeric_range(self, range_pattern: str) -&gt; list:\n    \"\"\"\n    Expand alphanumeric ranges.\n\n    Examples:\n        - Ethernet[1-3] -&gt; ['Ethernet1', 'Ethernet2', 'Ethernet3']\n        - [ge,xe]-0/0/[0-9] -&gt; ['ge-0/0/0', 'ge-0/0/1', ..., 'xe-0/0/9']\n    \"\"\"\n    # Find all bracketed patterns\n    bracket_pattern = r\"\\[([^\\]]+)\\]\"\n    matches = list(re.finditer(bracket_pattern, range_pattern))\n\n    if not matches:\n        # No ranges found, return as-is\n        return [range_pattern]\n\n    # Start with a single template\n    templates = [range_pattern]\n\n    # Process each bracket from left to right\n    for match in matches:\n        bracket_content = match.group(1)\n        new_templates = []\n\n        # Check if it's a comma-separated list\n        if \",\" in bracket_content:\n            options = [opt.strip() for opt in bracket_content.split(\",\")]\n            for template in templates:\n                for option in options:\n                    new_templates.append(\n                        template.replace(f\"[{bracket_content}]\", option, 1)\n                    )\n\n        # Check if it's a numeric range\n        elif (\n            \"-\" in bracket_content\n            and bracket_content.replace(\"-\", \"\").replace(\" \", \"\").isdigit()\n        ):\n            parts = bracket_content.split(\"-\")\n            if len(parts) == 2:\n                try:\n                    start = int(parts[0].strip())\n                    end = int(parts[1].strip())\n                    for template in templates:\n                        for num in range(start, end + 1):\n                            new_templates.append(\n                                template.replace(\n                                    f\"[{bracket_content}]\", str(num), 1\n                                )\n                            )\n                except ValueError:\n                    # If conversion fails, treat as literal\n                    for template in templates:\n                        new_templates.append(\n                            template.replace(\n                                f\"[{bracket_content}]\", bracket_content, 1\n                            )\n                        )\n        else:\n            # Treat as literal\n            for template in templates:\n                new_templates.append(\n                    template.replace(f\"[{bracket_content}]\", bracket_content, 1)\n                )\n\n        templates = new_templates\n\n    return templates\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.create_device_interfaces","title":"<code>create_device_interfaces(job: Job, devices: list, interface_name: Union[list, str], interface_type: str = 'other', instance: Union[None, str] = None, dry_run: bool = False, branch: str = None, **kwargs: dict) -&gt; Result</code>","text":"<p>Create interfaces for one or more devices in NetBox. This task creates interfaces in bulk and only if interfaces does not exist in Netbox.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job object containing execution context and metadata.</p> required <code>devices</code> <code>list</code> <p>List of device names or device objects to create interfaces for.</p> required <code>interface_name</code> <code>Union[list, str]</code> <p>Name(s) of the interface(s) to create. Can be a single interface name as a string or multiple names as a list. Alphanumeric ranges are supported for bulk creation:</p> <ul> <li>Ethernet[1-3] -&gt; Ethernet1, Ethernet2, Ethernet3</li> <li>[ge,xe]-0/0/[0-9] -&gt; ge-0/0/0, ..., xe-0/0/0 etc.</li> </ul> required <code>interface_type</code> <code>str</code> <p>Type of interface (e.g., \"other\", \"virtual\", \"lag\", \"1000base-t\"). Defaults to \"other\".</p> <code>'other'</code> <code>instance</code> <code>Union[None, str]</code> <p>NetBox instance identifier to use. If None, uses the default instance. Defaults to None.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, simulates the operation without making actual changes. Defaults to False.</p> <code>False</code> <code>branch</code> <code>str</code> <p>NetBox branch to use for the operation. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>Any additional interface attributes</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>Result object containing the task name, execution results, and affected resources. The result dictionary contains status and details of interface creation operations.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef create_device_interfaces(\n    self,\n    job: Job,\n    devices: list,\n    interface_name: Union[list, str],\n    interface_type: str = \"other\",\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    branch: str = None,\n    **kwargs: dict,\n) -&gt; Result:\n    \"\"\"\n    Create interfaces for one or more devices in NetBox. This task creates interfaces in bulk and only\n    if interfaces does not exist in Netbox.\n\n    Args:\n        job (Job): The job object containing execution context and metadata.\n        devices (list): List of device names or device objects to create interfaces for.\n        interface_name (Union[list, str]): Name(s) of the interface(s) to create. Can be a single\n            interface name as a string or multiple names as a list. Alphanumeric ranges are\n            supported for bulk creation:\n\n            - Ethernet[1-3] -&gt; Ethernet1, Ethernet2, Ethernet3\n            - [ge,xe]-0/0/[0-9] -&gt; ge-0/0/0, ..., xe-0/0/0 etc.\n\n        interface_type (str, optional): Type of interface (e.g., \"other\", \"virtual\", \"lag\",\n            \"1000base-t\"). Defaults to \"other\".\n        instance (Union[None, str], optional): NetBox instance identifier to use. If None,\n            uses the default instance. Defaults to None.\n        dry_run (bool, optional): If True, simulates the operation without making actual changes.\n            Defaults to False.\n        branch (str, optional): NetBox branch to use for the operation. Defaults to None.\n        kwargs (dict, optional): Any additional interface attributes\n\n    Returns:\n        Result: Result object containing the task name, execution results, and affected resources.\n            The result dictionary contains status and details of interface creation operations.\n    \"\"\"\n    instance = instance or self.default_instance\n    result = {}\n    kwargs = kwargs or {}\n    ret = Result(\n        task=f\"{self.name}:create_device_interfaces\",\n        result=result,\n        resources=[instance],\n    )\n    nb = self._get_pynetbox(instance, branch=branch)\n\n    # Normalize interface_name to a list\n    if isinstance(interface_name, str):\n        interface_names = [interface_name]\n    else:\n        interface_names = interface_name\n\n    # Expand all interface name patterns\n    all_interface_names = []\n    for name_pattern in interface_names:\n        all_interface_names.extend(self.expand_alphanumeric_range(name_pattern))\n\n    job.event(\n        f\"Expanded interface names to {len(all_interface_names)} interface(s)\"\n    )\n\n    # Process each device\n    for device_name in devices:\n        result[device_name] = {\n            \"created\": [],\n            \"skipped\": [],\n        }\n\n        try:\n            # Get device from NetBox\n            nb_device = nb.dcim.devices.get(name=device_name)\n            if not nb_device:\n                msg = f\"Device '{device_name}' not found in NetBox\"\n                ret.errors.append(msg)\n                job.event(msg)\n                continue\n\n            # Get existing interfaces for this device\n            existing_interfaces = nb.dcim.interfaces.filter(device=device_name)\n            existing_interface_names = {intf.name for intf in existing_interfaces}\n\n            # Prepare interfaces to create\n            interfaces_to_create = []\n\n            for intf_name in all_interface_names:\n                if intf_name in existing_interface_names:\n                    result[device_name][\"skipped\"].append(intf_name)\n                    continue\n\n                # Build interface data\n                intf_data = {\n                    \"device\": nb_device.id,\n                    \"name\": intf_name,\n                    \"type\": interface_type,\n                    **kwargs,\n                }\n\n                interfaces_to_create.append(intf_data)\n                result[device_name][\"created\"].append(intf_name)\n\n            # Create interfaces in bulk if not dry_run\n            if interfaces_to_create and not dry_run:\n                try:\n                    nb.dcim.interfaces.create(interfaces_to_create)\n                    msg = f\"Created {len(interfaces_to_create)} interface(s) on device '{device_name}'\"\n                    job.event(msg)\n                except Exception as e:\n                    msg = f\"Failed to create interfaces on device '{device_name}': {e}\"\n                    ret.errors.append(msg)\n                    log.error(msg)\n            elif interfaces_to_create and dry_run:\n                msg = f\"[DRY RUN] Would create {len(interfaces_to_create)} interface(s) on device '{device_name}'\"\n                job.event(msg)\n\n        except Exception as e:\n            msg = f\"Error processing device '{device_name}': {e}\"\n            ret.errors.append(msg)\n            log.error(msg)\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.NetboxWorker.get_bgp_peerings","title":"<code>get_bgp_peerings(job: Job, instance: Union[None, str] = None, devices: Union[None, list] = None, cache: Union[bool, str] = None) -&gt; Result</code>","text":"<p>Retrieve device BGP peerings from Netbox using REST API.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>instance</code> <code>str</code> <p>Netbox instance name.</p> <code>None</code> <code>devices</code> <code>list</code> <p>List of devices to retrieve BGP peerings for.</p> <code>None</code> <code>cache</code> <code>Union[bool, str]</code> <p>Cache usage options:</p> <ul> <li>True: Use data stored in cache if it is up to date, refresh it otherwise.</li> <li>False: Do not use cache and do not update cache.</li> <li>refresh: Ignore data in cache and replace it with data fetched from Netbox.</li> <li>force: Use data in cache without checking if it is up to date.</li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>Dictionary keyed by device name with BGP peerings details.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_bgp_peerings(\n    self,\n    job: Job,\n    instance: Union[None, str] = None,\n    devices: Union[None, list] = None,\n    cache: Union[bool, str] = None,\n) -&gt; Result:\n    \"\"\"\n    Retrieve device BGP peerings from Netbox using REST API.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        instance (str, optional): Netbox instance name.\n        devices (list, optional): List of devices to retrieve BGP peerings for.\n        cache (Union[bool, str], optional): Cache usage options:\n\n            - True: Use data stored in cache if it is up to date, refresh it otherwise.\n            - False: Do not use cache and do not update cache.\n            - refresh: Ignore data in cache and replace it with data fetched from Netbox.\n            - force: Use data in cache without checking if it is up to date.\n\n    Returns:\n        dict: Dictionary keyed by device name with BGP peerings details.\n    \"\"\"\n    instance = instance or self.default_instance\n    devices = devices or []\n    cache = self.cache_use if cache is None else cache\n    ret = Result(\n        task=f\"{self.name}:get_bgp_peerings\",\n        result={d: {} for d in devices},\n        resources=[instance],\n    )\n\n    # Check if BGP plugin is installed\n    if not self.has_plugin(\"netbox_bgp\", instance, strict=True):\n        ret.errors.append(f\"{instance} Netbox instance has no BGP Plugin installed\")\n        ret.failed = True\n        return ret\n\n    self.cache.expire()\n\n    # Get device details to collect device IDs\n    devices_result = self.get_devices(\n        job=job, devices=devices, instance=instance, cache=False\n    )\n    if devices_result.errors:\n        ret.errors.append(\n            f\"Failed to retrieve device details: {devices_result.errors}\"\n        )\n        return ret\n\n    nb = self._get_pynetbox(instance)\n\n    for device_name in devices:\n        # Skip devices not found in Netbox\n        if device_name not in devices_result.result:\n            msg = f\"Device '{device_name}' not found in Netbox\"\n            job.event(msg, resource=instance, severity=\"WARNING\")\n            log.warning(msg)\n            continue\n\n        device_id = devices_result.result[device_name][\"id\"]\n        cache_key = f\"get_bgp_peerings::{device_name}\"\n        cached_data = self.cache.get(cache_key)\n\n        # Mode: force with cached data - use cache directly\n        if cache == \"force\" and cached_data is not None:\n            ret.result[device_name] = cached_data\n            job.event(\n                f\"Using cached BGP peerings for '{device_name}' (forced)\",\n                resource=instance,\n            )\n            continue\n\n        # Mode: cache disabled - fetch without caching\n        if cache is False:\n            bgp_sessions = nb.plugins.bgp.session.filter(device_id=device_id)\n            ret.result[device_name] = {s.name: dict(s) for s in bgp_sessions}\n            job.event(\n                f\"Retrieved {len(ret.result[device_name])} BGP session(s) for '{device_name}'\",\n                resource=instance,\n            )\n            continue\n\n        # Mode: refresh or no cached data - fetch and cache\n        if cache == \"refresh\" or cached_data is None:\n            if cache == \"refresh\" and cached_data is not None:\n                self.cache.delete(cache_key, retry=True)\n            bgp_sessions = nb.plugins.bgp.session.filter(device_id=device_id)\n            ret.result[device_name] = {s.name: dict(s) for s in bgp_sessions}\n            self.cache.set(\n                cache_key, ret.result[device_name], expire=self.cache_ttl\n            )\n            job.event(\n                f\"Fetched and cached {len(ret.result[device_name])} BGP session(s) for '{device_name}'\",\n                resource=instance,\n            )\n            continue\n\n        # Mode: cache=True with cached data - smart update (only fetch changed sessions)\n        ret.result[device_name] = dict(cached_data)\n        job.event(\n            f\"Retrieved {len(cached_data)} BGP session(s) from cache for '{device_name}'\",\n            resource=instance,\n        )\n\n        # Fetch brief session info to compare timestamps\n        brief_sessions = nb.plugins.bgp.session.filter(\n            device_id=device_id, fields=\"id,last_updated,name\"\n        )\n        netbox_sessions = {\n            s.id: {\"name\": s.name, \"last_updated\": s.last_updated}\n            for s in brief_sessions\n        }\n\n        # Build lookup maps\n        cached_by_id = {s[\"id\"]: name for name, s in cached_data.items()}\n        session_ids_to_fetch = []\n        sessions_to_remove = []\n\n        # Find stale sessions (exist in both but timestamps differ) and deleted sessions\n        for session_name, cached_session in cached_data.items():\n            cached_id = cached_session[\"id\"]\n            if cached_id in netbox_sessions:\n                if (\n                    cached_session[\"last_updated\"]\n                    != netbox_sessions[cached_id][\"last_updated\"]\n                ):\n                    session_ids_to_fetch.append(cached_id)\n            else:\n                sessions_to_remove.append(session_name)\n\n        # Find new sessions in Netbox not in cache\n        for nb_id in netbox_sessions:\n            if nb_id not in cached_by_id:\n                session_ids_to_fetch.append(nb_id)\n\n        # Remove deleted sessions\n        for session_name in sessions_to_remove:\n            ret.result[device_name].pop(session_name, None)\n            job.event(\n                f\"Removed deleted session '{session_name}' from cache for '{device_name}'\",\n                resource=instance,\n            )\n\n        # Fetch updated/new sessions\n        if session_ids_to_fetch:\n            job.event(\n                f\"Fetching {len(session_ids_to_fetch)} updated BGP session(s) for '{device_name}'\",\n                resource=instance,\n            )\n            for session in nb.plugins.bgp.session.filter(id=session_ids_to_fetch):\n                ret.result[device_name][session.name] = dict(session)\n\n        # Update cache if any changes occurred\n        if session_ids_to_fetch or sessions_to_remove:\n            self.cache.set(\n                cache_key, ret.result[device_name], expire=self.cache_ttl\n            )\n            job.event(f\"Updated cache for '{device_name}'\", resource=instance)\n        else:\n            job.event(\n                f\"Using cache, it is up to date for '{device_name}'\",\n                resource=instance,\n            )\n\n    return ret\n</code></pre>"},{"location":"workers/netbox/api_reference_workers_netbox_worker/#norfab.workers.netbox_worker.compare_netbox_object_state","title":"<code>compare_netbox_object_state(desired_state: dict, current_state: dict, ignore_fields: Union[list, None] = None, ignore_if_not_empty: Union[list, None] = None, diff: dict = None) -&gt; tuple</code>","text":"<p>Compare desired state with current NetBox object state and return fields that need updating.</p> <p>Parameters:</p> Name Type Description Default <code>desired_state</code> <code>dict</code> <p>Dictionary with desired field values.</p> required <code>current_state</code> <code>dict</code> <p>Dictionary with current NetBox object field values.</p> required <code>ignore_fields</code> <code>list</code> <p>List of field names to ignore completely.</p> <code>None</code> <code>ignore_if_not_empty</code> <code>list</code> <p>List of field names to ignore if they have non-empty values in current_state (won't overwrite existing data).</p> <code>None</code> <code>diff</code> <code>dict</code> <p>Dictionary to accumulate field differences. If not provided, a new dictionary will be created.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing: - updates (dict): Dictionary containing only fields that need to be updated with their new values. - diff (dict): Dictionary containing the differences with '+' (new value) and '-' (old value) keys.</p> Example <p>desired = {\"serial\": \"ABC123\", \"asset_tag\": \"TAG001\", \"comments\": \"New comment\"} current = {\"serial\": \"OLD123\", \"asset_tag\": \"TAG001\", \"comments\": \"Existing\"} ignore_fields = [\"comments\"] ignore_if_not_empty = [] updates, diff = compare_netbox_object_state(desired, current, ignore_fields, ignore_if_not_empty) updates</p> <p>desired = {\"serial\": \"ABC123\", \"asset_tag\": \"TAG001\", \"comments\": \"New comment\"} current = {\"serial\": \"OLD123\", \"asset_tag\": \"\", \"comments\": \"Existing\"} ignore_fields = [] ignore_if_not_empty = [\"comments\"] updates, diff = compare_netbox_object_state(desired, current, ignore_fields, ignore_if_not_empty) updates</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>def compare_netbox_object_state(\n    desired_state: dict,\n    current_state: dict,\n    ignore_fields: Union[list, None] = None,\n    ignore_if_not_empty: Union[list, None] = None,\n    diff: dict = None,\n) -&gt; tuple:\n    \"\"\"\n    Compare desired state with current NetBox object state and return fields that need updating.\n\n    Args:\n        desired_state (dict): Dictionary with desired field values.\n        current_state (dict): Dictionary with current NetBox object field values.\n        ignore_fields (list, optional): List of field names to ignore completely.\n        ignore_if_not_empty (list, optional): List of field names to ignore if they have\n            non-empty values in current_state (won't overwrite existing data).\n        diff (dict, optional): Dictionary to accumulate field differences. If not provided,\n            a new dictionary will be created.\n\n    Returns:\n        tuple: A tuple containing:\n            - updates (dict): Dictionary containing only fields that need to be updated with their new values.\n            - diff (dict): Dictionary containing the differences with '+' (new value) and '-' (old value) keys.\n\n    Example:\n        &gt;&gt;&gt; desired = {\"serial\": \"ABC123\", \"asset_tag\": \"TAG001\", \"comments\": \"New comment\"}\n        &gt;&gt;&gt; current = {\"serial\": \"OLD123\", \"asset_tag\": \"TAG001\", \"comments\": \"Existing\"}\n        &gt;&gt;&gt; ignore_fields = [\"comments\"]\n        &gt;&gt;&gt; ignore_if_not_empty = []\n        &gt;&gt;&gt; updates, diff = compare_netbox_object_state(desired, current, ignore_fields, ignore_if_not_empty)\n        &gt;&gt;&gt; updates\n        {\"serial\": \"ABC123\"}\n\n        &gt;&gt;&gt; desired = {\"serial\": \"ABC123\", \"asset_tag\": \"TAG001\", \"comments\": \"New comment\"}\n        &gt;&gt;&gt; current = {\"serial\": \"OLD123\", \"asset_tag\": \"\", \"comments\": \"Existing\"}\n        &gt;&gt;&gt; ignore_fields = []\n        &gt;&gt;&gt; ignore_if_not_empty = [\"comments\"]\n        &gt;&gt;&gt; updates, diff = compare_netbox_object_state(desired, current, ignore_fields, ignore_if_not_empty)\n        &gt;&gt;&gt; updates\n        {\"serial\": \"ABC123\", \"asset_tag\": \"TAG001\"}\n    \"\"\"\n    ignore_fields = ignore_fields or []\n    ignore_if_not_empty = ignore_if_not_empty or []\n    updates = {}\n    diff = diff or {}\n\n    for field, desired_value in desired_state.items():\n        # Skip if field is in ignore list\n        if field in ignore_fields:\n            continue\n\n        # Get current value, default to None if field doesn't exist\n        current_value = current_state.get(field)\n\n        # Skip if field is in ignore_if_not_empty and current value is not empty\n        if field in ignore_if_not_empty and current_value:\n            continue\n\n        # Compare values and add to updates if different\n        if current_value != desired_value:\n            updates[field] = desired_value\n            diff[field] = {\n                \"-\": current_value,\n                \"+\": desired_value,\n            }\n\n    return updates, diff\n</code></pre>"},{"location":"workers/netbox/services_netbox_service/","title":"Netbox Service","text":"<p>The Netbox Service allows NorFab to integrate with Netbox - an open-source tool for documenting networks. This integration provides network engineers and administrators with powerful capabilities to manage and automate their network infrastructure using the rich data stored in Netbox.</p> <p></p>","tags":["Netbox"]},{"location":"workers/netbox/services_netbox_service/#overview","title":"Overview","text":"<p>Netbox is a comprehensive network documentation and management tool that includes features for IP address management (IPAM), data center infrastructure management (DCIM), and more. By integrating Netbox with NorFab, you can leverage the detailed network data stored in Netbox to automate various network tasks, ensuring consistency and accuracy across your network operations.</p>","tags":["Netbox"]},{"location":"workers/netbox/services_netbox_service/#netbox-compatibility","title":"Netbox Compatibility","text":"<p>NorFab supports these versions of Netbox:</p> <ol> <li>Netbox 3.6.x</li> <li>Netbox 4.0.x, 4.1.x</li> <li>Netbox 4.2.x starting with Norfab 0.8.0</li> </ol>","tags":["Netbox"]},{"location":"workers/netbox/services_netbox_service/#norfab-netbox-service-key-features","title":"NorFab Netbox Service Key Features","text":"","tags":["Netbox"]},{"location":"workers/netbox/services_netbox_service/#multi-instance-support","title":"Multi-Instance Support","text":"<p>With each NorFab Netbox worker capable of working with multiple Netbox instances, NorFab Netbox Service offers high flexibility. This allows you to manage and automate tasks across different Netbox instances, making it ideal for large-scale environments with multiple data centers or network segments.</p>","tags":["Netbox"]},{"location":"workers/netbox/services_netbox_service/#device-and-interface-management","title":"Device and Interface Management","text":"<p>The Netbox Service enables you to retrieve and manage detailed information about network devices and interfaces. This includes device configuration contexts, interface statuses, IP addresses, and more. By using this data you can  ensure that your network state is always up-to-date and accurate.</p>","tags":["Netbox"]},{"location":"workers/netbox/services_netbox_service/#graphql-and-rest-api-integration","title":"GraphQL and REST API Integration","text":"<p>The Netbox Service leverages both REST and GraphQL API provided by Netbox to perform complex queries and retrieve specific data sets. This allows for efficient data retrieval and manipulation, enabling you to automate tasks such as inventory updates, configuration audits, and compliance checks.</p>","tags":["Netbox"]},{"location":"workers/netbox/services_netbox_service/#customizable-filters-and-queries","title":"Customizable Filters and Queries","text":"<p>You can define custom filters and queries to retrieve specific data from Netbox. This flexibility allows you to tailor the data retrieval process to meet your specific needs, ensuring that you get the exact information required for your automation tasks.</p>","tags":["Netbox"]},{"location":"workers/netbox/services_netbox_service/#caching-and-performance-optimization","title":"Caching and Performance Optimization","text":"<p>The Netbox Service includes caching mechanisms to optimize performance and reduce the load on your Netbox instances. By caching frequently accessed data, you can improve the efficiency of your automation workflows and ensure faster response times.</p>","tags":["Netbox"]},{"location":"workers/netbox/services_netbox_service/#use-cases","title":"Use Cases","text":"","tags":["Netbox"]},{"location":"workers/netbox/services_netbox_service/#automated-inventory-management","title":"Automated Inventory Management","text":"<p>By integrating Netbox with NorFab, you can automate the process of updating and maintaining your network inventory. This ensures that your inventory data is always accurate and up-to-date, reducing the risk of configuration errors and improving overall network reliability.</p>","tags":["Netbox"]},{"location":"workers/netbox/services_netbox_service/#configuration-compliance-and-auditing","title":"Configuration, Compliance and Auditing","text":"<p>The Netbox Service allows you to automate configuration compliance checks and audits through integration with other services such as Nornir. By retrieving inventory data from Netbox and comparing it against network state, you can quickly identify and remediate any deviations, ensuring that your network remains compliant.</p>","tags":["Netbox"]},{"location":"workers/netbox/services_netbox_service/#getting-started","title":"Getting Started","text":"<p>To get started with the Netbox Service, you need to configure your Netbox instances and define the necessary connection parameters in your NorFab inventory. Refer to the Netbox Inventory section for detailed instructions on setting up your inventory and integrating Netbox with NorFab.</p>","tags":["Netbox"]},{"location":"workers/netbox/services_netbox_service/#conclusion","title":"Conclusion","text":"<p>The Netbox Service is a powerful addition to NorFab, providing seamless integration with Netbox and enabling advanced network automation capabilities. By leveraging the rich data stored in Netbox, you can enhance your network management processes, improve accuracy, and ensure consistency across your network operations.</p>","tags":["Netbox"]},{"location":"workers/netbox/services_netbox_service_inventory/","title":"Netbox Worker Inventory","text":"<p>Content of <code>inventory.yaml</code> need to be updated to include Netbox worker details:</p> inventory.yaml<pre><code>broker: \n  endpoint: \"tcp://127.0.0.1:5555\" \n  shared_key: \"5z1:yW}]n?UXhGmz+5CeHN1&gt;:S9k!eCh6JyIhJqO\"\n\nworkers:\n  fastapi-worker-1: \n    - netbox/netbox-worker-1.yaml\n\ntopology: \n  workers: \n    - netbox-worker-1\n</code></pre> <p>To obtain broker <code>shared_key</code> run this command on broker:</p> <pre><code>cd &lt;path/to/broker/inventory.yaml&gt;\nnfcli --show-broker-shared-key\n</code></pre> <p>Sample Netbox Worker Inventory:</p> netbox/netbox-worker-1.yaml<pre><code>service: netbox\ncache_use: True # or False, refresh, force\ncache_ttl: 31557600\nnetbox_connect_timeout: 10\nnetbox_read_timeout: 300\nbranch_create_timeout: 120\ninstances:\n  prod:\n    default: True\n    url: \"http://192.168.4.130:8000/\"\n    token: \"0123456789abcdef0123456789abcdef01234567\"\n    ssl_verify: False\n  dev:\n    url: \"http://192.168.4.131:8000/\"\n    token: \"0123456789abcdef0123456789abcdef01234567\"\n    ssl_verify: False\n  preprod:\n    url: \"http://192.168.4.132:8000/\"\n    token: \"0123456789abcdef0123456789abcdef01234567\"\n    ssl_verify: False\n</code></pre>"},{"location":"workers/netbox/services_netbox_service_tasks_create_device_interfaces/","title":"Netbox Create Device Interfaces Task","text":"<p>task api name: <code>create_device_interfaces</code></p> <p>Task to create network interfaces on one or more devices in NetBox. This task creates interfaces in bulk and only if the interfaces do not already exist in NetBox, making it idempotent and safe to run multiple times.</p> <p>The task supports alphanumeric range expansion, allowing you to create multiple interfaces with a single pattern. This is particularly useful for creating large numbers of similar interfaces efficiently.</p> <p>Tip</p> <p>The <code>create_device_interfaces</code> task automatically skips interfaces that already exist, preventing duplicate creation attempts and allowing for safe re-runs of automation tasks.</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_device_interfaces/#interface-name-range-expansion","title":"Interface Name Range Expansion","text":"<p>The task supports powerful range expansion patterns for creating multiple interfaces:</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_device_interfaces/#numeric-ranges","title":"Numeric Ranges","text":"<p>Use <code>[start-end]</code> syntax to expand numeric ranges:</p> <pre><code>interface_name: \"Ethernet[1-5]\"\n# Expands to: Ethernet1, Ethernet2, Ethernet3, Ethernet4, Ethernet5\n\ninterface_name: \"Loopback[10-12]\"\n# Expands to: Loopback10, Loopback11, Loopback12\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_device_interfaces/#comma-separated-lists","title":"Comma-Separated Lists","text":"<p>Use <code>[option1,option2,...]</code> syntax to expand lists:</p> <pre><code>interface_name: \"[ge,xe,fe]-0/0/0\"\n# Expands to: ge-0/0/0, xe-0/0/0, fe-0/0/0\n\ninterface_name: \"Port-[A,B,C]\"\n# Expands to: Port-A, Port-B, Port-C\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_device_interfaces/#multiple-range-patterns","title":"Multiple Range Patterns","text":"<p>Combine multiple range patterns in a single interface name:</p> <pre><code>interface_name: \"[ge,xe]-0/0/[0-3]\"\n# Expands to: ge-0/0/0, ge-0/0/1, ge-0/0/2, ge-0/0/3,\n#             xe-0/0/0, xe-0/0/1, xe-0/0/2, xe-0/0/3\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_device_interfaces/#multiple-interface-names","title":"Multiple Interface Names","text":"<p>Pass a list of interface names (with or without ranges):</p> <pre><code>interface_name: \n  - \"Loopback[1-3]\"\n  - \"Management1\"\n  - \"[ge,xe]-0/1/0\"\n# Expands to: Loopback1, Loopback2, Loopback3, Management1, ge-0/1/0, xe-0/1/0\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_device_interfaces/#branching-support","title":"Branching Support","text":"<p>Create Device Interfaces task is branch aware and can create interfaces within a branch. Netbox Branching Plugin needs to be installed on the Netbox instance.</p> <p>When using branches, interfaces are created in the specified branch and can be reviewed before merging into the main database.</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_device_interfaces/#norfab-netbox-create-device-interfaces-command-shell-reference","title":"NORFAB Netbox Create Device Interfaces Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>create_device_interfaces</code> task:</p> <pre><code>nf#man tree netbox.create.device-interfaces\nroot\n\u2514\u2500\u2500 netbox:    Netbox service\n    \u2514\u2500\u2500 create:    Create objects in Netbox\n        \u2514\u2500\u2500 device-interfaces:    Create devices interfaces\n            \u251c\u2500\u2500 instance:    Netbox instance name to target\n            \u251c\u2500\u2500 dry-run:    Do not commit to database\n            \u251c\u2500\u2500 branch:    Branching plugin branch name to use\n            \u251c\u2500\u2500 *devices:    List of device names or device objects to create interfaces for\n            \u251c\u2500\u2500 *interface_name:    Name(s) of the interface(s) to create\n            \u251c\u2500\u2500 interface-type:    Name(s) of the interface(s) to create, default 'other'\n            \u251c\u2500\u2500 description:    Interface description\n            \u251c\u2500\u2500 speed:    Interface speed in Kbps\n            \u251c\u2500\u2500 mtu:    Maximum transmission unit size in bytes\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u251c\u2500\u2500 workers:    Filter worker to target, default 'any'\n            \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n            \u2514\u2500\u2500 progress:    Display progress events, default 'True'\nnf#\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_device_interfaces/#usage-examples","title":"Usage Examples","text":"","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_device_interfaces/#basic-interface-creation","title":"Basic Interface Creation","text":"<p>Create a single interface on one device:</p> <pre><code>from norfab.core.nfapi import NorFab\n\nnf = NorFab()\nresult = nf.run_job(\n    service=\"netbox\",\n    task=\"create_device_interfaces\",\n    workers=\"any\",\n    kwargs={\n        \"devices\": [\"switch-01\", \"switch-02\"],\n        \"interface_name\": \"Loopbback[0-5]\",\n        \"interface_type\": \"virtual\",\n        \"description\": \"Test interfaces\"\n    }\n)\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_device_interfaces/#interface-types","title":"Interface Types","text":"<p>Common interface types supported by NetBox:</p> <ul> <li><code>virtual</code> - Virtual interfaces (loopbacks, tunnel interfaces)</li> <li><code>lag</code> - Link Aggregation Group (Port-Channel, bond)</li> <li><code>1000base-t</code> - 1G copper Ethernet</li> <li><code>10gbase-x-sfpp</code> - 10G SFP+ Ethernet</li> <li><code>25gbase-x-sfp28</code> - 25G SFP28 Ethernet</li> <li><code>40gbase-x-qsfpp</code> - 40G QSFP+ Ethernet</li> <li><code>100gbase-x-qsfp28</code> - 100G QSFP28 Ethernet</li> <li><code>other</code> - Generic interface type (default)</li> </ul> <p>Refer to your NetBox instance for the complete list of available interface types.</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_device_interfaces/#error-handling","title":"Error Handling","text":"<p>Task handles several error conditions gracefully:</p> <ol> <li>Non-existent Device: If a device doesn't exist in NetBox, an error is logged but processing continues for other devices</li> <li>Duplicate Interfaces: Existing interfaces are automatically skipped and listed in the <code>skipped</code> array</li> <li>Invalid Interface Type: NetBox will reject invalid interface types with an error message</li> <li>Branch Not Found: If a specified branch doesn't exist and the branching plugin is not available, the task will fail</li> </ol>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_device_interfaces/#best-practices","title":"Best Practices","text":"<ol> <li>Use Dry Run First: Always test with <code>dry_run: True</code> before creating interfaces in production</li> <li>Meaningful Names: Use descriptive interface names that match your device's actual interface naming</li> <li>Consistent Types: Use appropriate interface types that match the physical/virtual nature of the interfaces</li> <li>Batch Operations: Create multiple interfaces in a single call for efficiency</li> <li>Branch Usage: Use branches for testing bulk operations before committing to the main database</li> <li>Idempotency: The task is idempotent - running it multiple times with the same parameters is safe</li> </ol>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_device_interfaces/#python-api-reference","title":"Python API Reference","text":"<p>Create interfaces for one or more devices in NetBox. This task creates interfaces in bulk and only if interfaces does not exist in Netbox.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job object containing execution context and metadata.</p> required <code>devices</code> <code>list</code> <p>List of device names or device objects to create interfaces for.</p> required <code>interface_name</code> <code>Union[list, str]</code> <p>Name(s) of the interface(s) to create. Can be a single interface name as a string or multiple names as a list. Alphanumeric ranges are supported for bulk creation:</p> <ul> <li>Ethernet[1-3] -&gt; Ethernet1, Ethernet2, Ethernet3</li> <li>[ge,xe]-0/0/[0-9] -&gt; ge-0/0/0, ..., xe-0/0/0 etc.</li> </ul> required <code>interface_type</code> <code>str</code> <p>Type of interface (e.g., \"other\", \"virtual\", \"lag\", \"1000base-t\"). Defaults to \"other\".</p> <code>'other'</code> <code>instance</code> <code>Union[None, str]</code> <p>NetBox instance identifier to use. If None, uses the default instance. Defaults to None.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, simulates the operation without making actual changes. Defaults to False.</p> <code>False</code> <code>branch</code> <code>str</code> <p>NetBox branch to use for the operation. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>Any additional interface attributes</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>Result object containing the task name, execution results, and affected resources. The result dictionary contains status and details of interface creation operations.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef create_device_interfaces(\n    self,\n    job: Job,\n    devices: list,\n    interface_name: Union[list, str],\n    interface_type: str = \"other\",\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    branch: str = None,\n    **kwargs: dict,\n) -&gt; Result:\n    \"\"\"\n    Create interfaces for one or more devices in NetBox. This task creates interfaces in bulk and only\n    if interfaces does not exist in Netbox.\n\n    Args:\n        job (Job): The job object containing execution context and metadata.\n        devices (list): List of device names or device objects to create interfaces for.\n        interface_name (Union[list, str]): Name(s) of the interface(s) to create. Can be a single\n            interface name as a string or multiple names as a list. Alphanumeric ranges are\n            supported for bulk creation:\n\n            - Ethernet[1-3] -&gt; Ethernet1, Ethernet2, Ethernet3\n            - [ge,xe]-0/0/[0-9] -&gt; ge-0/0/0, ..., xe-0/0/0 etc.\n\n        interface_type (str, optional): Type of interface (e.g., \"other\", \"virtual\", \"lag\",\n            \"1000base-t\"). Defaults to \"other\".\n        instance (Union[None, str], optional): NetBox instance identifier to use. If None,\n            uses the default instance. Defaults to None.\n        dry_run (bool, optional): If True, simulates the operation without making actual changes.\n            Defaults to False.\n        branch (str, optional): NetBox branch to use for the operation. Defaults to None.\n        kwargs (dict, optional): Any additional interface attributes\n\n    Returns:\n        Result: Result object containing the task name, execution results, and affected resources.\n            The result dictionary contains status and details of interface creation operations.\n    \"\"\"\n    instance = instance or self.default_instance\n    result = {}\n    kwargs = kwargs or {}\n    ret = Result(\n        task=f\"{self.name}:create_device_interfaces\",\n        result=result,\n        resources=[instance],\n    )\n    nb = self._get_pynetbox(instance, branch=branch)\n\n    # Normalize interface_name to a list\n    if isinstance(interface_name, str):\n        interface_names = [interface_name]\n    else:\n        interface_names = interface_name\n\n    # Expand all interface name patterns\n    all_interface_names = []\n    for name_pattern in interface_names:\n        all_interface_names.extend(self.expand_alphanumeric_range(name_pattern))\n\n    job.event(\n        f\"Expanded interface names to {len(all_interface_names)} interface(s)\"\n    )\n\n    # Process each device\n    for device_name in devices:\n        result[device_name] = {\n            \"created\": [],\n            \"skipped\": [],\n        }\n\n        try:\n            # Get device from NetBox\n            nb_device = nb.dcim.devices.get(name=device_name)\n            if not nb_device:\n                msg = f\"Device '{device_name}' not found in NetBox\"\n                ret.errors.append(msg)\n                job.event(msg)\n                continue\n\n            # Get existing interfaces for this device\n            existing_interfaces = nb.dcim.interfaces.filter(device=device_name)\n            existing_interface_names = {intf.name for intf in existing_interfaces}\n\n            # Prepare interfaces to create\n            interfaces_to_create = []\n\n            for intf_name in all_interface_names:\n                if intf_name in existing_interface_names:\n                    result[device_name][\"skipped\"].append(intf_name)\n                    continue\n\n                # Build interface data\n                intf_data = {\n                    \"device\": nb_device.id,\n                    \"name\": intf_name,\n                    \"type\": interface_type,\n                    **kwargs,\n                }\n\n                interfaces_to_create.append(intf_data)\n                result[device_name][\"created\"].append(intf_name)\n\n            # Create interfaces in bulk if not dry_run\n            if interfaces_to_create and not dry_run:\n                try:\n                    nb.dcim.interfaces.create(interfaces_to_create)\n                    msg = f\"Created {len(interfaces_to_create)} interface(s) on device '{device_name}'\"\n                    job.event(msg)\n                except Exception as e:\n                    msg = f\"Failed to create interfaces on device '{device_name}': {e}\"\n                    ret.errors.append(msg)\n                    log.error(msg)\n            elif interfaces_to_create and dry_run:\n                msg = f\"[DRY RUN] Would create {len(interfaces_to_create)} interface(s) on device '{device_name}'\"\n                job.event(msg)\n\n        except Exception as e:\n            msg = f\"Error processing device '{device_name}': {e}\"\n            ret.errors.append(msg)\n            log.error(msg)\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_ip/","title":"Netbox Create IP Task","text":"<p>task api name: <code>create_ip</code></p> <p>Task to create next available IP from prefix or get existing IP address.</p> <p>Netbox service <code>create_ip</code> task integrated with Nornir service and can be called  using netbox.create_ip Jinja2 filter,  allowing to allocate IP addresses in Netbox on the fly while rendering configuration templates. </p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_ip/#branching-support","title":"Branching Support","text":"<p>Create IP task is branch aware and can create IP addresses within the branch. Netbox Branching Plugin need to be installed on Netbox instance.</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_ip/#norfab-netbox-create-ip-command-shell-reference","title":"NORFAB Netbox Create IP Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>create_ip</code> task:</p> <pre><code>nf#man tree netbox.create.ip\nroot\n\u2514\u2500\u2500 netbox:    Netbox service\n    \u2514\u2500\u2500 create:    Create objects in Netbox\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u251c\u2500\u2500 workers:    Filter worker to target, default 'any'\n            \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n            \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n            \u251c\u2500\u2500 instance:    Netbox instance name to target\n            \u251c\u2500\u2500 dry-run:    Do not commit to database\n            \u251c\u2500\u2500 *prefix:    Prefix to allocate IP address from, can also provide prefix name or filters\n            \u251c\u2500\u2500 device:    Device name to associate IP address with\n            \u251c\u2500\u2500 interface:    Device interface name to associate IP address with\n            \u251c\u2500\u2500 description:    IP address description\n            \u251c\u2500\u2500 vrf:    VRF to associate with IP address\n            \u251c\u2500\u2500 tags:    Tags to add to IP address\n            \u251c\u2500\u2500 dns_name:    IP address DNS name\n            \u251c\u2500\u2500 tenant:    Tenant name to associate with IP address\n            \u251c\u2500\u2500 comments:    IP address comments field\n            \u251c\u2500\u2500 role:    IP address functional role\n            \u2514\u2500\u2500 branch:    Branching plugin branch name to use\nnf#\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_ip/#python-api-reference","title":"Python API Reference","text":"<p>Allocate the next available IP address from a given subnet.</p> <p>This task finds or creates an IP address in NetBox, updates its metadata, optionally links it to a device/interface, and supports a dry run mode for previewing changes.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The prefix from which to allocate the IP address, could be:</p> <ul> <li>IPv4 prefix string e.g. 10.0.0.0/24</li> <li>IPv6 prefix string e.g. 2001::/64</li> <li>Prefix description string to filter by</li> <li>Dictionary with prefix filters to feed <code>pynetbox</code> get method     e.g. <code>{\"prefix\": \"10.0.0.0/24\", \"site__name\": \"foo\"}</code></li> </ul> required <code>description</code> <code>str</code> <p>A description for the allocated IP address.</p> <code>None</code> <code>device</code> <code>str</code> <p>The device associated with the IP address.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface associated with the IP address.</p> <code>None</code> <code>vrf</code> <code>str</code> <p>The VRF (Virtual Routing and Forwarding) instance.</p> <code>None</code> <code>tags</code> <code>list</code> <p>A list of tags to associate with the IP address.</p> <code>None</code> <code>dns_name</code> <code>str</code> <p>The DNS name for the IP address.</p> <code>None</code> <code>tenant</code> <code>str</code> <p>The tenant associated with the IP address.</p> <code>None</code> <code>comments</code> <code>str</code> <p>Additional comments for the IP address.</p> <code>None</code> <code>instance</code> <code>str</code> <p>The NetBox instance to use.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, do not actually allocate the IP address.</p> <code>False</code> <code>branch</code> <code>str</code> <p>Branch name to use, need to have branching plugin installed, automatically creates branch if it does not exist in Netbox.</p> <code>None</code> <code>mask_len</code> <code>int</code> <p>mask length to use for IP address on creation or to update existing IP address. On new IP address creation will create child subnet of <code>mask_len</code> within parent <code>prefix</code>, new subnet not created for existing IP addresses. <code>mask_len</code> argument ignored on dry run and ip allocated from parent prefix directly.</p> <code>None</code> <code>create_peer_ip</code> <code>bool</code> <p>If True creates IP address for link peer - remote device interface connected to requested device and interface</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the result of the IP allocation.</p> <p>Tasks execution follow these steps:</p> <ol> <li> <p>Tries to find an existing IP in NetBox matching the device/interface/description.     If found, uses it; otherwise, proceeds to create a new IP.</p> </li> <li> <p>If prefix is a string, determines if it\u2019s an IP network or a description.     Builds a filter dictionary for NetBox queries, optionally including VRF.</p> </li> <li> <p>Queries NetBox for the prefix using the constructed filter.</p> </li> <li> <p>If dry_run is True, fetches the next available IP but doesn\u2019t create it.</p> </li> <li> <p>If not a dry run, creates the next available IP in the prefix.</p> </li> <li> <p>Updates IP attributes (description, VRF, tenant, DNS name, comments, role, tags)     if provided and different from current values. Handles interface assignment and     can set the IP as primary for the device.</p> </li> <li> <p>If changes were made and not a dry run, saves the IP and device updates to NetBox.</p> </li> </ol> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"POST\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef create_ip(\n    self,\n    job: Job,\n    prefix: Union[str, dict],\n    device: Union[None, str] = None,\n    interface: Union[None, str] = None,\n    description: Union[None, str] = None,\n    vrf: Union[None, str] = None,\n    tags: Union[None, list] = None,\n    dns_name: Union[None, str] = None,\n    tenant: Union[None, str] = None,\n    comments: Union[None, str] = None,\n    role: Union[None, str] = None,\n    status: Union[None, str] = None,\n    is_primary: Union[None, bool] = None,\n    instance: Union[None, str] = None,\n    dry_run: Union[None, bool] = False,\n    branch: Union[None, str] = None,\n    mask_len: Union[None, int] = None,\n    create_peer_ip: Union[None, bool] = True,\n) -&gt; Result:\n    \"\"\"\n    Allocate the next available IP address from a given subnet.\n\n    This task finds or creates an IP address in NetBox, updates its metadata,\n    optionally links it to a device/interface, and supports a dry run mode for\n    previewing changes.\n\n    Args:\n        prefix (str): The prefix from which to allocate the IP address, could be:\n\n            - IPv4 prefix string e.g. 10.0.0.0/24\n            - IPv6 prefix string e.g. 2001::/64\n            - Prefix description string to filter by\n            - Dictionary with prefix filters to feed `pynetbox` get method\n                e.g. `{\"prefix\": \"10.0.0.0/24\", \"site__name\": \"foo\"}`\n\n        description (str, optional): A description for the allocated IP address.\n        device (str, optional): The device associated with the IP address.\n        interface (str, optional): The interface associated with the IP address.\n        vrf (str, optional): The VRF (Virtual Routing and Forwarding) instance.\n        tags (list, optional): A list of tags to associate with the IP address.\n        dns_name (str, optional): The DNS name for the IP address.\n        tenant (str, optional): The tenant associated with the IP address.\n        comments (str, optional): Additional comments for the IP address.\n        instance (str, optional): The NetBox instance to use.\n        dry_run (bool, optional): If True, do not actually allocate the IP address.\n        branch (str, optional): Branch name to use, need to have branching plugin\n            installed, automatically creates branch if it does not exist in Netbox.\n        mask_len (int, optional): mask length to use for IP address on creation or to\n            update existing IP address. On new IP address creation will create child\n            subnet of `mask_len` within parent `prefix`, new subnet not created for\n            existing IP addresses. `mask_len` argument ignored on dry run and ip allocated\n            from parent prefix directly.\n        create_peer_ip (bool, optional): If True creates IP address for link peer -\n            remote device interface connected to requested device and interface\n\n    Returns:\n        dict: A dictionary containing the result of the IP allocation.\n\n    Tasks execution follow these steps:\n\n    1. Tries to find an existing IP in NetBox matching the device/interface/description.\n        If found, uses it; otherwise, proceeds to create a new IP.\n\n    2. If prefix is a string, determines if it\u2019s an IP network or a description.\n        Builds a filter dictionary for NetBox queries, optionally including VRF.\n\n    3. Queries NetBox for the prefix using the constructed filter.\n\n    4. If dry_run is True, fetches the next available IP but doesn\u2019t create it.\n\n    5. If not a dry run, creates the next available IP in the prefix.\n\n    6. Updates IP attributes (description, VRF, tenant, DNS name, comments, role, tags)\n        if provided and different from current values. Handles interface assignment and\n        can set the IP as primary for the device.\n\n    7. If changes were made and not a dry run, saves the IP and device updates to NetBox.\n    \"\"\"\n    instance = instance or self.default_instance\n    ret = Result(task=f\"{self.name}:create_ip\", result={}, resources=[instance])\n    tags = tags or []\n    has_changes = False\n    nb_ip = None\n    nb_device = None\n    create_peer_ip_data = {}\n    nb = self._get_pynetbox(instance, branch=branch)\n\n    # source parent prefix from Netbox\n    if isinstance(prefix, str):\n        # try converting prefix to network, if fails prefix is not an IP network\n        try:\n            _ = ipaddress.ip_network(prefix)\n            is_network = True\n        except:\n            is_network = False\n        if is_network is True and vrf:\n            prefix = {\"prefix\": prefix, \"vrf__name\": vrf}\n        elif is_network is True:\n            prefix = {\"prefix\": prefix}\n        elif is_network is False and vrf:\n            prefix = {\"description\": prefix, \"vrf__name\": vrf}\n        elif is_network is False:\n            prefix = {\"description\": prefix}\n    nb_prefix = nb.ipam.prefixes.get(**prefix)\n    if not nb_prefix:\n        raise NetboxAllocationError(\n            f\"Unable to source parent prefix from Netbox - {prefix}\"\n        )\n    parent_prefix_len = int(str(nb_prefix).split(\"/\")[1])\n\n    # try to source existing IP from netbox\n    if device and interface and description:\n        nb_ip = nb.ipam.ip_addresses.get(\n            device=device,\n            interface=interface,\n            description=description,\n            parent=str(nb_prefix),\n        )\n    elif device and interface:\n        nb_ip = nb.ipam.ip_addresses.get(\n            device=device, interface=interface, parent=str(nb_prefix)\n        )\n    elif description:\n        nb_ip = nb.ipam.ip_addresses.get(\n            description=description, parent=str(nb_prefix)\n        )\n\n    # create new IP address\n    if not nb_ip:\n        # check if interface has link peer that has IP within parent prefix\n        if device and interface:\n            connection = self.get_connections(\n                job=job,\n                devices=[device],\n                interface_regex=interface,\n                instance=instance,\n                include_virtual=True,\n            )\n            if interface in connection.result[device]:\n                peer = connection.result[device][interface]\n                # do not process breakout cables\n                if isinstance(peer[\"remote_interface\"], list):\n                    peer[\"remote_interface\"] = None\n                # try to source peer ip subnet\n                nb_peer_ip = None\n                if peer[\"remote_device\"] and peer[\"remote_interface\"]:\n                    nb_peer_ip = nb.ipam.ip_addresses.get(\n                        device=peer[\"remote_device\"],\n                        interface=peer[\"remote_interface\"],\n                        parent=str(nb_prefix),\n                    )\n                # try to source peer ip subnet\n                nb_peer_prefix = None\n                if nb_peer_ip:\n                    peer_ip = ipaddress.ip_interface(nb_peer_ip.address)\n                    nb_peer_prefix = nb.ipam.prefixes.get(\n                        prefix=str(peer_ip.network),\n                        vrf__name=vrf,\n                    )\n                elif create_peer_ip and peer[\"remote_interface\"]:\n                    create_peer_ip_data = {\n                        \"device\": peer[\"remote_device\"],\n                        \"interface\": peer[\"remote_interface\"],\n                        \"vrf\": vrf,\n                        \"branch\": branch,\n                        \"tenant\": tenant,\n                        \"dry_run\": dry_run,\n                        \"tags\": tags,\n                        \"status\": status,\n                        \"create_peer_ip\": False,\n                        \"instance\": instance,\n                    }\n                # use peer subnet to create IP address\n                if nb_peer_prefix:\n                    nb_prefix = nb_peer_prefix\n                    mask_len = None  # cancel subnet creation\n                    job.event(\n                        f\"Using link peer '{peer['remote_device']}:{peer['remote_interface']}' \"\n                        f\"prefix '{nb_peer_prefix}' to create IP address\"\n                    )\n        # if mask_len provided create new subnet\n        if mask_len and not dry_run and mask_len != parent_prefix_len:\n            if mask_len &lt; parent_prefix_len:\n                raise ValueError(\n                    f\"Mask length '{mask_len}' must be longer then '{parent_prefix_len}' prefix length\"\n                )\n            prefix_status = status\n            if prefix_status not in [\"active\", \"reserved\", \"deprecated\"]:\n                prefix_status = None\n            child_subnet = self.create_prefix(\n                job=job,\n                parent=str(nb_prefix),\n                prefixlen=mask_len,\n                vrf=vrf,\n                tags=tags,\n                tenant=tenant,\n                status=prefix_status,\n                instance=instance,\n                branch=branch,\n            )\n            prefix = {\"prefix\": child_subnet.result[\"prefix\"]}\n            if vrf:\n                prefix[\"vrf__name\"] = vrf\n            nb_prefix = nb.ipam.prefixes.get(**prefix)\n\n            if not nb_prefix:\n                raise NetboxAllocationError(\n                    f\"Unable to source child prefix of mask length \"\n                    f\"'{mask_len}' from '{prefix}' parent prefix\"\n                )\n        # execute dry run on new IP\n        if dry_run is True:\n            nb_ip = nb_prefix.available_ips.list()[0]\n            ret.status = \"unchanged\"\n            ret.dry_run = True\n            ret.result = {\n                \"address\": str(nb_ip),\n                \"description\": description,\n                \"vrf\": vrf,\n                \"device\": device,\n                \"interface\": interface,\n            }\n            # add branch to results\n            if branch is not None:\n                ret.result[\"branch\"] = branch\n            return ret\n        # create new IP\n        else:\n            nb_ip = nb_prefix.available_ips.create()\n            job.event(\n                f\"Created '{nb_ip}' IP address for '{device}:{interface}' within '{nb_prefix}' prefix\"\n            )\n        ret.status = \"created\"\n    else:\n        job.event(f\"Using existing IP address {nb_ip}\")\n        ret.status = \"updated\"\n\n    # update IP address parameters\n    if description and description != nb_ip.description:\n        nb_ip.description = description\n        has_changes = True\n    if vrf and vrf != nb_ip.vrf:\n        nb_ip.vrf = {\"name\": vrf}\n        has_changes = True\n    if tenant and tenant != nb_ip.tenant:\n        nb_ip.tenant = {\"name\": tenant}\n        has_changes = True\n    if dns_name and dns_name != nb_ip.dns_name:\n        nb_ip.dns_name = dns_name\n        has_changes = True\n    if comments and comments != nb_ip.comments:\n        nb_ip.comments = comments\n        has_changes = True\n    if role and role != nb_ip.role:\n        nb_ip.role = role\n        has_changes = True\n    if tags and not any(t in nb_ip.tags for t in tags):\n        for t in tags:\n            if t not in nb_ip.tags:\n                nb_ip.tags.append({\"name\": t})\n                has_changes = True\n    if device and interface:\n        nb_interface = nb.dcim.interfaces.get(device=device, name=interface)\n        if not nb_interface:\n            raise NetboxAllocationError(\n                f\"Unable to source '{device}:{interface}' interface from Netbox\"\n            )\n        if (\n            hasattr(nb_ip, \"assigned_object\")\n            and nb_ip.assigned_object != nb_interface.id\n        ):\n            nb_ip.assigned_object_id = nb_interface.id\n            nb_ip.assigned_object_type = \"dcim.interface\"\n            if is_primary is not None:\n                nb_device = nb.dcim.devices.get(name=device)\n                nb_device.primary_ip4 = nb_ip.id\n            has_changes = True\n    if mask_len and not str(nb_ip).endswith(f\"/{mask_len}\"):\n        address = str(nb_ip).split(\"/\")[0]\n        nb_ip.address = f\"{address}/{mask_len}\"\n        has_changes = True\n\n    # save IP address into Netbox\n    if dry_run:\n        ret.status = \"unchanged\"\n        ret.dry_run = True\n    elif has_changes:\n        nb_ip.save()\n        job.event(f\"Updated '{str(nb_ip)}' IP address parameters\")\n        # make IP primary for device\n        if is_primary is True and nb_device:\n            nb_device.save()\n    else:\n        ret.status = \"unchanged\"\n\n    # form and return results\n    ret.result = {\n        \"address\": str(nb_ip),\n        \"description\": str(nb_ip.description),\n        \"vrf\": str(nb_ip.vrf) if not vrf else nb_ip.vrf[\"name\"],\n        \"device\": device,\n        \"interface\": interface,\n    }\n    # add branch to results\n    if branch is not None:\n        ret.result[\"branch\"] = branch\n\n    # create IP address for peer\n    if create_peer_ip and create_peer_ip_data:\n        job.event(\n            f\"Creating IP address for link peer '{create_peer_ip_data['device']}:{create_peer_ip_data['interface']}'\"\n        )\n        peer_ip = self.create_ip(\n            **create_peer_ip_data, prefix=str(nb_prefix), job=job\n        )\n        if peer_ip.failed == False:\n            ret.result[\"peer\"] = peer_ip.result\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_prefix/","title":"Netbox Create Prefix Task","text":"<p>task api name: <code>create_prefix</code></p> <p>Task to create next available prefix of given prefix length within parent prefix or get existing prefix. By default prefix length is <code>30</code> resulting in ptp subnet allocation.</p> <p>Netbox service <code>create_prefix</code> task integrated with Nornir service and can be called  using netbox.create_prefix Jinja2 filter,  allowing to allocate prefixes in Netbox on the fly while rendering configuration templates. </p> <p>Warning</p> <p>Netbox <code>create_prefix</code> task uses prefix description argument to deduplicate prefixes, calls to <code>create_prefix</code> task should contain identical prefix description value for same prefix.</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_prefix/#branching-support","title":"Branching Support","text":"<p>Create Prefix task is branch aware and can create IP addresses within the branch. Netbox Branching Plugin need to be installed on Netbox instance.</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_prefix/#norfab-netbox-create-prefix-command-shell-reference","title":"NORFAB Netbox Create Prefix Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>create_prefix</code> task:</p> <pre><code>nf#man tree netbox.create.prefix\nroot\n\u2514\u2500\u2500 netbox:    Netbox service\n    \u2514\u2500\u2500 create:    Create objects in Netbox\n            \u251c\u2500\u2500 instance:    Netbox instance name to target\n            \u251c\u2500\u2500 dry-run:    Do not commit to database\n            \u251c\u2500\u2500 *parent:    Parent prefix to allocate new prefix from\n            \u251c\u2500\u2500 *description:    Description for new prefix\n            \u251c\u2500\u2500 prefixlen:    The prefix length of the new prefix, default '30'\n            \u251c\u2500\u2500 vrf:    Name of the VRF to associate with the prefix\n            \u251c\u2500\u2500 tags:    List of tags to assign to the prefix\n            \u251c\u2500\u2500 tenant:    Name of the tenant to associate with the prefix\n            \u251c\u2500\u2500 comments:    Comments for the prefix\n            \u251c\u2500\u2500 role:    Role to assign to the prefix\n            \u251c\u2500\u2500 site:    Name of the site to associate with the prefix\n            \u251c\u2500\u2500 status:    Status of the prefix\n            \u251c\u2500\u2500 branch:    Branching plugin branch name to use\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u251c\u2500\u2500 workers:    Filter worker to target, default 'any'\n            \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n            \u2514\u2500\u2500 progress:    Display progress events, default 'True'\nnf#\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_create_prefix/#python-api-reference","title":"Python API Reference","text":"<p>Creates a new IP prefix in NetBox or updates an existing one.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>Union[str, dict]</code> <p>Parent prefix to allocate new prefix from, could be:</p> <ul> <li>IPv4 prefix string e.g. 10.0.0.0/24</li> <li>IPv6 prefix string e.g. 2001::/64</li> <li>Prefix description string to filter by</li> <li>Dictionary with prefix filters for <code>pynetbox</code> prefixes.get method     e.g. <code>{\"prefix\": \"10.0.0.0/24\", \"site__name\": \"foo\"}</code></li> </ul> required <code>description</code> <code>str</code> <p>Description for the new prefix, prefix description used for deduplication to source existing prefixes.</p> <code>None</code> <code>prefixlen</code> <code>int</code> <p>The prefix length of the new prefix to create, by default allocates next available /30 point-to-point prefix.</p> <code>30</code> <code>vrf</code> <code>str</code> <p>Name of the VRF to associate with the prefix.</p> <code>None</code> <code>tags</code> <code>Union[None, list]</code> <p>List of tags to assign to the prefix.</p> <code>None</code> <code>tenant</code> <code>str</code> <p>Name of the tenant to associate with the prefix.</p> <code>None</code> <code>comments</code> <code>str</code> <p>Comments for the prefix.</p> <code>None</code> <code>role</code> <code>str</code> <p>Role to assign to the prefix.</p> <code>None</code> <code>site</code> <code>str</code> <p>Name of the site to associate with the prefix.</p> <code>None</code> <code>status</code> <code>str</code> <p>Status of the prefix.</p> <code>None</code> <code>instance</code> <code>Union[None, str]</code> <p>NetBox instance identifier.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, simulates the creation without making changes.</p> <code>False</code> <code>branch</code> <code>str</code> <p>Branch name to use, need to have branching plugin installed, automatically creates branch if it does not exist in Netbox.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the outcome, including status, details of the prefix, and resources used.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    input=CreatePrefixInput,\n    fastapi={\"methods\": [\"POST\"], \"schema\": NetboxFastApiArgs.model_json_schema()},\n)\ndef create_prefix(\n    self,\n    job: Job,\n    parent: Union[str, dict],\n    description: str = None,\n    prefixlen: int = 30,\n    vrf: str = None,\n    tags: Union[None, list] = None,\n    tenant: str = None,\n    comments: str = None,\n    role: str = None,\n    site: str = None,\n    status: str = None,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    branch: str = None,\n) -&gt; Result:\n    \"\"\"\n    Creates a new IP prefix in NetBox or updates an existing one.\n\n    Args:\n        parent (Union[str, dict]): Parent prefix to allocate new prefix from, could be:\n\n            - IPv4 prefix string e.g. 10.0.0.0/24\n            - IPv6 prefix string e.g. 2001::/64\n            - Prefix description string to filter by\n            - Dictionary with prefix filters for `pynetbox` prefixes.get method\n                e.g. `{\"prefix\": \"10.0.0.0/24\", \"site__name\": \"foo\"}`\n\n        description (str): Description for the new prefix, prefix description used for\n            deduplication to source existing prefixes.\n        prefixlen (int, optional): The prefix length of the new prefix to create, by default\n            allocates next available /30 point-to-point prefix.\n        vrf (str, optional): Name of the VRF to associate with the prefix.\n        tags (Union[None, list], optional): List of tags to assign to the prefix.\n        tenant (str, optional): Name of the tenant to associate with the prefix.\n        comments (str, optional): Comments for the prefix.\n        role (str, optional): Role to assign to the prefix.\n        site (str, optional): Name of the site to associate with the prefix.\n        status (str, optional): Status of the prefix.\n        instance (Union[None, str], optional): NetBox instance identifier.\n        dry_run (bool, optional): If True, simulates the creation without making changes.\n        branch (str, optional): Branch name to use, need to have branching plugin installed,\n            automatically creates branch if it does not exist in Netbox.\n\n    Returns:\n        Result: An object containing the outcome, including status, details of the prefix, and resources used.\n    \"\"\"\n    instance = instance or self.default_instance\n    changed = {}\n    ret = Result(\n        task=f\"{self.name}:create_prefix\",\n        result={},\n        resources=[instance],\n        diff=changed,\n    )\n    tags = tags or []\n    nb_prefix = None\n    nb = self._get_pynetbox(instance, branch=branch)\n\n    job.event(\n        f\"Processing prefix create request within '{parent}' for '/{prefixlen}' subnet\"\n    )\n\n    # source parent prefix from Netbox\n    if isinstance(parent, str):\n        # check if parent prefix is IP network or description\n        try:\n            _ = ipaddress.ip_network(parent)\n            is_network = True\n        except:\n            is_network = False\n        if is_network is True and vrf:\n            parent_filters = {\"prefix\": parent, \"vrf__name\": vrf}\n        elif is_network is True:\n            parent_filters = {\"prefix\": parent}\n        elif is_network is False and vrf:\n            parent_filters = {\"description\": parent, \"vrf__name\": vrf}\n        elif is_network is False:\n            parent_filters = {\"description\": parent}\n    nb_parent_prefix = nb.ipam.prefixes.get(**parent_filters)\n    if not nb_parent_prefix:\n        raise NetboxAllocationError(\n            f\"Unable to source parent prefix from Netbox - {parent}\"\n        )\n\n    # check that parent vrf and new prefix vrf are same\n    if vrf and str(nb_parent_prefix.vrf) != vrf:\n        raise NetboxAllocationError(\n            f\"Parent prefix vrf '{nb_parent_prefix.vrf}' not same as requested child prefix vrf '{vrf}'\"\n        )\n\n    # try to source existing prefix from netbox\n    prefix_filters = {}\n    if vrf:\n        prefix_filters[\"vrf__name\"] = vrf\n    if site:\n        prefix_filters[\"site__name\"] = site\n    if description:\n        prefix_filters[\"description\"] = description\n    try:\n        if prefix_filters:\n            nb_prefix = nb.ipam.prefixes.get(\n                within=nb_parent_prefix.prefix, **prefix_filters\n            )\n    except Exception as e:\n        raise NetboxAllocationError(\n            f\"Failed to source existing prefix from Netbox using filters '{prefix_filters}', error: {e}\"\n        )\n\n    # create new prefix\n    if not nb_prefix:\n        job.event(f\"Creating new '/{prefixlen}' prefix within '{parent}' prefix\")\n        # execute dry run on new prefix\n        if dry_run is True:\n            nb_prefixes = nb_parent_prefix.available_prefixes.list()\n            if not nb_prefixes:\n                raise NetboxAllocationError(\n                    f\"Parent prefix '{parent}' has no child prefixes available\"\n                )\n            for pfx in nb_prefixes:\n                # parent prefix empty, can use first subnet as a child prefix\n                if pfx.prefix == nb_parent_prefix.prefix:\n                    nb_prefix = (\n                        nb_parent_prefix.prefix.split(\"/\")[0] + f\"/{prefixlen}\"\n                    )\n                    break\n                # find child prefix by prefixlenght\n                elif str(pfx).endswith(f\"/{prefixlen}\"):\n                    nb_prefix = str(pfx)\n                    break\n            else:\n                raise NetboxAllocationError(\n                    f\"Parent prefix '{parent}' has no child prefixes available with '/{prefixlen}' prefix length\"\n                )\n            ret.status = \"unchanged\"\n            ret.dry_run = True\n            ret.result = {\n                \"prefix\": nb_prefix,\n                \"description\": description,\n                \"parent\": nb_parent_prefix.prefix,\n                \"vrf\": vrf,\n                \"site\": site,\n            }\n            # add branch to results\n            if branch is not None:\n                ret.result[\"branch\"] = branch\n            return ret\n        # create new prefix\n        else:\n            try:\n                nb_prefix = nb_parent_prefix.available_prefixes.create(\n                    {\"prefix_length\": prefixlen}\n                )\n            except Exception as e:\n                raise NetboxAllocationError(\n                    f\"Failed creating child prefix of '/{prefixlen}' prefix length \"\n                    f\"within parent prefix '{str(nb_parent_prefix)}', error: {e}\"\n                )\n        job.event(f\"Created new '{nb_prefix}' prefix within '{parent}' prefix\")\n        ret.status = \"created\"\n    else:\n        # check existing prefix length matching requested length\n        if not nb_prefix.prefix.endswith(f\"/{prefixlen}\"):\n            raise NetboxAllocationError(\n                f\"Found existing child prefix '{nb_prefix.prefix}' with mismatch \"\n                f\"requested prefix length '/{prefixlen}'\"\n            )\n        job.event(f\"Using existing prefix {nb_prefix}\")\n\n    # update prefix parameters\n    if description and description != nb_prefix.description:\n        changed[\"description\"] = {\"-\": str(nb_prefix.description), \"+\": description}\n        nb_prefix.description = description\n    if vrf and vrf != str(nb_prefix.vrf):\n        changed[\"vrf\"] = {\"-\": str(nb_prefix.vrf), \"+\": vrf}\n        nb_prefix.vrf = {\"name\": vrf}\n    if tenant and tenant != str(nb_prefix.tenant):\n        changed[\"tenant\"] = {\n            \"-\": str(nb_prefix.tenant) if nb_prefix.tenant else None,\n            \"+\": tenant,\n        }\n        nb_prefix.tenant = {\"name\": tenant}\n    if site and str(nb_prefix.scope) != site:\n        nb_site = nb.dcim.sites.get(name=site)\n        if not nb_site:\n            raise NetboxAllocationError(f\"Failed to get '{site}' site from Netbox\")\n        changed[\"site\"] = {\n            \"-\": str(nb_prefix.scope) if nb_prefix.scope else None,\n            \"+\": nb_site.name,\n        }\n        nb_prefix.scope_type = \"dcim.site\"\n        nb_prefix.scope_id = nb_site.id\n    if status and status.lower() != nb_prefix.status:\n        changed[\"status\"] = {\"-\": str(nb_prefix.status), \"+\": status.title()}\n        nb_prefix.status = status.lower()\n    if comments and comments != nb_prefix.comments:\n        changed[\"comments\"] = {\"-\": str(nb_prefix.comments), \"+\": comments}\n        nb_prefix.comments = comments\n    if role and role != nb_prefix.role:\n        changed[\"role\"] = {\"-\": str(nb_prefix.role), \"+\": role}\n        nb_prefix.role = {\"name\": role}\n    existing_tags = [str(t) for t in nb_prefix.tags]\n    if tags and not any(t in existing_tags for t in tags):\n        changed[\"tags\"] = {\n            \"-\": existing_tags,\n            \"+\": [t for t in tags if t not in existing_tags] + existing_tags,\n        }\n        for t in tags:\n            if t not in existing_tags:\n                nb_prefix.tags.append({\"name\": t})\n\n    # save prefix into Netbox\n    if dry_run:\n        ret.status = \"unchanged\"\n        ret.dry_run = True\n        ret.diff = changed\n    elif changed:\n        ret.diff = changed\n        nb_prefix.save()\n        if ret.status != \"created\":\n            ret.status = \"updated\"\n    else:\n        ret.status = \"unchanged\"\n\n    # source vrf name\n    vrf_name = None\n    if nb_prefix.vrf:\n        if isinstance(nb_prefix.vrf, dict):\n            vrf_name = nb_prefix.vrf[\"name\"]\n        else:\n            vrf_name = nb_prefix.vrf.name\n\n    # form and return results\n    ret.result = {\n        \"prefix\": nb_prefix.prefix,\n        \"description\": nb_prefix.description,\n        \"vrf\": vrf_name,\n        \"site\": str(nb_prefix.scope) if nb_prefix.scope else site,\n        \"parent\": nb_parent_prefix.prefix,\n    }\n    # add branch to results\n    if branch is not None:\n        ret.result[\"branch\"] = branch\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_bgp_peerings/","title":"Netbox Get BGP Peerings Task","text":"<p>task api name: <code>get_bgp_peerings</code></p> <p>This task integrates with Netbox BGP Plugin and allows to fetch devices' BGP peerings.</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_bgp_peerings/#how-it-works","title":"How It Works","text":"<ul> <li>Requires NetBox BGP plugin: The worker verifies the plugin is installed on the target instance before proceeding.</li> <li>Resolves device IDs: It calls <code>get_devices()</code> to map provided device names to NetBox device IDs for accurate API queries.</li> <li>Fetches sessions via REST: Uses <code>pynetbox</code> <code>plugins.bgp.session.filter(device_id=...)</code> to retrieve sessions per device.</li> <li>Returns structured data: The result is a dictionary keyed by device name; each device contains a dictionary keyed by BGP session <code>name</code>, with the full session dict as value.</li> <li>Smart caching: Per-device cache key <code>get_bgp_peerings::&lt;device&gt;</code> is used. Modes:</li> <li><code>True</code>: Uses cache when up-to-date; performs smart update by comparing <code>last_updated</code> and fetching only changed/new sessions.</li> <li><code>False</code>: Bypasses cache entirely and does not write to cache.</li> <li><code>refresh</code>: Forces re-fetch from NetBox and overwrites cache. </li> <li><code>force</code>: Returns cached data if present without freshness checks.</li> </ul> <p>Sample BGP session data retrieved from Netbox:</p> <pre><code>\"fceos4\": {\n    \"fceos4-fceos5-eth105\": {\n        \"comments\": \"\",\n        \"created\": \"2025-12-31T08:17:39.168208Z\",\n        \"custom_fields\": {},\n        \"description\": \"BGP peering between fceos4 and fceos5 on eth105\",\n        \"device\": {\n            \"description\": \"\",\n            \"display\": \"fceos4 (UUID-123451)\",\n            \"id\": 111,\n            \"name\": \"fceos4\",\n            \"url\": \"http://192.168.1.210:8000/api/dcim/devices/111/\"\n        },\n        \"display\": \"fceos4 (UUID-123451):fceos4-fceos5-eth105\",\n        \"export_policies\": [],\n        \"id\": 4,\n        \"import_policies\": [],\n        \"last_updated\": \"2025-12-31T08:17:39.168231Z\",\n        \"local_address\": {\n            \"address\": \"10.0.2.1/30\",\n            \"description\": \"\",\n            \"display\": \"10.0.2.1/30\",\n            \"family\": {\n                \"label\": \"IPv4\",\n                \"value\": 4\n            },\n            \"id\": 123,\n            \"url\": \"http://192.168.1.210:8000/api/ipam/ip-addresses/123/\"\n        },\n        \"local_as\": {\n            \"asn\": 65100,\n            \"description\": \"BGP ASN for fceos4\",\n            \"display\": \"AS65100\",\n            \"id\": 3,\n            \"url\": \"http://192.168.1.210:8000/api/ipam/asns/3/\"\n        },\n        \"name\": \"fceos4-fceos5-eth105\",\n        \"peer_group\": {\n            \"description\": \"Test BGP peer group 1 for standard peerings\",\n            \"display\": \"TEST_BGP_PEER_GROUP_1\",\n            \"id\": 9,\n            \"name\": \"TEST_BGP_PEER_GROUP_1\",\n            \"url\": \"http://192.168.1.210:8000/api/plugins/bgp/bgppeergroup/9/\"\n        },\n        \"prefix_list_in\": null,\n        \"prefix_list_out\": null,\n        \"remote_address\": {\n            \"address\": \"10.0.2.2/30\",\n            \"description\": \"\",\n            \"display\": \"10.0.2.2/30\",\n            \"family\": {\n                \"label\": \"IPv4\",\n                \"value\": 4\n            },\n            \"id\": 124,\n            \"url\": \"http://192.168.1.210:8000/api/ipam/ip-addresses/124/\"\n        },\n        \"remote_as\": {\n            \"asn\": 65101,\n            \"description\": \"BGP ASN for fceos5\",\n            \"display\": \"AS65101\",\n            \"id\": 4,\n            \"url\": \"http://192.168.1.210:8000/api/ipam/asns/4/\"\n        },\n        \"site\": {\n            \"description\": \"\",\n            \"display\": \"SALTNORNIR-LAB\",\n            \"id\": 16,\n            \"name\": \"SALTNORNIR-LAB\",\n            \"slug\": \"saltnornir-lab\",\n            \"url\": \"http://192.168.1.210:8000/api/dcim/sites/16/\"\n        },\n        \"status\": {\n            \"label\": \"Active\",\n            \"value\": \"active\"\n        },\n        \"tags\": [],\n        \"tenant\": {\n            \"description\": \"\",\n            \"display\": \"SALTNORNIR\",\n            \"id\": 11,\n            \"name\": \"SALTNORNIR\",\n            \"slug\": \"saltnornir\",\n            \"url\": \"http://192.168.1.210:8000/api/tenancy/tenants/11/\"\n        },\n        \"url\": \"http://192.168.1.210:8000/api/plugins/bgp/bgpsession/4/\",\n        \"virtualmachine\": null\n    },\n    \"fceos4-fceos5-eth106\": {\n\n        ...etc...\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_bgp_peerings/#gotchas","title":"Gotchas","text":"<ul> <li>Supported and tested Netbox version is 4.4 onwards.</li> <li>NetBox BGP plugin required: If missing, the task fails early with an error. Confirm plugin availability and version compatibility.</li> <li>Device name must exist: Unknown devices are skipped with warnings; verify names beforehand or use <code>get_devices</code> to inspect inventory.</li> <li>Session key uniqueness: Sessions in the result are keyed by <code>name</code>. If session names are not unique per device, later entries overwrite earlier ones.</li> <li>Partial-field queries: Smart update relies on <code>fields=\"id,last_updated,name\"</code>. Older Netbox versions may not support <code>fields</code>, impacting cache comparison.</li> <li>Large datasets: Fetching many devices or sessions may be slow; prefer cache or limit <code>devices</code> for interactive runs.</li> </ul>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_bgp_peerings/#norfab-netbox-get-bgp-peerings-command-shell-reference","title":"NORFAB Netbox Get BGP Peerings Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>get_bgp_peerings</code> task:</p> <pre><code>nf#man tree netbox.get.bgp_peerings\nroot\n\u2514\u2500\u2500 netbox:    Netbox service\n    \u2514\u2500\u2500 get:    Query data from Netbox\n        \u2514\u2500\u2500 bgp-peerings:    Query Netbox BGP Peerings data\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u251c\u2500\u2500 workers:    Filter worker to target, default 'any'\n            \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n            \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n            \u251c\u2500\u2500 instance:    Netbox instance name to target\n            \u251c\u2500\u2500 dry-run:    Only return query content, do not run it\n            \u251c\u2500\u2500 branch:    Branching plugin branch name to use\n            \u251c\u2500\u2500 *devices:    Device names to query data for\n            \u2514\u2500\u2500 cache:    How to use cache, default 'True'\nnf#\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_bgp_peerings/#python-api-reference","title":"Python API Reference","text":"<p>Retrieve device BGP peerings from Netbox using REST API.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>instance</code> <code>str</code> <p>Netbox instance name.</p> <code>None</code> <code>devices</code> <code>list</code> <p>List of devices to retrieve BGP peerings for.</p> <code>None</code> <code>cache</code> <code>Union[bool, str]</code> <p>Cache usage options:</p> <ul> <li>True: Use data stored in cache if it is up to date, refresh it otherwise.</li> <li>False: Do not use cache and do not update cache.</li> <li>refresh: Ignore data in cache and replace it with data fetched from Netbox.</li> <li>force: Use data in cache without checking if it is up to date.</li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>Dictionary keyed by device name with BGP peerings details.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_bgp_peerings(\n    self,\n    job: Job,\n    instance: Union[None, str] = None,\n    devices: Union[None, list] = None,\n    cache: Union[bool, str] = None,\n) -&gt; Result:\n    \"\"\"\n    Retrieve device BGP peerings from Netbox using REST API.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        instance (str, optional): Netbox instance name.\n        devices (list, optional): List of devices to retrieve BGP peerings for.\n        cache (Union[bool, str], optional): Cache usage options:\n\n            - True: Use data stored in cache if it is up to date, refresh it otherwise.\n            - False: Do not use cache and do not update cache.\n            - refresh: Ignore data in cache and replace it with data fetched from Netbox.\n            - force: Use data in cache without checking if it is up to date.\n\n    Returns:\n        dict: Dictionary keyed by device name with BGP peerings details.\n    \"\"\"\n    instance = instance or self.default_instance\n    devices = devices or []\n    cache = self.cache_use if cache is None else cache\n    ret = Result(\n        task=f\"{self.name}:get_bgp_peerings\",\n        result={d: {} for d in devices},\n        resources=[instance],\n    )\n\n    # Check if BGP plugin is installed\n    if not self.has_plugin(\"netbox_bgp\", instance, strict=True):\n        ret.errors.append(f\"{instance} Netbox instance has no BGP Plugin installed\")\n        ret.failed = True\n        return ret\n\n    self.cache.expire()\n\n    # Get device details to collect device IDs\n    devices_result = self.get_devices(\n        job=job, devices=devices, instance=instance, cache=False\n    )\n    if devices_result.errors:\n        ret.errors.append(\n            f\"Failed to retrieve device details: {devices_result.errors}\"\n        )\n        return ret\n\n    nb = self._get_pynetbox(instance)\n\n    for device_name in devices:\n        # Skip devices not found in Netbox\n        if device_name not in devices_result.result:\n            msg = f\"Device '{device_name}' not found in Netbox\"\n            job.event(msg, resource=instance, severity=\"WARNING\")\n            log.warning(msg)\n            continue\n\n        device_id = devices_result.result[device_name][\"id\"]\n        cache_key = f\"get_bgp_peerings::{device_name}\"\n        cached_data = self.cache.get(cache_key)\n\n        # Mode: force with cached data - use cache directly\n        if cache == \"force\" and cached_data is not None:\n            ret.result[device_name] = cached_data\n            job.event(\n                f\"Using cached BGP peerings for '{device_name}' (forced)\",\n                resource=instance,\n            )\n            continue\n\n        # Mode: cache disabled - fetch without caching\n        if cache is False:\n            bgp_sessions = nb.plugins.bgp.session.filter(device_id=device_id)\n            ret.result[device_name] = {s.name: dict(s) for s in bgp_sessions}\n            job.event(\n                f\"Retrieved {len(ret.result[device_name])} BGP session(s) for '{device_name}'\",\n                resource=instance,\n            )\n            continue\n\n        # Mode: refresh or no cached data - fetch and cache\n        if cache == \"refresh\" or cached_data is None:\n            if cache == \"refresh\" and cached_data is not None:\n                self.cache.delete(cache_key, retry=True)\n            bgp_sessions = nb.plugins.bgp.session.filter(device_id=device_id)\n            ret.result[device_name] = {s.name: dict(s) for s in bgp_sessions}\n            self.cache.set(\n                cache_key, ret.result[device_name], expire=self.cache_ttl\n            )\n            job.event(\n                f\"Fetched and cached {len(ret.result[device_name])} BGP session(s) for '{device_name}'\",\n                resource=instance,\n            )\n            continue\n\n        # Mode: cache=True with cached data - smart update (only fetch changed sessions)\n        ret.result[device_name] = dict(cached_data)\n        job.event(\n            f\"Retrieved {len(cached_data)} BGP session(s) from cache for '{device_name}'\",\n            resource=instance,\n        )\n\n        # Fetch brief session info to compare timestamps\n        brief_sessions = nb.plugins.bgp.session.filter(\n            device_id=device_id, fields=\"id,last_updated,name\"\n        )\n        netbox_sessions = {\n            s.id: {\"name\": s.name, \"last_updated\": s.last_updated}\n            for s in brief_sessions\n        }\n\n        # Build lookup maps\n        cached_by_id = {s[\"id\"]: name for name, s in cached_data.items()}\n        session_ids_to_fetch = []\n        sessions_to_remove = []\n\n        # Find stale sessions (exist in both but timestamps differ) and deleted sessions\n        for session_name, cached_session in cached_data.items():\n            cached_id = cached_session[\"id\"]\n            if cached_id in netbox_sessions:\n                if (\n                    cached_session[\"last_updated\"]\n                    != netbox_sessions[cached_id][\"last_updated\"]\n                ):\n                    session_ids_to_fetch.append(cached_id)\n            else:\n                sessions_to_remove.append(session_name)\n\n        # Find new sessions in Netbox not in cache\n        for nb_id in netbox_sessions:\n            if nb_id not in cached_by_id:\n                session_ids_to_fetch.append(nb_id)\n\n        # Remove deleted sessions\n        for session_name in sessions_to_remove:\n            ret.result[device_name].pop(session_name, None)\n            job.event(\n                f\"Removed deleted session '{session_name}' from cache for '{device_name}'\",\n                resource=instance,\n            )\n\n        # Fetch updated/new sessions\n        if session_ids_to_fetch:\n            job.event(\n                f\"Fetching {len(session_ids_to_fetch)} updated BGP session(s) for '{device_name}'\",\n                resource=instance,\n            )\n            for session in nb.plugins.bgp.session.filter(id=session_ids_to_fetch):\n                ret.result[device_name][session.name] = dict(session)\n\n        # Update cache if any changes occurred\n        if session_ids_to_fetch or sessions_to_remove:\n            self.cache.set(\n                cache_key, ret.result[device_name], expire=self.cache_ttl\n            )\n            job.event(f\"Updated cache for '{device_name}'\", resource=instance)\n        else:\n            job.event(\n                f\"Using cache, it is up to date for '{device_name}'\",\n                resource=instance,\n            )\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_circuits/","title":"Netbox Get Circuits Task","text":"<p>task api name: <code>get_circuits</code></p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_circuits/#how-it-works","title":"How It Works","text":"<p>Sample devices' circuits data retrieved from Netbox:</p> <pre><code>{\n    \"netbox-worker-1.1\": {\n        \"fceos4\": {\n            \"CID1\": {\n                \"comments\": \"\",\n                \"commit_rate\": null,\n                \"custom_fields\": {},\n                \"description\": \"\",\n                \"interface\": \"eth101\",\n                \"is_active\": true,\n                \"last_updated\": \"2026-01-02T22:50:14.739796+00:00\",\n                \"provider\": \"Provider1\",\n                \"provider_account\": \"\",\n                \"remote_device\": \"fceos5\",\n                \"remote_interface\": \"eth101\",\n                \"status\": \"active\",\n                \"tags\": [],\n                \"tenant\": null,\n                \"termination_a\": {\n                    \"id\": \"36\",\n                    \"last_updated\": \"2026-01-02T22:50:12.085037+00:00\"\n                },\n                \"termination_z\": {\n                    \"id\": \"37\",\n                    \"last_updated\": \"2026-01-02T22:50:14.498313+00:00\"\n                },\n                \"type\": \"DarkFibre\"\n            },\n            \"CID2\": {\n              ... etc ...\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_circuits/#norfab-netbox-get-circuits-command-shell-reference","title":"NORFAB Netbox Get Circuits Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>get_circuits</code> task:</p> <pre><code>nf#man tree netbox.get.circuits\nroot\n\u2514\u2500\u2500 netbox:    Netbox service\n    \u2514\u2500\u2500 get:    Query data from Netbox\n        \u2514\u2500\u2500 circuits:    Query Netbox circuits data for devices\n            \u251c\u2500\u2500 instance:    Netbox instance name to target\n            \u251c\u2500\u2500 workers:    Filter worker to target, default 'any'\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u251c\u2500\u2500 device-list:    Device names to query data for\n            \u251c\u2500\u2500 dry-run:    Only return query content, do not run it\n            \u251c\u2500\u2500 cid:    List of circuit identifiers to retrieve data for\n            \u2514\u2500\u2500 cache:    How to use cache, default 'True'\nnf#\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_circuits/#python-api-reference","title":"Python API Reference","text":"<p>Retrieve circuit information for specified devices from Netbox.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>devices</code> <code>list</code> <p>List of device names to retrieve circuits for.</p> required <code>cid</code> <code>list</code> <p>List of circuit IDs to filter by.</p> <code>None</code> <code>instance</code> <code>str</code> <p>Netbox instance to query.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, perform a dry run without making changes. Defaults to False.</p> <code>False</code> <code>cache</code> <code>Union[bool, str]</code> <p>Cache usage options:</p> <ul> <li>True: Use data stored in cache if it is up to date, refresh it otherwise.</li> <li>False: Do not use cache and do not update cache.</li> <li>\"refresh\": Ignore data in cache and replace it with data fetched from Netbox.</li> <li>\"force\": Use data in cache without checking if it is up to date.</li> </ul> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>dictionary keyed by device names with circuits data.</p> <p>Task to retrieve device's circuits data from Netbox.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_circuits(\n    self,\n    job: Job,\n    devices: list,\n    cid: Union[None, list] = None,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    cache: Union[bool, str] = True,\n) -&gt; Result:\n    \"\"\"\n    Retrieve circuit information for specified devices from Netbox.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        devices (list): List of device names to retrieve circuits for.\n        cid (list, optional): List of circuit IDs to filter by.\n        instance (str, optional): Netbox instance to query.\n        dry_run (bool, optional): If True, perform a dry run without making changes. Defaults to False.\n        cache (Union[bool, str], optional): Cache usage options:\n\n            - True: Use data stored in cache if it is up to date, refresh it otherwise.\n            - False: Do not use cache and do not update cache.\n            - \"refresh\": Ignore data in cache and replace it with data fetched from Netbox.\n            - \"force\": Use data in cache without checking if it is up to date.\n\n    Returns:\n        dict: dictionary keyed by device names with circuits data.\n\n    Task to retrieve device's circuits data from Netbox.\n    \"\"\"\n    cid = cid or []\n    log.info(\n        f\"{self.name}:get_circuits - {instance or self.default_instance} Netbox, \"\n        f\"devices {', '.join(devices)}, cid {cid}\"\n    )\n    instance = instance or self.default_instance\n\n    # form final result object\n    ret = Result(\n        task=f\"{self.name}:get_circuits\",\n        result={d: {} for d in devices},\n        resources=[instance],\n    )\n    cache = self.cache_use if cache is None else cache\n    cid = cid or []\n    circuit_fields = [\n        \"cid\",\n        \"tags {name}\",\n        \"provider {name}\",\n        \"commit_rate\",\n        \"description\",\n        \"status\",\n        \"type {name}\",\n        \"provider_account {name}\",\n        \"tenant {name}\",\n        \"termination_a {id last_updated}\",\n        \"termination_z {id last_updated}\",\n        \"custom_fields\",\n        \"comments\",\n        \"last_updated\",\n    ]\n\n    # form initial circuits filters based on devices' sites and cid list\n    circuits_filters = {}\n    device_data = self.get_devices(\n        job=job, devices=copy.deepcopy(devices), instance=instance, cache=cache\n    )\n    sites = list(set([i[\"site\"][\"slug\"] for i in device_data.result.values()]))\n    if self.nb_version[instance] &gt;= (4, 4, 0):\n        slist = str(sites).replace(\"'\", '\"')  # swap quotes\n        if cid:\n            clist = str(cid).replace(\"'\", '\"')  # swap quotes\n            circuits_filters = \"{terminations: {site: {slug: {in_list: slist}}}, cid: {in_list: clist}}\"\n            circuits_filters = circuits_filters.replace(\"slist\", slist).replace(\n                \"clist\", clist\n            )\n        else:\n            circuits_filters = \"{terminations: {site: {slug: {in_list: slist }}}}\"\n            circuits_filters = circuits_filters.replace(\"slist\", slist)\n    else:\n        raise UnsupportedNetboxVersion(\n            f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n            f\"minimum required version is {self.compatible_ge_v4}\"\n        )\n\n    log.info(\n        f\"{self.name}:get_circuits - constructed circuits filters: '{circuits_filters}'\"\n    )\n\n    if cache == True or cache == \"force\":\n        log.info(f\"{self.name}:get_circuits - retrieving circuits data from cache\")\n        cid_list = []  #  new cid list for follow up query\n        # retrieve last updated data from Netbox for circuits and their terminations\n        last_updated = self.graphql(\n            job=job,\n            obj=\"circuit_list\",\n            filters=circuits_filters,\n            fields=[\n                \"cid\",\n                \"last_updated\",\n                \"termination_a {id last_updated}\",\n                \"termination_z {id last_updated}\",\n            ],\n            dry_run=dry_run,\n            instance=instance,\n        )\n        last_updated.raise_for_status(f\"{self.name} - get circuits query failed\")\n\n        # return dry run result\n        if dry_run:\n            ret.result[\"get_circuits_dry_run\"] = last_updated.result\n            return ret\n\n        # retrieve circuits data from cache\n        self.cache.expire()  # remove expired items from cache\n        for device in devices:\n            for circuit in last_updated.result:\n                circuit_cache_key = f\"get_circuits::{circuit['cid']}\"\n                log.info(\n                    f\"{self.name}:get_circuits - searching cache for key {circuit_cache_key}\"\n                )\n                # check if cache is up to date and use it if so\n                if circuit_cache_key in self.cache:\n                    cache_ckt = self.cache[circuit_cache_key]\n                    # check if device uses this circuit\n                    if device not in cache_ckt:\n                        continue\n                    # use cache forcefully\n                    if cache == \"force\":\n                        ret.result[device][circuit[\"cid\"]] = cache_ckt[device]\n                    # check circuit cache is up to date\n                    if cache_ckt[device][\"last_updated\"] != circuit[\"last_updated\"]:\n                        continue\n                    if (\n                        cache_ckt[device][\"termination_a\"]\n                        and circuit[\"termination_a\"]\n                        and cache_ckt[device][\"termination_a\"][\"last_updated\"]\n                        != circuit[\"termination_a\"][\"last_updated\"]\n                    ):\n                        continue\n                    if (\n                        cache_ckt[device][\"termination_z\"]\n                        and circuit[\"termination_z\"]\n                        and cache_ckt[device][\"termination_z\"][\"last_updated\"]\n                        != circuit[\"termination_z\"][\"last_updated\"]\n                    ):\n                        continue\n                    ret.result[device][circuit[\"cid\"]] = cache_ckt[device]\n                    log.info(\n                        f\"{self.name}:get_circuits - {circuit['cid']} retrieved data from cache\"\n                    )\n                elif circuit[\"cid\"] not in cid_list:\n                    cid_list.append(circuit[\"cid\"])\n                    log.info(\n                        f\"{self.name}:get_circuits - {circuit['cid']} no cache data found, fetching from Netbox\"\n                    )\n        # form new filters dictionary to fetch remaining circuits data\n        circuits_filters = {}\n        if cid_list:\n            cid_list = str(cid_list).replace(\"'\", '\"')  # swap quotes\n            if self.nb_version[instance] &gt;= (4, 4, 0):\n                circuits_filters = \"{cid: {in_list: cid_list}}\"\n                circuits_filters = circuits_filters.replace(\"cid_list\", cid_list)\n            else:\n                raise UnsupportedNetboxVersion(\n                    f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n                    f\"minimum required version is {self.compatible_ge_v4}\"\n                )\n    # ignore cache data, fetch circuits from netbox\n    elif cache == False or cache == \"refresh\":\n        pass\n\n    if circuits_filters:\n        query_result = self.graphql(\n            job=job,\n            obj=\"circuit_list\",\n            filters=circuits_filters,\n            fields=circuit_fields,\n            dry_run=dry_run,\n            instance=instance,\n        )\n        query_result.raise_for_status(f\"{self.name} - get circuits query failed\")\n\n        # return dry run result\n        if dry_run is True:\n            return query_result\n\n        all_circuits = query_result.result\n\n        # iterate over circuits and map them to devices\n        log.info(\n            f\"{self.name}:get_circuits - retrieved data for {len(all_circuits)} \"\n            f\"circuits from netbox, mapping circuits to devices\"\n        )\n        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n            results = [\n                executor.submit(\n                    self._map_circuit, job, circuit, ret, instance, devices, cache\n                )\n                for circuit in all_circuits\n            ]\n            for _ in concurrent.futures.as_completed(results):\n                continue\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_connections/","title":"Netbox Get Connections Task","text":"<p>task api name: <code>get_connections</code></p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_connections/#get-connections-sample-usage","title":"Get Connections Sample Usage","text":"","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_connections/#norfab-netbox-get-connections-command-shell-reference","title":"NORFAB Netbox Get Connections Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>get_connections</code> task:</p> <pre><code>nf#man tree netbox.get.connections\nroot\n\u2514\u2500\u2500 netbox:    Netbox service\n    \u2514\u2500\u2500 get:    Query data from Netbox\n        \u2514\u2500\u2500 connections:    Query Netbox connections data for devices\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u251c\u2500\u2500 workers:    Filter worker to target, default 'any'\n            \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n            \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n            \u251c\u2500\u2500 instance:    Netbox instance name to target\n            \u251c\u2500\u2500 dry-run:    Only return query content, do not run it\n            \u251c\u2500\u2500 devices:    Device names to query data for\n            \u251c\u2500\u2500 cache:    How to use cache, default 'True'\n            \u2514\u2500\u2500 add-cables:    Add interfaces directly attached cables details\nnf#\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_connections/#python-api-reference","title":"Python API Reference","text":"<p>Retrieve interface connection details for specified devices from Netbox.</p> <p>This task retrieves these connections:</p> <ul> <li>Physical interfaces connections</li> <li>Child/virtual interfaces connections using parent interface connections details</li> <li>Lag interfaces connections using member ports connections details</li> <li>Lag child interfaces connections using member ports connections details</li> <li>Console port and console server ports connections</li> <li>Connections to provider networks for physical, child/virtual and lag interfaces</li> </ul> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>devices</code> <code>list</code> <p>List of device names to retrieve connections for.</p> required <code>instance</code> <code>str</code> <p>Netbox instance name for the GraphQL query.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, perform a dry run without making actual changes.</p> <code>False</code> <code>cables</code> <code>bool</code> <p>if True includes interfaces' directly attached cables details</p> <code>False</code> <code>include_virtual</code> <code>bool</code> <p>if True include connections for virtual and LAG interfaces</p> <code>True</code> <code>interface_regex</code> <code>str</code> <p>Regex pattern to match interfaces, console ports and console server ports by name, case insensitive.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing connection details for each device:</p> <pre><code>{\n    \"netbox-worker-1.2\": {\n        \"r1\": {\n            \"Console\": {\n                \"breakout\": false,\n                \"remote_device\": \"termserv1\",\n                \"remote_device_status\": \"active\",\n                \"remote_interface\": \"ConsoleServerPort1\",\n                \"remote_termination_type\": \"consoleserverport\",\n                \"termination_type\": \"consoleport\"\n            },\n            \"eth1\": {\n                \"breakout\": false,\n                \"remote_device\": \"r2\",\n                \"remote_device_status\": \"active\",\n                \"remote_interface\": \"eth8\",\n                \"remote_termination_type\": \"interface\",\n                \"termination_type\": \"interface\"\n            }\n        }\n    }\n}\n</code></pre> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error in the GraphQL query or data retrieval process.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_connections(\n    self,\n    job: Job,\n    devices: list[str],\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    cables: bool = False,\n    cache: Union[bool, str] = None,\n    include_virtual: bool = True,\n    interface_regex: Union[None, str] = None,\n) -&gt; Result:\n    \"\"\"\n    Retrieve interface connection details for specified devices from Netbox.\n\n    This task retrieves these connections:\n\n    - Physical interfaces connections\n    - Child/virtual interfaces connections using parent interface connections details\n    - Lag interfaces connections using member ports connections details\n    - Lag child interfaces connections using member ports connections details\n    - Console port and console server ports connections\n    - Connections to provider networks for physical, child/virtual and lag interfaces\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        devices (list): List of device names to retrieve connections for.\n        instance (str, optional): Netbox instance name for the GraphQL query.\n        dry_run (bool, optional): If True, perform a dry run without making actual changes.\n        cables (bool, optional): if True includes interfaces' directly attached cables details\n        include_virtual (bool, optional): if True include connections for virtual and LAG interfaces\n        interface_regex (str, optional): Regex pattern to match interfaces, console ports and\n            console server ports by name, case insensitive.\n\n    Returns:\n        dict: A dictionary containing connection details for each device:\n\n            ```\n            {\n                \"netbox-worker-1.2\": {\n                    \"r1\": {\n                        \"Console\": {\n                            \"breakout\": false,\n                            \"remote_device\": \"termserv1\",\n                            \"remote_device_status\": \"active\",\n                            \"remote_interface\": \"ConsoleServerPort1\",\n                            \"remote_termination_type\": \"consoleserverport\",\n                            \"termination_type\": \"consoleport\"\n                        },\n                        \"eth1\": {\n                            \"breakout\": false,\n                            \"remote_device\": \"r2\",\n                            \"remote_device_status\": \"active\",\n                            \"remote_interface\": \"eth8\",\n                            \"remote_termination_type\": \"interface\",\n                            \"termination_type\": \"interface\"\n                        }\n                    }\n                }\n            }\n            ```\n\n    Raises:\n        Exception: If there is an error in the GraphQL query or data retrieval process.\n    \"\"\"\n    instance = instance or self.default_instance\n    ret = Result(\n        task=f\"{self.name}:get_connections\",\n        result={d: {} for d in devices},\n        resources=[instance],\n    )\n\n    # form lists of fields to request from netbox\n    cable_fields = \"\"\"\n        cable {\n            type\n            status\n            tenant {name}\n            label\n            tags {name}\n            length\n            length_unit\n            custom_fields\n        }\n    \"\"\"\n    interfaces_fields = [\n        \"name\",\n        \"type\",\n        \"device {name, status}\",\n        \"\"\"\n        member_interfaces {\n          name\n          connected_endpoints {\n            __typename\n            ... on ProviderNetworkType {name}\n            ... on InterfaceType {name, device {name, status}, child_interfaces {name}, lag {name child_interfaces {name}}}\n          }\n        }\n        \"\"\",\n        \"\"\"\n        parent {\n          name\n          type\n          member_interfaces {\n            name\n            connected_endpoints {\n              __typename\n              ... on ProviderNetworkType {name}\n              ... on InterfaceType {name, device {name, status}, child_interfaces {name}, lag {name child_interfaces {name}}}\n            }\n          }\n          connected_endpoints {\n            __typename\n            ... on ProviderNetworkType {name}\n            ... on InterfaceType {name, device {name, status}, child_interfaces {name}, lag {name child_interfaces {name}}}\n          }\n        }\n        \"\"\",\n        \"\"\"\n        connected_endpoints {\n            __typename \n            ... on ProviderNetworkType {name}\n            ... on InterfaceType {name, device {name, status}, child_interfaces {name}, lag {name child_interfaces {name}}}\n        }\n        \"\"\",\n    ]\n    interfaces_fields.append(\n        \"\"\"\n        link_peers {\n            __typename\n            ... on InterfaceType {name device {name, status}}\n            ... on FrontPortType {name device {name, status}}\n            ... on RearPortType {name device {name, status}}\n        }\n    \"\"\"\n    )\n    console_ports_fields = [\n        \"name\",\n        \"device {name, status}\",\n        \"type\",\n        \"\"\"connected_endpoints {\n          __typename \n          ... on ConsoleServerPortType {name device {name, status}}\n        }\"\"\",\n        \"\"\"link_peers {\n          __typename\n          ... on ConsoleServerPortType {name device {name, status}}\n          ... on FrontPortType {name device {name, status}}\n          ... on RearPortType {name device {name, status}}\n        }\"\"\",\n    ]\n    console_server_ports_fields = [\n        \"name\",\n        \"device {name, status}\",\n        \"type\",\n        \"\"\"connected_endpoints {\n          __typename \n          ... on ConsolePortType {name device {name, status}}\n        }\"\"\",\n        \"\"\"link_peers {\n          __typename\n          ... on ConsolePortType {name device {name, status}}\n          ... on FrontPortType {name device {name, status}}\n          ... on RearPortType {name device {name, status}}\n        }\"\"\",\n    ]\n    power_outlet_fields = [\n        \"name\",\n        \"device {name, status}\",\n        \"type\",\n        \"\"\"connected_endpoints {\n          __typename \n          ... on PowerPortType {name device {name, status}}\n        }\"\"\",\n        \"\"\"link_peers {\n          __typename\n          ... on PowerPortType {name device {name, status}}\n        }\"\"\",\n    ]\n\n    # check if need to include cables info\n    if cables is True:\n        interfaces_fields.append(cable_fields)\n        console_ports_fields.append(cable_fields)\n        console_server_ports_fields.append(cable_fields)\n        power_outlet_fields.append(cable_fields)\n\n    # form query dictionary with aliases to get data from Netbox\n    dlist = str(devices).replace(\"'\", '\"')  # swap quotes\n    if self.nb_version[instance] &gt;= (4, 4, 0):\n        if interface_regex:\n            filters = (\n                \"{device: {name: {in_list: \"\n                + dlist\n                + \"}}, \"\n                + \"name: {i_regex: \"\n                + f'\"{interface_regex}\"'\n                + \"}}\"\n            )\n        else:\n            filters = \"{device: {name: {in_list: \" + dlist + \"}}}\"\n    else:\n        raise UnsupportedNetboxVersion(\n            f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n            f\"minimum required version is {self.compatible_ge_v4}\"\n        )\n\n    queries = {\n        \"interface\": {\n            \"obj\": \"interface_list\",\n            \"filters\": filters,\n            \"fields\": interfaces_fields,\n        },\n        \"consoleport\": {\n            \"obj\": \"console_port_list\",\n            \"filters\": filters,\n            \"fields\": console_ports_fields,\n        },\n        \"consoleserverport\": {\n            \"obj\": \"console_server_port_list\",\n            \"filters\": filters,\n            \"fields\": console_server_ports_fields,\n        },\n        \"poweroutlet\": {\n            \"obj\": \"power_outlet_list\",\n            \"filters\": filters,\n            \"fields\": power_outlet_fields,\n        },\n    }\n\n    # retrieve full list of devices interface with all cables\n    query_result = self.graphql(\n        job=job, queries=queries, instance=instance, dry_run=dry_run\n    )\n\n    # return dry run result\n    if dry_run:\n        return query_result\n\n    all_ports = query_result.result\n    if not all_ports:\n        return ret\n\n    # extract physical interfaces connections\n    for port_type, ports in all_ports.items():\n        for port in ports:\n            # skip ports that have no remote device connected\n            endpoints = port[\"connected_endpoints\"]\n            if not endpoints or not all(i for i in endpoints):\n                continue\n\n            # extract required parameters\n            cable = port.get(\"cable\", {})\n            device_name = port[\"device\"][\"name\"]\n            port_name = port[\"name\"]\n            link_peers = port[\"link_peers\"]\n            remote_termination_type = endpoints[0][\"__typename\"].lower()\n            remote_termination_type = remote_termination_type.replace(\"type\", \"\")\n\n            # form initial connection dictionary\n            connection = {\n                \"breakout\": len(endpoints) &gt; 1,\n                \"remote_termination_type\": remote_termination_type,\n                \"termination_type\": port_type,\n            }\n\n            # add remote connection details\n            if remote_termination_type == \"providernetwork\":\n                connection[\"remote_device\"] = None\n                connection[\"remote_device_status\"] = None\n                connection[\"remote_interface\"] = None\n                connection[\"provider\"] = endpoints[0][\"name\"]\n            else:\n                remote_interface = endpoints[0][\"name\"]\n                if len(endpoints) &gt; 1:\n                    remote_interface = list(sorted([i[\"name\"] for i in endpoints]))\n                connection[\"remote_interface\"] = remote_interface\n                connection[\"remote_device\"] = endpoints[0][\"device\"][\"name\"]\n                connection[\"remote_device_status\"] = endpoints[0][\"device\"][\n                    \"status\"\n                ]\n\n            # add cable and its peer details\n            if cables:\n                peer_termination_type = link_peers[0][\"__typename\"].lower()\n                peer_termination_type = peer_termination_type.replace(\"type\", \"\")\n                cable[\"peer_termination_type\"] = peer_termination_type\n                cable[\"peer_device\"] = link_peers[0].get(\"device\", {}).get(\"name\")\n                cable[\"peer_interface\"] = link_peers[0].get(\"name\")\n                if len(link_peers) &gt; 1:  # handle breakout cable\n                    cable[\"peer_interface\"] = [i[\"name\"] for i in link_peers]\n                connection[\"cable\"] = cable\n\n            # add physical connection to the results\n            ret.result[device_name][port_name] = connection\n\n    # extract virtual interfaces connections\n    for port_type, ports in all_ports.items():\n        for port in ports:\n            # add child virtual interfaces connections\n            if (\n                not include_virtual\n                or port[\"type\"] != \"virtual\"\n                or not port[\"parent\"]\n            ):\n                continue\n            device_name = port[\"device\"][\"name\"]\n            interface_name = port[\"name\"]\n            parent = port[\"parent\"]\n            connection = {\n                \"remote_device\": None,\n                \"remote_device_status\": None,\n                \"remote_interface\": None,\n                \"remote_termination_type\": \"virtual\",\n                \"termination_type\": \"virtual\",\n            }\n            # find connection endpoint\n            if parent[\"type\"] == \"lag\":\n                try:\n                    endpoint = parent[\"member_interfaces\"][0][\n                        \"connected_endpoints\"\n                    ][0]\n                except:\n                    continue\n            elif parent[\"connected_endpoints\"]:\n                try:\n                    endpoint = parent[\"connected_endpoints\"][0]\n                except:\n                    continue\n            connection[\"remote_device\"] = endpoint[\"device\"][\"name\"]\n            connection[\"remote_device_status\"] = endpoint[\"device\"][\"status\"]\n            remote_termination_type = endpoint[\"__typename\"].lower()\n            remote_termination_type = remote_termination_type.replace(\"type\", \"\")\n            # collect virtual interfaces facing provider\n            if remote_termination_type == \"providernetwork\":\n                connection[\"provider\"] = endpoint[\"name\"]\n            # find matching remote virtual interface for LAG subif\n            elif \".\" in interface_name and parent[\"type\"] == \"lag\":\n                subif_id = interface_name.split(\".\")[1]\n                for remote_child in endpoint[\"lag\"][\"child_interfaces\"]:\n                    if remote_child[\"name\"].endswith(f\".{subif_id}\"):\n                        connection[\"remote_interface\"] = remote_child[\"name\"]\n                        break\n                # no matching subinterface found, associate child interface with remote interface\n                else:\n                    connection[\"remote_interface\"] = endpoint[\"lag\"][\"name\"]\n                    connection[\"remote_termination_type\"] = \"lag\"\n            # find matching remote virtual interface for physical interface subif\n            elif \".\" in interface_name:\n                subif_id = interface_name.split(\".\")[1]\n                for remote_child in endpoint[\"child_interfaces\"]:\n                    if remote_child[\"name\"].endswith(f\".{subif_id}\"):\n                        connection[\"remote_interface\"] = remote_child[\"name\"]\n                        break\n                # no matching subinterface found, associate child interface with remote interface\n                else:\n                    connection[\"remote_interface\"] = endpoint[\"name\"]\n                    connection[\"remote_termination_type\"] = remote_termination_type\n            # add virtual interface connection to results\n            ret.result[device_name][interface_name] = connection\n\n    # extract LAG interfaces connections\n    for port_type, ports in all_ports.items():\n        for port in ports:\n            if not include_virtual or port[\"type\"] != \"lag\":\n                continue\n            device_name = port[\"device\"][\"name\"]\n            interface_name = port[\"name\"]\n            connection = {\n                \"remote_device\": None,\n                \"remote_device_status\": None,\n                \"remote_interface\": None,\n                \"remote_termination_type\": \"lag\",\n                \"termination_type\": \"lag\",\n            }\n            try:\n                endpoint = port[\"member_interfaces\"][0][\"connected_endpoints\"][0]\n            except:\n                continue\n            remote_termination_type = endpoint[\"__typename\"].lower()\n            remote_termination_type = remote_termination_type.replace(\"type\", \"\")\n            # collect lag interfaces facing provider\n            if remote_termination_type == \"providernetwork\":\n                connection[\"provider\"] = endpoint[\"name\"]\n            # find remote lag interface\n            elif endpoint[\"lag\"]:\n                connection[\"remote_interface\"] = endpoint[\"lag\"][\"name\"]\n                connection[\"remote_device\"] = endpoint[\"device\"][\"name\"]\n                connection[\"remote_device_status\"] = endpoint[\"device\"][\"status\"]\n            # add lag interface connection to results\n            ret.result[device_name][interface_name] = connection\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_containerlab_inventory/","title":"Netbox Get Containerlab Inventory Task","text":"<p>task api name: <code>get_containerlab_inventory</code></p> <p>This task designed to provide Containerlab workers with inventory data sourced from Netbox to deploy lab topologies.</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_containerlab_inventory/#get-containerlab-inventory-sample-usage","title":"Get Containerlab Inventory Sample Usage","text":"<p>Below is an example of how to fetch Containerlab topology inventory data from Netbox for two devices named <code>fceos4</code> and <code>fceos5</code>.</p> <pre><code>nf#netbox get containerlab-inventory devices fceos4 fceos5 lab-name foobar\n--------------------------------------------- Job Events -----------------------------------------------\n31-May-2025 13:10:14.477 7d434ed4e24c4a69af5d52797d7a187e job started\n31-May-2025 13:10:14.525 INFO netbox-worker-1.1 running netbox.get_containerlab_inventory  - Fetching devices data from Netbox\n31-May-2025 13:10:14.594 INFO netbox-worker-1.1 running netbox.get_containerlab_inventory  - Node added fceos4\n31-May-2025 13:10:14.600 INFO netbox-worker-1.1 running netbox.get_containerlab_inventory  - Node added fceos5\n31-May-2025 13:10:14.606 INFO netbox-worker-1.1 running netbox.get_containerlab_inventory  - Fetching connections data from Netbox\n31-May-2025 13:10:15.211 INFO netbox-worker-1.1 running netbox.get_containerlab_inventory  - Link added fceos5:eth1 - fceos4:eth1\n31-May-2025 13:10:15.217 INFO netbox-worker-1.1 running netbox.get_containerlab_inventory  - Link added fceos5:eth2 - fceos4:eth2\n31-May-2025 13:10:15.225 INFO netbox-worker-1.1 running netbox.get_containerlab_inventory  - Link added fceos5:eth3 - fceos4:eth3\n31-May-2025 13:10:15.232 INFO netbox-worker-1.1 running netbox.get_containerlab_inventory  - Link added fceos5:eth4 - fceos4:eth4\n31-May-2025 13:10:15.238 INFO netbox-worker-1.1 running netbox.get_containerlab_inventory  - Link added fceos5:eth6 - fceos4:eth6\n31-May-2025 13:10:15.244 INFO netbox-worker-1.1 running netbox.get_containerlab_inventory  - Link added fceos5:eth7 - fceos4:eth7\n31-May-2025 13:10:15.250 INFO netbox-worker-1.1 running netbox.get_containerlab_inventory  - Link added fceos5:eth8 - fceos4:eth101\n31-May-2025 13:10:15.257 INFO netbox-worker-1.1 running netbox.get_containerlab_inventory  - Link added fceos5:eth11 - fceos4:eth11\n31-May-2025 13:10:15.580 INFO netbox-worker-1.1 running netbox.get_containerlab_inventory  - Renaming fceos4 interfaces\n31-May-2025 13:10:15.587 INFO netbox-worker-1.1 running netbox.get_containerlab_inventory  - Renaming fceos5 interfaces\n31-May-2025 13:10:15.808 7d434ed4e24c4a69af5d52797d7a187e job completed in 1.331 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\nnetbox-worker-1.1:\n  mgmt:\n    ipv4-subnet: 172.100.100.0/24\n    network: br-foobar\n  name: foobar\n  topology:\n    links:\n    - endpoints:\n      - interface: eth1\n        node: fceos5\n      - interface: eth1\n        node: fceos4\n      type: veth\n    - endpoints:\n      - interface: eth2\n        node: fceos5\n      - interface: eth2\n        node: fceos4\n      type: veth\n    - endpoints:\n      - interface: eth3\n        node: fceos5\n      - interface: eth3\n        node: fceos4\n      type: veth\n    - endpoints:\n      - interface: eth4\n        node: fceos5\n      - interface: eth4\n        node: fceos4\n      type: veth\n    - endpoints:\n      - interface: eth6\n        node: fceos5\n      - interface: eth6\n        node: fceos4\n      type: veth\n    - endpoints:\n      - interface: eth7\n        node: fceos5\n      - interface: eth7\n        node: fceos4\n      type: veth\n    - endpoints:\n      - interface: eth8\n        node: fceos5\n      - interface: eth101\n        node: fceos4\n      type: veth\n    - endpoints:\n      - interface: eth11\n        node: fceos5\n      - interface: eth11\n        node: fceos4\n      type: veth\n    nodes:\n      fceos4:\n        image: ceosimage:4.30.0F\n        kind: ceos\n        mgmt-ipv4: 172.100.100.2\n        ports:\n        - 12000:22/tcp\n        - 12001:23/tcp\n        - 12002:80/tcp\n        - 12003:161/udp\n        - 12005:830/tcp\n        - 12006:8080/tcp\n      fceos5:\n        image: ceosimage:4.30.0F\n        kind: ceos\n        mgmt-ipv4: 172.100.100.3\n        ports:\n        - 12007:22/tcp\n        - 12008:23/tcp\n        - 12009:80/tcp\n        - 12010:161/udp\n        - 12011:443/tcp\n        - 12012:830/tcp\n        - 12013:8080/tcp\n\nnf#\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_containerlab_inventory/#norfab-netbox-get-containerlab-inventory-command-shell-reference","title":"NORFAB Netbox Get Containerlab Inventory Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>get_containerlab_inventory</code> task:</p> <pre><code>nf#man tree netbox.get.containerlab-inventory\nroot\n\u2514\u2500\u2500 netbox:    Netbox service\n    \u2514\u2500\u2500 get:    Query data from Netbox\n        \u2514\u2500\u2500 containerlab-inventory:    Query Netbox and construct Containerlab inventory\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u251c\u2500\u2500 workers:    Filter worker to target, default 'any'\n            \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n            \u251c\u2500\u2500 lab-name:    Lab name to generate lab inventory for\n            \u251c\u2500\u2500 tenant:    Tenant name to generate lab inventory for\n            \u2502   \u251c\u2500\u2500 tenant:    Filter devices by tenants\n            \u2502   \u251c\u2500\u2500 device-name-contains:    Filter devices by name pattern\n            \u2502   \u251c\u2500\u2500 model:    Filter devices by models\n            \u2502   \u251c\u2500\u2500 platform:    Filter devices by platforms\n            \u2502   \u251c\u2500\u2500 region:    Filter devices by regions\n            \u2502   \u251c\u2500\u2500 role:    Filter devices by roles\n            \u2502   \u251c\u2500\u2500 site:    Filter devices by sites\n            \u2502   \u251c\u2500\u2500 status:    Filter devices by statuses\n            \u2502   \u2514\u2500\u2500 tag:    Filter devices by tags\n            \u251c\u2500\u2500 devices:    List of devices to generate lab inventory for\n            \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n            \u251c\u2500\u2500 netbox-instance:    Name of Netbox instance to pull inventory from\n            \u251c\u2500\u2500 ipv4-subnet:    IPv4 management subnet to use for lab, default '172.100.100.0/24'\n            \u251c\u2500\u2500 image:    Docker image to use for all nodes\n            \u2514\u2500\u2500 ports:    Range of TCP/UDP ports to use for nodes, default '[12000, 13000]'\nnf#\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_containerlab_inventory/#python-api-reference","title":"Python API Reference","text":"<p>Retrieve and construct Containerlab inventory from NetBox data.</p> <p>Containerlab node details must be defined under device configuration context <code>norfab.containerlab</code> path, for example:</p> <pre><code>{\n    \"norfab\": {\n        \"containerlab\": {\n            \"kind\": \"ceos\",\n            \"image\": \"ceos:latest\",\n            \"mgmt-ipv4\": \"172.100.100.10/24\",\n            \"ports\": [\n                {10000: 22},\n                {10001: 830}\n            ],\n\n            ... any other node parameters ...\n\n            \"interfaces_rename\": [\n                {\n                    \"find\": \"eth\",\n                    \"replace\": \"Eth\",\n                    \"use_regex\": false\n                }\n            ]\n        }\n    }\n}\n</code></pre> <p>For complete list of parameters refer to Containerlab nodes definition.</p> <p>Special handling given to these parameters:</p> <ul> <li><code>lab_name</code> - if not provided uses <code>tenant</code> argument value as a lab name</li> <li><code>kind</code> - uses device platform field value by default</li> <li><code>image</code> - uses <code>image</code> value if provided, otherwise uses <code>{kind}:latest</code></li> <li><code>interfaces_rename</code> - a list of one or more interface renaming instructions,     each item must have <code>find</code> and <code>replace</code> defined, optional <code>use_regex</code>     flag specifies whether to use regex based pattern substitution.</li> </ul> <p>To retrieve topology data from Netbox at least one of these arguments must be provided to identify a set of devices to include into Containerlab topology:</p> <ul> <li><code>tenant</code> - topology constructed using all devices and links that belong to this tenant</li> <li><code>devices</code> - creates topology only using devices in the lists</li> <li><code>filters</code> - list of device filters to retrieve from Netbox and add to topology</li> </ul> <p>If multiple of above arguments provided, resulting lab topology is a sum of all devices matched.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>lab_name</code> <code>(str, Mandatory)</code> <p>Name of containerlab to construct inventory for.</p> <code>None</code> <code>tenant</code> <code>str</code> <p>Construct topology using given tenant's devices</p> <code>None</code> <code>filters</code> <code>list</code> <p>List of filters to apply when retrieving devices from NetBox.</p> <code>None</code> <code>devices</code> <code>list</code> <p>List of specific devices to retrieve from NetBox.</p> <code>None</code> <code>instance</code> <code>str</code> <p>NetBox instance to use.</p> <code>None</code> <code>image</code> <code>str</code> <p>Default containerlab image to use,</p> <code>None</code> <code>ipv4_subnet</code> <code>(str, Optional)</code> <p>Management subnet to use to IP number nodes starting with 2nd IP in the subnet, in assumption that 1st IP is a default gateway.</p> <code>'172.100.100.0/24'</code> <code>ports</code> <code>(tuple, Optional)</code> <p>Ports range to use for nodes.</p> <code>(12000, 15000)</code> <code>ports_map</code> <code>(dict, Optional)</code> <p>dictionary keyed by node name with list of ports maps to use,</p> <code>None</code> <code>cache</code> <code>Union[bool, str]</code> <p>Cache usage options:</p> <ul> <li>True: Use data stored in cache if it is up to date, refresh it otherwise.</li> <li>False: Do not use cache and do not update cache.</li> <li>\"refresh\": Ignore data in cache and replace it with data fetched from Netbox.</li> <li>\"force\": Use data in cache without checking if it is up to date.</li> </ul> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>Containerlab inventory dictionary containing lab topology data</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_containerlab_inventory(\n    self,\n    job: Job,\n    lab_name: str = None,\n    tenant: Union[None, str] = None,\n    filters: Union[None, list] = None,\n    devices: Union[None, list] = None,\n    instance: Union[None, str] = None,\n    image: Union[None, str] = None,\n    ipv4_subnet: str = \"172.100.100.0/24\",\n    ports: tuple = (12000, 15000),\n    ports_map: Union[None, dict] = None,\n    cache: Union[bool, str] = False,\n) -&gt; Result:\n    \"\"\"\n    Retrieve and construct Containerlab inventory from NetBox data.\n\n    Containerlab node details must be defined under device configuration\n    context `norfab.containerlab` path, for example:\n\n    ```\n    {\n        \"norfab\": {\n            \"containerlab\": {\n                \"kind\": \"ceos\",\n                \"image\": \"ceos:latest\",\n                \"mgmt-ipv4\": \"172.100.100.10/24\",\n                \"ports\": [\n                    {10000: 22},\n                    {10001: 830}\n                ],\n\n                ... any other node parameters ...\n\n                \"interfaces_rename\": [\n                    {\n                        \"find\": \"eth\",\n                        \"replace\": \"Eth\",\n                        \"use_regex\": false\n                    }\n                ]\n            }\n        }\n    }\n    ```\n\n    For complete list of parameters refer to\n    [Containerlab nodes definition](https://containerlab.dev/manual/nodes/).\n\n    Special handling given to these parameters:\n\n    - `lab_name` - if not provided uses `tenant` argument value as a lab name\n    - `kind` - uses device platform field value by default\n    - `image` - uses `image` value if provided, otherwise uses `{kind}:latest`\n    - `interfaces_rename` - a list of one or more interface renaming instructions,\n        each item must have `find` and `replace` defined, optional `use_regex`\n        flag specifies whether to use regex based pattern substitution.\n\n    To retrieve topology data from Netbox at least one of these arguments must be provided\n    to identify a set of devices to include into Containerlab topology:\n\n    - `tenant` - topology constructed using all devices and links that belong to this tenant\n    - `devices` - creates topology only using devices in the lists\n    - `filters` - list of device filters to retrieve from Netbox and add to topology\n\n    If multiple of above arguments provided, resulting lab topology is a sum of all\n    devices matched.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        lab_name (str, Mandatory): Name of containerlab to construct inventory for.\n        tenant (str, optional): Construct topology using given tenant's devices\n        filters (list, optional): List of filters to apply when retrieving devices from NetBox.\n        devices (list, optional): List of specific devices to retrieve from NetBox.\n        instance (str, optional): NetBox instance to use.\n        image (str, optional): Default containerlab image to use,\n        ipv4_subnet (str, Optional): Management subnet to use to IP number nodes\n            starting with 2nd IP in the subnet, in assumption that 1st IP is a default gateway.\n        ports (tuple, Optional): Ports range to use for nodes.\n        ports_map (dict, Optional): dictionary keyed by node name with list of ports maps to use,\n        cache (Union[bool, str], optional): Cache usage options:\n\n            - True: Use data stored in cache if it is up to date, refresh it otherwise.\n            - False: Do not use cache and do not update cache.\n            - \"refresh\": Ignore data in cache and replace it with data fetched from Netbox.\n            - \"force\": Use data in cache without checking if it is up to date.\n\n    Returns:\n        dict: Containerlab inventory dictionary containing lab topology data\n    \"\"\"\n    devices = devices or []\n    filters = filters or []\n    nodes, links = {}, []\n    ports_map = ports_map or {}\n    endpts_done = []  # to deduplicate links\n    instance = instance or self.default_instance\n    # handle lab name and tenant name with filters\n    if lab_name is None and tenant:\n        lab_name = tenant\n    # add tenant filters\n    if tenant:\n        filters = filters or [{}]\n        for filter in filters:\n            if self.nb_version[instance] &gt;= (4, 4, 0):\n                filter[\"tenant\"] = f'{{name: {{exact: \"{tenant}\"}}}}'\n            else:\n                raise UnsupportedNetboxVersion(\n                    f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n                    f\"minimum required version is {self.compatible_ge_v4}\"\n                )\n\n    # construct inventory\n    inventory = {\n        \"name\": lab_name,\n        \"topology\": {\"nodes\": nodes, \"links\": links},\n        \"mgmt\": {\"ipv4-subnet\": ipv4_subnet, \"network\": f\"br-{lab_name}\"},\n    }\n    ret = Result(\n        task=f\"{self.name}:get_containerlab_inventory\",\n        result=inventory,\n        resources=[instance],\n    )\n    mgmt_net = ipaddress.ip_network(ipv4_subnet)\n    available_ips = list(mgmt_net.hosts())[1:]\n\n    # run checks\n    if not available_ips:\n        raise ValueError(f\"Need IPs to allocate, but '{ipv4_subnet}' given\")\n    if ports:\n        available_ports = list(range(ports[0], ports[1]))\n    else:\n        raise ValueError(f\"Need ports to allocate, but '{ports}' given\")\n\n    # check Netbox status\n    netbox_status = self.get_netbox_status(job=job, instance=instance)\n    if netbox_status.result[instance][\"status\"] is False:\n        ret.failed = True\n        ret.messages = [f\"Netbox status is no good: {netbox_status}\"]\n        return ret\n\n    # retrieve devices data\n    log.debug(\n        f\"Fetching devices from {instance} Netbox instance, devices '{devices}', filters '{filters}'\"\n    )\n    job.event(\"Fetching devices data from Netbox\")\n    nb_devices = self.get_devices(\n        job=job,\n        filters=filters,\n        devices=devices,\n        instance=instance,\n        cache=cache,\n    )\n\n    # form Containerlab nodes inventory\n    for device_name, device in nb_devices.result.items():\n        node = device[\"config_context\"].get(\"norfab\", {}).get(\"containerlab\", {})\n        # populate node parameters\n        if not node.get(\"kind\"):\n            if device[\"platform\"]:\n                node[\"kind\"] = device[\"platform\"][\"name\"]\n            else:\n                msg = (\n                    f\"{device_name} - has no 'kind' of 'platform' defined, skipping\"\n                )\n                log.warning(msg)\n                job.event(msg, severity=\"WARNING\")\n                continue\n        if not node.get(\"image\"):\n            if image:\n                node[\"image\"] = image\n            else:\n                node[\"image\"] = f\"{node['kind']}:latest\"\n        if not node.get(\"mgmt-ipv4\"):\n            if available_ips:\n                node[\"mgmt-ipv4\"] = f\"{available_ips.pop(0)}\"\n            else:\n                raise RuntimeError(\"Run out of IP addresses to allocate\")\n        if not node.get(\"ports\"):\n            node[\"ports\"] = []\n            # use ports map\n            if ports_map.get(device_name):\n                node[\"ports\"] = ports_map[device_name]\n            # allocate next-available ports\n            else:\n                for port in [\n                    \"22/tcp\",\n                    \"23/tcp\",\n                    \"80/tcp\",\n                    \"161/udp\",\n                    \"443/tcp\",\n                    \"830/tcp\",\n                    \"8080/tcp\",\n                ]:\n                    if available_ports:\n                        node[\"ports\"].append(f\"{available_ports.pop(0)}:{port}\")\n                    else:\n                        raise RuntimeError(\n                            \"Run out of TCP / UDP ports to allocate.\"\n                        )\n\n        # save node content\n        nodes[device_name] = node\n        job.event(f\"Node added {device_name}\")\n\n    # return if no nodes found for provided parameters\n    if not nodes:\n        msg = f\"{self.name} - no devices found in Netbox\"\n        log.error(msg)\n        ret.failed = True\n        ret.messages = [\n            f\"{self.name} - no devices found in Netbox, \"\n            f\"devices - '{devices}', filters - '{filters}'\"\n        ]\n        ret.errors = [msg]\n        return ret\n\n    job.event(\"Fetching connections data from Netbox\")\n\n    # query interface connections data from netbox\n    nb_connections = self.get_connections(\n        job=job, devices=list(nodes), instance=instance, cache=cache\n    )\n    # save connections data to links inventory\n    while nb_connections.result:\n        device, device_connections = nb_connections.result.popitem()\n        for interface, connection in device_connections.items():\n            # skip non ethernet links\n            if connection.get(\"termination_type\") != \"interface\":\n                continue\n            # skip orphaned links\n            if not connection.get(\"remote_interface\"):\n                continue\n            # skip connections to devices that are not part of lab\n            if connection[\"remote_device\"] not in nodes:\n                continue\n            endpoints = []\n            link = {\n                \"type\": \"veth\",\n                \"endpoints\": endpoints,\n            }\n            # add A node\n            endpoints.append(\n                {\n                    \"node\": device,\n                    \"interface\": interface,\n                }\n            )\n            # add B node\n            endpoints.append({\"node\": connection[\"remote_device\"]})\n            if connection.get(\"breakout\") is True:\n                endpoints[-1][\"interface\"] = connection[\"remote_interface\"][0]\n            else:\n                endpoints[-1][\"interface\"] = connection[\"remote_interface\"]\n            # save the link\n            a_end = (\n                endpoints[0][\"node\"],\n                endpoints[0][\"interface\"],\n            )\n            b_end = (\n                endpoints[1][\"node\"],\n                endpoints[1][\"interface\"],\n            )\n            if a_end not in endpts_done and b_end not in endpts_done:\n                endpts_done.append(a_end)\n                endpts_done.append(b_end)\n                links.append(link)\n                job.event(\n                    f\"Link added {endpoints[0]['node']}:{endpoints[0]['interface']}\"\n                    f\" - {endpoints[1]['node']}:{endpoints[1]['interface']}\"\n                )\n\n    # query circuits connections data from netbox\n    nb_circuits = self.get_circuits(\n        job=job, devices=list(nodes), instance=instance, cache=cache\n    )\n    # save circuits data to hosts' inventory\n    while nb_circuits.result:\n        device, device_circuits = nb_circuits.result.popitem()\n        for cid, circuit in device_circuits.items():\n            # skip circuits not connected to devices\n            if not circuit.get(\"remote_interface\"):\n                continue\n            # skip circuits to devices that are not part of lab\n            if circuit[\"remote_device\"] not in nodes:\n                continue\n            endpoints = []\n            link = {\n                \"type\": \"veth\",\n                \"endpoints\": endpoints,\n            }\n            # add A node\n            endpoints.append(\n                {\n                    \"node\": device,\n                    \"interface\": circuit[\"interface\"],\n                }\n            )\n            # add B node\n            endpoints.append(\n                {\n                    \"node\": circuit[\"remote_device\"],\n                    \"interface\": circuit[\"remote_interface\"],\n                }\n            )\n            # save the link\n            a_end = (\n                endpoints[0][\"node\"],\n                endpoints[0][\"interface\"],\n            )\n            b_end = (\n                endpoints[1][\"node\"],\n                endpoints[1][\"interface\"],\n            )\n            if a_end not in endpts_done and b_end not in endpts_done:\n                endpts_done.append(a_end)\n                endpts_done.append(b_end)\n                links.append(link)\n                job.event(\n                    f\"Link added {endpoints[0]['node']}:{endpoints[0]['interface']}\"\n                    f\" - {endpoints[1]['node']}:{endpoints[1]['interface']}\"\n                )\n\n    # rename links' interfaces\n    for node_name, node_data in nodes.items():\n        interfaces_rename = node_data.pop(\"interfaces_rename\", [])\n        if interfaces_rename:\n            job.event(f\"Renaming {node_name} interfaces\")\n        for item in interfaces_rename:\n            if not item.get(\"find\") or not item.get(\"replace\"):\n                log.error(\n                    f\"{self.name} - interface rename need to have\"\n                    f\" 'find' and 'replace' defined, skipping: {item}\"\n                )\n                continue\n            pattern = item[\"find\"]\n            replace = item[\"replace\"]\n            use_regex = item.get(\"use_regex\", False)\n            # go over links one by one and rename interfaces\n            for link in links:\n                for endpoint in link[\"endpoints\"]:\n                    if endpoint[\"node\"] != node_name:\n                        continue\n                    if use_regex:\n                        renamed = re.sub(\n                            pattern,\n                            replace,\n                            endpoint[\"interface\"],\n                        )\n                    else:\n                        renamed = endpoint[\"interface\"].replace(pattern, replace)\n                    if endpoint[\"interface\"] != renamed:\n                        msg = f\"{node_name} interface {endpoint['interface']} renamed to {renamed}\"\n                        log.debug(msg)\n                        job.event(msg)\n                        endpoint[\"interface\"] = renamed\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_devices/","title":"Netbox Get Devices Task","text":"<p>task api name: <code>get_devices</code></p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_devices/#get-devices-sample-usage","title":"Get Devices Sample Usage","text":"","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_devices/#norfab-netbox-get-devices-command-shell-reference","title":"NORFAB Netbox Get Devices Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>get_devices</code> task:</p> <pre><code>nf#man tree netbox.get.devices\nroot\n\u2514\u2500\u2500 netbox:    Netbox service\n    \u2514\u2500\u2500 get:    Query data from Netbox\n        \u2514\u2500\u2500 devices:    Query Netbox devices data\n            \u251c\u2500\u2500 instance:    Netbox instance name to target\n            \u251c\u2500\u2500 workers:    Filter worker to target, default 'any'\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u251c\u2500\u2500 filters:    List of device filters dictionaries as a JSON string, examples: [{\"q\": \"ceos1\"}]\n            \u251c\u2500\u2500 devices:    Device names to query data for\n            \u251c\u2500\u2500 dry-run:    Only return query content, do not run it\n            \u2514\u2500\u2500 cache:    How to use cache, default 'True'\nnf#\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_devices/#python-api-reference","title":"Python API Reference","text":"<p>Retrieves device data from Netbox using the GraphQL API.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>filters</code> <code>list</code> <p>A list of filter dictionaries to filter devices.</p> <code>None</code> <code>instance</code> <code>str</code> <p>The Netbox instance name.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, only returns the query content without executing it. Defaults to False.</p> <code>False</code> <code>devices</code> <code>list</code> <p>A list of device names to query data for.</p> <code>None</code> <code>cache</code> <code>Union[bool, str]</code> <p>Cache usage options:</p> <ul> <li>True: Use data stored in cache if it is up to date, refresh it otherwise.</li> <li>False: Do not use cache and do not update cache.</li> <li>\"refresh\": Ignore data in cache and replace it with data fetched from Netbox.</li> <li>\"force\": Use data in cache without checking if it is up to date.</li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary keyed by device name with device data.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the GraphQL query fails or if there are errors in the query result.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_devices(\n    self,\n    job: Job,\n    filters: Union[None, list] = None,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    devices: Union[None, list] = None,\n    cache: Union[bool, str] = None,\n) -&gt; Result:\n    \"\"\"\n    Retrieves device data from Netbox using the GraphQL API.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        filters (list, optional): A list of filter dictionaries to filter devices.\n        instance (str, optional): The Netbox instance name.\n        dry_run (bool, optional): If True, only returns the query content without executing it. Defaults to False.\n        devices (list, optional): A list of device names to query data for.\n        cache (Union[bool, str], optional): Cache usage options:\n\n            - True: Use data stored in cache if it is up to date, refresh it otherwise.\n            - False: Do not use cache and do not update cache.\n            - \"refresh\": Ignore data in cache and replace it with data fetched from Netbox.\n            - \"force\": Use data in cache without checking if it is up to date.\n\n    Returns:\n        dict: A dictionary keyed by device name with device data.\n\n    Raises:\n        Exception: If the GraphQL query fails or if there are errors in the query result.\n    \"\"\"\n    instance = instance or self.default_instance\n    ret = Result(task=f\"{self.name}:get_devices\", result={}, resources=[instance])\n    cache = self.cache_use if cache is None else cache\n    filters = filters or []\n    devices = devices or []\n    queries = {}  # devices queries\n    device_fields = [\n        \"name\",\n        \"last_updated\",\n        \"custom_field_data\",\n        \"tags {name}\",\n        \"device_type {model}\",\n        \"role {name}\",\n        \"config_context\",\n        \"tenant {name}\",\n        \"platform {name}\",\n        \"serial\",\n        \"asset_tag\",\n        \"site {name slug tags{name} }\",\n        \"location {name}\",\n        \"rack {name}\",\n        \"status\",\n        \"primary_ip4 {address}\",\n        \"primary_ip6 {address}\",\n        \"airflow\",\n        \"position\",\n        \"id\",\n    ]\n\n    if cache == True or cache == \"force\":\n        # retrieve last updated data from Netbox for devices\n        last_updated_query = {\n            f\"devices_by_filter_{index}\": {\n                \"obj\": \"device_list\",\n                \"filters\": filter_item,\n                \"fields\": [\"name\", \"last_updated\"],\n            }\n            for index, filter_item in enumerate(filters)\n        }\n        if devices:\n            # use cache data without checking if it is up to date for cached devices\n            if cache == \"force\":\n                for device_name in list(devices):\n                    device_cache_key = f\"get_devices::{device_name}\"\n                    if device_cache_key in self.cache:\n                        devices.remove(device_name)\n                        ret.result[device_name] = self.cache[device_cache_key]\n            # query netbox last updated data for devices\n            if self.nb_version[instance] &gt;= (4, 4, 0):\n                dlist = '[\"{dl}\"]'.format(dl='\", \"'.join(devices))\n                filters_dict = {\"name\": f\"{{in_list: {dlist}}}\"}\n            else:\n                raise UnsupportedNetboxVersion(\n                    f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n                    f\"minimum required version is {self.compatible_ge_v4}\"\n                )\n            last_updated_query[\"devices_by_devices_list\"] = {\n                \"obj\": \"device_list\",\n                \"filters\": filters_dict,\n                \"fields\": [\"name\", \"last_updated\"],\n            }\n        last_updated = self.graphql(\n            job=job,\n            queries=last_updated_query,\n            instance=instance,\n            dry_run=dry_run,\n        )\n        last_updated.raise_for_status(f\"{self.name} - get devices query failed\")\n\n        # return dry run result\n        if dry_run:\n            ret.result[\"get_devices_dry_run\"] = last_updated.result\n            return ret\n\n        # try to retrieve device data from cache\n        self.cache.expire()  # remove expired items from cache\n        for devices_list in last_updated.result.values():\n            for device in devices_list:\n                device_cache_key = f\"get_devices::{device['name']}\"\n                # check if cache is up to date and use it if so\n                if device_cache_key in self.cache and (\n                    self.cache[device_cache_key].get(\"last_updated\")\n                    == device[\"last_updated\"]\n                    or cache == \"force\"\n                ):\n                    ret.result[device[\"name\"]] = self.cache[device_cache_key]\n                    # remove device from list of devices to retrieve\n                    if device[\"name\"] in devices:\n                        devices.remove(device[\"name\"])\n                # cache old or no cache, fetch device data\n                elif device[\"name\"] not in devices:\n                    devices.append(device[\"name\"])\n    # ignore cache data, fetch data from netbox\n    elif cache == False or cache == \"refresh\":\n        queries = {\n            f\"devices_by_filter_{index}\": {\n                \"obj\": \"device_list\",\n                \"filters\": filter_item,\n                \"fields\": device_fields,\n            }\n            for index, filter_item in enumerate(filters)\n        }\n\n    # fetch devices data from Netbox\n    if devices or queries:\n        if devices:\n            if self.nb_version[instance] &gt;= (4, 4, 0):\n                dlist = '[\"{dl}\"]'.format(dl='\", \"'.join(devices))\n                filters_dict = {\"name\": f\"{{in_list: {dlist}}}\"}\n            else:\n                raise UnsupportedNetboxVersion(\n                    f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n                    f\"minimum required version is {self.compatible_ge_v4}\"\n                )\n            queries[\"devices_by_devices_list\"] = {\n                \"obj\": \"device_list\",\n                \"filters\": filters_dict,\n                \"fields\": device_fields,\n            }\n\n        # send queries\n        query_result = self.graphql(\n            job=job, queries=queries, instance=instance, dry_run=dry_run\n        )\n\n        # check for errors\n        if query_result.errors:\n            msg = f\"{self.name} - get devices query failed with errors:\\n{query_result.errors}\"\n            raise Exception(msg)\n\n        # return dry run result\n        if dry_run:\n            ret.result[\"get_devices_dry_run\"] = query_result.result\n            return ret\n\n        # process devices data\n        devices_data = query_result.result\n        for devices_list in devices_data.values():\n            for device in devices_list:\n                if device[\"name\"] not in ret.result:\n                    device_name = device.pop(\"name\")\n                    # cache device data\n                    if cache != False:\n                        cache_key = f\"get_devices::{device_name}\"\n                        self.cache.set(cache_key, device, expire=self.cache_ttl)\n                    # add device data to return result\n                    ret.result[device_name] = device\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_interfaces/","title":"Netbox Get Interfaces Task","text":"<p>task api name: <code>get_interfaces</code></p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_interfaces/#get-interfaces-sample-usage","title":"Get Interfaces Sample Usage","text":"","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_interfaces/#norfab-netbox-get-interfaces-command-shell-reference","title":"NORFAB Netbox Get Interfaces Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>get_interfaces</code> task:</p> <pre><code>nf#man tree netbox.get.interfaces\nroot\n\u2514\u2500\u2500 netbox:    Netbox service\n    \u2514\u2500\u2500 get:    Query data from Netbox\n        \u2514\u2500\u2500 interfaces:    Query Netbox device interfaces data\n            \u251c\u2500\u2500 instance:    Netbox instance name to target\n            \u251c\u2500\u2500 workers:    Filter worker to target, default 'any'\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u251c\u2500\u2500 *devices:    Devices to retrieve interface for\n            \u251c\u2500\u2500 ip-addresses:    Retrieves interface IP addresses\n            \u251c\u2500\u2500 inventory-items:    Retrieves interface inventory items\n            \u2514\u2500\u2500 dry-run:    Only return query content, do not run it\nnf#\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_interfaces/#python-api-reference","title":"Python API Reference","text":"<p>Retrieve device interfaces from Netbox using GraphQL API.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>instance</code> <code>str</code> <p>Netbox instance name.</p> <code>None</code> <code>devices</code> <code>list</code> <p>List of devices to retrieve interfaces for.</p> <code>None</code> <code>interface_regex</code> <code>str</code> <p>Regex pattern to match interfaces by name, case insensitive.</p> <code>None</code> <code>ip_addresses</code> <code>bool</code> <p>If True, retrieves interface IPs. Defaults to False.</p> <code>False</code> <code>inventory_items</code> <code>bool</code> <p>If True, retrieves interface inventory items. Defaults to False.</p> <code>False</code> <code>dry_run</code> <code>bool</code> <p>If True, only return query content, do not run it. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>Dictionary keyed by device name with interface details.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no interfaces data is returned for the specified devices.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_interfaces(\n    self,\n    job: Job,\n    instance: Union[None, str] = None,\n    devices: Union[None, list] = None,\n    interface_regex: Union[None, str] = None,\n    ip_addresses: bool = False,\n    inventory_items: bool = False,\n    dry_run: bool = False,\n    cache: Union[bool, str] = None,\n) -&gt; Result:\n    \"\"\"\n    Retrieve device interfaces from Netbox using GraphQL API.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        instance (str, optional): Netbox instance name.\n        devices (list, optional): List of devices to retrieve interfaces for.\n        interface_regex (str, optional): Regex pattern to match interfaces by name, case insensitive.\n        ip_addresses (bool, optional): If True, retrieves interface IPs. Defaults to False.\n        inventory_items (bool, optional): If True, retrieves interface inventory items. Defaults to False.\n        dry_run (bool, optional): If True, only return query content, do not run it. Defaults to False.\n\n    Returns:\n        dict: Dictionary keyed by device name with interface details.\n\n    Raises:\n        Exception: If no interfaces data is returned for the specified devices.\n    \"\"\"\n    instance = instance or self.default_instance\n    devices = devices or []\n    ret = Result(\n        task=f\"{self.name}:get_interfaces\",\n        result={d: {} for d in devices},\n        resources=[instance],\n    )\n\n    intf_fields = [\n        \"name\",\n        \"enabled\",\n        \"description\",\n        \"mtu\",\n        \"parent {name}\",\n        \"mode\",\n        \"untagged_vlan {vid name}\",\n        \"vrf {name}\",\n        \"tagged_vlans {vid name}\",\n        \"tags {name}\",\n        \"custom_fields\",\n        \"last_updated\",\n        \"bridge {name}\",\n        \"child_interfaces {name}\",\n        \"bridge_interfaces {name}\",\n        \"member_interfaces {name}\",\n        \"wwn\",\n        \"duplex\",\n        \"speed\",\n        \"id\",\n        \"device {name}\",\n        \"label\",\n        \"mark_connected\",\n    ]\n    intf_fields.append(\"mac_addresses {mac_address}\")\n\n    # add IP addresses to interfaces fields\n    if ip_addresses:\n        intf_fields.append(\n            \"ip_addresses {address status role dns_name description custom_fields last_updated tenant {name} tags {name}}\"\n        )\n\n    # form interfaces query dictionary\n    dlist = str(devices).replace(\"'\", '\"')  # swap quotes\n    if self.nb_version[instance] &gt;= (4, 4, 0):\n        # add interface name regex filter\n        if interface_regex:\n            filters = (\n                \"{device: {name: {in_list: \"\n                + dlist\n                + \"}}\"\n                + \", name: {i_regex: \"\n                + f'\"{interface_regex}\"'\n                + \"}}\"\n            )\n        else:\n            filters = \"{device: {name: {in_list: \" + dlist + \"}}}\"\n    else:\n        raise UnsupportedNetboxVersion(\n            f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n            f\"minimum required version is {self.compatible_ge_v4}\"\n        )\n\n    queries = {\n        \"interfaces\": {\n            \"obj\": \"interface_list\",\n            \"filters\": filters,\n            \"fields\": intf_fields,\n        }\n    }\n\n    # add query to retrieve inventory items\n    if inventory_items:\n        if self.nb_version[instance] &gt;= (4, 4, 0):\n            dlist = str(devices).replace(\"'\", '\"')  # swap quotes\n            inv_filters = (\n                \"{device: {name: {in_list: \"\n                + dlist\n                + '}}, component_type: {app_label: {exact: \"dcim\"}}}'\n            )\n        else:\n            raise UnsupportedNetboxVersion(\n                f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n                f\"minimum required version is {self.compatible_ge_v4}\"\n            )\n        inv_fields = [\n            \"name\",\n            \"component {... on InterfaceType {id}}\",\n            \"role {name}\",\n            \"manufacturer {name}\",\n            \"custom_fields\",\n            \"label\",\n            \"description\",\n            \"tags {name}\",\n            \"asset_tag\",\n            \"serial\",\n            \"part_id\",\n        ]\n        queries[\"inventor_items\"] = {\n            \"obj\": \"inventory_item_list\",\n            \"filters\": inv_filters,\n            \"fields\": inv_fields,\n        }\n\n    query_result = self.graphql(\n        job=job, instance=instance, queries=queries, dry_run=dry_run\n    )\n\n    # return dry run result\n    if dry_run:\n        return query_result\n\n    interfaces_data = query_result.result\n\n    # exit if no Interfaces returned\n    if interfaces_data is None or not interfaces_data.get(\"interfaces\"):\n        raise Exception(\n            f\"{self.name} - no interfaces data in '{interfaces_data}' returned by '{instance}' \"\n            f\"for devices {', '.join(devices)}\"\n        )\n\n    # process query results\n    interfaces = interfaces_data.pop(\"interfaces\")\n\n    # process inventory items\n    if inventory_items:\n        inventory_items_list = interfaces_data.pop(\"inventor_items\")\n        # transform inventory items list to a dictionary keyed by intf_id\n        inventory_items_dict = {}\n        while inventory_items_list:\n            inv_item = inventory_items_list.pop()\n            # skip inventory items that does not assigned to components\n            if inv_item.get(\"component\") is None:\n                continue\n            intf_id = str(inv_item.pop(\"component\").pop(\"id\"))\n            inventory_items_dict.setdefault(intf_id, [])\n            inventory_items_dict[intf_id].append(inv_item)\n        # iterate over interfaces and add inventory items\n        for intf in interfaces:\n            intf[\"inventory_items\"] = inventory_items_dict.pop(intf[\"id\"], [])\n\n    # transform interfaces list to dictionary keyed by device and interfaces names\n    while interfaces:\n        intf = interfaces.pop()\n        device_name = intf.pop(\"device\").pop(\"name\")\n        intf_name = intf.pop(\"name\")\n        if device_name in ret.result:  # Netbox issue #16299\n            ret.result[device_name][intf_name] = intf\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_nornir_inventory/","title":"Netbox Get Nornir Inventory Task","text":"<p>task api name: <code>get_nornir_inventory</code></p> <p>This task designed to provide Nornir workers with inventory data sourced from Netbox.</p> <p>How it works</p> <p>Netbox NorFab workers able to talk with Netbox GraphQL and REST API. Nornir workers can be configured to request Nornir inventory from Netbox workers on startup. Netbox workers in response to such a request will be fetching devices data from Netbox and constructing Nornir inventory returning it to Nornir worker.</p> <p></p> <ol> <li> <p>On startup Nornir worker sends <code>get_nornir_inventory</code> request to Netbox NorFab Workers</p> </li> <li> <p>Netbox worker fetches devices data from netbox - hostnames, interfaces, ip addresses, circuits, connections, configuration contexts etc.</p> </li> <li> <p>Netbox worker constructs Nornir inventory and sends it back to Nornir worker</p> </li> </ol>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_nornir_inventory/#get-nornir-inventory-sample-usage","title":"Get Nornir Inventory Sample Usage","text":"","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_nornir_inventory/#norfab-netbox-get-nornir-inventory-command-shell-reference","title":"NORFAB Netbox Get Nornir Inventory Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>get_nornir_inventory</code> task:</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_get_nornir_inventory/#python-api-reference","title":"Python API Reference","text":"<p>Retrieve and construct Nornir inventory from NetBox data.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>filters</code> <code>list</code> <p>List of filters to apply when retrieving devices from NetBox.</p> <code>None</code> <code>devices</code> <code>list</code> <p>List of specific devices to retrieve from NetBox.</p> <code>None</code> <code>instance</code> <code>str</code> <p>NetBox instance to use.</p> <code>None</code> <code>interfaces</code> <code>Union[dict, bool]</code> <p>If True, include interfaces data     in the inventory. If a dict, use it as arguments for the get_interfaces method.</p> <code>False</code> <code>connections</code> <code>Union[dict, bool]</code> <p>If True, include connections data     in the inventory. If a dict, use it as arguments for the get_connections method.</p> <code>False</code> <code>circuits</code> <code>Union[dict, bool]</code> <p>If True, include circuits data in the     inventory. If a dict, use it as arguments for the get_circuits method.</p> <code>False</code> <code>nbdata</code> <code>bool</code> <p>If True, include a copy of NetBox device's data in the host's data.</p> <code>True</code> <code>primary_ip</code> <code>str</code> <p>Specify whether to use 'ip4' or 'ip6' for the primary     IP address. Defaults to 'ip4'.</p> <code>'ip4'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>Nornir inventory dictionary containing hosts and their respective data.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"], \"schema\": NetboxFastApiArgs.model_json_schema()})\ndef get_nornir_inventory(\n    self,\n    job: Job,\n    filters: Union[None, list] = None,\n    devices: Union[None, list] = None,\n    instance: Union[None, str] = None,\n    interfaces: Union[dict, bool] = False,\n    connections: Union[dict, bool] = False,\n    circuits: Union[dict, bool] = False,\n    nbdata: bool = True,\n    bgp_peerings: Union[dict, bool] = False,\n    primary_ip: str = \"ip4\",\n) -&gt; Result:\n    \"\"\"\n    Retrieve and construct Nornir inventory from NetBox data.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        filters (list, optional): List of filters to apply when retrieving devices from NetBox.\n        devices (list, optional): List of specific devices to retrieve from NetBox.\n        instance (str, optional): NetBox instance to use.\n        interfaces (Union[dict, bool], optional): If True, include interfaces data\n                in the inventory. If a dict, use it as arguments for the get_interfaces method.\n        connections (Union[dict, bool], optional): If True, include connections data\n                in the inventory. If a dict, use it as arguments for the get_connections method.\n        circuits (Union[dict, bool], optional): If True, include circuits data in the\n                inventory. If a dict, use it as arguments for the get_circuits method.\n        nbdata (bool, optional): If True, include a copy of NetBox device's data in the host's data.\n        primary_ip (str, optional): Specify whether to use 'ip4' or 'ip6' for the primary\n                IP address. Defaults to 'ip4'.\n\n    Returns:\n        dict: Nornir inventory dictionary containing hosts and their respective data.\n    \"\"\"\n    hosts = {}\n    filters = filters or []\n    devices = devices or []\n    inventory = {\"hosts\": hosts}\n    ret = Result(task=f\"{self.name}:get_nornir_inventory\", result=inventory)\n\n    # check Netbox status\n    netbox_status = self.get_netbox_status(job=job, instance=instance)\n    if netbox_status.result[instance or self.default_instance][\"status\"] is False:\n        return ret\n\n    # retrieve devices data\n    nb_devices = self.get_devices(\n        job=job, filters=filters, devices=devices, instance=instance\n    )\n\n    # form Nornir hosts inventory\n    for device_name, device in nb_devices.result.items():\n        host = device[\"config_context\"].pop(\"nornir\", {})\n        host.setdefault(\"data\", {})\n        name = host.pop(\"name\", device_name)\n        hosts[name] = host\n        # add platform if not provided in device config context\n        if not host.get(\"platform\"):\n            if device[\"platform\"]:\n                host[\"platform\"] = device[\"platform\"][\"name\"]\n            else:\n                log.warning(f\"{self.name} - no platform found for '{name}' device\")\n        # add hostname if not provided in config context\n        if not host.get(\"hostname\"):\n            if device[\"primary_ip4\"] and primary_ip in [\"ip4\", \"ipv4\"]:\n                host[\"hostname\"] = device[\"primary_ip4\"][\"address\"].split(\"/\")[0]\n            elif device[\"primary_ip6\"] and primary_ip in [\"ip6\", \"ipv6\"]:\n                host[\"hostname\"] = device[\"primary_ip6\"][\"address\"].split(\"/\")[0]\n            else:\n                host[\"hostname\"] = name\n        # add netbox data to host's data\n        if nbdata is True:\n            host[\"data\"].update(device)\n\n    # return if no hosts found for provided parameters\n    if not hosts:\n        log.warning(f\"{self.name} - no viable hosts returned by Netbox\")\n        return ret\n\n    # add interfaces data\n    if interfaces:\n        # decide on get_interfaces arguments\n        kwargs = interfaces if isinstance(interfaces, dict) else {}\n        # add 'interfaces' key to all hosts' data\n        for host in hosts.values():\n            host[\"data\"].setdefault(\"interfaces\", {})\n        # query interfaces data from netbox\n        nb_interfaces = self.get_interfaces(\n            job=job, devices=list(hosts), instance=instance, **kwargs\n        )\n        # save interfaces data to hosts' inventory\n        while nb_interfaces.result:\n            device, device_interfaces = nb_interfaces.result.popitem()\n            hosts[device][\"data\"][\"interfaces\"] = device_interfaces\n\n    # add connections data\n    if connections:\n        # decide on get_interfaces arguments\n        kwargs = connections if isinstance(connections, dict) else {}\n        # add 'connections' key to all hosts' data\n        for host in hosts.values():\n            host[\"data\"].setdefault(\"connections\", {})\n        # query connections data from netbox\n        nb_connections = self.get_connections(\n            job=job, devices=list(hosts), instance=instance, **kwargs\n        )\n        # save connections data to hosts' inventory\n        while nb_connections.result:\n            device, device_connections = nb_connections.result.popitem()\n            hosts[device][\"data\"][\"connections\"] = device_connections\n\n    # add circuits data\n    if circuits:\n        # decide on get_interfaces arguments\n        kwargs = circuits if isinstance(circuits, dict) else {}\n        # add 'circuits' key to all hosts' data\n        for host in hosts.values():\n            host[\"data\"].setdefault(\"circuits\", {})\n        # query circuits data from netbox\n        nb_circuits = self.get_circuits(\n            job=job, devices=list(hosts), instance=instance, **kwargs\n        )\n        # save circuits data to hosts' inventory\n        while nb_circuits.result:\n            device, device_circuits = nb_circuits.result.popitem()\n            hosts[device][\"data\"][\"circuits\"] = device_circuits\n\n    # add bgp peerings data\n    if bgp_peerings:\n        # decide on get_interfaces arguments\n        kwargs = bgp_peerings if isinstance(bgp_peerings, dict) else {}\n        # add 'bgp_peerings' key to all hosts' data\n        for host in hosts.values():\n            host[\"data\"].setdefault(\"bgp_peerings\", {})\n        # query bgp_peerings data from netbox\n        nb_bgp_peerings = self.get_bgp_peerings(\n            job=job, devices=list(hosts), instance=instance, **kwargs\n        )\n        # save circuits data to hosts' inventory\n        while nb_bgp_peerings.result:\n            device, device_bgp_peerings = nb_bgp_peerings.result.popitem()\n            hosts[device][\"data\"][\"bgp_peerings\"] = device_bgp_peerings\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_graphql/","title":"Netbox GrapQL Inventory Task","text":"<p>task api name: <code>graphql</code></p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_graphql/#grapql-sample-usage","title":"GrapQL Sample Usage","text":"","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_graphql/#norfab-netbox-grapql-command-shell-reference","title":"NORFAB Netbox GrapQL Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>graphql</code> task:</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_graphql/#python-api-reference","title":"Python API Reference","text":"<p>Function to query Netbox v3 or Netbox v4 GraphQL API.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>instance</code> <code>Union[None, str]</code> <p>Netbox instance name</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>only return query content, do not run it</p> <code>False</code> <code>obj</code> <code>Union[str, dict]</code> <p>Object to query</p> <code>None</code> <code>filters</code> <code>Union[None, dict, str]</code> <p>Filters to apply to the query</p> <code>None</code> <code>fields</code> <code>Union[None, list]</code> <p>Fields to retrieve in the query</p> <code>None</code> <code>queries</code> <code>Union[None, dict]</code> <p>Dictionary of queries to execute</p> <code>None</code> <code>query_string</code> <code>str</code> <p>Raw query string to execute</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>GraphQL request data returned by Netbox</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If required arguments are not provided</p> <code>Exception</code> <p>If GraphQL query fails</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"POST\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef graphql(\n    self,\n    job: Job,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    obj: Union[str, dict] = None,\n    filters: Union[None, dict, str] = None,\n    fields: Union[None, list] = None,\n    queries: Union[None, dict] = None,\n    query_string: str = None,\n) -&gt; Result:\n    \"\"\"\n    Function to query Netbox v3 or Netbox v4 GraphQL API.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        instance: Netbox instance name\n        dry_run: only return query content, do not run it\n        obj: Object to query\n        filters: Filters to apply to the query\n        fields: Fields to retrieve in the query\n        queries: Dictionary of queries to execute\n        query_string: Raw query string to execute\n\n    Returns:\n        dict: GraphQL request data returned by Netbox\n\n    Raises:\n        RuntimeError: If required arguments are not provided\n        Exception: If GraphQL query fails\n    \"\"\"\n    nb_params = self._get_instance_params(instance)\n    instance = instance or self.default_instance\n    ret = Result(task=f\"{self.name}:graphql\", resources=[instance])\n\n    # form graphql query(ies) payload\n    if queries:\n        queries_list = []\n        for alias, query_data in queries.items():\n            query_data[\"alias\"] = alias\n            if self.nb_version[instance] &gt;= (4, 4, 0):\n                queries_list.append(_form_query_v4(**query_data))\n            else:\n                raise UnsupportedNetboxVersion(\n                    f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n                    f\"minimum required version is {self.compatible_ge_v4}\"\n                )\n        queries_strings = \"    \".join(queries_list)\n        query = f\"query {{{queries_strings}}}\"\n    elif obj and filters and fields:\n        if self.nb_version[instance] &gt;= (4, 4, 0):\n            query = _form_query_v4(obj, filters, fields)\n        else:\n            raise UnsupportedNetboxVersion(\n                f\"{self.name} - Netbox version {self.nb_version[instance]} is not supported, \"\n                f\"minimum required version is {self.compatible_ge_v4}\"\n            )\n        query = f\"query {{{query}}}\"\n    elif query_string:\n        query = query_string\n    else:\n        raise RuntimeError(\n            f\"{self.name} - graphql method expects queries argument or obj, filters, \"\n            f\"fields arguments or query_string argument provided\"\n        )\n    payload = json.dumps({\"query\": query})\n\n    # form and return dry run response\n    if dry_run:\n        ret.result = {\n            \"url\": f\"{nb_params['url']}/graphql/\",\n            \"data\": payload,\n            \"verify\": nb_params.get(\"ssl_verify\", True),\n            \"headers\": {\n                \"Content-Type\": \"application/json\",\n                \"Accept\": \"application/json\",\n                \"Authorization\": f\"Token ...{nb_params['token'][-6:]}\",\n            },\n        }\n        return ret\n\n    # send request to Netbox GraphQL API\n    log.debug(\n        f\"{self.name} - sending GraphQL query '{payload}' to URL '{nb_params['url']}/graphql/'\"\n    )\n    req = requests.post(\n        url=f\"{nb_params['url']}/graphql/\",\n        headers={\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\",\n            \"Authorization\": f\"Token {nb_params['token']}\",\n        },\n        data=payload,\n        verify=nb_params.get(\"ssl_verify\", True),\n        timeout=(self.netbox_connect_timeout, self.netbox_read_timeout),\n    )\n    try:\n        req.raise_for_status()\n    except Exception:\n        raise Exception(\n            f\"{self.name} -  Netbox GraphQL query failed, query '{query}', \"\n            f\"URL '{req.url}', status-code '{req.status_code}', reason '{req.reason}', \"\n            f\"response content '{req.text}'\"\n        )\n\n    # return results\n    reply = req.json()\n    if reply.get(\"errors\"):\n        msg = f\"{self.name} - GrapQL query error '{reply['errors']}', query '{payload}'\"\n        log.error(msg)\n        ret.errors.append(msg)\n        if reply.get(\"data\"):\n            ret.result = reply[\"data\"]  # at least return some data\n    elif queries or query_string:\n        ret.result = reply[\"data\"]\n    else:\n        ret.result = reply[\"data\"][obj]\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_rest/","title":"Netbox Rest Inventory Task","text":"<p>task api name: <code>rest</code></p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_rest/#rest-task-sample-usage","title":"Rest Task Sample Usage","text":"","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_rest/#norfab-netbox-grapql-command-shell-reference","title":"NORFAB Netbox GrapQL Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>rest</code> task:</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_rest/#python-api-reference","title":"Python API Reference","text":"<p>Sends a request to the Netbox REST API.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>The Netbox instance name to get parameters for.</p> <code>None</code> <code>method</code> <code>str</code> <p>The HTTP method to use for the request (e.g., 'get', 'post'). Defaults to \"get\".</p> <code>'get'</code> <code>api</code> <code>str</code> <p>The API endpoint to send the request to. Defaults to \"\".</p> <code>''</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the request (e.g., params, data, json).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Result</code> <p>Union[dict, list]: The JSON response from the API, parsed into a dictionary or list.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the HTTP request returned an unsuccessful status code.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"POST\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef rest(\n    self,\n    job: Job,\n    instance: Union[None, str] = None,\n    method: str = \"get\",\n    api: str = \"\",\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Sends a request to the Netbox REST API.\n\n    Args:\n        instance (str, optional): The Netbox instance name to get parameters for.\n        method (str, optional): The HTTP method to use for the request (e.g., 'get', 'post'). Defaults to \"get\".\n        api (str, optional): The API endpoint to send the request to. Defaults to \"\".\n        **kwargs: Additional arguments to pass to the request (e.g., params, data, json).\n\n    Returns:\n        Union[dict, list]: The JSON response from the API, parsed into a dictionary or list.\n\n    Raises:\n        requests.exceptions.HTTPError: If the HTTP request returned an unsuccessful status code.\n    \"\"\"\n    ret = Result(task=f\"{self.name}:rest\", result={})\n    nb_params = self._get_instance_params(instance)\n\n    # send request to Netbox REST API\n    response = getattr(requests, method)(\n        url=f\"{nb_params['url']}/api/{api}/\",\n        headers={\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\",\n            \"Authorization\": f\"Token {nb_params['token']}\",\n        },\n        verify=nb_params.get(\"ssl_verify\", True),\n        **kwargs,\n    )\n\n    response.raise_for_status()\n    try:\n        ret.result = response.json()\n    except Exception as e:\n        log.debug(f\"Failed to decode json, error: {e}\")\n        ret.result = response.text if response.text else response.status_code\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_facts/","title":"Netbox Sync Device Facts Task","text":"<p>task api name: <code>sync_device_facts</code></p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_facts/#limitations","title":"Limitations","text":"<p>Datasource <code>nornir</code> uses NAPALM <code>get_facts</code> getter and as such only supports these device platforms:</p> <ul> <li>Arista EOS</li> <li>Cisco IOS</li> <li>Cisco IOSXR</li> <li>Cisco NXOS</li> <li>Juniper JUNOS</li> </ul>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_facts/#branching-support","title":"Branching Support","text":"<p>Update device facts task is branch aware and can push updates to the branch. Netbox Branching Plugin need to be installed on Netbox instance.</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_facts/#sync-device-facts-sample-usage","title":"Sync Device Facts Sample Usage","text":"","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_facts/#norfab-netbox-update-device-facts-command-shell-reference","title":"NORFAB Netbox Update Device Facts Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>sync_device_facts</code> task:</p> <pre><code>nf# man tree netbox.update.device.facts\nroot\n\u2514\u2500\u2500 netbox:    Netbox service\n    \u2514\u2500\u2500 sync:    Update Netbox data\n        \u2514\u2500\u2500 device:    Update device data\n            \u2514\u2500\u2500 facts:    Update device serial, OS version\n                \u251c\u2500\u2500 timeout:    Job timeout\n                \u251c\u2500\u2500 workers:    Filter worker to target, default 'any'\n                \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n                \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n                \u251c\u2500\u2500 instance:    Netbox instance name to target\n                \u251c\u2500\u2500 dry-run:    Return information that would be pushed to Netbox but do not push it\n                \u251c\u2500\u2500 devices:    List of Netbox devices to update\n                \u251c\u2500\u2500 batch-size:    Number of devices to process at a time, default '10'\n                \u251c\u2500\u2500 datasource:    Service to use to retrieve device data, default 'nornir'\n                \u2502   \u2514\u2500\u2500 nornir:    Use Nornir service to retrieve data from devices\n                \u2502       \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n                \u2502       \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n                \u2502       \u251c\u2500\u2500 FH:    Filter hosts by hostname\n                \u2502       \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n                \u2502       \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n                \u2502       \u251c\u2500\u2500 FG:    Filter hosts by group\n                \u2502       \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n                \u2502       \u251c\u2500\u2500 FL:    Filter hosts by names list\n                \u2502       \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n                \u2502       \u251c\u2500\u2500 FN:    Negate the match\n                \u2502       \u251c\u2500\u2500 add-details:    Add task details to results, default 'False'\n                \u2502       \u251c\u2500\u2500 num-workers:    RetryRunner number of threads for tasks execution\n                \u2502       \u251c\u2500\u2500 num-connectors:    RetryRunner number of threads for device connections\n                \u2502       \u251c\u2500\u2500 connect-retry:    RetryRunner number of connection attempts\n                \u2502       \u251c\u2500\u2500 task-retry:    RetryRunner number of attempts to run task\n                \u2502       \u251c\u2500\u2500 reconnect-on-fail:    RetryRunner perform reconnect to host on task failure\n                \u2502       \u251c\u2500\u2500 connect-check:    RetryRunner test TCP connection before opening actual connection\n                \u2502       \u251c\u2500\u2500 connect-timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n                \u2502       \u251c\u2500\u2500 creds-retry:    RetryRunner list of connection credentials and parameters to retry\n                \u2502       \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n                \u2502       \u251c\u2500\u2500 tf-skip-failed:    Save results to file for failed tasks\n                \u2502       \u251c\u2500\u2500 diff:    File group name to run the diff for\n                \u2502       \u251c\u2500\u2500 diff-last:    File version number to diff, default is 1 (last)\n                \u2502       \u2514\u2500\u2500 progress:    Display progress events, default 'True'\n                \u2514\u2500\u2500 branch:    Branching plugin branch name to use\nnf#\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_facts/#python-api-reference","title":"Python API Reference","text":"<p>Updates device facts in NetBox, this task updates this device attributes:</p> <ul> <li>serial number</li> </ul> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>instance</code> <code>str</code> <p>The NetBox instance to use.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, no changes will be made to NetBox.</p> <code>False</code> <code>datasource</code> <code>str</code> <p>The data source to use. Supported datasources:</p> <ul> <li>nornir - uses Nornir Service parse task to retrieve devices' data     using NAPALM <code>get_facts</code> getter</li> </ul> <code>'nornir'</code> <code>timeout</code> <code>int</code> <p>The timeout for the job execution. Defaults to 60.</p> <code>60</code> <code>devices</code> <code>list</code> <p>The list of devices to update.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The number of devices to process in each batch.</p> <code>10</code> <code>branch</code> <code>str</code> <p>Branch name to use, need to have branching plugin installed, automatically creates branch if it does not exist in Netbox.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the datasource job.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the results of the update operation.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If a device does not exist in NetBox.</p> <code>UnsupportedServiceError</code> <p>If the specified datasource is not supported.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"PATCH\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef sync_device_facts(\n    self,\n    job: Job,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    datasource: str = \"nornir\",\n    timeout: int = 60,\n    devices: Union[None, list] = None,\n    batch_size: int = 10,\n    branch: str = None,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Updates device facts in NetBox, this task updates this device attributes:\n\n    - serial number\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        instance (str, optional): The NetBox instance to use.\n        dry_run (bool, optional): If True, no changes will be made to NetBox.\n        datasource (str, optional): The data source to use. Supported datasources:\n\n            - **nornir** - uses Nornir Service parse task to retrieve devices' data\n                using NAPALM `get_facts` getter\n\n        timeout (int, optional): The timeout for the job execution. Defaults to 60.\n        devices (list, optional): The list of devices to update.\n        batch_size (int, optional): The number of devices to process in each batch.\n        branch (str, optional): Branch name to use, need to have branching plugin installed,\n            automatically creates branch if it does not exist in Netbox.\n        **kwargs: Additional keyword arguments to pass to the datasource job.\n\n    Returns:\n        dict: A dictionary containing the results of the update operation.\n\n    Raises:\n        Exception: If a device does not exist in NetBox.\n        UnsupportedServiceError: If the specified datasource is not supported.\n    \"\"\"\n    devices = devices or []\n    instance = instance or self.default_instance\n    ret = Result(\n        task=f\"{self.name}:sync_device_facts\",\n        resources=[instance],\n        dry_run=dry_run,\n        diff={},\n        result={},\n    )\n    nb = self._get_pynetbox(instance, branch=branch)\n    kwargs[\"add_details\"] = True\n\n    if datasource == \"nornir\":\n        # source hosts list from Nornir\n        if kwargs:\n            devices.extend(self.get_nornir_hosts(kwargs, timeout))\n            devices = list(set(devices))\n            job.event(f\"Syncing {len(devices)} devices\")\n        # fetch devices data from Netbox\n        nb_devices = self.get_devices(\n            job=job,\n            instance=instance,\n            devices=copy.copy(devices),\n            cache=\"refresh\",\n        ).result\n        # remove devices that does not exist in Netbox\n        for d in list(devices):\n            if d not in nb_devices:\n                msg = f\"'{d}' device does not exist in Netbox\"\n                ret.errors.append(msg)\n                log.error(msg)\n                devices.remove(d)\n        # iterate over devices in batches\n        for i in range(0, len(devices), batch_size):\n            kwargs[\"FL\"] = devices[i : i + batch_size]\n            kwargs[\"getters\"] = \"get_facts\"\n            job.event(f\"retrieving facts for devices {', '.join(kwargs['FL'])}\")\n            data = self.client.run_job(\n                \"nornir\",\n                \"parse\",\n                kwargs=kwargs,\n                workers=\"all\",\n                timeout=timeout,\n            )\n\n            # Collect devices to update in bulk\n            devices_to_update = []\n\n            for worker, results in data.items():\n                if results[\"failed\"]:\n                    msg = f\"{worker} get_facts failed, errors: {'; '.join(results['errors'])}\"\n                    ret.errors.append(msg)\n                    log.error(msg)\n                    continue\n                for host, host_data in results[\"result\"].items():\n                    if host_data[\"napalm_get\"][\"failed\"]:\n                        msg = f\"{host} facts update failed: '{host_data['napalm_get']['exception']}'\"\n                        ret.errors.append(msg)\n                        log.error(msg)\n                        continue\n\n                    nb_device = nb_devices[host]\n\n                    facts = host_data[\"napalm_get\"][\"result\"][\"get_facts\"]\n                    desired_state = {\n                        \"serial\": facts[\"serial_number\"],\n                    }\n                    current_state = {\n                        \"serial\": nb_device[\"serial\"],\n                    }\n\n                    # Compare and get fields that need updating\n                    updates, diff = compare_netbox_object_state(\n                        desired_state=desired_state,\n                        current_state=current_state,\n                    )\n\n                    # Only update if there are changes\n                    if updates:\n                        updates[\"id\"] = int(nb_device[\"id\"])\n                        devices_to_update.append(updates)\n                        ret.diff[host] = diff\n\n                    ret.result[host] = {\n                        (\n                            \"sync_device_facts_dry_run\"\n                            if dry_run\n                            else \"sync_device_facts\"\n                        ): (updates if updates else \"Device facts in sync\")\n                    }\n                    if branch is not None:\n                        ret.result[host][\"branch\"] = branch\n\n            # Perform bulk update\n            if devices_to_update and not dry_run:\n                try:\n                    nb.dcim.devices.update(devices_to_update)\n                except Exception as e:\n                    ret.errors.append(f\"Bulk update failed: {e}\")\n    else:\n        raise UnsupportedServiceError(\n            f\"'{datasource}' datasource service not supported\"\n        )\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_interfaces/","title":"Netbox Sync Device Interfaces Task","text":"<p>task api name: <code>sync_device_interfaces</code></p> <p>The Netbox Sync Device Interfaces Task is a feature of the NorFab Netbox Service that allows you to synchronize and update the interface data of your network devices in Netbox. This task ensures that the interface configurations in Netbox are accurate and up-to-date, reflecting the current state of your network infrastructure.</p> <p>Keeping interface data accurate and up-to-date is crucial for effective network management. The Netbox Update Device Interfaces Task automates the process of updating interface information, such as interface names, statuses, mac addresses, and other relevant details.</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_interfaces/#branching-support","title":"Branching Support","text":"<p>Update device interfaces task is branch aware and can push updates to the branch. Netbox Branching Plugin need to be installed on Netbox instance.</p> <p>How it works - Netbox worker on a call to update interfaces task fetches live data from network devices using nominated datasource, by default it is Nornir service parse task using NAPALM <code>get_interfaces</code> getter. Once data retrieved from network, Netbox worker updates records in Netbox database for device interfaces.</p> <p></p> <ol> <li> <p>Client submits and on-demand request to NorFab Netbox worker to update device interfaces</p> </li> <li> <p>Netbox worker sends job request to nominated datasource service to fetch live data from network devices</p> </li> <li> <p>Datasource service fetches data from the network</p> </li> <li> <p>Datasource returns devices interfaces data back to Netbox Service worker</p> </li> <li> <p>Netbox worker processes device interfaces data and updates records in Netbox for requested devices</p> </li> </ol>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_interfaces/#limitations","title":"Limitations","text":"<p>Datasource <code>nornir</code> uses NAPALM <code>get_interfaces</code> getter and as such only supports these device platforms:</p> <ul> <li>Arista EOS</li> <li>Cisco IOS</li> <li>Cisco IOSXR</li> <li>Cisco NXOS</li> <li>Juniper JUNOS</li> </ul>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_interfaces/#update-device-interfaces-sample-usage","title":"Update Device Interfaces Sample Usage","text":"","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_interfaces/#norfab-netbox-update-device-interfaces-command-shell-reference","title":"NORFAB Netbox Update Device Interfaces Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>sync_device_interfaces</code> task:</p> <pre><code>nf# man tree netbox.update.device.interfaces\nroot\n\u2514\u2500\u2500 netbox:    Netbox service\n    \u2514\u2500\u2500 sync:    Update Netbox data\n        \u2514\u2500\u2500 device:    Update device data\n            \u2514\u2500\u2500 interfaces:    Update device interfaces\n                \u251c\u2500\u2500 timeout:    Job timeout\n                \u251c\u2500\u2500 workers:    Filter worker to target, default 'any'\n                \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n                \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n                \u251c\u2500\u2500 instance:    Netbox instance name to target\n                \u251c\u2500\u2500 dry-run:    Return information that would be pushed to Netbox but do not push it\n                \u251c\u2500\u2500 devices:    List of Netbox devices to update\n                \u251c\u2500\u2500 datasource:    Service to use to retrieve device data, default 'nornir'\n                \u2502   \u2514\u2500\u2500 nornir:    Use Nornir service to retrieve data from devices\n                \u2502       \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n                \u2502       \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n                \u2502       \u251c\u2500\u2500 FH:    Filter hosts by hostname\n                \u2502       \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n                \u2502       \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n                \u2502       \u251c\u2500\u2500 FG:    Filter hosts by group\n                \u2502       \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n                \u2502       \u251c\u2500\u2500 FL:    Filter hosts by names list\n                \u2502       \u251c\u2500\u2500 FM:    Filter hosts by platform\n                \u2502       \u251c\u2500\u2500 FN:    Negate the match\n                \u2502       \u251c\u2500\u2500 add-details:    Add task details to results, default 'False'\n                \u2502       \u251c\u2500\u2500 num-workers:    RetryRunner number of threads for tasks execution\n                \u2502       \u251c\u2500\u2500 num-connectors:    RetryRunner number of threads for device connections\n                \u2502       \u251c\u2500\u2500 connect-retry:    RetryRunner number of connection attempts\n                \u2502       \u251c\u2500\u2500 task-retry:    RetryRunner number of attempts to run task\n                \u2502       \u251c\u2500\u2500 reconnect-on-fail:    RetryRunner perform reconnect to host on task failure\n                \u2502       \u251c\u2500\u2500 connect-check:    RetryRunner test TCP connection before opening actual connection\n                \u2502       \u251c\u2500\u2500 connect-timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n                \u2502       \u251c\u2500\u2500 creds-retry:    RetryRunner list of connection credentials and parameters to retry\n                \u2502       \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n                \u2502       \u251c\u2500\u2500 tf-skip-failed:    Save results to file for failed tasks\n                \u2502       \u251c\u2500\u2500 diff:    File group name to run the diff for\n                \u2502       \u251c\u2500\u2500 diff-last:    File version number to diff, default is 1 (last)\n                \u2502       \u2514\u2500\u2500 progress:    Display progress events, default 'True'\n                \u251c\u2500\u2500 batch-size:    Number of devices to process at a time, default '10'\n                \u2514\u2500\u2500 branch:    Branching plugin branch name to use\nnf#\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_interfaces/#python-api-reference","title":"Python API Reference","text":"<p>Update or create device interfaces in Netbox using devices interfaces data sourced via Nornir service <code>parse</code> task using NAPALM getter.</p> <p>Interface parameters updated:</p> <ul> <li>interface name</li> <li>interface description</li> <li>mtu</li> <li>mac address</li> <li>admin status</li> <li>speed</li> </ul> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata.</p> required <code>instance</code> <code>str</code> <p>The Netbox instance name to use.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, no changes will be made to Netbox.</p> <code>False</code> <code>datasource</code> <code>str</code> <p>The data source to use. Supported datasources:</p> <ul> <li>nornir - uses Nornir Service parse task to retrieve devices' data     using NAPALM get_interfaces getter</li> </ul> <code>'nornir'</code> <code>timeout</code> <code>int</code> <p>The timeout for the job.</p> <code>60</code> <code>devices</code> <code>list</code> <p>List of devices to update.</p> <code>None</code> <code>create</code> <code>bool</code> <p>If True, new interfaces will be created if they do not exist.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>The number of devices to process in each batch.</p> <code>10</code> <code>branch</code> <code>str</code> <p>Branch name to use, need to have branching plugin installed, automatically creates branch if it does not exist in Netbox.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the datasource job.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the results of the update operation.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If a device does not exist in Netbox.</p> <code>UnsupportedServiceError</code> <p>If the specified datasource is not supported.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"PATCH\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef sync_device_interfaces(\n    self,\n    job: Job,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    datasource: str = \"nornir\",\n    timeout: int = 60,\n    devices: Union[None, list] = None,\n    create: bool = True,\n    batch_size: int = 10,\n    branch: str = None,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Update or create device interfaces in Netbox using devices interfaces\n    data sourced via Nornir service `parse` task using NAPALM getter.\n\n    Interface parameters updated:\n\n    - interface name\n    - interface description\n    - mtu\n    - mac address\n    - admin status\n    - speed\n\n    Args:\n        job: NorFab Job object containing relevant metadata.\n        instance (str, optional): The Netbox instance name to use.\n        dry_run (bool, optional): If True, no changes will be made to Netbox.\n        datasource (str, optional): The data source to use. Supported datasources:\n\n            - **nornir** - uses Nornir Service parse task to retrieve devices' data\n                using NAPALM get_interfaces getter\n\n        timeout (int, optional): The timeout for the job.\n        devices (list, optional): List of devices to update.\n        create (bool, optional): If True, new interfaces will be created if they do not exist.\n        batch_size (int, optional): The number of devices to process in each batch.\n        branch (str, optional): Branch name to use, need to have branching plugin installed,\n            automatically creates branch if it does not exist in Netbox.\n        **kwargs: Additional keyword arguments to pass to the datasource job.\n\n    Returns:\n        dict: A dictionary containing the results of the update operation.\n\n    Raises:\n        Exception: If a device does not exist in Netbox.\n        UnsupportedServiceError: If the specified datasource is not supported.\n    \"\"\"\n    devices = devices or []\n    instance = instance or self.default_instance\n    ret = Result(\n        task=f\"{self.name}:sync_device_interfaces\",\n        result={},\n        resources=[instance],\n        dry_run=dry_run,\n        diff={},\n    )\n    nb = self._get_pynetbox(instance, branch=branch)\n    kwargs[\"add_details\"] = True\n\n    if datasource == \"nornir\":\n        # source hosts list from Nornir\n        if kwargs:\n            devices.extend(self.get_nornir_hosts(kwargs, timeout))\n            devices = list(set(devices))\n            job.event(f\"syncing {len(devices)} devices\")\n\n        # fetch devices interfaces data from Netbox\n        nb_interfaces_data = self.get_interfaces(\n            job=job,\n            instance=instance,\n            devices=copy.copy(devices),\n            cache=\"refresh\",\n        ).result\n\n        # fetch devices data from Netbox\n        nb_devices_data = self.get_devices(\n            job=job,\n            instance=instance,\n            devices=copy.copy(devices),\n        ).result\n\n        # iterate over devices in batches\n        for i in range(0, len(devices), batch_size):\n            kwargs[\"FL\"] = devices[i : i + batch_size]\n            kwargs[\"getters\"] = \"get_interfaces\"\n            job.event(\n                f\"retrieving interfaces for devices {', '.join(kwargs['FL'])}\"\n            )\n            data = self.client.run_job(\n                \"nornir\",\n                \"parse\",\n                kwargs=kwargs,\n                workers=\"all\",\n                timeout=timeout,\n            )\n\n            # Collect interfaces to update and create in bulk\n            interfaces_to_update = []\n            interfaces_to_create = []\n            mac_addresses_to_create = []\n\n            for worker, results in data.items():\n                if results[\"failed\"]:\n                    msg = f\"{worker} get_interfaces failed, errors: {'; '.join(results['errors'])}\"\n                    ret.errors.append(msg)\n                    log.error(msg)\n                    continue\n\n                for host, host_data in results[\"result\"].items():\n                    if host_data[\"napalm_get\"][\"failed\"]:\n                        msg = f\"{host} interfaces update failed: '{host_data['napalm_get']['exception']}'\"\n                        ret.errors.append(msg)\n                        log.error(msg)\n                        continue\n\n                    nb_interfaces = nb_interfaces_data.get(host, {})\n                    if not nb_interfaces:\n                        msg = f\"'{host}' has no interfaces in Netbox, skipping\"\n                        ret.errors.append(msg)\n                        log.warning(msg)\n                        continue\n\n                    # Get device ID for creating new interfaces\n                    nb_device = nb_devices_data.get(host)\n                    if not nb_device:\n                        msg = f\"'{host}' does not exist in Netbox\"\n                        ret.errors.append(msg)\n                        log.error(msg)\n                        continue\n\n                    interfaces = host_data[\"napalm_get\"][\"result\"][\"get_interfaces\"]\n\n                    sync_key = \"sync_device_interfaces\"\n                    create_key = \"created_device_interfaces\"\n                    if dry_run:\n                        sync_key = \"sync_device_interfaces_dry_run\"\n                        create_key = \"created_device_interfaces_dry_run\"\n                    ret.result[host] = {\n                        sync_key: {},\n                        create_key: {},\n                    }\n                    if branch is not None:\n                        ret.result[host][\"branch\"] = branch\n\n                    # Process network device interfaces\n                    for intf_name, interface_data in interfaces.items():\n                        if intf_name in nb_interfaces:\n                            # Interface exists - prepare update\n                            nb_intf = nb_interfaces[intf_name]\n\n                            # Build desired state\n                            desired_state = {\n                                \"description\": interface_data.get(\n                                    \"description\", \"\"\n                                ),\n                                \"enabled\": interface_data.get(\"is_enabled\", True),\n                            }\n                            if 10000 &gt; interface_data.get(\"mtu\", 0) &gt; 0:\n                                desired_state[\"mtu\"] = interface_data[\"mtu\"]\n                            if interface_data.get(\"speed\", 0) &gt; 0:\n                                desired_state[\"speed\"] = (\n                                    interface_data[\"speed\"] * 1000\n                                )\n\n                            # Build current state\n                            current_state = {\n                                \"description\": nb_intf.get(\"description\", \"\"),\n                                \"enabled\": nb_intf.get(\"enabled\", True),\n                            }\n                            if nb_intf.get(\"mtu\"):\n                                current_state[\"mtu\"] = nb_intf[\"mtu\"]\n                            if nb_intf.get(\"speed\"):\n                                current_state[\"speed\"] = nb_intf[\"speed\"]\n\n                            # Compare and get fields that need updating\n                            updates, diff = compare_netbox_object_state(\n                                desired_state=desired_state,\n                                current_state=current_state,\n                            )\n\n                            # Only update if there are changes\n                            if updates:\n                                updates[\"id\"] = int(nb_intf[\"id\"])\n                                interfaces_to_update.append(updates)\n                                ret.diff.setdefault(host, {})[intf_name] = diff\n\n                            ret.result[host][sync_key][intf_name] = (\n                                updates if updates else \"Interface in sync\"\n                            )\n\n                            mac_address = (\n                                interface_data.get(\"mac_address\", \"\")\n                                .strip()\n                                .lower()\n                            )\n                            if mac_address and mac_address not in [\"none\", \"\"]:\n                                # Check if MAC already exists\n                                for nb_mac in nb_intf.get(\"mac_addresses\") or []:\n                                    if (\n                                        nb_mac.get(\"mac_address\", \"\").lower()\n                                        == mac_address\n                                    ):\n                                        break\n                                else:\n                                    # Prepare MAC address for creation\n                                    mac_addresses_to_create.append(\n                                        {\n                                            \"mac_address\": mac_address,\n                                            \"assigned_object_type\": \"dcim.interface\",\n                                            \"assigned_object_id\": int(\n                                                nb_intf[\"id\"]\n                                            ),\n                                        }\n                                    )\n                        elif create:\n                            # Interface doesn't exist - prepare creation\n                            new_intf = {\n                                \"name\": intf_name,\n                                \"device\": int(nb_device[\"id\"]),\n                                \"type\": \"other\",\n                                \"description\": interface_data.get(\n                                    \"description\", \"\"\n                                ),\n                                \"enabled\": interface_data.get(\"is_enabled\", True),\n                            }\n                            if 10000 &gt; interface_data.get(\"mtu\", 0) &gt; 0:\n                                new_intf[\"mtu\"] = interface_data[\"mtu\"]\n                            if interface_data.get(\"speed\", 0) &gt; 0:\n                                new_intf[\"speed\"] = interface_data[\"speed\"] * 1000\n\n                            mac_address = (\n                                interface_data.get(\"mac_address\", \"\")\n                                .strip()\n                                .lower()\n                            )\n                            if mac_address and mac_address not in [\"none\", \"\"]:\n                                mac_addresses_to_create.append(\n                                    {\n                                        \"mac_address\": mac_address,\n                                        \"assigned_object_type\": \"dcim.interface\",\n                                        \"assigned_object_id\": int(nb_intf[\"id\"]),\n                                    }\n                                )\n\n                            interfaces_to_create.append(new_intf)\n                            ret.result[host][create_key][intf_name] = new_intf\n\n            # Perform bulk updates and creations\n            if interfaces_to_update and not dry_run:\n                try:\n                    nb.dcim.interfaces.update(interfaces_to_update)\n                    job.event(\n                        f\"Bulk updated {len(interfaces_to_update)} interfaces\"\n                    )\n                except Exception as e:\n                    msg = f\"Bulk interface update failed: {e}\"\n                    ret.errors.append(msg)\n                    log.error(msg)\n\n            if interfaces_to_create and not dry_run:\n                try:\n                    _ = nb.dcim.interfaces.create(interfaces_to_create)\n                    job.event(\n                        f\"Bulk created {len(interfaces_to_create)} interfaces\"\n                    )\n                except Exception as e:\n                    msg = f\"Bulk interface creation failed: {e}\"\n                    ret.errors.append(msg)\n                    log.error(msg)\n\n            # Bulk create MAC addresses\n            if mac_addresses_to_create and not dry_run:\n                try:\n                    nb.dcim.mac_addresses.create(mac_addresses_to_create)\n                    job.event(\n                        f\"Bulk created {len(mac_addresses_to_create)} MAC addresses\"\n                    )\n                except Exception as e:\n                    msg = f\"Bulk MAC address creation failed: {e}\"\n                    ret.errors.append(msg)\n                    log.error(msg)\n\n    else:\n        raise UnsupportedServiceError(\n            f\"'{datasource}' datasource service not supported\"\n        )\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_ip/","title":"Netbox Sync Device IP Task","text":"<p>task api name: <code>sync_device_ip</code></p> <p>The Netbox Sync Device IP Task is a feature of the NorFab Netbox Service that allows you to synchronize and update the IP addresses data of your network devices in Netbox. This task ensures that the IP address records in Netbox are accurate and up-to-date, reflecting the current state of your network infrastructure.</p> <p>How it works - Netbox worker on a call to update IP addresses task fetches live data from network devices using nominated datasource, by default it is Nornir service parse task using NAPALM <code>get_interfaces_ip</code> getter. Once data retrieved from network, Netbox worker updates records in Netbox database for device interfaces.</p> <p></p> <ol> <li> <p>Client submits and on-demand request to NorFab Netbox worker to update device IP addresses</p> </li> <li> <p>Netbox worker sends job request to nominated datasource service to fetch live data from network devices</p> </li> <li> <p>Datasource service fetches data from the network</p> </li> <li> <p>Datasource returns devices IP addresses data back to Netbox Service worker</p> </li> <li> <p>Netbox worker processes device data and updates or creates IP address records in Netbox for requested devices</p> </li> </ol>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_ip/#branching-support","title":"Branching Support","text":"<p>Update device IP task is branch aware and can push updates to the branch. Netbox Branching Plugin need to be installed on Netbox instance.</p>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_ip/#limitations","title":"Limitations","text":"<p>Datasource <code>nornir</code> uses NAPALM <code>get_interfaces_ip</code> getter and as such only supports these device platforms:</p> <ul> <li>Arista EOS</li> <li>Cisco IOS</li> <li>Cisco IOSXR</li> <li>Cisco NXOS</li> <li>Juniper JUNOS</li> </ul>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_ip/#update-device-ip-sample-usage","title":"Update Device IP Sample Usage","text":"","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_ip/#norfab-netbox-update-device-ip-command-shell-reference","title":"NORFAB Netbox Update Device IP Command Shell Reference","text":"<p>NorFab shell supports these command options for Netbox <code>sync_device_ip</code> task:</p> <pre><code>nf# man tree netbox.update.device.ip-addresses\nroot\n\u2514\u2500\u2500 netbox:    Netbox service\n    \u2514\u2500\u2500 sync:    Update Netbox data\n        \u2514\u2500\u2500 device:    Update device data\n            \u2514\u2500\u2500 ip-addresses:    Update device interface IP addresses\n                \u251c\u2500\u2500 timeout:    Job timeout\n                \u251c\u2500\u2500 workers:    Filter worker to target, default 'any'\n                \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n                \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n                \u251c\u2500\u2500 instance:    Netbox instance name to target\n                \u251c\u2500\u2500 dry-run:    Return information that would be pushed to Netbox but do not push it\n                \u251c\u2500\u2500 devices:    List of Netbox devices to update\n                \u251c\u2500\u2500 datasource:    Service to use to retrieve device data, default 'nornir'\n                \u2502   \u2514\u2500\u2500 nornir:    Use Nornir service to retrieve data from devices\n                \u2502       \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n                \u2502       \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n                \u2502       \u251c\u2500\u2500 FH:    Filter hosts by hostname\n                \u2502       \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n                \u2502       \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n                \u2502       \u251c\u2500\u2500 FG:    Filter hosts by group\n                \u2502       \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n                \u2502       \u251c\u2500\u2500 FL:    Filter hosts by names list\n                \u2502       \u251c\u2500\u2500 FM:    Filter hosts by platform\n                \u2502       \u251c\u2500\u2500 FN:    Negate the match\n                \u2502       \u251c\u2500\u2500 add-details:    Add task details to results, default 'False'\n                \u2502       \u251c\u2500\u2500 num-workers:    RetryRunner number of threads for tasks execution\n                \u2502       \u251c\u2500\u2500 num-connectors:    RetryRunner number of threads for device connections\n                \u2502       \u251c\u2500\u2500 connect-retry:    RetryRunner number of connection attempts\n                \u2502       \u251c\u2500\u2500 task-retry:    RetryRunner number of attempts to run task\n                \u2502       \u251c\u2500\u2500 reconnect-on-fail:    RetryRunner perform reconnect to host on task failure\n                \u2502       \u251c\u2500\u2500 connect-check:    RetryRunner test TCP connection before opening actual connection\n                \u2502       \u251c\u2500\u2500 connect-timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n                \u2502       \u251c\u2500\u2500 creds-retry:    RetryRunner list of connection credentials and parameters to retry\n                \u2502       \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n                \u2502       \u251c\u2500\u2500 tf-skip-failed:    Save results to file for failed tasks\n                \u2502       \u251c\u2500\u2500 diff:    File group name to run the diff for\n                \u2502       \u251c\u2500\u2500 diff-last:    File version number to diff, default is 1 (last)\n                \u2502       \u2514\u2500\u2500 progress:    Display progress events, default 'True'\n                \u251c\u2500\u2500 batch-size:    Number of devices to process at a time, default '10'\n                \u2514\u2500\u2500 branch:    Branching plugin branch name to use\nnf#\n</code></pre>","tags":["netbox"]},{"location":"workers/netbox/services_netbox_service_tasks_sync_device_ip/#python-api-reference","title":"Python API Reference","text":"<p>Update the IP addresses of devices in Netbox.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>instance</code> <code>str</code> <p>The Netbox instance name to use.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, no changes will be made.</p> <code>False</code> <code>datasource</code> <code>str</code> <p>The data source to use. Supported datasources:</p> <ul> <li>nornir - uses Nornir Service parse task to retrieve devices' data     using NAPALM get_interfaces_ip getter</li> </ul> <code>'nornir'</code> <code>timeout</code> <code>int</code> <p>The timeout for the operation.</p> <code>60</code> <code>devices</code> <code>list</code> <p>The list of devices to update.</p> <code>None</code> <code>create</code> <code>bool</code> <p>If True, new IP addresses will be created if they do not exist.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>The number of devices to process in each batch.</p> <code>10</code> <code>branch</code> <code>str</code> <p>Branch name to use, need to have branching plugin installed, automatically creates branch if it does not exist in Netbox.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the results of the update operation.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If a device does not exist in Netbox.</p> <code>UnsupportedServiceError</code> <p>If the specified datasource is not supported.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"PATCH\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef sync_device_ip(\n    self,\n    job: Job,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    datasource: str = \"nornir\",\n    timeout: int = 60,\n    devices: Union[None, list] = None,\n    create: bool = True,\n    batch_size: int = 10,\n    branch: str = None,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Update the IP addresses of devices in Netbox.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        instance (str, optional): The Netbox instance name to use.\n        dry_run (bool, optional): If True, no changes will be made.\n        datasource (str, optional): The data source to use. Supported datasources:\n\n            - **nornir** - uses Nornir Service parse task to retrieve devices' data\n                using NAPALM get_interfaces_ip getter\n\n        timeout (int, optional): The timeout for the operation.\n        devices (list, optional): The list of devices to update.\n        create (bool, optional): If True, new IP addresses will be created if they do not exist.\n        batch_size (int, optional): The number of devices to process in each batch.\n        branch (str, optional): Branch name to use, need to have branching plugin installed,\n            automatically creates branch if it does not exist in Netbox.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the results of the update operation.\n\n    Raises:\n        Exception: If a device does not exist in Netbox.\n        UnsupportedServiceError: If the specified datasource is not supported.\n    \"\"\"\n    result = {}\n    devices = devices or []\n    instance = instance or self.default_instance\n    ret = Result(\n        task=f\"{self.name}:sync_device_ip\", result=result, resources=[instance]\n    )\n    nb = self._get_pynetbox(instance, branch=branch)\n\n    if datasource == \"nornir\":\n        # source hosts list from Nornir\n        if kwargs:\n            devices.extend(self.get_nornir_hosts(kwargs, timeout))\n        # iterate over devices in batches\n        for i in range(0, len(devices), batch_size):\n            kwargs[\"FL\"] = devices[i : i + batch_size]\n            kwargs[\"getters\"] = \"get_interfaces_ip\"\n            data = self.client.run_job(\n                \"nornir\",\n                \"parse\",\n                kwargs=kwargs,\n                workers=\"all\",\n                timeout=timeout,\n            )\n            for worker, results in data.items():\n                if results[\"failed\"]:\n                    log.error(\n                        f\"{worker} get_interfaces_ip failed, errors: {'; '.join(results['errors'])}\"\n                    )\n                    continue\n                for host, host_data in results[\"result\"].items():\n                    updated, created = {}, {}\n                    result[host] = {\n                        \"sync_ip_dry_run\" if dry_run else \"sync_ip\": updated,\n                        \"created_ip_dry_run\" if dry_run else \"created_ip\": created,\n                    }\n                    if branch is not None:\n                        result[host][\"branch\"] = branch\n                    interfaces = host_data[\"napalm_get\"][\"get_interfaces_ip\"]\n                    nb_device = nb.dcim.devices.get(name=host)\n                    if not nb_device:\n                        raise Exception(f\"'{host}' does not exist in Netbox\")\n                    nb_interfaces = nb.dcim.interfaces.filter(\n                        device_id=nb_device.id\n                    )\n                    # update interface IP addresses\n                    for nb_interface in nb_interfaces:\n                        if nb_interface.name not in interfaces:\n                            continue\n                        interface = interfaces.pop(nb_interface.name)\n                        # merge v6 into v4 addresses to save code repetition\n                        ips = {\n                            **interface.get(\"ipv4\", {}),\n                            **interface.get(\"ipv6\", {}),\n                        }\n                        # update/create IP addresses\n                        for ip, ip_data in ips.items():\n                            prefix_length = ip_data[\"prefix_length\"]\n                            # get IP address info from Netbox\n                            nb_ip = nb.ipam.ip_addresses.filter(\n                                address=f\"{ip}/{prefix_length}\"\n                            )\n                            if len(nb_ip) &gt; 1:\n                                log.warning(\n                                    f\"{host} got multiple {ip}/{prefix_length} IP addresses from Netbox, \"\n                                    f\"NorFab Netbox Service only supports handling of non-duplicate IPs.\"\n                                )\n                                continue\n                            # decide what to do\n                            if not nb_ip and create is False:\n                                continue\n                            elif not nb_ip and create is True:\n                                if dry_run is not True:\n                                    try:\n                                        nb_ip = nb.ipam.ip_addresses.create(\n                                            address=f\"{ip}/{prefix_length}\"\n                                        )\n                                    except Exception as e:\n                                        msg = f\"{host} failed to create {ip}/{prefix_length}, error: {e}\"\n                                        log.error(msg)\n                                        job.event(msg, resource=instance)\n                                        continue\n                                    nb_ip.assigned_object_type = \"dcim.interface\"\n                                    nb_ip.assigned_object_id = nb_interface.id\n                                    nb_ip.status = \"active\"\n                                    nb_ip.save()\n                                created[f\"{ip}/{prefix_length}\"] = nb_interface.name\n                                job.event(\n                                    f\"{host} created IP address {ip}/{prefix_length} for {nb_interface.name} interface\",\n                                    resource=instance,\n                                )\n                            elif nb_ip:\n                                nb_ip = list(nb_ip)[0]\n                                if dry_run is not True:\n                                    nb_ip.assigned_object_type = \"dcim.interface\"\n                                    nb_ip.assigned_object_id = nb_interface.id\n                                    nb_ip.status = \"active\"\n                                    nb_ip.save()\n                                updated[nb_ip.address] = nb_interface.name\n                                job.event(\n                                    f\"{host} updated IP address {ip}/{prefix_length} for {nb_interface.name} interface\",\n                                    resource=instance,\n                                )\n\n    else:\n        raise UnsupportedServiceError(\n            f\"'{datasource}' datasource service not supported\"\n        )\n\n    return ret\n</code></pre>","tags":["netbox"]},{"location":"workers/nornir/api_reference_workers_nornir_worker/","title":"Nornir Worker","text":""},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.WatchDog","title":"<code>WatchDog(worker)</code>","text":"<p>               Bases: <code>WorkerWatchDog</code></p> <p>Class to monitor Nornir worker performance.</p> <p>Parameters:</p> Name Type Description Default <code>worker</code> <code>Worker</code> <p>The worker instance that this NornirWorker will manage.</p> required <p>Attributes:</p> Name Type Description <code>worker</code> <code>Worker</code> <p>The worker instance being monitored.</p> <code>connections_idle_timeout</code> <code>int</code> <p>Timeout value for idle connections.</p> <code>connections_data</code> <code>dict</code> <p>Dictionary to store connection use timestamps.</p> <code>started_at</code> <code>float</code> <p>Timestamp when the watchdog was started.</p> <code>idle_connections_cleaned</code> <code>int</code> <p>Counter for idle connections cleaned.</p> <code>dead_connections_cleaned</code> <code>int</code> <p>Counter for dead connections cleaned.</p> <code>watchdog_tasks</code> <code>list</code> <p>List of tasks for the watchdog to run in a given order.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def __init__(self, worker):\n    super().__init__(worker)\n    self.worker = worker\n    self.connections_idle_timeout = worker.nornir_worker_inventory.get(\n        \"connections_idle_timeout\", None\n    )\n    self.connections_data = {}  # store connections use timestamps\n    self.started_at = time.time()\n\n    # stats attributes\n    self.idle_connections_cleaned = 0\n    self.dead_connections_cleaned = 0\n\n    # list of tasks for watchdog to run in given order\n    self.watchdog_tasks = [\n        self.connections_clean,\n        self.connections_keepalive,\n    ]\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.WatchDog.stats","title":"<code>stats() -&gt; Dict</code>","text":"<p>Collects and returns statistics about the worker.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict</code> <p>A dictionary containing the following keys:</p> <ul> <li>runs (int): The number of runs executed by the worker.</li> <li>timestamp (str): The current time in a human-readable format.</li> <li>alive (int): The time in seconds since the worker started.</li> <li>dead_connections_cleaned (int): The number of dead connections cleaned.</li> <li>idle_connections_cleaned (int): The number of idle connections cleaned.</li> <li>worker_ram_usage_mbyte (float): The current RAM usage of the worker in megabytes.</li> </ul> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def stats(self) -&gt; Dict:\n    \"\"\"\n    Collects and returns statistics about the worker.\n\n    Returns:\n        dict: A dictionary containing the following keys:\n\n            - runs (int): The number of runs executed by the worker.\n            - timestamp (str): The current time in a human-readable format.\n            - alive (int): The time in seconds since the worker started.\n            - dead_connections_cleaned (int): The number of dead connections cleaned.\n            - idle_connections_cleaned (int): The number of idle connections cleaned.\n            - worker_ram_usage_mbyte (float): The current RAM usage of the worker in megabytes.\n    \"\"\"\n    return {\n        \"runs\": self.runs,\n        \"timestamp\": time.ctime(),\n        \"alive\": int(time.time() - self.started_at),\n        \"dead_connections_cleaned\": self.dead_connections_cleaned,\n        \"idle_connections_cleaned\": self.idle_connections_cleaned,\n        \"worker_ram_usage_mbyte\": self.get_ram_usage(),\n    }\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.WatchDog.configuration","title":"<code>configuration() -&gt; Dict</code>","text":"<p>Returns the configuration settings for the worker.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>A dictionary containing the configuration settings:</p> <ul> <li>\"watchdog_interval\" (int): The interval for the watchdog timer.</li> <li>\"connections_idle_timeout\" (int): The timeout for idle connections.</li> </ul> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def configuration(self) -&gt; Dict:\n    \"\"\"\n    Returns the configuration settings for the worker.\n\n    Returns:\n        Dict: A dictionary containing the configuration settings:\n\n            - \"watchdog_interval\" (int): The interval for the watchdog timer.\n            - \"connections_idle_timeout\" (int): The timeout for idle connections.\n    \"\"\"\n    return {\n        \"watchdog_interval\": self.watchdog_interval,\n        \"connections_idle_timeout\": self.connections_idle_timeout,\n    }\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.WatchDog.connections_get","title":"<code>connections_get() -&gt; Dict</code>","text":"<p>Retrieve the current connections data.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>A dictionary containing the current connections data.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def connections_get(self) -&gt; Dict:\n    \"\"\"\n    Retrieve the current connections data.\n\n    Returns:\n        Dict: A dictionary containing the current connections data.\n    \"\"\"\n    return {\n        \"connections\": self.connections_data,\n    }\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.WatchDog.connections_update","title":"<code>connections_update(nr: Any, plugin: str) -&gt; None</code>","text":"<p>Function to update connection use timestamps for each host</p> <p>Parameters:</p> Name Type Description Default <code>nr</code> <code>Any</code> <p>Nornir object</p> required <code>plugin</code> <code>str</code> <p>connection plugin name</p> required Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def connections_update(self, nr: Any, plugin: str) -&gt; None:\n    \"\"\"\n    Function to update connection use timestamps for each host\n\n    Args:\n        nr: Nornir object\n        plugin: connection plugin name\n    \"\"\"\n    conn_stats = {\n        \"last_use\": None,\n        \"last_keepalive\": None,\n        \"keepalive_count\": 0,\n    }\n    for host_name in nr.inventory.hosts:\n        self.connections_data.setdefault(host_name, {})\n        self.connections_data[host_name].setdefault(plugin, conn_stats.copy())\n        self.connections_data[host_name][plugin][\"last_use\"] = time.ctime()\n    log.info(\n        f\"{self.worker.name} - updated connections use timestamps for '{plugin}'\"\n    )\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.WatchDog.connections_clean","title":"<code>connections_clean()</code>","text":"<p>Cleans up idle connections based on the configured idle timeout.</p> <p>This method checks for connections that have been idle for longer than the specified <code>connections_idle_timeout</code> and disconnects them. The behavior varies depending on the value of <code>connections_idle_timeout</code>:</p> <ul> <li>If <code>connections_idle_timeout</code> is None, no connections are disconnected.</li> <li>If <code>connections_idle_timeout</code> is 0, all connections are disconnected.</li> <li>If <code>connections_idle_timeout</code> is greater than 0, only connections that   have been idle for longer than the specified timeout are disconnected.</li> </ul> <p>The method acquires a lock to ensure thread safety while modifying the connections data. It logs the disconnection actions and updates the <code>idle_connections_cleaned</code> counter.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while attempting to disconnect idle connections, an error message is logged.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def connections_clean(self):\n    \"\"\"\n    Cleans up idle connections based on the configured idle timeout.\n\n    This method checks for connections that have been idle for longer than the\n    specified `connections_idle_timeout` and disconnects them. The behavior\n    varies depending on the value of `connections_idle_timeout`:\n\n    - If `connections_idle_timeout` is None, no connections are disconnected.\n    - If `connections_idle_timeout` is 0, all connections are disconnected.\n    - If `connections_idle_timeout` is greater than 0, only connections that\n      have been idle for longer than the specified timeout are disconnected.\n\n    The method acquires a lock to ensure thread safety while modifying the\n    connections data. It logs the disconnection actions and updates the\n    `idle_connections_cleaned` counter.\n\n    Raises:\n        Exception: If an error occurs while attempting to disconnect idle connections, an error message is logged.\n    \"\"\"\n    # dictionary keyed by plugin name and value as a list of hosts\n    disconnect = {}\n    if not self.worker.connections_lock.acquire(blocking=False):\n        return\n    try:\n        # if idle timeout not set, connections don't age out\n        if self.connections_idle_timeout is None:\n            disconnect = {}\n        # disconnect all connections for all hosts\n        elif self.connections_idle_timeout == 0:\n            disconnect = {\"all\": list(self.connections_data.keys())}\n        # only disconnect aged/idle connections\n        elif self.connections_idle_timeout &gt; 0:\n            for host_name, plugins in self.connections_data.items():\n                for plugin, conn_data in plugins.items():\n                    last_use = time.mktime(time.strptime(conn_data[\"last_use\"]))\n                    age = time.time() - last_use\n                    if age &gt; self.connections_idle_timeout:\n                        disconnect.setdefault(plugin, [])\n                        disconnect[plugin].append(host_name)\n        # run task to disconnect connections for aged hosts\n        for plugin, hosts in disconnect.items():\n            if not hosts:\n                continue\n            aged_hosts = FFun(self.worker.nr, FL=hosts)\n            aged_hosts.run(task=nr_connections, call=\"close\", conn_name=plugin)\n            log.debug(\n                f\"{self.worker.name} watchdog, disconnected '{plugin}' \"\n                f\"connections for '{', '.join(hosts)}'\"\n            )\n            self.idle_connections_cleaned += len(hosts)\n            # wipe out connections data if all connection closed\n            if plugin == \"all\":\n                self.connections_data = {}\n                break\n            # remove disconnected plugin from host's connections_data\n            for host in hosts:\n                self.connections_data[host].pop(plugin)\n                if not self.connections_data[host]:\n                    self.connections_data.pop(host)\n    except Exception as e:\n        msg = f\"{self.worker.name} - watchdog failed to close idle connections, error: {e}\"\n        log.error(msg)\n    finally:\n        self.worker.connections_lock.release()\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.WatchDog.connections_keepalive","title":"<code>connections_keepalive()</code>","text":"<p>Keepalive connections and clean up dead connections if any.</p> <p>This method performs the following tasks:</p> <ul> <li>If <code>connections_idle_timeout</code> is 0, it returns immediately without performing any actions.</li> <li>Attempts to acquire a lock on <code>worker.connections_lock</code> to ensure thread safety.</li> <li>Logs a debug message indicating that the keepalive process is running.</li> <li>Uses <code>HostsKeepalive</code> to check and clean up dead connections, updating the <code>dead_connections_cleaned</code> counter.</li> <li>Removes connections that are no longer present in the Nornir inventory.</li> <li>Removes hosts from <code>connections_data</code> if they have no remaining connections.</li> <li>Updates the keepalive statistics for each connection plugin, including the last keepalive time and keepalive count.</li> <li>Logs an error message if an exception occurs during the keepalive process.</li> <li>Releases the lock on <code>worker.connections_lock</code> in the <code>finally</code> block to ensure it is always released.</li> </ul> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during the keepalive process, it is logged as an error.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def connections_keepalive(self):\n    \"\"\"\n    Keepalive connections and clean up dead connections if any.\n\n    This method performs the following tasks:\n\n    - If `connections_idle_timeout` is 0, it returns immediately without performing any actions.\n    - Attempts to acquire a lock on `worker.connections_lock` to ensure thread safety.\n    - Logs a debug message indicating that the keepalive process is running.\n    - Uses `HostsKeepalive` to check and clean up dead connections, updating the `dead_connections_cleaned` counter.\n    - Removes connections that are no longer present in the Nornir inventory.\n    - Removes hosts from `connections_data` if they have no remaining connections.\n    - Updates the keepalive statistics for each connection plugin, including the last keepalive time and keepalive count.\n    - Logs an error message if an exception occurs during the keepalive process.\n    - Releases the lock on `worker.connections_lock` in the `finally` block to ensure it is always released.\n\n    Raises:\n        Exception: If an error occurs during the keepalive process, it is logged as an error.\n    \"\"\"\n    if self.connections_idle_timeout == 0:  # do not keepalive if idle is 0\n        return\n    if not self.worker.connections_lock.acquire(blocking=False):\n        return\n    try:\n        log.debug(f\"{self.worker.name} - watchdog running connections keepalive\")\n        stats = HostsKeepalive(self.worker.nr)\n        self.dead_connections_cleaned += stats[\"dead_connections_cleaned\"]\n        # remove connections that are no longer present in Nornir inventory\n        for host_name, host_connections in self.connections_data.items():\n            # check if host is still in Nornir inventory\n            if host_name not in self.worker.nr.inventory.hosts:\n                self.connections_data.pop(host_name, None)\n                continue\n            # clean up specific connections for host\n            for connection_name in list(host_connections.keys()):\n                if not self.worker.nr.inventory.hosts[host_name].connections.get(\n                    connection_name\n                ):\n                    self.connections_data[host_name].pop(connection_name)\n        # remove host if no connections left\n        for host_name in list(self.connections_data.keys()):\n            if self.connections_data[host_name] == {}:\n                self.connections_data.pop(host_name)\n        # update connections statistics\n        for plugins in self.connections_data.values():\n            for plugin in plugins.values():\n                plugin[\"last_keepalive\"] = time.ctime()\n                plugin[\"keepalive_count\"] += 1\n    except Exception as e:\n        msg = f\"{self.worker.name} - watchdog HostsKeepalive check error: {e}\"\n        log.error(msg)\n    finally:\n        self.worker.connections_lock.release()\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker","title":"<code>NornirWorker(inventory: str, broker: str, worker_name: str, exit_event=None, init_done_event=None, log_level: str = None, log_queue: object = None)</code>","text":"<p>               Bases: <code>NFPWorker</code></p> <p>NornirWorker class for managing Nornir Service tasks.</p> <p>Parameters:</p> Name Type Description Default <code>inventory</code> <code>str</code> <p>Path to the inventory file.</p> required <code>broker</code> <code>str</code> <p>Broker address.</p> required <code>worker_name</code> <code>str</code> <p>Name of the worker.</p> required <code>exit_event</code> <code>Event</code> <p>Event to signal worker exit. Defaults to None.</p> <code>None</code> <code>init_done_event</code> <code>Event</code> <p>Event to signal initialization completion. Defaults to None.</p> <code>None</code> <code>log_level</code> <code>str</code> <p>Logging level. Defaults to None.</p> <code>None</code> <code>log_queue</code> <code>object</code> <p>Queue for logging. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>init_done_event</code> <code>Event</code> <p>Event to signal initialization completion.</p> <code>tf_base_path</code> <code>str</code> <p>Base path for files folder saved using <code>tf</code> processor.</p> <code>connections_lock</code> <code>Lock</code> <p>Lock for managing connections.</p> <code>nornir_inventory</code> <code>dict</code> <p>Inventory data for Nornir.</p> <code>watchdog</code> <code>WatchDog</code> <p>Watchdog instance for monitoring.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def __init__(\n    self,\n    inventory: str,\n    broker: str,\n    worker_name: str,\n    exit_event=None,\n    init_done_event=None,\n    log_level: str = None,\n    log_queue: object = None,\n):\n    super().__init__(\n        inventory, broker, SERVICE, worker_name, exit_event, log_level, log_queue\n    )\n    self.init_done_event = init_done_event\n    self.tf_base_path = os.path.join(self.base_dir, \"tf\")\n\n    # misc attributes\n    self.connections_lock = Lock()\n\n    # initiate Nornir\n    self.refresh_nornir(job=Job())\n\n    # initiate watchdog\n    self.watchdog = WatchDog(self)\n    self.watchdog.start()\n\n    # run startup hooks\n    for f in self.inventory.hooks.get(\"nornir-startup\", []):\n        f[\"function\"](self, *f.get(\"args\", []), **f.get(\"kwargs\", {}))\n\n    if self.init_done_event is not None:\n        self.init_done_event.set()\n\n    log.info(f\"{self.name} - Started\")\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.worker_exit","title":"<code>worker_exit()</code>","text":"<p>Executes all functions registered under the \"nornir-exit\" hook in the inventory.</p> <p>This method iterates through the list of hooks associated with the \"nornir-exit\" key in the inventory's hooks.</p> <p>For each hook, it calls the function specified in the hook, passing the current instance (<code>self</code>) as the first argument, followed by any additional positional and keyword arguments specified in the hook.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def worker_exit(self):\n    \"\"\"\n    Executes all functions registered under the \"nornir-exit\" hook in the inventory.\n\n    This method iterates through the list of hooks associated with the \"nornir-exit\"\n    key in the inventory's hooks.\n\n    For each hook, it calls the function specified in the hook, passing the current\n    instance (`self`) as the first argument, followed by any additional positional\n    and keyword arguments specified in the hook.\n    \"\"\"\n    # run exit hooks\n    for f in self.inventory.hooks.get(\"nornir-exit\", []):\n        f[\"function\"](self, *f.get(\"args\", []), **f.get(\"kwargs\", {}))\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.init_nornir","title":"<code>init_nornir(inventory: dict) -&gt; None</code>","text":"<p>Initializes the Nornir automation framework with the provided inventory.</p> <p>This method first closes any existing Nornir connections if present, optionally emitting a progress event. It then creates a new Nornir instance using the supplied inventory dictionary, which should contain configuration for logging, runner, hosts, groups, defaults, and user-defined settings.</p> <p>Parameters:</p> Name Type Description Default <code>inventory</code> <code>dict</code> <p>A dictionary containing Nornir inventory and configuration options.</p> required Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def init_nornir(self, inventory: dict) -&gt; None:\n    \"\"\"\n    Initializes the Nornir automation framework with the provided inventory.\n\n    This method first closes any existing Nornir connections if present, optionally emitting a progress event.\n    It then creates a new Nornir instance using the supplied inventory dictionary, which should contain\n    configuration for logging, runner, hosts, groups, defaults, and user-defined settings.\n\n    Args:\n        inventory (dict): A dictionary containing Nornir inventory and configuration options.\n    \"\"\"\n    # clean up existing Nornir instance\n    with self.connections_lock:\n        if self.nr is not None and self.nr.inventory.hosts:\n            self.nr.close_connections()\n\n        # initiate Nornir\n        self.nr = InitNornir(\n            logging=inventory.get(\"logging\", {\"enabled\": False}),\n            runner=inventory.get(\"runner\", {}),\n            inventory={\n                \"plugin\": \"DictInventory\",\n                \"options\": {\n                    \"hosts\": inventory.get(\"hosts\", {}),\n                    \"groups\": inventory.get(\"groups\", {}),\n                    \"defaults\": inventory.get(\"defaults\", {}),\n                },\n            },\n            user_defined=inventory.get(\"user_defined\", {}),\n        )\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.filter_hosts_and_validate","title":"<code>filter_hosts_and_validate(kwargs: Dict[str, Any], ret: Result) -&gt; Tuple[Any, Result]</code>","text":"<p>Helper method to filter hosts and validate results.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Any, Result]</code> <p>(filtered_nornir, Result) where Result status set to <code>no_match</code> if no hosts matched.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def filter_hosts_and_validate(\n    self, kwargs: Dict[str, Any], ret: Result\n) -&gt; Tuple[Any, Result]:\n    \"\"\"\n    Helper method to filter hosts and validate results.\n\n    Returns:\n        tuple: (filtered_nornir, Result) where Result status set to\n            `no_match` if no hosts matched.\n    \"\"\"\n    self.nr.data.reset_failed_hosts()  # reset failed hosts before filtering\n    filters = {k: kwargs.pop(k) for k in list(kwargs.keys()) if k in FFun_functions}\n    filtered_nornir = FFun(self.nr, **filters)\n\n    if not filtered_nornir.inventory.hosts:\n        msg = (\n            f\"{self.name} - nothing to do, no hosts matched by filters '{filters}'\"\n        )\n        log.debug(msg)\n        ret.messages.append(msg)\n        ret.status = \"no_match\"\n\n    return filtered_nornir, ret\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.refresh_nornir","title":"<code>refresh_nornir(job: Job, progress: bool = False) -&gt; Result</code>","text":"<p>Refreshes the Nornir instance by reloading the inventory from configured sources.</p> <p>This method performs the following steps:</p> <pre><code>1. Loads the inventory configuration from the broker.\n2. If Netbox is specified in the inventory, pulls inventory data from Netbox.\n3. If Containerlab is specified in the inventory, pulls inventory data from Containerlab.\n4. Initializes the Nornir instance with the refreshed inventory.\n5. Optionally emits progress events at each stage if `progress` is True.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>progress</code> <code>bool</code> <p>If True, emits progress events during the refresh process. Defaults to False.</p> <code>False</code> <p>The inventory configuration is expected to be a dictionary with the following keys:</p> <ul> <li>\"logging\": A dictionary specifying logging configuration (default: {\"enabled\": False}).</li> <li>\"runner\": A dictionary specifying runner options (default: {}).</li> <li>\"hosts\": A dictionary specifying host details (default: {}).</li> <li>\"groups\": A dictionary specifying group details (default: {}).</li> <li>\"defaults\": A dictionary specifying default values (default: {}).</li> <li>\"user_defined\": A dictionary specifying user-defined options (default: {}).</li> </ul> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>A Result object indicating the outcome of the refresh operation.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef refresh_nornir(\n    self,\n    job: Job,\n    progress: bool = False,\n) -&gt; Result:\n    \"\"\"\n    Refreshes the Nornir instance by reloading the inventory from configured sources.\n\n    This method performs the following steps:\n\n        1. Loads the inventory configuration from the broker.\n        2. If Netbox is specified in the inventory, pulls inventory data from Netbox.\n        3. If Containerlab is specified in the inventory, pulls inventory data from Containerlab.\n        4. Initializes the Nornir instance with the refreshed inventory.\n        5. Optionally emits progress events at each stage if `progress` is True.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        progress (bool, optional): If True, emits progress events during the refresh process. Defaults to False.\n\n    The inventory configuration is expected to be a dictionary with the following keys:\n\n    - \"logging\": A dictionary specifying logging configuration (default: {\"enabled\": False}).\n    - \"runner\": A dictionary specifying runner options (default: {}).\n    - \"hosts\": A dictionary specifying host details (default: {}).\n    - \"groups\": A dictionary specifying group details (default: {}).\n    - \"defaults\": A dictionary specifying default values (default: {}).\n    - \"user_defined\": A dictionary specifying user-defined options (default: {}).\n\n    Returns:\n        Result: A Result object indicating the outcome of the refresh operation.\n    \"\"\"\n    ret = Result(task=f\"{self.name}:refresh_nornir\", result=True)\n\n    # get inventory from broker\n    self.nornir_worker_inventory = self.load_inventory()\n\n    # pull Nornir inventory from Netbox\n    if \"netbox\" in self.nornir_worker_inventory:\n        self.nornir_inventory_load_netbox(job=job)\n        job.event(\"Pulled Nornir inventory data from Netbox\")\n\n    # pull Nornir inventory from Containerlab\n    if \"containerlab\" in self.nornir_worker_inventory:\n        self.nornir_inventory_load_containerlab(\n            job=job,\n            **self.nornir_worker_inventory[\"containerlab\"],\n            re_init_nornir=False,\n        )\n        job.event(\"Pulled Nornir inventory data from Containerlab\")\n\n    job.event(\"Pulled inventories, refreshing Nornir instance\")\n\n    self.init_nornir(self.nornir_worker_inventory)\n\n    job.event(\"Nornir instance refreshed\")\n\n    return ret\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.nornir_inventory_load_netbox","title":"<code>nornir_inventory_load_netbox(job: Job, progress: bool = False) -&gt; Result</code>","text":"<p>Queries inventory data from Netbox Service and merges it into the Nornir inventory.</p> <p>This function checks if there is Netbox data in the inventory and retrieves it if available. It handles retries and timeout configurations, and ensures that necessary filters or devices are specified. The retrieved inventory data is then merged into the existing Nornir inventory.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required Logs <ul> <li>Critical: If the inventory has no hosts, filters, or devices defined.</li> <li>Error: If no inventory data is returned from Netbox.</li> <li>Warning: If the Netbox instance returns no hosts data.</li> </ul> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef nornir_inventory_load_netbox(\n    self,\n    job: Job,\n    progress: bool = False,\n) -&gt; Result:\n    \"\"\"\n    Queries inventory data from Netbox Service and merges it into the Nornir inventory.\n\n    This function checks if there is Netbox data in the inventory and retrieves\n    it if available. It handles retries and timeout configurations, and ensures\n    that necessary filters or devices are specified. The retrieved inventory\n    data is then merged into the existing Nornir inventory.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n\n    Logs:\n        - Critical: If the inventory has no hosts, filters, or devices defined.\n        - Error: If no inventory data is returned from Netbox.\n        - Warning: If the Netbox instance returns no hosts data.\n    \"\"\"\n    ret = Result(task=f\"{self.name}:nornir_inventory_load_netbox\", result=True)\n\n    # form Netbox inventory load arguments\n    if isinstance(self.nornir_worker_inventory.get(\"netbox\"), dict):\n        kwargs = self.nornir_worker_inventory[\"netbox\"]\n    elif self.nornir_worker_inventory.get(\"netbox\") is True:\n        kwargs = {}\n    timeout = max(10, kwargs.pop(\"timeout\", 100))\n\n    # check if need to add devices list\n    if \"filters\" not in kwargs and \"devices\" not in kwargs:\n        if self.nornir_worker_inventory.get(\"hosts\"):\n            kwargs[\"devices\"] = list(self.nornir_worker_inventory[\"hosts\"])\n        else:\n            msg = f\"{self.name} - inventory has no hosts, Netbox filters or devices defined\"\n            log.warning(msg)\n            ret.result = False\n            ret.messages = [msg]\n            return ret\n\n    nb_inventory_data = self.client.run_job(\n        service=\"netbox\",\n        task=\"get_nornir_inventory\",\n        workers=\"any\",\n        kwargs=kwargs,\n        timeout=timeout,\n    )\n\n    if nb_inventory_data is None:\n        msg = f\"{self.name} - Netbox get_nornir_inventory no inventory returned\"\n        log.error(msg)\n        raise RuntimeError(msg)\n\n    # merge Netbox inventory into Nornir inventory\n    for wname, wdata in nb_inventory_data.items():\n        if wdata[\"failed\"] is False and wdata[\"result\"].get(\"hosts\"):\n            merge_recursively(self.nornir_worker_inventory, wdata[\"result\"])\n            break\n    else:\n        msg = (\n            f\"{self.name} - Netbox worker(s) \"\n            f\"'{', '.join(list(nb_inventory_data.keys()))}' returned no hosts data.\"\n        )\n        log.error(msg)\n        raise RuntimeError(msg)\n\n    job.event(\"Pulled Nornir inventory from Netbox\")\n\n    return ret\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.nornir_inventory_load_containerlab","title":"<code>nornir_inventory_load_containerlab(job: Job, lab_name: str = None, groups: Union[None, list] = None, clab_workers: str = 'all', use_default_credentials: bool = True, progress: bool = False, dry_run: bool = False, re_init_nornir: bool = True) -&gt; Result</code>","text":"<p>Pulls the Nornir inventory from a Containerlab lab instance and merges it with the existing Nornir inventory.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>lab_name</code> <code>str</code> <p>The name of the Containerlab lab to retrieve the inventory from.</p> <code>None</code> <code>groups</code> <code>list</code> <p>A list of group names to include into the hosts' inventory.</p> <code>None</code> <code>use_default_credentials</code> <code>bool</code> <p>Whether to use default credentials for the hosts.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>A Result object indicating the success or failure of the operation.     If successful, the Nornir inventory is updated with the retrieved data.</p> Notes <ul> <li>The method retrieves inventory data from a Containerlab lab using a client job.</li> <li>If the retrieved inventory contains host data, it is merged into the existing   Nornir inventory using the <code>merge_recursively</code> function.</li> <li>If no inventory or host data is returned, the method logs an error and marks   the operation as failed.</li> <li>After successful merging of inventory, Nornir instance is re-initialized with the   updated inventory.</li> </ul> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef nornir_inventory_load_containerlab(\n    self,\n    job: Job,\n    lab_name: str = None,\n    groups: Union[None, list] = None,\n    clab_workers: str = \"all\",\n    use_default_credentials: bool = True,\n    progress: bool = False,\n    dry_run: bool = False,\n    re_init_nornir: bool = True,\n) -&gt; Result:\n    \"\"\"\n    Pulls the Nornir inventory from a Containerlab lab instance and merges it with the\n    existing Nornir inventory.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        lab_name (str): The name of the Containerlab lab to retrieve the inventory from.\n        groups (list, optional): A list of group names to include into the hosts' inventory.\n        use_default_credentials (bool): Whether to use default credentials for the hosts.\n\n    Returns:\n        Result: A Result object indicating the success or failure of the operation.\n                If successful, the Nornir inventory is updated with the retrieved data.\n\n    Notes:\n        - The method retrieves inventory data from a Containerlab lab using a client job.\n        - If the retrieved inventory contains host data, it is merged into the existing\n          Nornir inventory using the `merge_recursively` function.\n        - If no inventory or host data is returned, the method logs an error and marks\n          the operation as failed.\n        - After successful merging of inventory, Nornir instance is re-initialized with the\n          updated inventory.\n    \"\"\"\n    groups = groups or []\n    ret = Result(\n        task=f\"{self.name}:nornir_inventory_load_containerlab\", result=True\n    )\n    job.event(\n        f\"Pulling Containerlab '{lab_name or 'all'}' inventory from '{clab_workers}' workers\"\n    )\n\n    clab_inventory_data = self.client.run_job(\n        service=\"containerlab\",\n        task=\"get_nornir_inventory\",\n        workers=clab_workers,\n        kwargs={\n            \"lab_name\": lab_name,\n            \"groups\": groups,\n            \"use_default_credentials\": use_default_credentials,\n        },\n    )\n\n    if clab_inventory_data is None:\n        msg = f\"{self.name} - Containerlab get_nornir_inventory no data returned\"\n        log.error(msg)\n        raise RuntimeError(msg)\n\n    job.event(f\"Pulled Containerlab '{lab_name or 'all'}' lab inventory\")\n\n    if dry_run is True:\n        ret.result = {w: r[\"result\"] for w, r in clab_inventory_data.items()}\n        return ret\n\n    for wname, wdata in clab_inventory_data.items():\n        # use inventory from first worker that returned hosts data\n        if wdata[\"failed\"] is False and wdata[\"result\"].get(\"hosts\"):\n            merge_recursively(self.nornir_worker_inventory, wdata[\"result\"])\n            break\n    else:\n        msg = (\n            f\"{self.name} - Containerlab worker(s) '{', '.join(list(clab_inventory_data.keys()))}' \"\n            f\"returned no hosts data for '{lab_name}' lab.\"\n        )\n        log.error(msg)\n        raise RuntimeError(msg)\n\n    job.event(\n        f\"Merged Containerlab '{lab_name or 'all'}' lab inventory with Nornir runtime inventory\"\n    )\n\n    if re_init_nornir is True:\n        self.init_nornir(self.nornir_worker_inventory)\n        job.event(\"Nornir instance re-initialized\")\n\n    return ret\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.load_job_data","title":"<code>load_job_data(job_data: Any) -&gt; Dict</code>","text":"<p>Helper function to download job data YAML files and load it.</p> <p>Parameters:</p> Name Type Description Default <code>job_data</code> <code>str</code> <p>job data NorFab file path to download and load using YAML.</p> required <p>Returns:dict     data: The job data loaded from the YAML string.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the job data is a URL and the file download fails.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def load_job_data(self, job_data: Any) -&gt; Dict:\n    \"\"\"\n    Helper function to download job data YAML files and load it.\n\n    Args:\n        job_data (str): job data NorFab file path to download and load using YAML.\n\n    Returns:dict\n        data: The job data loaded from the YAML string.\n\n    Raises:\n        FileNotFoundError: If the job data is a URL and the file download fails.\n    \"\"\"\n    if self.is_url(job_data):\n        job_data = self.fetch_file(job_data)\n        if job_data is None:\n            msg = f\"{self.name} - '{job_data}' job data file download failed\"\n            raise FileNotFoundError(msg)\n        job_data = yaml.safe_load(job_data)\n\n    return job_data\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.jinja2_network_hosts","title":"<code>jinja2_network_hosts(network: str, pfxlen: bool = False) -&gt; list</code>","text":"<p>Custom Jinja2 filter that return a list of hosts for a given IP network.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>str</code> <p>The network address in CIDR notation.</p> required <code>pfxlen</code> <code>bool</code> <p>If True, include the prefix length in the returned host addresses. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of host addresses as strings. If pfxlen is True, each address will include the prefix length.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def jinja2_network_hosts(self, network: str, pfxlen: bool = False) -&gt; list:\n    \"\"\"\n    Custom Jinja2 filter that return a list of hosts for a given IP network.\n\n    Args:\n        network (str): The network address in CIDR notation.\n        pfxlen (bool, optional): If True, include the prefix length\n            in the returned host addresses. Defaults to False.\n\n    Returns:\n        list: A list of host addresses as strings. If pfxlen is True,\n            each address will include the prefix length.\n    \"\"\"\n    ret = []\n    ip_interface = ipaddress.ip_interface(network)\n    prefixlen = ip_interface.network.prefixlen\n    for ip in ip_interface.network.hosts():\n        ret.append(f\"{ip}/{prefixlen}\" if pfxlen else str(ip))\n    return ret\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.jinja2_nb_create_ip","title":"<code>jinja2_nb_create_ip(prefix: str, device: str = None, interface: str = None, **kwargs) -&gt; str</code>","text":"<p>Jinja2 filter to get or create next available IP address from prefix using Netbox service.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def jinja2_nb_create_ip(\n    self, prefix: str, device: str = None, interface: str = None, **kwargs\n) -&gt; str:\n    \"\"\"\n    Jinja2 filter to get or create next available IP address from\n    prefix using Netbox service.\n    \"\"\"\n    kwargs[\"prefix\"] = prefix\n    kwargs[\"device\"] = device\n    kwargs[\"interface\"] = interface\n    reply = self.client.run_job(\n        \"netbox\",\n        \"create_ip\",\n        kwargs=kwargs,\n        workers=\"any\",\n        timeout=30,\n    )\n    # reply is a dict of {worker_name: results_dict}\n    res = list(reply.values())[0]\n    if res[\"failed\"]:\n        raise RuntimeError(res[\"messages\"])\n    return res[\"result\"][\"address\"]\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.jinja2_nb_create_prefix","title":"<code>jinja2_nb_create_prefix(parent: str, description: str, prefixlen: int = 30, **kwargs) -&gt; str</code>","text":"<p>Jinja2 filter to get or create next available prefix from parent prefix using Netbox service.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def jinja2_nb_create_prefix(\n    self, parent: str, description: str, prefixlen: int = 30, **kwargs\n) -&gt; str:\n    \"\"\"\n    Jinja2 filter to get or create next available prefix from\n    parent prefix using Netbox service.\n    \"\"\"\n    kwargs[\"parent\"] = parent\n    kwargs[\"description\"] = description\n    kwargs[\"prefixlen\"] = prefixlen\n    reply = self.client.run_job(\n        \"netbox\",\n        \"create_prefix\",\n        kwargs=kwargs,\n        workers=\"any\",\n        timeout=30,\n    )\n    # reply is a dict of {worker_name: results_dict}\n    res = list(reply.values())[0]\n    if res[\"failed\"]:\n        raise RuntimeError(res[\"messages\"])\n\n    return res[\"result\"][\"prefix\"]\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.jinja2_call_netbox","title":"<code>jinja2_call_netbox(netbox_task: str) -&gt; callable</code>","text":"<p>Returns a callable function to execute arbitrary NetBox service task.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def jinja2_call_netbox(self, netbox_task: str) -&gt; callable:\n    \"\"\"\n    Returns a callable function to execute arbitrary NetBox service task.\n    \"\"\"\n\n    def call_netbox(*args, **kwargs) -&gt; dict:\n        reply = self.client.run_job(\n            \"netbox\",\n            netbox_task,\n            args=args,\n            kwargs=kwargs,\n            workers=\"any\",\n            timeout=300,\n        )\n        res = list(reply.values())[0]\n\n        # check if has an error\n        if res[\"failed\"]:\n            raise RuntimeError(res[\"messages\"])\n\n        # return result for single host only\n        if len(kwargs.get(\"devices\", [])) == 1:\n            return res[\"result\"][kwargs[\"devices\"][0]]\n        # return full results\n        else:\n            return res[\"result\"]\n\n    return call_netbox\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.add_jinja2_netbox","title":"<code>add_jinja2_netbox() -&gt; Dict</code>","text":"<p>Aggregates Jinja2 NetBox-related methods and functions into a dictionary for the ease of use within Jinja2 templates.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def add_jinja2_netbox(self) -&gt; Dict:\n    \"\"\"\n    Aggregates Jinja2 NetBox-related methods and functions into a dictionary\n    for the ease of use within Jinja2 templates.\n    \"\"\"\n    return {\n        \"get_connections\": self.jinja2_call_netbox(\"get_connections\"),\n        \"get_interfaces\": self.jinja2_call_netbox(\"get_interfaces\"),\n        \"get_circuits\": self.jinja2_call_netbox(\"get_circuits\"),\n        \"get_devices\": self.jinja2_call_netbox(\"get_devices\"),\n        \"rest\": self.jinja2_call_netbox(\"rest\"),\n        \"graphql\": self.jinja2_call_netbox(\"graphql\"),\n        \"create_ip\": self.jinja2_nb_create_ip,\n        \"create_prefix\": self.jinja2_nb_create_prefix,\n    }\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.add_jinja2_filters","title":"<code>add_jinja2_filters() -&gt; Dict</code>","text":"<p>Adds custom filters for use in Jinja2 templates using <code>|</code> syntaxis.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict</code> <p>A dictionary where the keys are the names of the filters and the values are the corresponding filter functions.</p> <ul> <li>\"nb_create_ip\": Method to get the next IP address.</li> <li>\"nb_create_prefix\": Method to get next available prefix.</li> <li>\"network_hosts\": Method to get IP network hosts.</li> </ul> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>def add_jinja2_filters(self) -&gt; Dict:\n    \"\"\"\n    Adds custom filters for use in Jinja2 templates using `|` syntaxis.\n\n    Returns:\n        dict (Dict): A dictionary where the keys are the names of the filters\n            and the values are the corresponding filter functions.\n\n            - \"nb_create_ip\": Method to get the next IP address.\n            - \"nb_create_prefix\": Method to get next available prefix.\n            - \"network_hosts\": Method to get IP network hosts.\n    \"\"\"\n    return {\n        \"netbox.create_ip\": self.jinja2_nb_create_ip,\n        \"netbox.create_prefix\": self.jinja2_nb_create_prefix,\n        \"network_hosts\": self.jinja2_network_hosts,\n    }\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.get_nornir_hosts","title":"<code>get_nornir_hosts(details: bool = False, **kwargs: dict) -&gt; Result</code>","text":"<p>Retrieve a list of Nornir hosts managed by this worker.</p> <p>Parameters:</p> Name Type Description Default <code>details</code> <code>bool</code> <p>If True, returns detailed information about each host.</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Hosts filters to apply when retrieving hosts.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Result</code> <p>List[Dict]: A list of hosts with optional detailed information.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"GET\"]},\n    input=GetNornirHosts,\n    output=GetNornirHostsResponse,\n)\ndef get_nornir_hosts(self, details: bool = False, **kwargs: dict) -&gt; Result:\n    \"\"\"\n    Retrieve a list of Nornir hosts managed by this worker.\n\n    Args:\n        details (bool): If True, returns detailed information about each host.\n        **kwargs (dict): Hosts filters to apply when retrieving hosts.\n\n    Returns:\n        List[Dict]: A list of hosts with optional detailed information.\n    \"\"\"\n    ret = Result(task=f\"{self.name}:get_nornir_hosts\", result={} if details else [])\n    filtered_nornir, ret = self.filter_hosts_and_validate(kwargs, ret)\n    if ret.status == \"no_match\":\n        ret.result = None\n    elif details:\n        ret.result = {\n            host_name: {\n                \"platform\": str(host.platform),\n                \"hostname\": str(host.hostname),\n                \"port\": str(host.port),\n                \"groups\": [str(g) for g in host.groups],\n                \"username\": str(host.username),\n            }\n            for host_name, host in filtered_nornir.inventory.hosts.items()\n        }\n    else:\n        ret.result = list(filtered_nornir.inventory.hosts)\n    return ret\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.get_inventory","title":"<code>get_inventory(**kwargs: dict) -&gt; Result</code>","text":"<p>Retrieve running Nornir inventory for requested hosts</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>dict</code> <p>Fx filters used to filter the inventory.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Result</code> <p>A dictionary representation of the filtered inventory.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_inventory(self, **kwargs: dict) -&gt; Result:\n    \"\"\"\n    Retrieve running Nornir inventory for requested hosts\n\n    Args:\n        **kwargs (dict): Fx filters used to filter the inventory.\n\n    Returns:\n        Dict: A dictionary representation of the filtered inventory.\n    \"\"\"\n    ret = Result(task=f\"{self.name}:get_inventory\", result={})\n    filtered_nornir, ret = self.filter_hosts_and_validate(kwargs, ret)\n    if ret.status != \"no_match\":\n        ret.result = filtered_nornir.inventory.dict()\n    return ret\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.get_version","title":"<code>get_version() -&gt; Result</code>","text":"<p>Retrieve the versions of various libraries and system information.</p> <p>This method collects the version information of a predefined set of libraries and system details such as the Python version and platform. It attempts to import each library and fetch its version. If a library is not found, it is skipped.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>a dictionary with the library names as keys and their respective version numbers as values. If a library is not found, its value will be an empty string.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_version(self) -&gt; Result:\n    \"\"\"\n    Retrieve the versions of various libraries and system information.\n\n    This method collects the version information of a predefined set of libraries\n    and system details such as the Python version and platform. It attempts to\n    import each library and fetch its version. If a library is not found, it is\n    skipped.\n\n    Returns:\n        dict: a dictionary with the library names as keys and their respective\n            version numbers as values. If a library is not found, its value will be\n            an empty string.\n    \"\"\"\n    libs = {\n        \"norfab\": \"\",\n        \"scrapli\": \"\",\n        \"scrapli-netconf\": \"\",\n        \"scrapli-community\": \"\",\n        \"paramiko\": \"\",\n        \"netmiko\": \"\",\n        \"napalm\": \"\",\n        \"nornir\": \"\",\n        \"ncclient\": \"\",\n        \"nornir-netmiko\": \"\",\n        \"nornir-napalm\": \"\",\n        \"nornir-scrapli\": \"\",\n        \"nornir-utils\": \"\",\n        \"tabulate\": \"\",\n        \"xmltodict\": \"\",\n        \"puresnmp\": \"\",\n        \"pygnmi\": \"\",\n        \"pyyaml\": \"\",\n        \"jmespath\": \"\",\n        \"jinja2\": \"\",\n        \"ttp\": \"\",\n        \"nornir-salt\": \"\",\n        \"lxml\": \"\",\n        \"ttp-templates\": \"\",\n        \"ntc-templates\": \"\",\n        \"cerberus\": \"\",\n        \"pydantic\": \"\",\n        \"requests\": \"\",\n        \"textfsm\": \"\",\n        \"N2G\": \"\",\n        \"dnspython\": \"\",\n        \"pythonping\": \"\",\n        \"python\": sys.version.split(\" \")[0],\n        \"platform\": sys.platform,\n    }\n    # get version of packages installed\n    for pkg in libs.keys():\n        try:\n            libs[pkg] = importlib.metadata.version(pkg)\n        except importlib.metadata.PackageNotFoundError:\n            pass\n\n    return Result(result=libs)\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.get_watchdog_stats","title":"<code>get_watchdog_stats() -&gt; Result</code>","text":"<p>Retrieve the statistics from the watchdog.</p> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the statistics from the watchdog.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_watchdog_stats(self) -&gt; Result:\n    \"\"\"\n    Retrieve the statistics from the watchdog.\n\n    Returns:\n        Result: An object containing the statistics from the watchdog.\n    \"\"\"\n    return Result(result=self.watchdog.stats())\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.get_watchdog_configuration","title":"<code>get_watchdog_configuration() -&gt; Result</code>","text":"<p>Retrieves the current configuration of the watchdog.</p> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the watchdog configuration.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_watchdog_configuration(self) -&gt; Result:\n    \"\"\"\n    Retrieves the current configuration of the watchdog.\n\n    Returns:\n        Result: An object containing the watchdog configuration.\n    \"\"\"\n    return Result(result=self.watchdog.configuration())\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.get_watchdog_connections","title":"<code>get_watchdog_connections() -&gt; Result</code>","text":"<p>Retrieve the list of connections currently managed by watchdog.</p> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An instance of the Result class containing the current watchdog connections.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_watchdog_connections(self) -&gt; Result:\n    \"\"\"\n    Retrieve the list of connections currently managed by watchdog.\n\n    Returns:\n        Result: An instance of the Result class containing the current\n            watchdog connections.\n    \"\"\"\n    return Result(result=self.watchdog.connections_get())\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.task","title":"<code>task(job: Job, plugin: str, **kwargs: Any) -&gt; Result</code>","text":"<p>Execute a Nornir task plugin.</p> <p>This method dynamically imports and executes a specified Nornir task plugin, using the provided arguments and keyword arguments. The <code>plugin</code> attribute can refer to a file to fetch from a file service, which must contain a function named <code>task</code> that accepts a Nornir task object as the first positional argument.</p> <p>Example of a custom task function file:</p> <pre><code># define connection name for RetryRunner to properly detect it\nCONNECTION_NAME = \"netmiko\"\n\n# create task function\ndef task(nornir_task_object, **kwargs):\n    pass\n</code></pre> Note <p>The <code>CONNECTION_NAME</code> must be defined within the custom task function file if RetryRunner is in use. Otherwise, the connection retry logic is skipped, and connections to all hosts are initiated simultaneously up to the number of <code>num_workers</code>.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>plugin</code> <code>str</code> <p>The path to the plugin function to import, or a NorFab URL to download a custom task or template URL that resolves to a file.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the specified task plugin.</p> <code>{}</code> Notes <ul> <li><code>add_details</code> (bool): If True, adds task execution details to the results.</li> <li><code>to_dict</code> (bool): If True, returns results as a dictionary. Defaults to True.</li> <li>Host filters: keys matching <code>FFun_functions</code> are treated as Nornir host filters.</li> </ul> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An instance of the Result class containing the task execution results.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified plugin file cannot be downloaded.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef task(self, job: Job, plugin: str, **kwargs: Any) -&gt; Result:\n    \"\"\"\n    Execute a Nornir task plugin.\n\n    This method dynamically imports and executes a specified Nornir task plugin,\n    using the provided arguments and keyword arguments. The `plugin` attribute\n    can refer to a file to fetch from a file service, which must contain a function\n    named `task` that accepts a Nornir task object as the first positional argument.\n\n    Example of a custom task function file:\n\n    ```python\n    # define connection name for RetryRunner to properly detect it\n    CONNECTION_NAME = \"netmiko\"\n\n    # create task function\n    def task(nornir_task_object, **kwargs):\n        pass\n    ```\n\n    Note:\n        The `CONNECTION_NAME` must be defined within the custom task function file if\n        RetryRunner is in use. Otherwise, the connection retry logic is skipped, and\n        connections to all hosts are initiated simultaneously up to the number of `num_workers`.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        plugin (str): The path to the plugin function to import, or a NorFab\n            URL to download a custom task or template URL that resolves to a file.\n        **kwargs (Any): Additional arguments to pass to the specified task plugin.\n\n    Notes:\n        - `add_details` (bool): If True, adds task execution details to the results.\n        - `to_dict` (bool): If True, returns results as a dictionary. Defaults to True.\n        - Host filters: keys matching `FFun_functions` are treated as Nornir host filters.\n\n    Returns:\n        Result: An instance of the Result class containing the task execution results.\n\n    Raises:\n        FileNotFoundError: If the specified plugin file cannot be downloaded.\n    \"\"\"\n    # extract attributes\n    add_details = kwargs.pop(\"add_details\", False)  # ResultSerializer\n    to_dict = kwargs.pop(\"to_dict\", True)  # ResultSerializer\n    ret = Result(task=f\"{self.name}:task\", result={} if to_dict else [])\n\n    filtered_nornir, no_match_result = self.filter_hosts_and_validate(kwargs, ret)\n    if ret.status == \"no_match\":\n        return ret\n\n    # download task from broker and load it\n    if plugin.startswith(\"nf://\"):\n        function_text = self.fetch_file(plugin)\n        if function_text is None:\n            raise FileNotFoundError(\n                f\"{self.name} - '{plugin}' task plugin download failed\"\n            )\n\n        # load task function running exec\n        globals_dict = {}\n        exec(function_text, globals_dict, globals_dict)\n        task_function = globals_dict[\"task\"]\n    # import task function\n    elif \".\" in plugin:\n        # below same as \"from nornir.plugins.tasks import task_fun as task_function\"\n        module_name, func_name = plugin.rsplit(\".\", 1)\n        module = __import__(module_name, fromlist=[func_name])  # nosec\n        task_function = getattr(module, func_name)\n    else:\n        raise RuntimeError(\n            f\"{self.name} - '{plugin}' task should either be a path \"\n            f\"to a file or a module import string\"\n        )\n\n    nr = self._add_processors(filtered_nornir, kwargs, job)  # add processors\n\n    # run task\n    log.debug(f\"{self.name} - running Nornir task '{plugin}', kwargs '{kwargs}'\")\n    with self.connections_lock:\n        result = nr.run(task=task_function, **kwargs)\n\n    ret.failed = result.failed  # failed is true if any of the hosts failed\n    ret.result = ResultSerializer(result, to_dict=to_dict, add_details=add_details)\n\n    self.watchdog.connections_clean()\n\n    return ret\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.cli","title":"<code>cli(job: Job, commands: Union[str, list] = None, plugin: str = 'netmiko', dry_run: bool = False, run_ttp: str = None, job_data: Any = None, to_dict: bool = True, add_details: bool = False, **kwargs: Any) -&gt; Result</code>","text":"<p>Task to collect/retrieve show commands output from network devices using Command Line Interface (CLI).</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>commands</code> <code>list</code> <p>List of commands to send to devices or URL to a file or template URL that resolves to a file.</p> <code>None</code> <code>plugin</code> <code>str</code> <p>Plugin name to use. Valid options are <code>netmiko</code>, <code>scrapli</code>, <code>napalm</code>.</p> <code>'netmiko'</code> <code>dry_run</code> <code>bool</code> <p>If True, do not send commands to devices, just return them.</p> <code>False</code> <code>run_ttp</code> <code>str</code> <p>TTP Template to run.</p> <code>None</code> <code>job_data</code> <code>str</code> <p>URL to YAML file with data or dictionary/list of data to pass on to Jinja2 rendering context.</p> <code>None</code> <code>to_dict</code> <code>bool</code> <p>If True, returns results as a dictionary.</p> <code>True</code> <code>add_details</code> <code>bool</code> <p>If True, adds task execution details to the results.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the specified task plugin.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary with the results of the CLI task.</p> <p>Raises:</p> Type Description <code>UnsupportedPluginError</code> <p>If the specified plugin is not supported.</p> <code>FileNotFoundError</code> <p>If the specified TTP template or job data file cannot be downloaded.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"POST\"]},\n    input=NorniCliInput,\n)\ndef cli(\n    self,\n    job: Job,\n    commands: Union[str, list] = None,\n    plugin: str = \"netmiko\",\n    dry_run: bool = False,\n    run_ttp: str = None,\n    job_data: Any = None,\n    to_dict: bool = True,\n    add_details: bool = False,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Task to collect/retrieve show commands output from network devices using\n    Command Line Interface (CLI).\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        commands (list, optional): List of commands to send to devices or URL to a file or template\n            URL that resolves to a file.\n        plugin (str, optional): Plugin name to use. Valid options are\n            ``netmiko``, ``scrapli``, ``napalm``.\n        dry_run (bool, optional): If True, do not send commands to devices,\n            just return them.\n        run_ttp (str, optional): TTP Template to run.\n        job_data (str, optional): URL to YAML file with data or dictionary/list\n            of data to pass on to Jinja2 rendering context.\n        to_dict (bool, optional): If True, returns results as a dictionary.\n        add_details (bool, optional): If True, adds task execution details\n            to the results.\n        **kwargs: Additional arguments to pass to the specified task plugin.\n\n    Returns:\n        dict: A dictionary with the results of the CLI task.\n\n    Raises:\n        UnsupportedPluginError: If the specified plugin is not supported.\n        FileNotFoundError: If the specified TTP template or job data file\n            cannot be downloaded.\n    \"\"\"\n    job_data = job_data or {}\n    timeout = job.timeout * 0.9\n    ret = Result(task=f\"{self.name}:cli\", result={} if to_dict else [])\n\n    filtered_nornir, no_match_result = self.filter_hosts_and_validate(kwargs, ret)\n    if ret.status == \"no_match\":\n        return ret\n\n    # decide on what send commands task plugin to use\n    if plugin == \"netmiko\":\n        task_plugin = netmiko_send_commands\n        if kwargs.get(\"use_ps\"):\n            kwargs.setdefault(\"timeout\", timeout)\n        else:\n            kwargs.setdefault(\"read_timeout\", timeout)\n    elif plugin == \"scrapli\":\n        task_plugin = scrapli_send_commands\n        kwargs.setdefault(\"timeout_ops\", timeout)\n    elif plugin == \"napalm\":\n        task_plugin = napalm_send_commands\n    else:\n        raise UnsupportedPluginError(f\"Plugin '{plugin}' not supported\")\n\n    # download TTP template\n    if self.is_url(run_ttp):\n        downloaded = self.fetch_file(run_ttp)\n        kwargs[\"run_ttp\"] = downloaded\n        if downloaded is None:\n            msg = f\"{self.name} - TTP template download failed '{run_ttp}'\"\n            raise FileNotFoundError(msg)\n    # use TTP template as is - inline template or ttp://xyz path\n    elif run_ttp:\n        kwargs[\"run_ttp\"] = run_ttp\n\n    # download job data\n    job_data = self.load_job_data(job_data)\n\n    nr = self._add_processors(filtered_nornir, kwargs, job)  # add processors\n\n    # render commands using Jinja2 on a per-host basis\n    if commands:\n        commands = commands if isinstance(commands, list) else [commands]\n        for host in nr.inventory.hosts.values():\n            rendered = self.jinja2_render_templates(\n                templates=commands,\n                context={\n                    \"host\": host,\n                    \"norfab\": self.client,\n                    \"job_data\": job_data,\n                    \"netbox\": self.add_jinja2_netbox(),\n                },\n                filters=self.add_jinja2_filters(),\n            )\n            host.data[\"__task__\"] = {\"commands\": rendered}\n\n    # run task\n    log.debug(\n        f\"{self.name} - running cli commands '{commands}', kwargs '{kwargs}', is cli dry run - '{dry_run}'\"\n    )\n    if dry_run is True:\n        result = nr.run(\n            task=nr_test, use_task_data=\"commands\", name=\"dry_run\", **kwargs\n        )\n    else:\n        with self.connections_lock:\n            result = nr.run(task=task_plugin, **kwargs)\n\n    ret.failed = result.failed  # failed is true if any of the hosts failed\n    ret.result = ResultSerializer(result, to_dict=to_dict, add_details=add_details)\n\n    # remove __task__ data\n    for host_name, host_object in nr.inventory.hosts.items():\n        _ = host_object.data.pop(\"__task__\", None)\n\n    self.watchdog.connections_update(nr, plugin)\n    self.watchdog.connections_clean()\n\n    return ret\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.cfg","title":"<code>cfg(job: Job, config: Union[str, list], plugin: str = 'netmiko', dry_run: bool = False, to_dict: bool = True, add_details: bool = False, job_data: Any = None, **kwargs: Any) -&gt; Result</code>","text":"<p>Task to send configuration commands to devices using Command Line Interface (CLI).</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>config</code> <code>list</code> <p>List of commands to send to devices or URL to a file or template URL that resolves to a file.</p> required <code>plugin</code> <code>str</code> <p>Plugin name to use. Valid options are:</p> <ul> <li>netmiko - use Netmiko to configure devices</li> <li>scrapli - use Scrapli to configure devices</li> <li>napalm - use NAPALM to configure devices</li> </ul> <code>'netmiko'</code> <code>dry_run</code> <code>bool</code> <p>If True, will not send commands to devices but just return them.</p> <code>False</code> <code>to_dict</code> <code>bool</code> <p>If True, returns results as a dictionary. Defaults to True.</p> <code>True</code> <code>add_details</code> <code>bool</code> <p>If True, adds task execution details to the results.</p> <code>False</code> <code>job_data</code> <code>str</code> <p>URL to YAML file with data or dictionary/list of data to pass on to Jinja2 rendering context.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the task plugin.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary with the results of the configuration task.</p> <p>Raises:</p> Type Description <code>UnsupportedPluginError</code> <p>If the specified plugin is not supported.</p> <code>FileNotFoundError</code> <p>If the specified job data file cannot be downloaded.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"POST\"]},\n)\ndef cfg(\n    self,\n    job: Job,\n    config: Union[str, list],\n    plugin: str = \"netmiko\",\n    dry_run: bool = False,\n    to_dict: bool = True,\n    add_details: bool = False,\n    job_data: Any = None,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Task to send configuration commands to devices using Command Line Interface (CLI).\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        config (list): List of commands to send to devices or URL to a file or template\n            URL that resolves to a file.\n        plugin (str, optional): Plugin name to use. Valid options are:\n\n            - netmiko - use Netmiko to configure devices\n            - scrapli - use Scrapli to configure devices\n            - napalm - use NAPALM to configure devices\n\n        dry_run (bool, optional): If True, will not send commands to devices but just return them.\n        to_dict (bool, optional): If True, returns results as a dictionary. Defaults to True.\n        add_details (bool, optional): If True, adds task execution details to the results.\n        job_data (str, optional): URL to YAML file with data or dictionary/list of data to pass on to Jinja2 rendering context.\n        **kwargs: Additional arguments to pass to the task plugin.\n\n    Returns:\n        dict: A dictionary with the results of the configuration task.\n\n    Raises:\n        UnsupportedPluginError: If the specified plugin is not supported.\n        FileNotFoundError: If the specified job data file cannot be downloaded.\n    \"\"\"\n    config = config if isinstance(config, list) else [config]\n    ret = Result(task=f\"{self.name}:cfg\", result={} if to_dict else [])\n\n    filtered_nornir, no_match_result = self.filter_hosts_and_validate(kwargs, ret)\n    if ret.status == \"no_match\":\n        return ret\n\n    # decide on what send commands task plugin to use\n    if plugin == \"netmiko\":\n        task_plugin = netmiko_send_config\n    elif plugin == \"scrapli\":\n        task_plugin = scrapli_send_config\n    elif plugin == \"napalm\":\n        task_plugin = napalm_configure\n    else:\n        raise UnsupportedPluginError(f\"Plugin '{plugin}' not supported\")\n\n    job_data = self.load_job_data(job_data)\n\n    nr = self._add_processors(filtered_nornir, kwargs, job)  # add processors\n\n    # render config using Jinja2 on a per-host basis\n    for host in nr.inventory.hosts.values():\n        rendered = self.jinja2_render_templates(\n            templates=config,\n            context={\n                \"host\": host,\n                \"norfab\": self.client,\n                \"job_data\": job_data,\n                \"netbox\": self.add_jinja2_netbox(),\n            },\n            filters=self.add_jinja2_filters(),\n        )\n        host.data[\"__task__\"] = {\"config\": rendered}\n\n    # run task\n    log.debug(\n        f\"{self.name} - sending config commands '{config}', kwargs '{kwargs}', is dry_run - '{dry_run}'\"\n    )\n    if dry_run is True:\n        result = nr.run(\n            task=nr_test, use_task_data=\"config\", name=\"dry_run\", **kwargs\n        )\n    else:\n        with self.connections_lock:\n            result = nr.run(task=task_plugin, **kwargs)\n\n    ret.failed = result.failed  # failed is true if any of the hosts failed\n    ret.result = ResultSerializer(result, to_dict=to_dict, add_details=add_details)\n\n    # remove __task__ data\n    for host_name, host_object in nr.inventory.hosts.items():\n        _ = host_object.data.pop(\"__task__\", None)\n\n    self.watchdog.connections_update(nr, plugin)\n    self.watchdog.connections_clean()\n\n    return ret\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.test","title":"<code>test(job: Job, suite: Union[list, str], subset: str = None, dry_run: bool = False, remove_tasks: bool = True, failed_only: bool = False, return_tests_suite: bool = False, job_data: Any = None, extensive: bool = False, **kwargs: Any) -&gt; Result</code>","text":"<p>Function to test networks using a suite of tests.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>suite</code> <code>Union[list, str]</code> <p>URL Path to YAML file with tests or a list of test definitions or template URL that resolves to a file path.</p> required <code>subset</code> <code>str</code> <p>List or string with comma-separated non-case-sensitive glob patterns to filter tests by name. Ignored if dry_run is True.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, returns produced per-host tests suite content only.</p> <code>False</code> <code>remove_tasks</code> <code>bool</code> <p>If False, results will include other tasks output.</p> <code>True</code> <code>failed_only</code> <code>bool</code> <p>If True, returns test results for failed tests only.</p> <code>False</code> <code>return_tests_suite</code> <code>bool</code> <p>If True, returns rendered per-host tests suite content in addition to test results using a dictionary with <code>results</code> and <code>suite</code> keys.</p> <code>False</code> <code>job_data</code> <code>str</code> <p>URL to YAML file with data or dictionary/list of data to pass on to Jinja2 rendering context.</p> <code>None</code> <code>extensive</code> <code>bool</code> <p>return extensive results, equivalent to using these arguments:</p> <ul> <li>remove_tasks = False</li> <li>return_tests_suite = True</li> <li>add_details = True</li> <li>to_dict = False</li> </ul> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Any additional arguments to pass on to the Nornir service task.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the test results. If <code>return_tests_suite</code> is True, the dictionary will contain both the test results and the rendered test suite.</p> Note <p>Result <code>failed</code> attribute is set to True if any of the tests failed for any of the hosts.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error in rendering the Jinja2 templates or loading the YAML.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef test(\n    self,\n    job: Job,\n    suite: Union[list, str],\n    subset: str = None,\n    dry_run: bool = False,\n    remove_tasks: bool = True,\n    failed_only: bool = False,\n    return_tests_suite: bool = False,\n    job_data: Any = None,\n    extensive: bool = False,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Function to test networks using a suite of tests.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        suite (Union[list, str]): URL Path to YAML file with tests or a list of test definitions\n            or template URL that resolves to a file path.\n        subset (str, optional): List or string with comma-separated non-case-sensitive glob\n            patterns to filter tests by name. Ignored if dry_run is True.\n        dry_run (bool, optional): If True, returns produced per-host tests suite content only.\n        remove_tasks (bool, optional): If False, results will include other tasks output.\n        failed_only (bool, optional): If True, returns test results for failed tests only.\n        return_tests_suite (bool, optional): If True, returns rendered per-host tests suite\n            content in addition to test results using a dictionary with ``results`` and ``suite`` keys.\n        job_data (str, optional): URL to YAML file with data or dictionary/list of data\n            to pass on to Jinja2 rendering context.\n        extensive (bool, optional): return extensive results, equivalent to using these arguments:\n\n            - remove_tasks = False\n            - return_tests_suite = True\n            - add_details = True\n            - to_dict = False\n\n        **kwargs: Any additional arguments to pass on to the Nornir service task.\n\n    Returns:\n        dict: A dictionary containing the test results. If `return_tests_suite` is True,\n            the dictionary will contain both the test results and the rendered test suite.\n\n    Note:\n        Result `failed` attribute is set to True if any of the tests failed for any of the hosts.\n\n    Raises:\n        RuntimeError: If there is an error in rendering the Jinja2 templates or loading the YAML.\n    \"\"\"\n    tests = {}  # dictionary to hold per-host test suites\n    # set extensive details flags\n    if extensive is True:\n        kwargs[\"add_details\"] = True\n        kwargs[\"to_dict\"] = False\n        remove_tasks = False\n        return_tests_suite = True\n    add_details = kwargs.get(\"add_details\", False)  # ResultSerializer\n    to_dict = kwargs.get(\"to_dict\", True)  # ResultSerializer\n    filters = {k: kwargs.get(k) for k in list(kwargs.keys()) if k in FFun_functions}\n    ret = Result(task=f\"{self.name}:test\", result={} if to_dict else [])\n    suites = {}  # dictionary to hold combined test suites\n\n    filtered_nornir, ret = self.filter_hosts_and_validate(kwargs, ret)\n    if ret.status == \"no_match\":\n        if return_tests_suite is True:\n            ret.result = {\"test_results\": [], \"suite\": {}}\n        return ret\n\n    # download job data\n    job_data = self.load_job_data(job_data)\n\n    # generate per-host test suites\n    for host_name, host in filtered_nornir.inventory.hosts.items():\n        # render suite using Jinja2\n        try:\n            rendered_suite = self.jinja2_render_templates(\n                templates=[suite],\n                context={\n                    \"host\": host,\n                    \"norfab\": self.client,\n                    \"job_data\": job_data,\n                    \"netbox\": self.add_jinja2_netbox(),\n                },\n                filters=self.add_jinja2_filters(),\n            )\n        except Exception as e:\n            msg = f\"{self.name} - '{suite}' Jinja2 rendering failed: '{type(e).__name__}:{e}'\"\n            raise RuntimeError(msg) from e\n        # load suit using YAML\n        try:\n            tests[host_name] = yaml.safe_load(rendered_suite) or []\n        except Exception as e:\n            msg = f\"{self.name} - '{suite}' YAML load failed: '{type(e).__name__}:{e}'\"\n            raise RuntimeError(msg) from e\n\n    # validate tests suite\n    try:\n        _ = modelTestsProcessorSuite(tests=tests)\n    except Exception as e:\n        msg = f\"{self.name} - '{suite}' suite validation failed: '{type(e).__name__}:{e}'\"\n        raise RuntimeError(msg) from e\n\n    # download pattern, schema and custom function files\n    for host_name in tests.keys():\n        for index, item in enumerate(tests[host_name]):\n            for k in [\"pattern\", \"schema\", \"function_file\"]:\n                if self.is_url(item.get(k)):\n                    item[k] = self.fetch_file(\n                        item[k], raise_on_fail=True, read=True\n                    )\n                    if k == \"function_file\":\n                        item[\"function_text\"] = item.pop(k)\n            tests[host_name][index] = item\n\n    # save per-host tests suite content before mutating it\n    if return_tests_suite is True:\n        return_suite = copy.deepcopy(tests)\n\n    log.debug(f\"{self.name} - running test '{suite}', is dry run - '{dry_run}'\")\n    # run dry run task\n    if dry_run is True:\n        result = filtered_nornir.run(\n            task=nr_test, name=\"tests_dry_run\", ret_data_per_host=tests\n        )\n        ret.result = ResultSerializer(\n            result, to_dict=to_dict, add_details=add_details\n        )\n    # combine per-host tests in suites based on task and arguments\n    # Why - to run tests using any nornir service tasks with various arguments\n    else:\n        for host_name, host_tests in tests.items():\n            for test in host_tests:\n                dhash = hashlib.md5()\n                test_args = test.pop(\"norfab\", {})\n                nrtask = test_args.get(\"nrtask\", \"cli\")\n                assert nrtask in [\n                    \"cli\",\n                    \"network\",\n                    \"cfg\",\n                    \"task\",\n                ], f\"{self.name} - unsupported NorFab Nornir Service task '{nrtask}'\"\n                test_json = json.dumps(test_args, sort_keys=True).encode()\n                dhash.update(test_json)\n                test_hash = dhash.hexdigest()\n                suites.setdefault(test_hash, {\"params\": test_args, \"tests\": {}})\n                suites[test_hash][\"tests\"].setdefault(host_name, [])\n                suites[test_hash][\"tests\"][host_name].append(test)\n        log.debug(\n            f\"{self.name} - combined per-host tests suites based on NorFab Nornir Service task and arguments:\\n{suites}\"\n        )\n        # run test suites collecting output from devices\n        for tests_suite in suites.values():\n            nrtask = tests_suite[\"params\"].pop(\"nrtask\", \"cli\")\n            function_kwargs = {\n                **tests_suite[\"params\"],\n                **kwargs,\n                **filters,\n                \"tests\": tests_suite[\"tests\"],\n                \"remove_tasks\": remove_tasks,\n                \"failed_only\": failed_only,\n                \"subset\": subset,\n            }\n            result = getattr(self, nrtask)(\n                job=job, **function_kwargs\n            )  # returns Result object\n            # save test results into overall results\n            if to_dict == True:\n                for host_name, host_res in result.result.items():\n                    ret.result.setdefault(host_name, {})\n                    ret.result[host_name].update(host_res)\n                    # set return result failed to true if any of the tests failed\n                    for test_res in host_res.values():\n                        if add_details:\n                            if test_res[\"result\"] != \"PASS\":\n                                ret.failed = True\n                        elif test_res != \"PASS\":\n                            ret.failed = True\n            else:\n                ret.result.extend(result.result)\n                # set return result failed to true if any of the tests failed\n                if any(r[\"result\"] != \"PASS\" for r in result.result):\n                    ret.failed = True\n\n    # check if need to return tests suite content\n    if return_tests_suite is True:\n        ret.result = {\"test_results\": ret.result, \"suite\": return_suite}\n\n    return ret\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.network","title":"<code>network(job: Job, fun: str, **kwargs) -&gt; Result</code>","text":"<p>Task to call various network-related utility functions.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>fun</code> <code>str</code> <p>The name of the utility function to call.</p> required <code>kwargs</code> <code>dict</code> <p>Arguments to pass to the utility function.</p> <code>{}</code> <p>Available utility functions:</p> <ul> <li>resolve_dns Resolves hosts' hostname DNS, returning IP addresses using     <code>nornir_salt.plugins.tasks.network.resolve_dns</code> Nornir-Salt function.</li> <li>ping Executes ICMP ping to host using <code>nornir_salt.plugins.tasks.network.ping</code>     Nornir-Salt function.</li> </ul> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the results of the network utility function.</p> <p>Raises:</p> Type Description <code>UnsupportedPluginError</code> <p>If the specified utility function is not supported.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef network(self, job: Job, fun: str, **kwargs) -&gt; Result:\n    \"\"\"\n    Task to call various network-related utility functions.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        fun (str): The name of the utility function to call.\n        kwargs (dict): Arguments to pass to the utility function.\n\n    Available utility functions:\n\n    - **resolve_dns** Resolves hosts' hostname DNS, returning IP addresses using\n        `nornir_salt.plugins.tasks.network.resolve_dns` Nornir-Salt function.\n    - **ping** Executes ICMP ping to host using `nornir_salt.plugins.tasks.network.ping`\n        Nornir-Salt function.\n\n    Returns:\n        dict: A dictionary containing the results of the network utility function.\n\n    Raises:\n        UnsupportedPluginError: If the specified utility function is not supported.\n    \"\"\"\n    kwargs[\"call\"] = fun\n    return self.task(\n        job=job,\n        plugin=\"nornir_salt.plugins.tasks.network\",\n        **kwargs,\n    )\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.parse","title":"<code>parse(job: Job, plugin: str = 'napalm', getters: Union[str, list] = 'get_facts', template: str = None, commands: Union[str, list] = None, to_dict: bool = True, add_details: bool = False, **kwargs: Any) -&gt; Result</code>","text":"<p>Parse network device output using specified plugin and options.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>plugin</code> <code>str</code> <p>The plugin to use for parsing. Options are:</p> <ul> <li>napalm - parse devices output using NAPALM getters</li> <li>ttp - use TTP Templates to parse devices output</li> <li>textfsm - use TextFSM templates to parse devices output</li> </ul> <code>'napalm'</code> <code>getters</code> <code>str</code> <p>The getters to use with the \"napalm\" plugin.</p> <code>'get_facts'</code> <code>template</code> <code>str</code> <p>The template to use with the \"ttp\" or \"textfsm\" plugin.</p> <code>None</code> <code>commands</code> <code>list</code> <p>The list of commands to run with the \"ttp\" or \"textfsm\" plugin.</p> <code>None</code> <code>to_dict</code> <code>bool</code> <p>Whether to convert the result to a dictionary.</p> <code>True</code> <code>add_details</code> <code>bool</code> <p>Whether to add details to the result.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the plugin.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>A Result object containing the parsed data.</p> <p>Raises:</p> Type Description <code>UnsupportedPluginError</code> <p>If the specified plugin is not supported.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef parse(\n    self,\n    job: Job,\n    plugin: str = \"napalm\",\n    getters: Union[str, list] = \"get_facts\",\n    template: str = None,\n    commands: Union[str, list] = None,\n    to_dict: bool = True,\n    add_details: bool = False,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Parse network device output using specified plugin and options.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        plugin (str): The plugin to use for parsing. Options are:\n\n            - napalm - parse devices output using NAPALM getters\n            - ttp - use TTP Templates to parse devices output\n            - textfsm - use TextFSM templates to parse devices output\n\n        getters (str): The getters to use with the \"napalm\" plugin.\n        template (str): The template to use with the \"ttp\" or \"textfsm\" plugin.\n        commands (list): The list of commands to run with the \"ttp\" or \"textfsm\" plugin.\n        to_dict (bool): Whether to convert the result to a dictionary.\n        add_details (bool): Whether to add details to the result.\n        **kwargs: Additional keyword arguments to pass to the plugin.\n\n    Returns:\n        Result: A Result object containing the parsed data.\n\n    Raises:\n        UnsupportedPluginError: If the specified plugin is not supported.\n    \"\"\"\n    filters = {k: kwargs.get(k) for k in list(kwargs.keys()) if k in FFun_functions}\n    ret = Result(task=f\"{self.name}:parse\", result={} if to_dict else [])\n\n    filtered_nornir, ret = self.filter_hosts_and_validate(kwargs, ret)\n    if ret.status == \"no_match\":\n        return ret\n\n    if plugin == \"napalm\":\n        nr = self._add_processors(filtered_nornir, kwargs, job)  # add processors\n        result = nr.run(task=napalm_get, getters=getters, **kwargs)\n        ret.result = ResultSerializer(\n            result, to_dict=to_dict, add_details=add_details\n        )\n        ret.failed = result.failed  # failed is true if any of the hosts failed\n    elif plugin == \"ttp\":\n        result = self.cli(\n            job=job,\n            commands=commands or [],\n            run_ttp=template,\n            **filters,\n            **kwargs,\n            to_dict=to_dict,\n            add_details=add_details,\n            plugin=\"netmiko\",\n        )\n        ret.result = result.result\n    elif plugin == \"textfsm\":\n        result = self.cli(\n            job=job,\n            commands=commands,\n            **filters,\n            **kwargs,\n            to_dict=to_dict,\n            add_details=add_details,\n            use_textfsm=True,\n            textfsm_template=template,\n            plugin=\"netmiko\",\n        )\n        ret.result = result.result\n    else:\n        raise UnsupportedPluginError(f\"Plugin '{plugin}' not supported\")\n\n    return ret\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.file_copy","title":"<code>file_copy(job: Job, source_file: str, plugin: str = 'netmiko', to_dict: bool = True, add_details: bool = False, dry_run: bool = False, **kwargs: Any) -&gt; Result</code>","text":"<p>Task to transfer files to and from hosts using SCP.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>source_file</code> <code>str</code> <p>The path or URL of the source file to be copied in <code>nf://path/to/file</code> format</p> required <code>plugin</code> <code>str</code> <p>The plugin to use for file transfer. Supported plugins:</p> <ul> <li>netmiko - uses <code>netmiko_file_transfer</code> task plugin.</li> </ul> <code>'netmiko'</code> <code>to_dict</code> <code>bool</code> <p>Whether to return the result as a dictionary. Defaults to True.</p> <code>True</code> <code>add_details</code> <code>bool</code> <p>Whether to add detailed information to the result. Defaults to False.</p> <code>False</code> <code>dry_run</code> <code>bool</code> <p>If True, performs a dry run without making any changes. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the file transfer plugin.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>The result of the file copy operation.</p> <p>Raises:</p> Type Description <code>UnsupportedPluginError</code> <p>If the specified plugin is not supported.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef file_copy(\n    self,\n    job: Job,\n    source_file: str,\n    plugin: str = \"netmiko\",\n    to_dict: bool = True,\n    add_details: bool = False,\n    dry_run: bool = False,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Task to transfer files to and from hosts using SCP.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        source_file (str): The path or URL of the source file to be copied in\n            ``nf://path/to/file`` format\n        plugin (str, optional): The plugin to use for file transfer. Supported plugins:\n\n            - netmiko - uses `netmiko_file_transfer` task plugin.\n\n        to_dict (bool, optional): Whether to return the result as a dictionary. Defaults to True.\n        add_details (bool, optional): Whether to add detailed information to the result. Defaults to False.\n        dry_run (bool, optional): If True, performs a dry run without making any changes. Defaults to False.\n        **kwargs: Additional arguments to pass to the file transfer plugin.\n\n    Returns:\n        dict: The result of the file copy operation.\n\n    Raises:\n        UnsupportedPluginError: If the specified plugin is not supported.\n    \"\"\"\n    timeout = job.timeout * 0.9\n    ret = Result(task=f\"{self.name}:file_copy\", result={} if to_dict else [])\n\n    filtered_nornir, ret = self.filter_hosts_and_validate(kwargs, ret)\n    if ret.status == \"no_match\":\n        return ret\n\n    # download file from broker\n    if self.is_url(source_file):\n        source_file_local = self.fetch_file(\n            source_file, raise_on_fail=True, read=False\n        )\n\n    # decide on what send commands task plugin to use\n    if plugin == \"netmiko\":\n        task_plugin = netmiko_file_transfer\n        kwargs[\"source_file\"] = source_file_local\n        kwargs.setdefault(\"socket_timeout\", timeout / 5)\n        kwargs.setdefault(\"dest_file\", os.path.split(source_file_local)[-1])\n    else:\n        raise UnsupportedPluginError(f\"Plugin '{plugin}' not supported\")\n\n    nr = self._add_processors(filtered_nornir, kwargs, job)  # add processors\n\n    # run task\n    log.debug(\n        f\"{self.name} - running file copy with arguments '{kwargs}', is dry run - '{dry_run}'\"\n    )\n    if dry_run is True:\n        result = nr.run(task=nr_test, name=\"file_copy_dry_run\", **kwargs)\n    else:\n        with self.connections_lock:\n            result = nr.run(task=task_plugin, **kwargs)\n\n    ret.failed = result.failed  # failed is true if any of the hosts failed\n    ret.result = ResultSerializer(result, to_dict=to_dict, add_details=add_details)\n\n    self.watchdog.connections_update(nr, plugin)\n    self.watchdog.connections_clean()\n\n    return ret\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.runtime_inventory","title":"<code>runtime_inventory(job: Job, action: str, **kwargs: Any) -&gt; Result</code>","text":"<p>Task to work with Nornir runtime (in-memory) inventory.</p> <p>Supported actions:</p> <ul> <li><code>create_host</code> or <code>create</code> - creates new host or replaces existing host object</li> <li><code>read_host</code> or <code>read</code> - read host inventory content</li> <li><code>update_host</code> or <code>update</code> - non recursively update host attributes if host exists     in Nornir inventory, do not create host if it does not exist</li> <li><code>delete_host</code> or <code>delete</code> - deletes host object from Nornir Inventory</li> <li><code>load</code> - to simplify calling multiple functions</li> <li><code>read_inventory</code> - read inventory content for groups, default and hosts</li> <li><code>read_host_data</code> - to return host's data under provided path keys</li> <li><code>list_hosts</code> - return a list of inventory's host names</li> <li><code>list_hosts_platforms</code> - return a dictionary of hosts' platforms</li> <li><code>update_defaults</code> - non recursively update defaults attributes</li> </ul> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>action</code> <code>str</code> <p>action to perform on inventory</p> required <code>kwargs</code> <code>Any</code> <p>arguments to use with the calling action</p> <code>{}</code> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef runtime_inventory(self, job: Job, action: str, **kwargs: Any) -&gt; Result:\n    \"\"\"\n    Task to work with Nornir runtime (in-memory) inventory.\n\n    Supported actions:\n\n    - `create_host` or `create` - creates new host or replaces existing host object\n    - `read_host` or `read` - read host inventory content\n    - `update_host` or `update` - non recursively update host attributes if host exists\n        in Nornir inventory, do not create host if it does not exist\n    - `delete_host` or `delete` - deletes host object from Nornir Inventory\n    - `load` - to simplify calling multiple functions\n    - `read_inventory` - read inventory content for groups, default and hosts\n    - `read_host_data` - to return host's data under provided path keys\n    - `list_hosts` - return a list of inventory's host names\n    - `list_hosts_platforms` - return a dictionary of hosts' platforms\n    - `update_defaults` - non recursively update defaults attributes\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        action: action to perform on inventory\n        kwargs: arguments to use with the calling action\n    \"\"\"\n    # clean up kwargs\n    _ = kwargs.pop(\"progress\", None)\n    job.event(f\"Performing '{action}' action\")\n    return Result(result=InventoryFun(self.nr, call=action, **kwargs))\n</code></pre>"},{"location":"workers/nornir/api_reference_workers_nornir_worker/#norfab.workers.nornir_worker.NornirWorker.netconf","title":"<code>netconf(job: Job, call: str, plugin: str = 'ncclient', data: str = None, **kwargs: Any) -&gt; Result</code>","text":"<p>Interact with devices using NETCONF protocol utilizing one of the supported plugins.</p> <p>This method provides a unified interface to interact with network devices using NETCONF protocol through different backend plugins.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata.</p> required <code>call</code> <code>str</code> <p>The ncclient manager or scrapli netconf object method to call.</p> required <code>plugin</code> <code>str</code> <p>Name of netconf plugin to use. Available plugins:</p> <ul> <li><code>ncclient</code> (default): <code>nornir-salt</code> built-in plugin that uses <code>ncclient</code>   library to interact with devices. Uses <code>ncclient_call</code>_ task plugin.</li> <li><code>scrapli</code>: Uses <code>scrapli_netconf</code> connection plugin that is part of   <code>nornir_scrapli</code> library. Uses <code>scrapli_netconf_call</code>_ task plugin.</li> </ul> <code>'ncclient'</code> <code>data</code> <code>str</code> <p>Path to file for <code>rpc</code> method call or rpc content.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the underlying plugin.</p> <ul> <li>method_name (str): Name of method to provide docstring for, used only by <code>help</code> call.</li> </ul> <code>{}</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>A Result object containing the NETCONF operation results.</p> Note <p>Special <code>call</code> arguments/methods available:</p> <ul> <li><code>dir</code>: Returns methods supported by Ncclient connection manager object.</li> <li><code>help</code>: Returns <code>method_name</code> docstring.</li> <li> <p><code>transaction</code>: Same as <code>edit_config</code>, but runs a more reliable workflow:</p> <ol> <li>Lock target configuration datastore</li> <li>If server supports it - Discard previous changes if any</li> <li>Perform configuration edit using RPC specified in <code>edit_rpc</code> argument</li> <li>If server supports it - validate configuration if <code>validate</code> argument is True</li> <li>If server supports it - do commit confirmed if <code>confirmed</code> argument is True    using <code>confirm_delay</code> timer with <code>commit_arg</code> argument</li> <li>If confirmed commit requested, wait for <code>commit_final_delay</code> timer before    sending final commit, final commit does not use <code>commit_arg</code> arguments</li> <li>If server supports it - do commit operation</li> <li>Unlock target configuration datastore</li> <li>If server supports it - discard all changes if any of steps 3, 4, 5 or 7 fail</li> <li>Return results list of dictionaries keyed by step name</li> </ol> </li> </ul> Warning <p>Beware of differences in keywords required by different plugins, e.g. <code>filter</code> for <code>ncclient</code> vs <code>filter_</code>/<code>filters</code> for <code>scrapli_netconf</code>. Refer to modules' API documentation for required arguments.</p> <p>Examples:</p> <p>Examples using <code>ncclient</code> plugin::</p> <pre><code>salt nrp1 nr.nc server_capabilities FB=\"*\"\nsalt nrp1 nr.nc get_config filter='[\"subtree\", \"salt://rpc/get_config_data.xml\"]' source=\"running\"\nsalt nrp1 nr.nc edit_config target=\"running\" config=\"salt://rpc/edit_config_data.xml\" FB=\"ceos1\"\nsalt nrp1 nr.nc transaction target=\"candidate\" config=\"salt://rpc/edit_config_data.xml\"\nsalt nrp1 nr.nc commit\nsalt nrp1 nr.nc rpc data=\"salt://rpc/iosxe_rpc_edit_interface.xml\"\nsalt nrp1 nr.nc get_schema identifier=\"ietf-interfaces\"\nsalt nrp1 nr.nc get filter='&lt;system-time xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-shellutil-oper\"/&gt;'\n</code></pre> <p>Examples using <code>scrapli_netconf</code> plugin::</p> <pre><code>salt nrp1 nr.nc get filter_=salt://rpc/get_config_filter_ietf_interfaces.xml plugin=scrapli\nsalt nrp1 nr.nc get_config source=running plugin=scrapli\nsalt nrp1 nr.nc server_capabilities FB=\"*\" plugin=scrapli\nsalt nrp1 nr.nc rpc filter_=salt://rpc/get_config_rpc_ietf_interfaces.xml plugin=scrapli\nsalt nrp1 nr.nc transaction target=\"candidate\" config=\"salt://rpc/edit_config_ietf_interfaces.xml\" plugin=scrapli\n</code></pre> <p>Python API usage from Salt-Master::</p> <pre><code>import salt.client\nclient = salt.client.LocalClient()\n\ntask_result = client.cmd(\n    tgt=\"nrp1\",\n    fun=\"nr.nc\",\n    arg=[\"get_config\"],\n    kwarg={\"source\": \"running\", \"plugin\": \"ncclient\"},\n)\n</code></pre> <p>Using special calls::</p> <pre><code>salt nrp1 nr.nc dir\nsalt nrp1 nr.nc help method_name=edit_config\nsalt nrp1 nr.nc transaction target=\"candidate\" config=\"salt://path/to/config_file.xml\" FB=\"*core-1\"\n</code></pre> <p>.. _ncclient_call: https://nornir-salt.readthedocs.io/en/latest/Tasks/ncclient_call.html .. _scrapli_netconf_call: https://nornir-salt.readthedocs.io/en/latest/Tasks/scrapli_netconf_call.html</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef netconf(\n    self,\n    job: Job,\n    call: str,\n    plugin: str = \"ncclient\",\n    data: str = None,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Interact with devices using NETCONF protocol utilizing one of the supported plugins.\n\n    This method provides a unified interface to interact with network devices using\n    NETCONF protocol through different backend plugins.\n\n    Args:\n        job (Job): NorFab Job object containing relevant metadata.\n        call (str): The ncclient manager or scrapli netconf object method to call.\n        plugin (str, optional): Name of netconf plugin to use. Available plugins:\n\n            - ``ncclient`` (default): ``nornir-salt`` built-in plugin that uses ``ncclient``\n              library to interact with devices. Uses `ncclient_call`_ task plugin.\n            - ``scrapli``: Uses ``scrapli_netconf`` connection plugin that is part of\n              ``nornir_scrapli`` library. Uses `scrapli_netconf_call`_ task plugin.\n\n        data (str, optional): Path to file for ``rpc`` method call or rpc content.\n        **kwargs (Any): Additional arguments to pass to the underlying plugin.\n\n            - method_name (str): Name of method to provide docstring for, used only by ``help`` call.\n\n    Returns:\n        Result: A Result object containing the NETCONF operation results.\n\n    Note:\n        Special ``call`` arguments/methods available:\n\n        - ``dir``: Returns methods supported by Ncclient connection manager object.\n        - ``help``: Returns ``method_name`` docstring.\n        - ``transaction``: Same as ``edit_config``, but runs a more reliable workflow:\n\n            1. Lock target configuration datastore\n            2. If server supports it - Discard previous changes if any\n            3. Perform configuration edit using RPC specified in ``edit_rpc`` argument\n            4. If server supports it - validate configuration if ``validate`` argument is True\n            5. If server supports it - do commit confirmed if ``confirmed`` argument is True\n               using ``confirm_delay`` timer with ``commit_arg`` argument\n            6. If confirmed commit requested, wait for ``commit_final_delay`` timer before\n               sending final commit, final commit does not use ``commit_arg`` arguments\n            7. If server supports it - do commit operation\n            8. Unlock target configuration datastore\n            9. If server supports it - discard all changes if any of steps 3, 4, 5 or 7 fail\n            10. Return results list of dictionaries keyed by step name\n\n    Warning:\n        Beware of differences in keywords required by different plugins, e.g. ``filter`` for\n        ``ncclient`` vs ``filter_``/``filters`` for ``scrapli_netconf``. Refer to modules' API\n        documentation for required arguments.\n\n    Examples:\n        Examples using ``ncclient`` plugin::\n\n            salt nrp1 nr.nc server_capabilities FB=\"*\"\n            salt nrp1 nr.nc get_config filter='[\"subtree\", \"salt://rpc/get_config_data.xml\"]' source=\"running\"\n            salt nrp1 nr.nc edit_config target=\"running\" config=\"salt://rpc/edit_config_data.xml\" FB=\"ceos1\"\n            salt nrp1 nr.nc transaction target=\"candidate\" config=\"salt://rpc/edit_config_data.xml\"\n            salt nrp1 nr.nc commit\n            salt nrp1 nr.nc rpc data=\"salt://rpc/iosxe_rpc_edit_interface.xml\"\n            salt nrp1 nr.nc get_schema identifier=\"ietf-interfaces\"\n            salt nrp1 nr.nc get filter='&lt;system-time xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-shellutil-oper\"/&gt;'\n\n        Examples using ``scrapli_netconf`` plugin::\n\n            salt nrp1 nr.nc get filter_=salt://rpc/get_config_filter_ietf_interfaces.xml plugin=scrapli\n            salt nrp1 nr.nc get_config source=running plugin=scrapli\n            salt nrp1 nr.nc server_capabilities FB=\"*\" plugin=scrapli\n            salt nrp1 nr.nc rpc filter_=salt://rpc/get_config_rpc_ietf_interfaces.xml plugin=scrapli\n            salt nrp1 nr.nc transaction target=\"candidate\" config=\"salt://rpc/edit_config_ietf_interfaces.xml\" plugin=scrapli\n\n        Python API usage from Salt-Master::\n\n            import salt.client\n            client = salt.client.LocalClient()\n\n            task_result = client.cmd(\n                tgt=\"nrp1\",\n                fun=\"nr.nc\",\n                arg=[\"get_config\"],\n                kwarg={\"source\": \"running\", \"plugin\": \"ncclient\"},\n            )\n\n        Using special calls::\n\n            salt nrp1 nr.nc dir\n            salt nrp1 nr.nc help method_name=edit_config\n            salt nrp1 nr.nc transaction target=\"candidate\" config=\"salt://path/to/config_file.xml\" FB=\"*core-1\"\n\n    .. _ncclient_call: https://nornir-salt.readthedocs.io/en/latest/Tasks/ncclient_call.html\n    .. _scrapli_netconf_call: https://nornir-salt.readthedocs.io/en/latest/Tasks/scrapli_netconf_call.html\n    \"\"\"\n    # TODO implement files download and rendering\n    # kwargs.setdefault(\n    #     \"render\", [\"rpc\", \"config\", \"data\", \"filter\", \"filter_\", \"filters\"]\n    # )\n\n    # decide on plugin to use\n    if plugin.lower() == \"ncclient\":\n        task_fun = \"nornir_salt.plugins.tasks.ncclient_call\"\n        kwargs[\"connection_name\"] = \"ncclient\"\n        # TODO implement filters handling\n\n    elif plugin.lower() == \"scrapli\":\n        task_fun = \"nornir_salt.plugins.tasks.scrapli_netconf_call\"\n        kwargs[\"connection_name\"] = \"scrapli_netconf\"\n        # TODO implement filters handling\n\n    # run task\n    return self.task(job=job, plugin=task_fun, call=call, **kwargs)\n</code></pre>"},{"location":"workers/nornir/services_nornir_service/","title":"Nornir Service","text":"<p>Nornir Service is based on Nornir library - a well adopted open-source tool for automating network devices operations.</p> <p> </p> <p>With each Nornir worker capable of handling multiple devices simultaneously,  Nornir Service offers high scalability, allowing efficient management of  large device fleets. By optimizing compute resources such as CPU, RAM, and  storage, it delivers cost-effective performance.</p> <p>Additionally, Nornir Service supports various interfaces and libraries for  seamless integration. For instance, the <code>cli</code> task can interact with devices  via the SSH Command Line Interface (CLI) using popular libraries like Netmiko,  Scrapli or NAPALM, providing flexibility for diverse network environments.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service/#nornir-service-tasks","title":"Nornir Service Tasks","text":"<p>Nornir Service supports a number of tasks to interact with network devices using  some of the most popular open source libraries such as Netmiko, NAPALM, Scrapli,  Ncclient, Scrapli NETCONF, pygnmi, puresnmp, TextFSM, TTP etc.</p> Task Description Use Cases task Run Nornir custom tasks Pure Python per device workflows, do anything you want, it is pure python cli Executes CLI commands on network devices using libraries like Netmiko, Scrapli or NAPALM. Device diagnostics, retrieving device information. cfg Manages device configurations, including pushing configurations. Automated configuration management. test Run test suites against network devices. Network testing, troubleshooting, device compliance, configuration verification. network A collection of network utilities such as ping and DNS. Check device connectivity, verify and resolve DNS records. parse Parses command outputs using TextFSM, NAPALM getters or TTP to extract structured data. Data extraction from CLI outputs, automated report generation, configuration validation. diagram Produce Network L2,  L3, OSPF or ISIS routing diagrams in DrawIO or yED formats. Automated network documentation, network validation. file_copy Copy files to network devices over SCP. Device software upgrades, certificates or license renewal. runtime_inventory Modify Nornir service runtime inventory. Add, update or remove Nornir hosts at a runtime.","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service/#nornir-service-show-commands","title":"Nornir Service Show Commands","text":"<p>Nornir service shell comes with a set of show commands to query service details:</p> <pre><code>nf#man tree show.nornir\nroot\n\u2514\u2500\u2500 show:    NorFab show commands\n    \u2514\u2500\u2500 nornir:    Show Nornir service\n        \u251c\u2500\u2500 inventory:    show Nornir inventory data\n        \u2502   \u251c\u2500\u2500 timeout:    Job timeout\n        \u2502   \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u2502   \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n        \u2502   \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u2502   \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u2502   \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u2502   \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u2502   \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u2502   \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u2502   \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u2502   \u251c\u2500\u2500 FM:    Filter hosts by platform\n        \u2502   \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n        \u2502   \u251c\u2500\u2500 FN:    Negate the match\n        \u2502   \u2514\u2500\u2500 hosts:    Filter hosts to target\n        \u251c\u2500\u2500 hosts:    show Nornir hosts\n        \u2502   \u251c\u2500\u2500 table:    Table format (brief, terse, extend) or parameters or True\n        \u2502   \u251c\u2500\u2500 headers:    Table headers\n        \u2502   \u251c\u2500\u2500 headers-exclude:    Table headers to exclude\n        \u2502   \u251c\u2500\u2500 sortby:    Table header column to sort by\n        \u2502   \u251c\u2500\u2500 reverse:    Table reverse the sort by order\n        \u2502   \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n        \u2502   \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u2502   \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u2502   \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u2502   \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u2502   \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u2502   \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u2502   \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u2502   \u251c\u2500\u2500 FM:    Filter hosts by platform\n        \u2502   \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n        \u2502   \u251c\u2500\u2500 FN:    Negate the match\n        \u2502   \u251c\u2500\u2500 hosts:    Filter hosts to target\n        \u2502   \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u2502   \u2514\u2500\u2500 details:    show hosts details\n        \u251c\u2500\u2500 version:    show Nornir service version report\n        \u251c\u2500\u2500 watchdog:    show Nornir service version report\n        \u2502   \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n        \u2502   \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u2502   \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u2502   \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u2502   \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u2502   \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u2502   \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u2502   \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u2502   \u251c\u2500\u2500 FM:    Filter hosts by platform\n        \u2502   \u251c\u2500\u2500 FN:    Negate the match\n        \u2502   \u251c\u2500\u2500 hosts:    Filter hosts to target\n        \u2502   \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u2502   \u251c\u2500\u2500 statistics:    show Nornir watchdog statistics\n        \u2502   \u251c\u2500\u2500 configuration:    show Nornir watchdog configuration\n        \u2502   \u2514\u2500\u2500 connections:    show Nornir watchdog connections monitoring data\n        \u2514\u2500\u2500 jobs:    Show Nornir Jobs\n            \u251c\u2500\u2500 summary:    List jobs\n            \u2502   \u251c\u2500\u2500 timeout:    Job timeout\n            \u2502   \u251c\u2500\u2500 workers:    Workers to return jobs for, default 'all'\n            \u2502   \u251c\u2500\u2500 last:    Return last N completed and last N pending jobs\n            \u2502   \u251c\u2500\u2500 pending:    Return pending jobs, default 'True'\n            \u2502   \u251c\u2500\u2500 completed:    Return completed jobs, default 'True'\n            \u2502   \u251c\u2500\u2500 client:    Client name to return jobs for\n            \u2502   \u251c\u2500\u2500 uuid:    Job UUID to return\n            \u2502   \u2514\u2500\u2500 task:    Task name to return jobs for\n            \u2514\u2500\u2500 details:    Show job details\n                \u251c\u2500\u2500 timeout:    Job timeout\n                \u251c\u2500\u2500 workers:    Workers to return jobs for, default 'all'\n                \u251c\u2500\u2500 *uuid:    Job UUID\n                \u251c\u2500\u2500 data:    Return job data received from client, default 'True'\n                \u251c\u2500\u2500 result:    Return job result, default 'True'\n                \u2514\u2500\u2500 events:    Return job events, default 'True'\nnf# \n</code></pre>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_inventory/","title":"Nornir Worker Inventory","text":"<p>Content of <code>inventory.yaml</code> need to be updated to include Nornir worker details:</p> inventory.yaml<pre><code>broker: \n  endpoint: \"tcp://127.0.0.1:5555\" \n  shared_key: \"5z1:yW}]n?UXhGmz+5CeHN1&gt;:S9k!eCh6JyIhJqO\"\n\nworkers:\n  nornir-worker-1: \n    - nornir/nornir-worker-1.yaml\n\ntopology: \n  workers: \n    - nornir-worker-1\n</code></pre> <p>To obtain broker <code>shared_key</code> run this command on broker:</p> <pre><code>cd &lt;path/to/broker/inventory.yaml&gt;\nnfcli --show-broker-shared-key\n</code></pre> <p>Sample Nornir worker inventory definition</p> <p>Example</p> Netbox &gt;= 4.3.0 <p>This inventory <code>filters</code> section contains GraphQL query examples  compatible with Netbox 4.3.0 and above.</p> nornir/nornir-worker-1.yaml<pre><code>service: nornir\nwatchdog_interval: 30\nconnections_idle_timeout: null\n\n# these parameters mapped to Nornir inventory\n# https://nornir.readthedocs.io/en/latest/tutorial/inventory.html\nrunner:\n  plugin: RetryRunner\n  options: \n    num_workers: 100\n    num_connectors: 10\n    connect_retry: 1\n    connect_backoff: 1000\n    connect_splay: 100\n    task_retry: 1\n    task_backoff: 1000\n    task_splay: 100\n    reconnect_on_fail: True\n    task_timeout: 600\nhosts: {}\ngroups: {}\ndefaults: {}\nlogging: {}\nuser_defined: {}\n\n# Netbox Service Nornir Inventory integration\nnetbox:\n  instance: prod\n  interfaces:\n    ip_addresses: True\n    inventory_items: True\n  connections:\n    cables: True\n  nbdata: True\n  circuits: True\n  primary_ip: \"ipv4\"\n  devices:\n    - fceos4\n    - fceos5\n    - fceos8\n    - ceos1\n  filters: \n    - name: '{i_contains: \"fceos3\"}'\n    - '{platform: {name: {exact: \"cisco_xr\"}}}'\n</code></pre> <p>watchdog_interval</p> <p>Watchdog run interval in seconds, default is 30</p> <p>connections_idle_timeout</p> <p>Watchdog connection idle timeout, default is <code>None</code> - no timeout, connection always kept alive, if set to 0, connections disconnected right after task completed, if positive number, connection disconnected after not being used for over <code>connections_idle_timeout</code></p>"},{"location":"workers/nornir/services_nornir_service_inventory/#netbox-inventory-integration","title":"Netbox Inventory Integration","text":"<p>NorFab Nornir Worker supports tight integration with Netbox to fetch devices data such as device interfaces, ip addresses, circuits, configuration context. Netbox 3.7.x and 4.x.x supported. </p> <p>Sample Nornir Worker inventory parameters to fetch devices data from Netbox</p> <pre><code>netbox:\n  instance: prod\n  interfaces:\n    ip_addresses: True\n    inventory_items: True\n  connections:\n    cables: True\n  nbdata: True\n  circuits: True\n  primary_ip: \"ipv4\"\n  devices:\n    - fceos4\n    - fceos5\n    - fceos8\n    - ceos1\n  filters: \n    - q: fceos3\n    - manufacturer: cisco\n      platform: cisco_xr\n</code></pre> <p>filters</p> <p>List of Netbox GraphQL filters to pull devices data. Up to 10 filters supported.</p> <p>devices</p> <p>List of exact device names to retrieve from Netbox, names used as hosts' names in Nornir inventory.</p> <p>instance</p> <p>Specifies the name of the NetBox instance to be used. This parameter is useful for environments with multiple NetBox instances, allowing to target a specific instance to fetch devices data.</p> <p>interfaces</p> <p>Indicates whether to include interface data in the results.</p> <p>Extras:</p> <ul> <li>ip_addresses: When set to <code>True</code>, includes IP address information associated with the interfaces in Netbox. </li> <li>inventory_items: When set to <code>True</code>, includes inventory items associated with the interfaces in Netbox. </li> </ul> <p>connections</p> <p>Specifies whether to include connection data in the results. </p> <p>Extras:</p> <ul> <li>cables: When set to <code>True</code>, includes cable information associated with the interface connections. </li> </ul> <p>nbdata</p> <p>Specifies whether to merge NetBox devices data into Nornir hosts' <code>data</code>. This is useful when need to make Netbox device <code>config_context</code> available in Nornir hosts' <code>data</code> together with other device information such as Netbox <code>site</code>, <code>tags</code>, <code>role</code> etc.</p> <p>circuits</p> <p>Indicates whether to fetch circuits data from Netbox and map it to hosts data.</p> <p>primary_ip</p> <p>Specifies what Netbox device IP address to use for Nornir host's <code>hostname</code> parameter, supported values are <code>ipv4</code>, <code>ip4</code>, <code>ipv6</code> or <code>ip6</code>, uses Netbox device name instead if no primary IP address mapped to the device in Netbox.</p>"},{"location":"workers/nornir/services_nornir_service_jinja2_filters/","title":"Nornir Service Jinja2 Templates Filters","text":"<p>Below listed additional Jinja2 filters that supported by Nornir service for templates rendering by all service tasks such as <code>cfg</code>, <code>cli</code>, <code>tests</code> etc.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_jinja2_filters/#network_hosts","title":"network_hosts","text":"<p>Returns a list of hosts for given network.</p> <p>Arguments:</p> <ul> <li><code>pfxlen</code> - boolean, default is True, if False skips prefix length for IP addresses </li> </ul> <p>Example:</p> <pre><code>{{ '192.168.1.0/30' | network_hosts }}\n\n{{ '192.168.2.0/30' | network_hosts(pfxlen=False) }}\n</code></pre> <p>Returns:</p> <pre><code>[\"192.168.1.1/30\", \"192.168.1.2/30\"]\n\n[\"192.168.2.1\", \"192.168.2.2\"]\n</code></pre>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_jinja2_filters/#netboxcreate_ip","title":"netbox.create_ip","text":"<p>This Jinja2 filter queries Netbox to get existing or create next available IP in prefix.</p> <p><code>netbox.create_ip</code> can be invoked using Jinja2 filter syntax where value it is applied against must be  a prefix recorded in Netbox:</p> <pre><code>{% for interface in host.interfaces %}\ninterface {{ interface }}\n  ip address {{ \"10.0.0.0/24\" | netbox.create_ip(host.name, interface) }}\n!\n{% endfor %}\n</code></pre> <p>Alternatively, <code>netbox.create_ip</code> can be called within <code>set</code> block to assign result to a variable:</p> <pre><code>{% for interface in host.interfaces %}\n{% set ip = netbox.create_ip(\"10.0.0.0/24\", host.name, interface, description=\"Primary interface ip\") %}\ninterface {{ interface }}\n  ip address {{ ip }}\n!\n{% endfor %}\n</code></pre> <p>All the same arguments supported by Netbox service create_ip task can be passed onto <code>netbox.create_ip</code> call:</p> <p>Allocate the next available IP address from a given subnet.</p> <p>This task finds or creates an IP address in NetBox, updates its metadata, optionally links it to a device/interface, and supports a dry run mode for previewing changes.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The prefix from which to allocate the IP address, could be:</p> <ul> <li>IPv4 prefix string e.g. 10.0.0.0/24</li> <li>IPv6 prefix string e.g. 2001::/64</li> <li>Prefix description string to filter by</li> <li>Dictionary with prefix filters to feed <code>pynetbox</code> get method     e.g. <code>{\"prefix\": \"10.0.0.0/24\", \"site__name\": \"foo\"}</code></li> </ul> required <code>description</code> <code>str</code> <p>A description for the allocated IP address.</p> <code>None</code> <code>device</code> <code>str</code> <p>The device associated with the IP address.</p> <code>None</code> <code>interface</code> <code>str</code> <p>The interface associated with the IP address.</p> <code>None</code> <code>vrf</code> <code>str</code> <p>The VRF (Virtual Routing and Forwarding) instance.</p> <code>None</code> <code>tags</code> <code>list</code> <p>A list of tags to associate with the IP address.</p> <code>None</code> <code>dns_name</code> <code>str</code> <p>The DNS name for the IP address.</p> <code>None</code> <code>tenant</code> <code>str</code> <p>The tenant associated with the IP address.</p> <code>None</code> <code>comments</code> <code>str</code> <p>Additional comments for the IP address.</p> <code>None</code> <code>instance</code> <code>str</code> <p>The NetBox instance to use.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, do not actually allocate the IP address.</p> <code>False</code> <code>branch</code> <code>str</code> <p>Branch name to use, need to have branching plugin installed, automatically creates branch if it does not exist in Netbox.</p> <code>None</code> <code>mask_len</code> <code>int</code> <p>mask length to use for IP address on creation or to update existing IP address. On new IP address creation will create child subnet of <code>mask_len</code> within parent <code>prefix</code>, new subnet not created for existing IP addresses. <code>mask_len</code> argument ignored on dry run and ip allocated from parent prefix directly.</p> <code>None</code> <code>create_peer_ip</code> <code>bool</code> <p>If True creates IP address for link peer - remote device interface connected to requested device and interface</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the result of the IP allocation.</p> <p>Tasks execution follow these steps:</p> <ol> <li> <p>Tries to find an existing IP in NetBox matching the device/interface/description.     If found, uses it; otherwise, proceeds to create a new IP.</p> </li> <li> <p>If prefix is a string, determines if it\u2019s an IP network or a description.     Builds a filter dictionary for NetBox queries, optionally including VRF.</p> </li> <li> <p>Queries NetBox for the prefix using the constructed filter.</p> </li> <li> <p>If dry_run is True, fetches the next available IP but doesn\u2019t create it.</p> </li> <li> <p>If not a dry run, creates the next available IP in the prefix.</p> </li> <li> <p>Updates IP attributes (description, VRF, tenant, DNS name, comments, role, tags)     if provided and different from current values. Handles interface assignment and     can set the IP as primary for the device.</p> </li> <li> <p>If changes were made and not a dry run, saves the IP and device updates to NetBox.</p> </li> </ol> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"POST\"], \"schema\": NetboxFastApiArgs.model_json_schema()}\n)\ndef create_ip(\n    self,\n    job: Job,\n    prefix: Union[str, dict],\n    device: Union[None, str] = None,\n    interface: Union[None, str] = None,\n    description: Union[None, str] = None,\n    vrf: Union[None, str] = None,\n    tags: Union[None, list] = None,\n    dns_name: Union[None, str] = None,\n    tenant: Union[None, str] = None,\n    comments: Union[None, str] = None,\n    role: Union[None, str] = None,\n    status: Union[None, str] = None,\n    is_primary: Union[None, bool] = None,\n    instance: Union[None, str] = None,\n    dry_run: Union[None, bool] = False,\n    branch: Union[None, str] = None,\n    mask_len: Union[None, int] = None,\n    create_peer_ip: Union[None, bool] = True,\n) -&gt; Result:\n    \"\"\"\n    Allocate the next available IP address from a given subnet.\n\n    This task finds or creates an IP address in NetBox, updates its metadata,\n    optionally links it to a device/interface, and supports a dry run mode for\n    previewing changes.\n\n    Args:\n        prefix (str): The prefix from which to allocate the IP address, could be:\n\n            - IPv4 prefix string e.g. 10.0.0.0/24\n            - IPv6 prefix string e.g. 2001::/64\n            - Prefix description string to filter by\n            - Dictionary with prefix filters to feed `pynetbox` get method\n                e.g. `{\"prefix\": \"10.0.0.0/24\", \"site__name\": \"foo\"}`\n\n        description (str, optional): A description for the allocated IP address.\n        device (str, optional): The device associated with the IP address.\n        interface (str, optional): The interface associated with the IP address.\n        vrf (str, optional): The VRF (Virtual Routing and Forwarding) instance.\n        tags (list, optional): A list of tags to associate with the IP address.\n        dns_name (str, optional): The DNS name for the IP address.\n        tenant (str, optional): The tenant associated with the IP address.\n        comments (str, optional): Additional comments for the IP address.\n        instance (str, optional): The NetBox instance to use.\n        dry_run (bool, optional): If True, do not actually allocate the IP address.\n        branch (str, optional): Branch name to use, need to have branching plugin\n            installed, automatically creates branch if it does not exist in Netbox.\n        mask_len (int, optional): mask length to use for IP address on creation or to\n            update existing IP address. On new IP address creation will create child\n            subnet of `mask_len` within parent `prefix`, new subnet not created for\n            existing IP addresses. `mask_len` argument ignored on dry run and ip allocated\n            from parent prefix directly.\n        create_peer_ip (bool, optional): If True creates IP address for link peer -\n            remote device interface connected to requested device and interface\n\n    Returns:\n        dict: A dictionary containing the result of the IP allocation.\n\n    Tasks execution follow these steps:\n\n    1. Tries to find an existing IP in NetBox matching the device/interface/description.\n        If found, uses it; otherwise, proceeds to create a new IP.\n\n    2. If prefix is a string, determines if it\u2019s an IP network or a description.\n        Builds a filter dictionary for NetBox queries, optionally including VRF.\n\n    3. Queries NetBox for the prefix using the constructed filter.\n\n    4. If dry_run is True, fetches the next available IP but doesn\u2019t create it.\n\n    5. If not a dry run, creates the next available IP in the prefix.\n\n    6. Updates IP attributes (description, VRF, tenant, DNS name, comments, role, tags)\n        if provided and different from current values. Handles interface assignment and\n        can set the IP as primary for the device.\n\n    7. If changes were made and not a dry run, saves the IP and device updates to NetBox.\n    \"\"\"\n    instance = instance or self.default_instance\n    ret = Result(task=f\"{self.name}:create_ip\", result={}, resources=[instance])\n    tags = tags or []\n    has_changes = False\n    nb_ip = None\n    nb_device = None\n    create_peer_ip_data = {}\n    nb = self._get_pynetbox(instance, branch=branch)\n\n    # source parent prefix from Netbox\n    if isinstance(prefix, str):\n        # try converting prefix to network, if fails prefix is not an IP network\n        try:\n            _ = ipaddress.ip_network(prefix)\n            is_network = True\n        except:\n            is_network = False\n        if is_network is True and vrf:\n            prefix = {\"prefix\": prefix, \"vrf__name\": vrf}\n        elif is_network is True:\n            prefix = {\"prefix\": prefix}\n        elif is_network is False and vrf:\n            prefix = {\"description\": prefix, \"vrf__name\": vrf}\n        elif is_network is False:\n            prefix = {\"description\": prefix}\n    nb_prefix = nb.ipam.prefixes.get(**prefix)\n    if not nb_prefix:\n        raise NetboxAllocationError(\n            f\"Unable to source parent prefix from Netbox - {prefix}\"\n        )\n    parent_prefix_len = int(str(nb_prefix).split(\"/\")[1])\n\n    # try to source existing IP from netbox\n    if device and interface and description:\n        nb_ip = nb.ipam.ip_addresses.get(\n            device=device,\n            interface=interface,\n            description=description,\n            parent=str(nb_prefix),\n        )\n    elif device and interface:\n        nb_ip = nb.ipam.ip_addresses.get(\n            device=device, interface=interface, parent=str(nb_prefix)\n        )\n    elif description:\n        nb_ip = nb.ipam.ip_addresses.get(\n            description=description, parent=str(nb_prefix)\n        )\n\n    # create new IP address\n    if not nb_ip:\n        # check if interface has link peer that has IP within parent prefix\n        if device and interface:\n            connection = self.get_connections(\n                job=job,\n                devices=[device],\n                interface_regex=interface,\n                instance=instance,\n                include_virtual=True,\n            )\n            if interface in connection.result[device]:\n                peer = connection.result[device][interface]\n                # do not process breakout cables\n                if isinstance(peer[\"remote_interface\"], list):\n                    peer[\"remote_interface\"] = None\n                # try to source peer ip subnet\n                nb_peer_ip = None\n                if peer[\"remote_device\"] and peer[\"remote_interface\"]:\n                    nb_peer_ip = nb.ipam.ip_addresses.get(\n                        device=peer[\"remote_device\"],\n                        interface=peer[\"remote_interface\"],\n                        parent=str(nb_prefix),\n                    )\n                # try to source peer ip subnet\n                nb_peer_prefix = None\n                if nb_peer_ip:\n                    peer_ip = ipaddress.ip_interface(nb_peer_ip.address)\n                    nb_peer_prefix = nb.ipam.prefixes.get(\n                        prefix=str(peer_ip.network),\n                        vrf__name=vrf,\n                    )\n                elif create_peer_ip and peer[\"remote_interface\"]:\n                    create_peer_ip_data = {\n                        \"device\": peer[\"remote_device\"],\n                        \"interface\": peer[\"remote_interface\"],\n                        \"vrf\": vrf,\n                        \"branch\": branch,\n                        \"tenant\": tenant,\n                        \"dry_run\": dry_run,\n                        \"tags\": tags,\n                        \"status\": status,\n                        \"create_peer_ip\": False,\n                        \"instance\": instance,\n                    }\n                # use peer subnet to create IP address\n                if nb_peer_prefix:\n                    nb_prefix = nb_peer_prefix\n                    mask_len = None  # cancel subnet creation\n                    job.event(\n                        f\"Using link peer '{peer['remote_device']}:{peer['remote_interface']}' \"\n                        f\"prefix '{nb_peer_prefix}' to create IP address\"\n                    )\n        # if mask_len provided create new subnet\n        if mask_len and not dry_run and mask_len != parent_prefix_len:\n            if mask_len &lt; parent_prefix_len:\n                raise ValueError(\n                    f\"Mask length '{mask_len}' must be longer then '{parent_prefix_len}' prefix length\"\n                )\n            prefix_status = status\n            if prefix_status not in [\"active\", \"reserved\", \"deprecated\"]:\n                prefix_status = None\n            child_subnet = self.create_prefix(\n                job=job,\n                parent=str(nb_prefix),\n                prefixlen=mask_len,\n                vrf=vrf,\n                tags=tags,\n                tenant=tenant,\n                status=prefix_status,\n                instance=instance,\n                branch=branch,\n            )\n            prefix = {\"prefix\": child_subnet.result[\"prefix\"]}\n            if vrf:\n                prefix[\"vrf__name\"] = vrf\n            nb_prefix = nb.ipam.prefixes.get(**prefix)\n\n            if not nb_prefix:\n                raise NetboxAllocationError(\n                    f\"Unable to source child prefix of mask length \"\n                    f\"'{mask_len}' from '{prefix}' parent prefix\"\n                )\n        # execute dry run on new IP\n        if dry_run is True:\n            nb_ip = nb_prefix.available_ips.list()[0]\n            ret.status = \"unchanged\"\n            ret.dry_run = True\n            ret.result = {\n                \"address\": str(nb_ip),\n                \"description\": description,\n                \"vrf\": vrf,\n                \"device\": device,\n                \"interface\": interface,\n            }\n            # add branch to results\n            if branch is not None:\n                ret.result[\"branch\"] = branch\n            return ret\n        # create new IP\n        else:\n            nb_ip = nb_prefix.available_ips.create()\n            job.event(\n                f\"Created '{nb_ip}' IP address for '{device}:{interface}' within '{nb_prefix}' prefix\"\n            )\n        ret.status = \"created\"\n    else:\n        job.event(f\"Using existing IP address {nb_ip}\")\n        ret.status = \"updated\"\n\n    # update IP address parameters\n    if description and description != nb_ip.description:\n        nb_ip.description = description\n        has_changes = True\n    if vrf and vrf != nb_ip.vrf:\n        nb_ip.vrf = {\"name\": vrf}\n        has_changes = True\n    if tenant and tenant != nb_ip.tenant:\n        nb_ip.tenant = {\"name\": tenant}\n        has_changes = True\n    if dns_name and dns_name != nb_ip.dns_name:\n        nb_ip.dns_name = dns_name\n        has_changes = True\n    if comments and comments != nb_ip.comments:\n        nb_ip.comments = comments\n        has_changes = True\n    if role and role != nb_ip.role:\n        nb_ip.role = role\n        has_changes = True\n    if tags and not any(t in nb_ip.tags for t in tags):\n        for t in tags:\n            if t not in nb_ip.tags:\n                nb_ip.tags.append({\"name\": t})\n                has_changes = True\n    if device and interface:\n        nb_interface = nb.dcim.interfaces.get(device=device, name=interface)\n        if not nb_interface:\n            raise NetboxAllocationError(\n                f\"Unable to source '{device}:{interface}' interface from Netbox\"\n            )\n        if (\n            hasattr(nb_ip, \"assigned_object\")\n            and nb_ip.assigned_object != nb_interface.id\n        ):\n            nb_ip.assigned_object_id = nb_interface.id\n            nb_ip.assigned_object_type = \"dcim.interface\"\n            if is_primary is not None:\n                nb_device = nb.dcim.devices.get(name=device)\n                nb_device.primary_ip4 = nb_ip.id\n            has_changes = True\n    if mask_len and not str(nb_ip).endswith(f\"/{mask_len}\"):\n        address = str(nb_ip).split(\"/\")[0]\n        nb_ip.address = f\"{address}/{mask_len}\"\n        has_changes = True\n\n    # save IP address into Netbox\n    if dry_run:\n        ret.status = \"unchanged\"\n        ret.dry_run = True\n    elif has_changes:\n        nb_ip.save()\n        job.event(f\"Updated '{str(nb_ip)}' IP address parameters\")\n        # make IP primary for device\n        if is_primary is True and nb_device:\n            nb_device.save()\n    else:\n        ret.status = \"unchanged\"\n\n    # form and return results\n    ret.result = {\n        \"address\": str(nb_ip),\n        \"description\": str(nb_ip.description),\n        \"vrf\": str(nb_ip.vrf) if not vrf else nb_ip.vrf[\"name\"],\n        \"device\": device,\n        \"interface\": interface,\n    }\n    # add branch to results\n    if branch is not None:\n        ret.result[\"branch\"] = branch\n\n    # create IP address for peer\n    if create_peer_ip and create_peer_ip_data:\n        job.event(\n            f\"Creating IP address for link peer '{create_peer_ip_data['device']}:{create_peer_ip_data['interface']}'\"\n        )\n        peer_ip = self.create_ip(\n            **create_peer_ip_data, prefix=str(nb_prefix), job=job\n        )\n        if peer_ip.failed == False:\n            ret.result[\"peer\"] = peer_ip.result\n\n    return ret\n</code></pre>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_jinja2_filters/#netboxcreate_prefix","title":"netbox.create_prefix","text":"<p>This Jinja2 filter queries Netbox to get existing or create next available prefix within parent prefix. The intention is to use <code>netbox.create_prefix</code> together with <code>netbox.create_ip</code> function to automate the process of IP addressing devices interfaces and Netbox updates.</p> <p>Warning</p> <p><code>netbox.create_prefix</code> functions uses prefix description argument to deduplicate prefixes, calls to <code>netbox.create_prefix</code> should contain identical prefix description value for same prefix.</p> <p><code>netbox.create_prefix</code> can be invoked using Jinja2 filter syntax where value it is applied against must be  a parent prefix recorded in Netbox:</p> <pre><code>{% set connections = netbox.get_connections(devices=[host.name]) -%}\n\n{% for interface, connection in connections.items() -%}\n{% set subnet_description = [host.name + \":\" + interface, connection[\"remote_device\"] + \":\" + connection[\"remote_interface\"]] | sort | join(\" - ptp - \") -%}\ninterface {{ interface }}\n  ip address {{ \"10.1.0.0/24\" | netbox.create_prefix(subnet_description, 30) | netbox.create_ip(host.name, interface) }}\n!\n{% endfor %}\n</code></pre> <p>above Jinja2 template will first invoke <code>netbox.create_prefix</code> to allocate next available <code>/30</code> subnet in 10.0.0.0/24 prefix with <code>netbox.create_ip</code> subsequently allocating next available IP address within newely allocated <code>/30</code> subnet.</p> <p>Alternatively, <code>netbox.create_prefix</code> can be called within <code>set</code> block to assign result to a variable:</p> <pre><code>{% set connections = netbox.get_connections(devices=[host.name]) -%}\n\n{% for interface, connection in connections.items() -%}\n{% set subnet_description = [host.name + \":\" + interface, connection[\"remote_device\"] + \":\" + connection[\"remote_interface\"]] | sort | join(\" - ptp - \") -%}\n{% set ip = netbox.create_prefix(\"10.1.0.0/24\", subnet_description, 30) | netbox.create_ip(host.name, interface) %}\ninterface {{ interface }}\n  ip address {{ ip }}\n!\n{% endfor %}\n</code></pre> <p>All the same arguments supported by Netbox service create_prefix task can be passed onto <code>netbox.create_prefix</code> call:</p> <p>Creates a new IP prefix in NetBox or updates an existing one.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>Union[str, dict]</code> <p>Parent prefix to allocate new prefix from, could be:</p> <ul> <li>IPv4 prefix string e.g. 10.0.0.0/24</li> <li>IPv6 prefix string e.g. 2001::/64</li> <li>Prefix description string to filter by</li> <li>Dictionary with prefix filters for <code>pynetbox</code> prefixes.get method     e.g. <code>{\"prefix\": \"10.0.0.0/24\", \"site__name\": \"foo\"}</code></li> </ul> required <code>description</code> <code>str</code> <p>Description for the new prefix, prefix description used for deduplication to source existing prefixes.</p> <code>None</code> <code>prefixlen</code> <code>int</code> <p>The prefix length of the new prefix to create, by default allocates next available /30 point-to-point prefix.</p> <code>30</code> <code>vrf</code> <code>str</code> <p>Name of the VRF to associate with the prefix.</p> <code>None</code> <code>tags</code> <code>Union[None, list]</code> <p>List of tags to assign to the prefix.</p> <code>None</code> <code>tenant</code> <code>str</code> <p>Name of the tenant to associate with the prefix.</p> <code>None</code> <code>comments</code> <code>str</code> <p>Comments for the prefix.</p> <code>None</code> <code>role</code> <code>str</code> <p>Role to assign to the prefix.</p> <code>None</code> <code>site</code> <code>str</code> <p>Name of the site to associate with the prefix.</p> <code>None</code> <code>status</code> <code>str</code> <p>Status of the prefix.</p> <code>None</code> <code>instance</code> <code>Union[None, str]</code> <p>NetBox instance identifier.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, simulates the creation without making changes.</p> <code>False</code> <code>branch</code> <code>str</code> <p>Branch name to use, need to have branching plugin installed, automatically creates branch if it does not exist in Netbox.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing the outcome, including status, details of the prefix, and resources used.</p> Source code in <code>norfab\\workers\\netbox_worker.py</code> <pre><code>@Task(\n    input=CreatePrefixInput,\n    fastapi={\"methods\": [\"POST\"], \"schema\": NetboxFastApiArgs.model_json_schema()},\n)\ndef create_prefix(\n    self,\n    job: Job,\n    parent: Union[str, dict],\n    description: str = None,\n    prefixlen: int = 30,\n    vrf: str = None,\n    tags: Union[None, list] = None,\n    tenant: str = None,\n    comments: str = None,\n    role: str = None,\n    site: str = None,\n    status: str = None,\n    instance: Union[None, str] = None,\n    dry_run: bool = False,\n    branch: str = None,\n) -&gt; Result:\n    \"\"\"\n    Creates a new IP prefix in NetBox or updates an existing one.\n\n    Args:\n        parent (Union[str, dict]): Parent prefix to allocate new prefix from, could be:\n\n            - IPv4 prefix string e.g. 10.0.0.0/24\n            - IPv6 prefix string e.g. 2001::/64\n            - Prefix description string to filter by\n            - Dictionary with prefix filters for `pynetbox` prefixes.get method\n                e.g. `{\"prefix\": \"10.0.0.0/24\", \"site__name\": \"foo\"}`\n\n        description (str): Description for the new prefix, prefix description used for\n            deduplication to source existing prefixes.\n        prefixlen (int, optional): The prefix length of the new prefix to create, by default\n            allocates next available /30 point-to-point prefix.\n        vrf (str, optional): Name of the VRF to associate with the prefix.\n        tags (Union[None, list], optional): List of tags to assign to the prefix.\n        tenant (str, optional): Name of the tenant to associate with the prefix.\n        comments (str, optional): Comments for the prefix.\n        role (str, optional): Role to assign to the prefix.\n        site (str, optional): Name of the site to associate with the prefix.\n        status (str, optional): Status of the prefix.\n        instance (Union[None, str], optional): NetBox instance identifier.\n        dry_run (bool, optional): If True, simulates the creation without making changes.\n        branch (str, optional): Branch name to use, need to have branching plugin installed,\n            automatically creates branch if it does not exist in Netbox.\n\n    Returns:\n        Result: An object containing the outcome, including status, details of the prefix, and resources used.\n    \"\"\"\n    instance = instance or self.default_instance\n    changed = {}\n    ret = Result(\n        task=f\"{self.name}:create_prefix\",\n        result={},\n        resources=[instance],\n        diff=changed,\n    )\n    tags = tags or []\n    nb_prefix = None\n    nb = self._get_pynetbox(instance, branch=branch)\n\n    job.event(\n        f\"Processing prefix create request within '{parent}' for '/{prefixlen}' subnet\"\n    )\n\n    # source parent prefix from Netbox\n    if isinstance(parent, str):\n        # check if parent prefix is IP network or description\n        try:\n            _ = ipaddress.ip_network(parent)\n            is_network = True\n        except:\n            is_network = False\n        if is_network is True and vrf:\n            parent_filters = {\"prefix\": parent, \"vrf__name\": vrf}\n        elif is_network is True:\n            parent_filters = {\"prefix\": parent}\n        elif is_network is False and vrf:\n            parent_filters = {\"description\": parent, \"vrf__name\": vrf}\n        elif is_network is False:\n            parent_filters = {\"description\": parent}\n    nb_parent_prefix = nb.ipam.prefixes.get(**parent_filters)\n    if not nb_parent_prefix:\n        raise NetboxAllocationError(\n            f\"Unable to source parent prefix from Netbox - {parent}\"\n        )\n\n    # check that parent vrf and new prefix vrf are same\n    if vrf and str(nb_parent_prefix.vrf) != vrf:\n        raise NetboxAllocationError(\n            f\"Parent prefix vrf '{nb_parent_prefix.vrf}' not same as requested child prefix vrf '{vrf}'\"\n        )\n\n    # try to source existing prefix from netbox\n    prefix_filters = {}\n    if vrf:\n        prefix_filters[\"vrf__name\"] = vrf\n    if site:\n        prefix_filters[\"site__name\"] = site\n    if description:\n        prefix_filters[\"description\"] = description\n    try:\n        if prefix_filters:\n            nb_prefix = nb.ipam.prefixes.get(\n                within=nb_parent_prefix.prefix, **prefix_filters\n            )\n    except Exception as e:\n        raise NetboxAllocationError(\n            f\"Failed to source existing prefix from Netbox using filters '{prefix_filters}', error: {e}\"\n        )\n\n    # create new prefix\n    if not nb_prefix:\n        job.event(f\"Creating new '/{prefixlen}' prefix within '{parent}' prefix\")\n        # execute dry run on new prefix\n        if dry_run is True:\n            nb_prefixes = nb_parent_prefix.available_prefixes.list()\n            if not nb_prefixes:\n                raise NetboxAllocationError(\n                    f\"Parent prefix '{parent}' has no child prefixes available\"\n                )\n            for pfx in nb_prefixes:\n                # parent prefix empty, can use first subnet as a child prefix\n                if pfx.prefix == nb_parent_prefix.prefix:\n                    nb_prefix = (\n                        nb_parent_prefix.prefix.split(\"/\")[0] + f\"/{prefixlen}\"\n                    )\n                    break\n                # find child prefix by prefixlenght\n                elif str(pfx).endswith(f\"/{prefixlen}\"):\n                    nb_prefix = str(pfx)\n                    break\n            else:\n                raise NetboxAllocationError(\n                    f\"Parent prefix '{parent}' has no child prefixes available with '/{prefixlen}' prefix length\"\n                )\n            ret.status = \"unchanged\"\n            ret.dry_run = True\n            ret.result = {\n                \"prefix\": nb_prefix,\n                \"description\": description,\n                \"parent\": nb_parent_prefix.prefix,\n                \"vrf\": vrf,\n                \"site\": site,\n            }\n            # add branch to results\n            if branch is not None:\n                ret.result[\"branch\"] = branch\n            return ret\n        # create new prefix\n        else:\n            try:\n                nb_prefix = nb_parent_prefix.available_prefixes.create(\n                    {\"prefix_length\": prefixlen}\n                )\n            except Exception as e:\n                raise NetboxAllocationError(\n                    f\"Failed creating child prefix of '/{prefixlen}' prefix length \"\n                    f\"within parent prefix '{str(nb_parent_prefix)}', error: {e}\"\n                )\n        job.event(f\"Created new '{nb_prefix}' prefix within '{parent}' prefix\")\n        ret.status = \"created\"\n    else:\n        # check existing prefix length matching requested length\n        if not nb_prefix.prefix.endswith(f\"/{prefixlen}\"):\n            raise NetboxAllocationError(\n                f\"Found existing child prefix '{nb_prefix.prefix}' with mismatch \"\n                f\"requested prefix length '/{prefixlen}'\"\n            )\n        job.event(f\"Using existing prefix {nb_prefix}\")\n\n    # update prefix parameters\n    if description and description != nb_prefix.description:\n        changed[\"description\"] = {\"-\": str(nb_prefix.description), \"+\": description}\n        nb_prefix.description = description\n    if vrf and vrf != str(nb_prefix.vrf):\n        changed[\"vrf\"] = {\"-\": str(nb_prefix.vrf), \"+\": vrf}\n        nb_prefix.vrf = {\"name\": vrf}\n    if tenant and tenant != str(nb_prefix.tenant):\n        changed[\"tenant\"] = {\n            \"-\": str(nb_prefix.tenant) if nb_prefix.tenant else None,\n            \"+\": tenant,\n        }\n        nb_prefix.tenant = {\"name\": tenant}\n    if site and str(nb_prefix.scope) != site:\n        nb_site = nb.dcim.sites.get(name=site)\n        if not nb_site:\n            raise NetboxAllocationError(f\"Failed to get '{site}' site from Netbox\")\n        changed[\"site\"] = {\n            \"-\": str(nb_prefix.scope) if nb_prefix.scope else None,\n            \"+\": nb_site.name,\n        }\n        nb_prefix.scope_type = \"dcim.site\"\n        nb_prefix.scope_id = nb_site.id\n    if status and status.lower() != nb_prefix.status:\n        changed[\"status\"] = {\"-\": str(nb_prefix.status), \"+\": status.title()}\n        nb_prefix.status = status.lower()\n    if comments and comments != nb_prefix.comments:\n        changed[\"comments\"] = {\"-\": str(nb_prefix.comments), \"+\": comments}\n        nb_prefix.comments = comments\n    if role and role != nb_prefix.role:\n        changed[\"role\"] = {\"-\": str(nb_prefix.role), \"+\": role}\n        nb_prefix.role = {\"name\": role}\n    existing_tags = [str(t) for t in nb_prefix.tags]\n    if tags and not any(t in existing_tags for t in tags):\n        changed[\"tags\"] = {\n            \"-\": existing_tags,\n            \"+\": [t for t in tags if t not in existing_tags] + existing_tags,\n        }\n        for t in tags:\n            if t not in existing_tags:\n                nb_prefix.tags.append({\"name\": t})\n\n    # save prefix into Netbox\n    if dry_run:\n        ret.status = \"unchanged\"\n        ret.dry_run = True\n        ret.diff = changed\n    elif changed:\n        ret.diff = changed\n        nb_prefix.save()\n        if ret.status != \"created\":\n            ret.status = \"updated\"\n    else:\n        ret.status = \"unchanged\"\n\n    # source vrf name\n    vrf_name = None\n    if nb_prefix.vrf:\n        if isinstance(nb_prefix.vrf, dict):\n            vrf_name = nb_prefix.vrf[\"name\"]\n        else:\n            vrf_name = nb_prefix.vrf.name\n\n    # form and return results\n    ret.result = {\n        \"prefix\": nb_prefix.prefix,\n        \"description\": nb_prefix.description,\n        \"vrf\": vrf_name,\n        \"site\": str(nb_prefix.scope) if nb_prefix.scope else site,\n        \"parent\": nb_parent_prefix.prefix,\n    }\n    # add branch to results\n    if branch is not None:\n        ret.result[\"branch\"] = branch\n\n    return ret\n</code></pre>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_jinja2_filters/#netboxget_connections","title":"netbox.get_connections","text":"<p>TBD</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_jinja2_filters/#netboxget_interfaces","title":"netbox.get_interfaces","text":"<p>TBD</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_jinja2_filters/#netboxget_devices","title":"netbox.get_devices","text":"<p>TBD</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_jinja2_filters/#netboxget_circuits","title":"netbox.get_circuits","text":"<p>TBD</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_jinja2_filters/#netboxrest","title":"netbox.rest","text":"<p>TBD</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_jinja2_filters/#netboxgraphql","title":"netbox.graphql","text":"<p>TBD</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cfg/","title":"Nornir Service CFG Task","text":"<p>task api name: <code>cfg</code></p> <p>Nornir service <code>cfg</code> task designed to send configuration to devices using SSH and Telnet. Nornir <code>cfg</code> can use Netmiko, Scrapli and NAPALM libraries to configure devices.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cfg/#nornir-cfg-sample-usage","title":"Nornir CFG Sample Usage","text":"<p>Example of sending configuration commands to devices.</p> <p>Example</p> CLIPython <pre><code>C:\\nf&gt;nfcli\nWelcome to NorFab Interactive Shell.\nnf#\n</code></pre> <p>Demo</p> <p></p> <p>Above sends configuration commands to all Nornir hosts that contain <code>spine</code> or <code>leaf</code> in their hostname as we use <code>FC</code> - \"Filter Contains\" Nornir hosts targeting filter.</p> <p><code>inventory.yaml</code> should be located in same folder where we start nfcli, unless <code>nfcli -i path_to_inventory.yaml</code> flag used. Refer to Getting Started section on how to construct  <code>inventory.yaml</code> file</p> <p>This code is complete and can run as is</p> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"nornir\",\n        task=\"cfg\",\n        kwargs={\n            \"config\": [\"ntp server 10.0.0.1\", \"ntp server 10.0.0.2\"],\n            \"FC\": \"spine,leaf\"              \n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre> <p>Refer to Getting Started section on how to construct  <code>inventory.yaml</code> file.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cfg/#use-different-configuration-plugins","title":"Use Different Configuration Plugins","text":"<p>NorFab supports various configuration plugins such as <code>netmiko</code>, <code>napalm</code> and <code>scrapli</code>. These plugins enable you to push configurations to a wide range of network devices. Each plugin has its own set of capabilities and requirements, so it is essential to ensure that your Nornir inventory is properly configured for the chosen plugin. This includes specifying the necessary connection parameters and device-specific settings. By leveraging these plugins, you can standardize and automate the configuration management process across different network environments.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cfg/#using-dry-run","title":"Using Dry Run","text":"<p>The dry run feature in NorFab allows you to simulate the application of configurations without actually pushing them to the devices. This is particularly useful for testing and validation purposes, as it enables you to verify the correctness of your configurations before making any changes to the network. Additionally, the dry run feature can be used for generating and rendering device configurations, which is beneficial for staging environments where you need to prepare configurations in advance. By using dry run, you can ensure that your configurations are accurate and ready for deployment.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cfg/#using-commit-confirmed","title":"Using Commit Confirmed","text":"<p>The commit confirmed feature provides an added layer of safety when pushing configurations to network devices. With this feature, you can apply a configuration with a rollback timer. If the configuration is not explicitly confirmed within the specified time, it will be automatically rolled back to the previous state. This is particularly useful in scenarios where you need to ensure that a configuration change does not negatively impact the network. By using commit confirmed, you can mitigate the risk of configuration errors and ensure network stability.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cfg/#sourcing-configuration-from-files","title":"Sourcing Configuration From Files","text":"<p>NorFab allows you to source configurations from text files stored on the broker. This approach enables you to manage configurations as files, making it easier to version control and maintain them. By storing configurations in files, you can apply them as needed, ensuring consistency and repeatability in your configuration management process. This method is particularly useful for large-scale deployments where configurations need to be applied to multiple devices in a controlled and organized manner.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cfg/#using-jinja2-templates","title":"Using Jinja2 Templates","text":"<p>Jinja2 templates provide a powerful way to create dynamic configurations based on variables defined in your inventory or passed as job data. By using templates, you can generate configurations that are tailored to the specific requirements of each device. This approach allows you to automate the creation of complex configurations and ensures consistency across your network. Jinja2 templates are highly flexible and can be used to incorporate conditional logic, loops, and other advanced features, making them an essential tool for network automation.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cfg/#templating-configuration-with-inline-job-data","title":"Templating Configuration with Inline Job Data","text":"","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cfg/#parsing-and-generating-configuration-in-templates","title":"Parsing and Generating Configuration in Templates","text":"<p>NorFab supports parsing of device output and the generation of new configurations within same template using parsing results. This capability allows you to create configurations based on the current state of the device, ensuring that your changes are applied accurately and efficiently. By parsing existing configurations, you can extract relevant information and use it to generate new configurations that are consistent with the device's current setup. </p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cfg/#outputting-text-tables","title":"Outputting Text Tables","text":"<p>The NorFab interactive shell supports the table command, which can be used to format output into text tables. This feature relies on the tabulate module and supports most of its functionalities. By outputting results in table format, you can easily visualize and analyze the data, making it easier to interpret and act upon. This is particularly useful for displaying configuration results in a structured, concise and readable manner.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cfg/#formatting-output-results","title":"Formatting Output Results","text":"<p>You can format the output results using various options provided by the Nornir worker. The output of the commands can be formatted using the <code>to_dict</code> parameter. When set to <code>True</code>, the results will be returned as a dictionary. When set to <code>False</code>, the results will be returned as a list. In addition <code>add_details</code> argument can be used to control the verbosity of the output and return additional Nornir result information such as:</p> <ul> <li><code>changed</code> flag</li> <li><code>diff</code> content if supported by plugin</li> <li><code>failed</code> status</li> <li><code>exception</code> details if task execution failed with error</li> <li><code>connection_retry</code> counter to show how many times RetryRunner tried to establish a connection</li> <li><code>task_retry</code> counter to show how many times RetryRunner tried to run this task</li> </ul>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cfg/#using-promptless-mode","title":"Using Promptless Mode","text":"<p>NorFab supports a proprietary promptless mode that can be used with Netmiko. This mode is particularly useful when dealing with devices that do not have a consistent prompt or when the default Netmiko output collection functions are not reliable enough. By enabling promptless mode, you can ensure that configurations are applied accurately and efficiently, even in challenging environments. This feature enhances the robustness of your configuration management process and ensures that your network devices are configured correctly.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cfg/#norfab-nornir-cfg-shell-reference","title":"NORFAB Nornir CFG Shell Reference","text":"<p>NorFab shell supports these command options for Nornir <code>cfg</code> task:</p> <pre><code>nf#man tree nornir.cfg\nroot\n\u2514\u2500\u2500 nornir:    Nornir service\n    \u2514\u2500\u2500 cfg:    Configure devices over CLI interface\n        \u251c\u2500\u2500 timeout:    Job timeout\n        \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u251c\u2500\u2500 add-details:    Add task details to results, default 'False'\n        \u251c\u2500\u2500 num-workers:    RetryRunner number of threads for tasks execution\n        \u251c\u2500\u2500 num-connectors:    RetryRunner number of threads for device connections\n        \u251c\u2500\u2500 connect-retry:    RetryRunner number of connection attempts\n        \u251c\u2500\u2500 task-retry:    RetryRunner number of attempts to run task\n        \u251c\u2500\u2500 reconnect-on-fail:    RetryRunner perform reconnect to host on task failure\n        \u251c\u2500\u2500 connect-check:    RetryRunner test TCP connection before opening actual connection\n        \u251c\u2500\u2500 connect-timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n        \u251c\u2500\u2500 creds-retry:    RetryRunner list of connection credentials and parameters to retry\n        \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n        \u251c\u2500\u2500 tf-skip-failed:    Save results to file for failed tasks\n        \u251c\u2500\u2500 diff:    File group name to run the diff for\n        \u251c\u2500\u2500 diff-last:    File version number to diff, default is 1 (last)\n        \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n        \u251c\u2500\u2500 table:    Table format (brief, terse, extend) or parameters or True\n        \u251c\u2500\u2500 headers:    Table headers\n        \u251c\u2500\u2500 headers-exclude:    Table headers to exclude\n        \u251c\u2500\u2500 sortby:    Table header column to sort by\n        \u251c\u2500\u2500 reverse:    Table reverse the sort by order\n        \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n        \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u251c\u2500\u2500 FM:    Filter hosts by platform\n        \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n        \u251c\u2500\u2500 FN:    Negate the match\n        \u251c\u2500\u2500 hosts:    Filter hosts to target\n        \u251c\u2500\u2500 dry-run:    Dry run cfg function\n        \u251c\u2500\u2500 *config:    List of configuration commands to send to devices\n        \u251c\u2500\u2500 plugin:    Configuration plugin parameters\n        \u2502   \u251c\u2500\u2500 netmiko:    Use Netmiko plugin to configure devices\n        \u2502   \u2502   \u251c\u2500\u2500 enable:    Attempt to enter enable-mode\n        \u2502   \u2502   \u251c\u2500\u2500 exit-config-mode:    Determines whether or not to exit config mode after complete\n        \u2502   \u2502   \u251c\u2500\u2500 strip-command:    Determines whether or not to strip the command\n        \u2502   \u2502   \u251c\u2500\u2500 read-timeout:    Absolute timer to send to read_channel_timing\n        \u2502   \u2502   \u251c\u2500\u2500 config-mode-command:    The command to enter into config mode\n        \u2502   \u2502   \u251c\u2500\u2500 cmd-verify:    Whether or not to verify command echo for each command in config_set\n        \u2502   \u2502   \u251c\u2500\u2500 enter-config-mode:    Do you enter config mode before sending config commands\n        \u2502   \u2502   \u251c\u2500\u2500 error-pattern:    Regular expression pattern to detect config errors in the output\n        \u2502   \u2502   \u251c\u2500\u2500 terminator:    Regular expression pattern to use as an alternate terminator\n        \u2502   \u2502   \u251c\u2500\u2500 bypass-commands:    Regular expression pattern indicating configuration commands, cmd_verify is automatically disabled\n        \u2502   \u2502   \u251c\u2500\u2500 commit:    Commit configuration, default 'True'\n        \u2502   \u2502   \u251c\u2500\u2500 commit-confirm:    Perform commit confirm on supported platforms\n        \u2502   \u2502   \u251c\u2500\u2500 commit-confirm-delay:    Confirmed commit rollback timeout in minutes, used with commit-confirm\n        \u2502   \u2502   \u251c\u2500\u2500 commit-final-delay:    Time to wait in seconds before doing final commit, used with commit-confirm\n        \u2502   \u2502   \u251c\u2500\u2500 commit-comment:    Commit operation comment\n        \u2502   \u2502   \u2514\u2500\u2500 batch:    Commands count to send in batches\n        \u2502   \u251c\u2500\u2500 scrapli:    Use Scrapli plugin to configure devices\n        \u2502   \u2502   \u251c\u2500\u2500 dry-run:    Apply changes or not, also tests if possible to enter config mode\n        \u2502   \u2502   \u251c\u2500\u2500 strip-prompt:    Strip prompt from returned output\n        \u2502   \u2502   \u251c\u2500\u2500 failed-when-contains:    String or list of strings indicating failure if found in response\n        \u2502   \u2502   \u251c\u2500\u2500 stop-on-failed:    Stop executing commands if command fails\n        \u2502   \u2502   \u251c\u2500\u2500 privilege-level:    Name of configuration privilege level to acquire\n        \u2502   \u2502   \u251c\u2500\u2500 eager:    Do not read until prompt is seen at each command sent to the channel\n        \u2502   \u2502   \u2514\u2500\u2500 timeout-ops:    Timeout ops value for this operation\n        \u2502   \u2514\u2500\u2500 napalm:    Use NAPALM plugin to configure devices\n        \u2502       \u251c\u2500\u2500 replace:    Whether to replace or merge the configuration\n        \u2502       \u251c\u2500\u2500 dry-run:    Apply changes or not, also tests if possible to enter config mode\n        \u2502       \u2514\u2500\u2500 revert-in:    Amount of time in seconds after which to revert the commit\n        \u2514\u2500\u2500 job-data:    Path to YAML file with job data\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cfg/#python-api-reference","title":"Python API Reference","text":"<p>Task to send configuration commands to devices using Command Line Interface (CLI).</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>config</code> <code>list</code> <p>List of commands to send to devices or URL to a file or template URL that resolves to a file.</p> required <code>plugin</code> <code>str</code> <p>Plugin name to use. Valid options are:</p> <ul> <li>netmiko - use Netmiko to configure devices</li> <li>scrapli - use Scrapli to configure devices</li> <li>napalm - use NAPALM to configure devices</li> </ul> <code>'netmiko'</code> <code>dry_run</code> <code>bool</code> <p>If True, will not send commands to devices but just return them.</p> <code>False</code> <code>to_dict</code> <code>bool</code> <p>If True, returns results as a dictionary. Defaults to True.</p> <code>True</code> <code>add_details</code> <code>bool</code> <p>If True, adds task execution details to the results.</p> <code>False</code> <code>job_data</code> <code>str</code> <p>URL to YAML file with data or dictionary/list of data to pass on to Jinja2 rendering context.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the task plugin.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary with the results of the configuration task.</p> <p>Raises:</p> Type Description <code>UnsupportedPluginError</code> <p>If the specified plugin is not supported.</p> <code>FileNotFoundError</code> <p>If the specified job data file cannot be downloaded.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"POST\"]},\n)\ndef cfg(\n    self,\n    job: Job,\n    config: Union[str, list],\n    plugin: str = \"netmiko\",\n    dry_run: bool = False,\n    to_dict: bool = True,\n    add_details: bool = False,\n    job_data: Any = None,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Task to send configuration commands to devices using Command Line Interface (CLI).\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        config (list): List of commands to send to devices or URL to a file or template\n            URL that resolves to a file.\n        plugin (str, optional): Plugin name to use. Valid options are:\n\n            - netmiko - use Netmiko to configure devices\n            - scrapli - use Scrapli to configure devices\n            - napalm - use NAPALM to configure devices\n\n        dry_run (bool, optional): If True, will not send commands to devices but just return them.\n        to_dict (bool, optional): If True, returns results as a dictionary. Defaults to True.\n        add_details (bool, optional): If True, adds task execution details to the results.\n        job_data (str, optional): URL to YAML file with data or dictionary/list of data to pass on to Jinja2 rendering context.\n        **kwargs: Additional arguments to pass to the task plugin.\n\n    Returns:\n        dict: A dictionary with the results of the configuration task.\n\n    Raises:\n        UnsupportedPluginError: If the specified plugin is not supported.\n        FileNotFoundError: If the specified job data file cannot be downloaded.\n    \"\"\"\n    config = config if isinstance(config, list) else [config]\n    ret = Result(task=f\"{self.name}:cfg\", result={} if to_dict else [])\n\n    filtered_nornir, no_match_result = self.filter_hosts_and_validate(kwargs, ret)\n    if ret.status == \"no_match\":\n        return ret\n\n    # decide on what send commands task plugin to use\n    if plugin == \"netmiko\":\n        task_plugin = netmiko_send_config\n    elif plugin == \"scrapli\":\n        task_plugin = scrapli_send_config\n    elif plugin == \"napalm\":\n        task_plugin = napalm_configure\n    else:\n        raise UnsupportedPluginError(f\"Plugin '{plugin}' not supported\")\n\n    job_data = self.load_job_data(job_data)\n\n    nr = self._add_processors(filtered_nornir, kwargs, job)  # add processors\n\n    # render config using Jinja2 on a per-host basis\n    for host in nr.inventory.hosts.values():\n        rendered = self.jinja2_render_templates(\n            templates=config,\n            context={\n                \"host\": host,\n                \"norfab\": self.client,\n                \"job_data\": job_data,\n                \"netbox\": self.add_jinja2_netbox(),\n            },\n            filters=self.add_jinja2_filters(),\n        )\n        host.data[\"__task__\"] = {\"config\": rendered}\n\n    # run task\n    log.debug(\n        f\"{self.name} - sending config commands '{config}', kwargs '{kwargs}', is dry_run - '{dry_run}'\"\n    )\n    if dry_run is True:\n        result = nr.run(\n            task=nr_test, use_task_data=\"config\", name=\"dry_run\", **kwargs\n        )\n    else:\n        with self.connections_lock:\n            result = nr.run(task=task_plugin, **kwargs)\n\n    ret.failed = result.failed  # failed is true if any of the hosts failed\n    ret.result = ResultSerializer(result, to_dict=to_dict, add_details=add_details)\n\n    # remove __task__ data\n    for host_name, host_object in nr.inventory.hosts.items():\n        _ = host_object.data.pop(\"__task__\", None)\n\n    self.watchdog.connections_update(nr, plugin)\n    self.watchdog.connections_clean()\n\n    return ret\n</code></pre>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/","title":"Nornir Service CLI Task","text":"<p>task api name: <code>cli</code></p> <p>Nornir service <code>cli</code> task designed to retrieve show commands output  from devices using SSH and Telnet. Nornir <code>cli</code> uses Netmiko, Scrapli  and NAPALM libraries to communicate with devices.</p> <ul> <li>Netmiko: A multi-vendor library that simplifies SSH connections to network devices.</li> <li>Scrapli: A fast and flexible library for interacting with network devices.</li> <li>NAPALM: A library that provides a unified API to interact with different network device operating systems.</li> </ul>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#nornir-cli-sample-usage","title":"Nornir CLI Sample Usage","text":"<p>Below is an example of how to use the Nornir CLI task to retrieve command outputs from devices.</p> <p>Example</p> CLIPython <pre><code>C:\\nf&gt;nfcli\nWelcome to NorFab Interactive Shell.\nnf#\nnf#nornir\nnf[nornir]#cli\nnf[nornir-cli]#\nnf[nornir-cli]#commands \"show clock\" \"show hostname\" FC ceos-spine\nceos-spine-1:\n    show clock:\n        Sun Dec  1 10:49:58 2024\n        Timezone: UTC\n        Clock source: local\n    show hostname:\n        Hostname: ceos-spine-1\n        FQDN:     ceos-spine-1\nceos-spine-2:\n    show clock:\n        Sun Dec  1 10:49:58 2024\n        Timezone: UTC\n        Clock source: local\n    show hostname:\n        Hostname: ceos-spine-2\n        FQDN:     ceos-spine-2\nnf[nornir-cli]#\n</code></pre> <p>Demo</p> <p></p> <p>In this example:</p> <ul> <li><code>nfcli</code> command starts the NorFab Interactive Shell.</li> <li><code>nornir</code> command switches to the Nornir sub-shell.</li> <li><code>cli</code> command switches to the CLI task sub-shell.</li> <li><code>commands</code> command retrieves the output of \"show clock\" and \"show hostname\" from the devices  that contain <code>ceos-spine</code> in their hostname as we use <code>FC</code> - \"Filter Contains\" Nornir hosts targeting filter.</li> </ul> <p><code>inventory.yaml</code> should be located in same folder where we start nfcli, unless <code>nfcli -i path_to_inventory.yaml</code> flag used. Refer to Getting Started section on how to construct  <code>inventory.yaml</code> file</p> <p>This code is complete and can run as is</p> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"nornir\",\n        task=\"cli\",\n        kwargs={\n            \"commands\": [\"show clock\", \"show hostname\"],\n            \"FC\": \"ceos-spine\"              \n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre> <p>Once executed, above code should produce this output:</p> <pre><code>C:\\nf&gt;python nornir_cli.py\n{'nornir-worker-1': {'errors': [],\n                     'failed': False,\n                     'messages': [],\n                     'result': {'ceos-spine-1': {'show clock': 'Sun Dec  1 '\n                                                               '11:10:53 2024\\n'\n                                                               'Timezone: UTC\\n'\n                                                               'Clock source: '\n                                                               'local',\n                                                 'show hostname': 'Hostname: '\n                                                                  'ceos-spine-1\\n'\n                                                                  'FQDN:     '\n                                                                  'ceos-spine-1'},\n                                'ceos-spine-2': {'show clock': 'Sun Dec  1 '\n                                                               '11:10:53 2024\\n'\n                                                               'Timezone: UTC\\n'\n                                                               'Clock source: '\n                                                               'local',\n                                                 'show hostname': 'Hostname: '\n                                                                  'ceos-spine-2\\n'\n                                                                  'FQDN:     '\n                                                                  'ceos-spine-2'}},\n                     'task': 'nornir-worker-1:cli'}}\nC:\\nf&gt;                   \n</code></pre> <p>Refer to Getting Started section on  how to construct  <code>inventory.yaml</code> file.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#use-different-connection-plugins","title":"Use Different Connection Plugins","text":"<p>The Nornir Service CLI Task supports various connection plugins, such as <code>netmiko</code>, <code>napalm</code>, and <code>scrapli</code>, to interact with network devices. These plugins provide the flexibility to choose the most suitable method for connecting to and managing your devices, depending on your specific requirements and preferences.</p> <p>To use a specific connection plugin, ensure that your Nornir inventory is properly configured with the necessary connection parameters and device-specific settings. This includes specifying the plugin type, authentication details, and any additional options required by the plugin.</p> <p>Example</p> CLIPython <pre><code>C:\\nf&gt;nfcli\nWelcome to NorFab Interactive Shell.\nnf#\nnf#nornir cli\nnf[nornir-cli]#commands \"show clock\" FC spine plugin netmiko\n--------------------------------------------- Job Events -----------------------------------------------\n04-Jan-2025 22:37:57 5ed5775b183a404181f004753f583f0c job started\n04-Jan-2025 22:37:57.085 nornir nornir-worker-1 ceos-spine-1, ceos-spine-2 task started - 'netmiko_send_commands'\n04-Jan-2025 22:37:57.114 nornir nornir-worker-1 ceos-spine-1 task_instance started - 'netmiko_send_commands'\n04-Jan-2025 22:37:57.114 nornir nornir-worker-1 ceos-spine-2 task_instance started - 'netmiko_send_commands'\n04-Jan-2025 22:37:57.124 nornir nornir-worker-1 ceos-spine-1 subtask started - 'show clock'\n04-Jan-2025 22:37:57.136 nornir nornir-worker-1 ceos-spine-2 subtask started - 'show clock'\n04-Jan-2025 22:37:57.237 nornir nornir-worker-1 ceos-spine-1 subtask completed - 'show clock'\n04-Jan-2025 22:37:57.237 nornir nornir-worker-1 ceos-spine-2 subtask completed - 'show clock'\n04-Jan-2025 22:37:57.244 nornir nornir-worker-1 ceos-spine-2 task_instance completed - 'netmiko_send_commands'\n04-Jan-2025 22:37:57.245 nornir nornir-worker-1 ceos-spine-1 task_instance completed - 'netmiko_send_commands'\n04-Jan-2025 22:37:57.425 nornir nornir-worker-1 ceos-spine-1, ceos-spine-2 task completed - 'netmiko_send_commands'\n04-Jan-2025 22:37:57 5ed5775b183a404181f004753f583f0c job completed in 0.476 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\nceos-spine-1:\n    show clock:\n        Sat Jan  4 12:37:57 2025\n        Timezone: UTC\n        Clock source: local\nceos-spine-2:\n    show clock:\n        Sat Jan  4 12:37:57 2025\n        Timezone: UTC\n        Clock source: local\nnf[nornir-cli]#commands \"show clock\" FC spine plugin scrapli\n--------------------------------------------- Job Events -----------------------------------------------\n04-Jan-2025 22:38:01 c6bd014aac4c42249594a6197175012e job started\n04-Jan-2025 22:38:01.116 nornir nornir-worker-1 ceos-spine-1, ceos-spine-2 task started - 'scrapli_send_commands'\n04-Jan-2025 22:38:01.119 nornir nornir-worker-1 ceos-spine-2 task_instance started - 'scrapli_send_commands'\n04-Jan-2025 22:38:01.128 nornir nornir-worker-1 ceos-spine-2 subtask started - 'show clock'\n04-Jan-2025 22:38:01.141 nornir nornir-worker-1 ceos-spine-2 subtask completed - 'show clock'\n04-Jan-2025 22:38:01.148 nornir nornir-worker-1 ceos-spine-2 task_instance completed - 'scrapli_send_commands'\n04-Jan-2025 22:38:01.192 nornir nornir-worker-1 ceos-spine-1 task_instance started - 'scrapli_send_commands'\n04-Jan-2025 22:38:01.202 nornir nornir-worker-1 ceos-spine-1 subtask started - 'show clock'\n04-Jan-2025 22:38:01.215 nornir nornir-worker-1 ceos-spine-1 subtask completed - 'show clock'\n04-Jan-2025 22:38:01.221 nornir nornir-worker-1 ceos-spine-1 task_instance completed - 'scrapli_send_commands'\n04-Jan-2025 22:38:01.364 nornir nornir-worker-1 ceos-spine-1, ceos-spine-2 task completed - 'scrapli_send_commands'\n04-Jan-2025 22:38:01 c6bd014aac4c42249594a6197175012e job completed in 0.497 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\nceos-spine-1:\n    show clock:\n        Sat Jan  4 12:38:01 2025\n        Sat Jan  4 12:38:01 2025\n        Timezone: UTC\n        Clock source: local\nceos-spine-2:\n    show clock:\n        Sat Jan  4 12:38:01 2025\n        Timezone: UTC\n        Clock source: local\nnf[nornir-cli]#commands \"show clock\" FC spine plugin napalm\n--------------------------------------------- Job Events -----------------------------------------------\n04-Jan-2025 22:43:41 02eed090a7bb4652b27cccec1a49dab6 job started\n04-Jan-2025 22:43:41.360 nornir nornir-worker-1 ceos-spine-1, ceos-spine-2 task started - 'napalm_send_commands'\n04-Jan-2025 22:43:41.382 nornir nornir-worker-1 ceos-spine-2 task_instance started - 'napalm_send_commands'\n04-Jan-2025 22:43:41.382 nornir nornir-worker-1 ceos-spine-1 task_instance started - 'napalm_send_commands'\n04-Jan-2025 22:43:41.388 nornir nornir-worker-1 ceos-spine-1 subtask started - 'napalm_cli'\n04-Jan-2025 22:43:41.389 nornir nornir-worker-1 ceos-spine-2 subtask started - 'napalm_cli'\n04-Jan-2025 22:43:41.419 nornir nornir-worker-1 ceos-spine-1 subtask completed - 'napalm_cli'\n04-Jan-2025 22:43:41.424 nornir nornir-worker-1 ceos-spine-2 subtask completed - 'napalm_cli'\n04-Jan-2025 22:43:41.425 nornir nornir-worker-1 ceos-spine-1 task_instance completed - 'napalm_send_commands'\n04-Jan-2025 22:43:41.432 nornir nornir-worker-1 ceos-spine-2 task_instance completed - 'napalm_send_commands'\n04-Jan-2025 22:43:41.599 nornir nornir-worker-1 ceos-spine-1, ceos-spine-2 task completed - 'napalm_send_commands'\n04-Jan-2025 22:43:41 02eed090a7bb4652b27cccec1a49dab6 job completed in 0.576 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\nceos-spine-1:\n    show clock:\n        Sat Jan  4 12:43:41 2025\n        Timezone: UTC\n        Clock source: local\nceos-spine-2:\n    show clock:\n        Sat Jan  4 12:43:41 2025\n        Timezone: UTC\n        Clock source: local\nnf[nornir-cli]#\nnf#\n</code></pre> <p>Demo</p> <p></p> <p>In this example:</p> <ul> <li><code>nfcli</code> command starts the NorFab Interactive Shell.</li> <li><code>nornir</code> command switches to the Nornir sub-shell.</li> <li><code>cli</code> command switches to the CLI task sub-shell.</li> <li><code>commands</code> command retrieves the output of \"show clock\" from the devices  that contain <code>spine</code> in their hostname as we use <code>FC</code> - \"Filter Contains\" Nornir hosts targeting filter, <code>plugin</code> argument used to inform Nornir service to use <code>netmiko</code>, <code>scrapli</code> or <code>napalm</code> modules to retrieve command output from devices.</li> </ul> <p><code>inventory.yaml</code> should be located in same folder where we start nfcli, unless <code>nfcli -i path_to_inventory.yaml</code> flag used. Refer to Getting Started section on how to construct  <code>inventory.yaml</code> file</p> <p>This code is complete and can run as is</p> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"nornir\",\n        task=\"cli\",\n        kwargs={\n            \"commands\": [\"show clock\"],\n            \"FC\": \"ceos-spine\",\n            \"plugin\": \"scrapli\"              \n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre> <p>Once executed, above code should produce this output:</p> <pre><code>C:\\nf&gt;python nornir_cli.py\n{'nornir-worker-1': {'errors': [],\n                     'failed': False,\n                     'messages': [],\n                     'result': {'ceos-spine-1': {'show clock': 'Sun Dec  1 '\n                                                               '11:10:53 2024\\n'\n                                                               'Timezone: UTC\\n'\n                                                               'Clock source: '\n                                                               'local'},\n                                'ceos-spine-2': {'show clock': 'Sun Dec  1 '\n                                                               '11:10:53 2024\\n'\n                                                               'Timezone: UTC\\n'\n                                                               'Clock source: '\n                                                               'local'}},\n                     'task': 'nornir-worker-1:cli'}}\nC:\\nf&gt;                   \n</code></pre> <p>Refer to Getting Started section on  how to construct  <code>inventory.yaml</code> file.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#outputting-text-tables","title":"Outputting Text Tables","text":"<p>NorFab interactive shell supports <code>table</code> argument  that can be used to format output into text tables. Internally it relies on tabulate module and most of its features are supported.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#sourcing-commands-from-file","title":"Sourcing Commands From File","text":"<p>Commands can be provided inline in the shell itself, but it is also possible to source commands from text files stored on broker.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#using-jinja2-templates","title":"Using Jinja2 Templates","text":"<p>Commands can be templated using Jinja2. This allows you to create dynamic commands based on variables defined in your inventory or passed as job data.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#templating-commands-with-inline-job-data","title":"Templating Commands with Inline Job Data","text":"<p>Templating commands with inline job data allows you to dynamically generate command strings based on variables defined directly within the job data. This approach provides flexibility and customization, enabling you to tailor commands to specific devices or scenarios without the need for external sourced of data.</p> <p>When defining a job, you can include variables directly within the <code>job_data</code> argument. These variables can then be referenced within the command strings using Jinja2 templating syntax. The Nornir worker will process these templates, substituting the variables with their corresponding values from the job data.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#using-dry-run","title":"Using Dry Run","text":"<p>The dry run feature allows you to see the commands that would be executed without actually sending them to the devices. This is useful for testing and validation. When set to <code>True</code>, the commands will not be sent to the devices, but will be returned as part of the result.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#formatting-output-results","title":"Formatting Output Results","text":"<p>You can format the output results using various options provided by the Nornir worker. The output of the commands can be formatted using the <code>to_dict</code> parameter. When set to <code>True</code>, the results will be returned as a dictionary. When set to <code>False</code>, the results will be returned as a list. In addition <code>add_details</code> argument can be used to control the verbosity of the output and return additional Nornir result information such as:</p> <ul> <li><code>changed</code> flag</li> <li><code>diff</code> content if supported by plugin</li> <li><code>failed</code> status</li> <li><code>exception</code> details if task execution failed with error</li> <li><code>connection_retry</code> counter to show how many times RetryRunner tried to establish a connection</li> <li><code>task_retry</code> counter to show how many times RetryRunner tried to run this task</li> </ul>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#running-show-commands-multiple-times","title":"Running Show Commands Multiple Times","text":"<p>You can run show commands multiple times using the <code>repeat</code> parameter. This is useful for monitoring changes over time. The <code>repeat</code> parameter can be used to run the same command multiple times. You can also specify the interval between each repeat using the <code>repeat_interval</code> parameter.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#using-netmiko-promptless-mode","title":"Using Netmiko Promptless Mode","text":"<p>NorFab support proprietary promptless mode that can be used with Netmiko, it can be useful when dealing with devices that do not have a consistent prompt, or default Netmiko output collection functions are not reliable enough. This mode can be enabled by setting the <code>use_ps</code> parameter to <code>True</code>.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#parsing-commands-output","title":"Parsing Commands Output","text":"<p>When using Netmiko plugin the output of commands can be parsed using various parsers such as <code>textfsm</code>, <code>ttp</code> and <code>genie</code>. This allows you to convert the raw output into structured data. </p> <p>Using TTP parsing templates supported by all Netmiko, Scrapli and NAPALM connection plugins, to invoke TTP can us <code>run_ttp</code> command specifying path to parsing template stored on broker. </p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#filtering-commands-output","title":"Filtering Commands Output","text":"<p>The output of commands can be filtered to only include specific information. This can be done using <code>match</code> command with containment patterns.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#sending-new-line-character","title":"Sending New Line Character","text":"<p>You can send a new line character as part of the command to devices. This is useful for commands that require a new line to be executed properly. To send new-line character need to include <code>_br_</code> into command text.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#saving-task-results-to-files","title":"Saving Task Results to Files","text":"<p>The results of tasks can be saved to files for later analysis and record-keeping. This is particularly useful for maintaining logs of command outputs, configuration changes, and other important data. By saving task results to files, you can create a historical record of network operations, which can be invaluable for troubleshooting, auditing, and compliance purposes.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#using-diff-function-to-compare-results","title":"Using Diff Function to Compare Results","text":"<p>The diff function allows you to compare the results of different task results for same commands. This is useful for identifying changes in configurations or device state, detecting anomalies, and verifying the impact of network modifications. By using the diff function, you can ensure that your network remains consistent and identify any unintended changes that may have occurred.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#norfab-nornir-cli-shell-reference","title":"NORFAB Nornir CLI Shell Reference","text":"<p>The NorFab shell provides a comprehensive set of commands for the Nornir <code>cli</code> task, allowing you to perform various network utility functions. These commands include options for setting job timeouts, specifying connection parameters, and controlling the execution of CLI commands. The shell reference details the available commands and their descriptions, providing you with the flexibility to tailor the behavior of the tasks to meet your specific network management needs.</p> <pre><code>nf#man tree nornir.cli\nroot\n\u2514\u2500\u2500 nornir:    Nornir service\n    \u2514\u2500\u2500 cli:    Send CLI commands to devices\n        \u251c\u2500\u2500 timeout:    Job timeout\n        \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u251c\u2500\u2500 add-details:    Add task details to results, default 'False'\n        \u251c\u2500\u2500 num-workers:    RetryRunner number of threads for tasks execution\n        \u251c\u2500\u2500 num-connectors:    RetryRunner number of threads for device connections\n        \u251c\u2500\u2500 connect-retry:    RetryRunner number of connection attempts\n        \u251c\u2500\u2500 task-retry:    RetryRunner number of attempts to run task\n        \u251c\u2500\u2500 reconnect-on-fail:    RetryRunner perform reconnect to host on task failure\n        \u251c\u2500\u2500 connect-check:    RetryRunner test TCP connection before opening actual connection\n        \u251c\u2500\u2500 connect-timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n        \u251c\u2500\u2500 creds-retry:    RetryRunner list of connection credentials and parameters to retry\n        \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n        \u251c\u2500\u2500 tf-skip-failed:    Save results to file for failed tasks\n        \u251c\u2500\u2500 diff:    File group name to run the diff for\n        \u251c\u2500\u2500 diff-last:    File version number to diff, default is 1 (last)\n        \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n        \u251c\u2500\u2500 table:    Table format (brief, terse, extend) or parameters or True\n        \u251c\u2500\u2500 headers:    Table headers\n        \u251c\u2500\u2500 headers-exclude:    Table headers to exclude\n        \u251c\u2500\u2500 sortby:    Table header column to sort by\n        \u251c\u2500\u2500 reverse:    Table reverse the sort by order\n        \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n        \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u251c\u2500\u2500 FM:    Filter hosts by platform\n        \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n        \u251c\u2500\u2500 FN:    Negate the match\n        \u251c\u2500\u2500 hosts:    Filter hosts to target\n        \u251c\u2500\u2500 *commands:    List of commands to collect form devices\n        \u251c\u2500\u2500 plugin:    Connection plugin parameters\n        \u2502   \u251c\u2500\u2500 netmiko:    Use Netmiko plugin to configure devices\n        \u2502   \u2502   \u251c\u2500\u2500 enable:    Attempt to enter enable-mode\n        \u2502   \u2502   \u251c\u2500\u2500 use-timing:    switch to send command timing method\n        \u2502   \u2502   \u251c\u2500\u2500 expect-string:    Regular expression pattern to use for determining end of output\n        \u2502   \u2502   \u251c\u2500\u2500 read-timeout:    Maximum time to wait looking for pattern\n        \u2502   \u2502   \u251c\u2500\u2500 auto-find-prompt:    Use find_prompt() to override base prompt\n        \u2502   \u2502   \u251c\u2500\u2500 strip-prompt:    Remove the trailing router prompt from the output\n        \u2502   \u2502   \u251c\u2500\u2500 strip-command:    Remove the echo of the command from the output\n        \u2502   \u2502   \u251c\u2500\u2500 normalize:    Ensure the proper enter is sent at end of command\n        \u2502   \u2502   \u251c\u2500\u2500 use-textfsm:    Process command output through TextFSM template\n        \u2502   \u2502   \u251c\u2500\u2500 textfsm-template:    Name of template to parse output with\n        \u2502   \u2502   \u251c\u2500\u2500 use-ttp:    Process command output through TTP template\n        \u2502   \u2502   \u251c\u2500\u2500 ttp-template:    Name of template to parse output with\n        \u2502   \u2502   \u251c\u2500\u2500 use-genie:    Process command output through PyATS/Genie parser\n        \u2502   \u2502   \u251c\u2500\u2500 cmd-verify:    Verify command echo before proceeding\n        \u2502   \u2502   \u251c\u2500\u2500 use-ps:    Use send command promptless method\n        \u2502   \u2502   \u251c\u2500\u2500 use-ps-timeout:    Promptless mode absolute timeout\n        \u2502   \u2502   \u251c\u2500\u2500 split-lines:    Split multiline string to individual commands\n        \u2502   \u2502   \u251c\u2500\u2500 new-line-char:    Character to replace with new line before sending to device, default is _br_\n        \u2502   \u2502   \u251c\u2500\u2500 repeat:    Number of times to repeat the commands\n        \u2502   \u2502   \u251c\u2500\u2500 stop-pattern:    Stop commands repeat if output matches provided glob pattern\n        \u2502   \u2502   \u251c\u2500\u2500 repeat-interval:    Time in seconds to wait between repeating commands\n        \u2502   \u2502   \u2514\u2500\u2500 return-last:    Returns requested last number of commands outputs\n        \u2502   \u251c\u2500\u2500 scrapli:    Use Scrapli plugin to configure devices\n        \u2502   \u2502   \u251c\u2500\u2500 strip-prompt:    Strip prompt from returned output\n        \u2502   \u2502   \u251c\u2500\u2500 failed-when-contains:    String or list of strings indicating failure if found in response\n        \u2502   \u2502   \u251c\u2500\u2500 timeout-ops:    Timeout ops value for this operation\n        \u2502   \u2502   \u251c\u2500\u2500 interval:    Interval between sending commands\n        \u2502   \u2502   \u251c\u2500\u2500 split-lines:    Split multiline string to individual commands\n        \u2502   \u2502   \u251c\u2500\u2500 new-line-char:    Character to replace with new line before sending to device, default is _br_\n        \u2502   \u2502   \u251c\u2500\u2500 repeat:    Number of times to repeat the commands\n        \u2502   \u2502   \u251c\u2500\u2500 stop-pattern:    Stop commands repeat if output matches provided glob pattern\n        \u2502   \u2502   \u251c\u2500\u2500 repeat-interval:    Time in seconds to wait between repeating commands\n        \u2502   \u2502   \u2514\u2500\u2500 return-last:    Returns requested last number of commands outputs\n        \u2502   \u2514\u2500\u2500 napalm:    Use NAPALM plugin to configure devices\n        \u2502       \u251c\u2500\u2500 interval:    Interval between sending commands\n        \u2502       \u251c\u2500\u2500 split-lines:    Split multiline string to individual commands\n        \u2502       \u2514\u2500\u2500 new-line-char:    Character to replace with new line before sending to device, default is _br_\n        \u251c\u2500\u2500 dry-run:    Dry run the commands\n        \u251c\u2500\u2500 enable:    Enter exec mode\n        \u251c\u2500\u2500 run-ttp:    TTP Template to run\n        \u2514\u2500\u2500 job-data:    Path to YAML file with job data\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_cli/#python-api-reference","title":"Python API Reference","text":"<p>Task to collect/retrieve show commands output from network devices using Command Line Interface (CLI).</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>commands</code> <code>list</code> <p>List of commands to send to devices or URL to a file or template URL that resolves to a file.</p> <code>None</code> <code>plugin</code> <code>str</code> <p>Plugin name to use. Valid options are <code>netmiko</code>, <code>scrapli</code>, <code>napalm</code>.</p> <code>'netmiko'</code> <code>dry_run</code> <code>bool</code> <p>If True, do not send commands to devices, just return them.</p> <code>False</code> <code>run_ttp</code> <code>str</code> <p>TTP Template to run.</p> <code>None</code> <code>job_data</code> <code>str</code> <p>URL to YAML file with data or dictionary/list of data to pass on to Jinja2 rendering context.</p> <code>None</code> <code>to_dict</code> <code>bool</code> <p>If True, returns results as a dictionary.</p> <code>True</code> <code>add_details</code> <code>bool</code> <p>If True, adds task execution details to the results.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the specified task plugin.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary with the results of the CLI task.</p> <p>Raises:</p> Type Description <code>UnsupportedPluginError</code> <p>If the specified plugin is not supported.</p> <code>FileNotFoundError</code> <p>If the specified TTP template or job data file cannot be downloaded.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(\n    fastapi={\"methods\": [\"POST\"]},\n    input=NorniCliInput,\n)\ndef cli(\n    self,\n    job: Job,\n    commands: Union[str, list] = None,\n    plugin: str = \"netmiko\",\n    dry_run: bool = False,\n    run_ttp: str = None,\n    job_data: Any = None,\n    to_dict: bool = True,\n    add_details: bool = False,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Task to collect/retrieve show commands output from network devices using\n    Command Line Interface (CLI).\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        commands (list, optional): List of commands to send to devices or URL to a file or template\n            URL that resolves to a file.\n        plugin (str, optional): Plugin name to use. Valid options are\n            ``netmiko``, ``scrapli``, ``napalm``.\n        dry_run (bool, optional): If True, do not send commands to devices,\n            just return them.\n        run_ttp (str, optional): TTP Template to run.\n        job_data (str, optional): URL to YAML file with data or dictionary/list\n            of data to pass on to Jinja2 rendering context.\n        to_dict (bool, optional): If True, returns results as a dictionary.\n        add_details (bool, optional): If True, adds task execution details\n            to the results.\n        **kwargs: Additional arguments to pass to the specified task plugin.\n\n    Returns:\n        dict: A dictionary with the results of the CLI task.\n\n    Raises:\n        UnsupportedPluginError: If the specified plugin is not supported.\n        FileNotFoundError: If the specified TTP template or job data file\n            cannot be downloaded.\n    \"\"\"\n    job_data = job_data or {}\n    timeout = job.timeout * 0.9\n    ret = Result(task=f\"{self.name}:cli\", result={} if to_dict else [])\n\n    filtered_nornir, no_match_result = self.filter_hosts_and_validate(kwargs, ret)\n    if ret.status == \"no_match\":\n        return ret\n\n    # decide on what send commands task plugin to use\n    if plugin == \"netmiko\":\n        task_plugin = netmiko_send_commands\n        if kwargs.get(\"use_ps\"):\n            kwargs.setdefault(\"timeout\", timeout)\n        else:\n            kwargs.setdefault(\"read_timeout\", timeout)\n    elif plugin == \"scrapli\":\n        task_plugin = scrapli_send_commands\n        kwargs.setdefault(\"timeout_ops\", timeout)\n    elif plugin == \"napalm\":\n        task_plugin = napalm_send_commands\n    else:\n        raise UnsupportedPluginError(f\"Plugin '{plugin}' not supported\")\n\n    # download TTP template\n    if self.is_url(run_ttp):\n        downloaded = self.fetch_file(run_ttp)\n        kwargs[\"run_ttp\"] = downloaded\n        if downloaded is None:\n            msg = f\"{self.name} - TTP template download failed '{run_ttp}'\"\n            raise FileNotFoundError(msg)\n    # use TTP template as is - inline template or ttp://xyz path\n    elif run_ttp:\n        kwargs[\"run_ttp\"] = run_ttp\n\n    # download job data\n    job_data = self.load_job_data(job_data)\n\n    nr = self._add_processors(filtered_nornir, kwargs, job)  # add processors\n\n    # render commands using Jinja2 on a per-host basis\n    if commands:\n        commands = commands if isinstance(commands, list) else [commands]\n        for host in nr.inventory.hosts.values():\n            rendered = self.jinja2_render_templates(\n                templates=commands,\n                context={\n                    \"host\": host,\n                    \"norfab\": self.client,\n                    \"job_data\": job_data,\n                    \"netbox\": self.add_jinja2_netbox(),\n                },\n                filters=self.add_jinja2_filters(),\n            )\n            host.data[\"__task__\"] = {\"commands\": rendered}\n\n    # run task\n    log.debug(\n        f\"{self.name} - running cli commands '{commands}', kwargs '{kwargs}', is cli dry run - '{dry_run}'\"\n    )\n    if dry_run is True:\n        result = nr.run(\n            task=nr_test, use_task_data=\"commands\", name=\"dry_run\", **kwargs\n        )\n    else:\n        with self.connections_lock:\n            result = nr.run(task=task_plugin, **kwargs)\n\n    ret.failed = result.failed  # failed is true if any of the hosts failed\n    ret.result = ResultSerializer(result, to_dict=to_dict, add_details=add_details)\n\n    # remove __task__ data\n    for host_name, host_object in nr.inventory.hosts.items():\n        _ = host_object.data.pop(\"__task__\", None)\n\n    self.watchdog.connections_update(nr, plugin)\n    self.watchdog.connections_clean()\n\n    return ret\n</code></pre>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_diagram/","title":"Nornir Service Diagram Task","text":"<p>task api name: <code>diagram</code></p> <p>The Nornir Service Diagram Task is a powerful component of NorFab's Nornir service, designed to create detailed network diagrams. By leveraging the N2G (Need to Graph) module, this task enables network engineers and architects to visualize network topologies and configurations, facilitating better network management and planning.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_diagram/#creating-layer-2-network-diagram","title":"Creating Layer-2 Network Diagram","text":"<p>Layer-2 network diagrams illustrate the data link layer of the OSI model, showing how devices are interconnected within a local area network (LAN) based on the output provided by LLDP and CDP protocols. These diagrams are essential for understanding the physical and logical connections between switches, routers, and other network devices. By creating Layer-2 network diagrams, you can identify potential bottlenecks, optimize traffic flow, and ensure efficient network design. The Nornir Service Diagram Task uses the N2G module to automatically generate these diagrams, providing a clear and accurate representation of your Layer-2 topology.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_diagram/#creating-layer-3-network-diagram","title":"Creating Layer-3 Network Diagram","text":"<p>Layer-3 network diagrams depict the network layer of the OSI model, highlighting the routing and IP addressing within a network. These diagrams are crucial for understanding how data is routed between different subnets and networks. By creating Layer-3 network diagrams, you can visualize the routing paths, identify potential routing issues, and ensure proper IP address allocation. The Nornir Service Diagram Task leverages the N2G module to construct these diagrams, offering a comprehensive view of your Layer-3 network infrastructure.</p> <p>Example</p> CLI <pre><code>nf#\nnf#\nnf#nornir\nnf[nornir]#diagram\nnf[nornir-diagram]#layer3 FC spine,leaf\n--------------------------------------------- Job Events -----------------------------------------------\n04-Jan-2025 22:59:56 85fd42146327446cae3c26ceb2077abf job started\n04-Jan-2025 22:59:56.664 nornir nornir-worker-1 ceos-spine-1, ceos-spine-2 task started - 'netmiko_send_commands'\n&lt;omitted for brevity&gt;\n04-Jan-2025 22:59:58 85fd42146327446cae3c26ceb2077abf job completed in 2.117 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\ndiagram: 'layer3', format: 'yed'\nsaved at: './diagrams\\layer3_2025-01-04_22-59-56.graphml'\nhosts: ceos-leaf-1, ceos-leaf-2, ceos-leaf-3, ceos-spine-1, ceos-spine-2\nnf[nornir-diagram]#\n</code></pre> <p>Demo</p> <p></p> <p>In this example:</p> <ul> <li><code>nornir</code> command switches to the Nornir sub-shell.</li> <li><code>diagram</code> command switches to the diagram task sub-shell.</li> <li><code>layer3</code> command run commands output collection for devices that have <code>spine</code> or <code>leaf</code> in their hostname as we use <code>FC</code> - \"Filter Contains\" Nornir hosts targeting filter, once output collected N2G parses commands output and constructs L3 Network diagram of subnets and IP addresses saving diagram in yEd compatible format at <code>./diagrams\\layer3_2025-01-04_22-59-56.graphml</code> file.</li> </ul>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_diagram/#creating-ospf-routing-protocol-network-diagram","title":"Creating OSPF Routing Protocol Network Diagram","text":"<p>OSPF (Open Shortest Path First) is a widely used interior gateway protocol for routing within an autonomous system. Creating OSPF routing protocol network diagrams helps you visualize the OSPF areas, router adjacencies, and link metrics. These diagrams are useful for troubleshooting OSPF-related issues, optimizing OSPF configurations, and ensuring efficient routing. The Nornir Service Diagram Task utilizes the N2G module to generate OSPF network diagrams, providing a detailed view of your OSPF topology and configurations.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_diagram/#creating-isis-routing-protocol-network-diagram","title":"Creating ISIS Routing Protocol Network Diagram","text":"<p>ISIS (Intermediate System to Intermediate System) is a popular interior gateway protocol used for routing within large networks. Creating ISIS routing protocol network diagrams allows you to visualize the ISIS areas, router adjacencies, and link metrics. These diagrams are vital for understanding the ISIS routing process, identifying potential issues and optimizing the network. The Nornir Service Diagram Task utilizes the N2G module to generate ISIS network diagrams, providing a detailed view of your ISIS topology and configurations.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_diagram/#creating-drawio-diagrams","title":"Creating draw.io Diagrams","text":"<p>N2G module can produce diagrams in several formats, to create draw.io diagram need to use <code>format</code> argument with <code>drawio</code> value.</p> <p>Example</p> CLI <pre><code>nf#\nnf#\nnf#nornir\nnf[nornir]#diagram\nnf[nornir-diagram]#format drawio layer3 FC spine,leaf\n--------------------------------------------- Job Events -----------------------------------------------\n04-Jan-2025 23:16:13 a2d39b5b1268488a95805baed96699a1 job started\n04-Jan-2025 23:16:14.277 nornir nornir-worker-1 ceos-spine-1, ceos-spine-2 task started - 'netmiko_send_commands'\n04-Jan-2025 23:16:14.289 nornir nornir-worker-2 ceos-leaf-1, ceos-leaf-2, ceos-leaf-3 task started - 'netmiko_send_commands'\n&lt;omitted for brevity&gt;\n04-Jan-2025 23:16:16 a2d39b5b1268488a95805baed96699a1 job completed in 2.606 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\ndiagram: 'layer3', format: 'drawio'\nsaved at: './diagrams\\layer3_2025-01-04_23-16-13.drawio'\nhosts: ceos-leaf-1, ceos-leaf-2, ceos-leaf-3, ceos-spine-1, ceos-spine-2\nnf[nornir-diagram]#\n</code></pre> <ul> <li><code>nornir</code> command switches to the Nornir sub-shell.</li> <li><code>diagram</code> command switches to the diagram task sub-shell.</li> <li><code>format</code> argument specifies what diagram format to create, draw.io in this case.</li> <li><code>layer3</code> command run commands output collection for devices that have <code>spine</code> or <code>leaf</code> in their hostname as we use <code>FC</code> - \"Filter Contains\" Nornir hosts targeting filter, once output collected N2G parses commands output and constructs L3 Network diagram of subnets and IP addresses saving diagram in draw.io compatible format at <code>./diagrams\\layer3_2025-01-04_23-16-13.drawio</code> file.</li> </ul>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_diagram/#norfab-nornir-diagram-shell-reference","title":"NORFAB Nornir Diagram Shell Reference","text":"<p>NorFab shell supports these command options for Nornir <code>diagram</code> task:</p> <pre><code>nf#man tree nornir.diagram\nroot\n\u2514\u2500\u2500 nornir:    Nornir service\n    \u2514\u2500\u2500 diagram:    Produce network diagrams\n        \u251c\u2500\u2500 timeout:    Job timeout\n        \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u251c\u2500\u2500 format:    Diagram application format, default 'yed'\n        \u251c\u2500\u2500 layer3:    Create L3 Network diagram using IP data\n        \u2502   \u251c\u2500\u2500 add-details:    Add task details to results, default 'False'\n        \u2502   \u251c\u2500\u2500 num-workers:    RetryRunner number of threads for tasks execution\n        \u2502   \u251c\u2500\u2500 num-connectors:    RetryRunner number of threads for device connections\n        \u2502   \u251c\u2500\u2500 connect-retry:    RetryRunner number of connection attempts\n        \u2502   \u251c\u2500\u2500 task-retry:    RetryRunner number of attempts to run task\n        \u2502   \u251c\u2500\u2500 reconnect-on-fail:    RetryRunner perform reconnect to host on task failure\n        \u2502   \u251c\u2500\u2500 connect-check:    RetryRunner test TCP connection before opening actual connection\n        \u2502   \u251c\u2500\u2500 connect-timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n        \u2502   \u251c\u2500\u2500 creds-retry:    RetryRunner list of connection credentials and parameters to retry\n        \u2502   \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n        \u2502   \u251c\u2500\u2500 tf-skip-failed:    Save results to file for failed tasks\n        \u2502   \u251c\u2500\u2500 diff:    File group name to run the diff for\n        \u2502   \u251c\u2500\u2500 diff-last:    File version number to diff, default is 1 (last)\n        \u2502   \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n        \u2502   \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n        \u2502   \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u2502   \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u2502   \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u2502   \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u2502   \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u2502   \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u2502   \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u2502   \u251c\u2500\u2500 FM:    Filter hosts by platform\n        \u2502   \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n        \u2502   \u251c\u2500\u2500 FN:    Negate the match\n        \u2502   \u251c\u2500\u2500 hosts:    Filter hosts to target\n        \u2502   \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u2502   \u251c\u2500\u2500 group_links:    Group links between same nodes\n        \u2502   \u251c\u2500\u2500 add-arp:    Add IP nodes from ARP cache parsing results\n        \u2502   \u251c\u2500\u2500 label-interface:    Add interface name to the link\u2019s source and target labels\n        \u2502   \u251c\u2500\u2500 label-vrf:    Add VRF name to the link\u2019s source and target labels\n        \u2502   \u251c\u2500\u2500 collapse-ptp:    Combines links for /31 and /30 IPv4 and /127 IPv6 subnets into a single link\n        \u2502   \u251c\u2500\u2500 add-fhrp:    Add HSRP and VRRP IP addresses to the diagram\n        \u2502   \u251c\u2500\u2500 bottom-label-length:    Length of interface description to use for subnet labels, if 0, label not set\n        \u2502   \u2514\u2500\u2500 lbl-next-to-subnet:    Put link port:vrf:ip label next to subnet node\n        \u251c\u2500\u2500 layer2:    Create L2 Network diagram using CDP/LLDP data\n        \u2502   \u251c\u2500\u2500 add-details:    Add task details to results, default 'False'\n        \u2502   \u251c\u2500\u2500 num-workers:    RetryRunner number of threads for tasks execution\n        \u2502   \u251c\u2500\u2500 num-connectors:    RetryRunner number of threads for device connections\n        \u2502   \u251c\u2500\u2500 connect-retry:    RetryRunner number of connection attempts\n        \u2502   \u251c\u2500\u2500 task-retry:    RetryRunner number of attempts to run task\n        \u2502   \u251c\u2500\u2500 reconnect-on-fail:    RetryRunner perform reconnect to host on task failure\n        \u2502   \u251c\u2500\u2500 connect-check:    RetryRunner test TCP connection before opening actual connection\n        \u2502   \u251c\u2500\u2500 connect-timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n        \u2502   \u251c\u2500\u2500 creds-retry:    RetryRunner list of connection credentials and parameters to retry\n        \u2502   \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n        \u2502   \u251c\u2500\u2500 tf-skip-failed:    Save results to file for failed tasks\n        \u2502   \u251c\u2500\u2500 diff:    File group name to run the diff for\n        \u2502   \u251c\u2500\u2500 diff-last:    File version number to diff, default is 1 (last)\n        \u2502   \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n        \u2502   \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n        \u2502   \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u2502   \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u2502   \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u2502   \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u2502   \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u2502   \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u2502   \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u2502   \u251c\u2500\u2500 FM:    Filter hosts by platform\n        \u2502   \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n        \u2502   \u251c\u2500\u2500 FN:    Negate the match\n        \u2502   \u251c\u2500\u2500 hosts:    Filter hosts to target\n        \u2502   \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u2502   \u251c\u2500\u2500 add-interfaces-data:    Add interfaces configuration and state data to links\n        \u2502   \u251c\u2500\u2500 group-links:    Group links between nodes\n        \u2502   \u251c\u2500\u2500 add-lag:    Add LAG/MLAG links to diagram\n        \u2502   \u251c\u2500\u2500 add-all-connected:    Add all nodes connected to devices based on interfaces state\n        \u2502   \u251c\u2500\u2500 combine-peers:    Combine CDP/LLDP peers behind same interface by adding L2 node\n        \u2502   \u2514\u2500\u2500 skip-lag:    Skip CDP peers for LAG, some platforms send CDP/LLDP PDU from LAG ports\n        \u251c\u2500\u2500 isis:    Create ISIS Network diagram using LSDB data\n        \u2502   \u251c\u2500\u2500 add-details:    Add task details to results, default 'False'\n        \u2502   \u251c\u2500\u2500 num-workers:    RetryRunner number of threads for tasks execution\n        \u2502   \u251c\u2500\u2500 num-connectors:    RetryRunner number of threads for device connections\n        \u2502   \u251c\u2500\u2500 connect-retry:    RetryRunner number of connection attempts\n        \u2502   \u251c\u2500\u2500 task-retry:    RetryRunner number of attempts to run task\n        \u2502   \u251c\u2500\u2500 reconnect-on-fail:    RetryRunner perform reconnect to host on task failure\n        \u2502   \u251c\u2500\u2500 connect-check:    RetryRunner test TCP connection before opening actual connection\n        \u2502   \u251c\u2500\u2500 connect-timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n        \u2502   \u251c\u2500\u2500 creds-retry:    RetryRunner list of connection credentials and parameters to retry\n        \u2502   \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n        \u2502   \u251c\u2500\u2500 tf-skip-failed:    Save results to file for failed tasks\n        \u2502   \u251c\u2500\u2500 diff:    File group name to run the diff for\n        \u2502   \u251c\u2500\u2500 diff-last:    File version number to diff, default is 1 (last)\n        \u2502   \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n        \u2502   \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n        \u2502   \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u2502   \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u2502   \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u2502   \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u2502   \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u2502   \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u2502   \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u2502   \u251c\u2500\u2500 FM:    Filter hosts by platform\n        \u2502   \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n        \u2502   \u251c\u2500\u2500 FN:    Negate the match\n        \u2502   \u251c\u2500\u2500 hosts:    Filter hosts to target\n        \u2502   \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u2502   \u251c\u2500\u2500 ip-lookup-data:    IP Lookup dictionary or OS path to CSV file\n        \u2502   \u251c\u2500\u2500 add-connected:    Add connected subnets as nodes\n        \u2502   \u251c\u2500\u2500 ptp-filter:    List of glob patterns to filter point-to-point links based on link IP\n        \u2502   \u2514\u2500\u2500 add-data:    Add data information to nodes and links\n        \u251c\u2500\u2500 ospf:    Create OSPF Network diagram using LSDB data\n        \u2502   \u251c\u2500\u2500 add-details:    Add task details to results, default 'False'\n        \u2502   \u251c\u2500\u2500 num-workers:    RetryRunner number of threads for tasks execution\n        \u2502   \u251c\u2500\u2500 num-connectors:    RetryRunner number of threads for device connections\n        \u2502   \u251c\u2500\u2500 connect-retry:    RetryRunner number of connection attempts\n        \u2502   \u251c\u2500\u2500 task-retry:    RetryRunner number of attempts to run task\n        \u2502   \u251c\u2500\u2500 reconnect-on-fail:    RetryRunner perform reconnect to host on task failure\n        \u2502   \u251c\u2500\u2500 connect-check:    RetryRunner test TCP connection before opening actual connection\n        \u2502   \u251c\u2500\u2500 connect-timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n        \u2502   \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n        \u2502   \u251c\u2500\u2500 tf-skip-failed:    Save results to file for failed tasks\n        \u2502   \u251c\u2500\u2500 diff:    File group name to run the diff for\n        \u2502   \u251c\u2500\u2500 diff-last:    File version number to diff, default is 1 (last)\n        \u2502   \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n        \u2502   \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n        \u2502   \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u2502   \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u2502   \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u2502   \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u2502   \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u2502   \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u2502   \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u2502   \u251c\u2500\u2500 FM:    Filter hosts by platform\n        \u2502   \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n        \u2502   \u251c\u2500\u2500 FN:    Negate the match\n        \u2502   \u251c\u2500\u2500 hosts:    Filter hosts to target\n        \u2502   \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u2502   \u251c\u2500\u2500 ip-lookup-data:    IP Lookup dictionary or OS path to CSV file\n        \u2502   \u251c\u2500\u2500 add-connected:    Add connected subnets as nodes\n        \u2502   \u251c\u2500\u2500 ptp-filter:    List of glob patterns to filter point-to-point links based on link IP\n        \u2502   \u2514\u2500\u2500 add-data:    Add data information to nodes and links\n        \u2514\u2500\u2500 filename:    Name of the file to save diagram content\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_file_copy/","title":"Nornir Service File Copy Task","text":"<p>task api name: <code>file_copy</code></p> <p>The Nornir Service File Copy Task is a component of NorFab's Nornir service, designed to facilitate the transfer of files to and from network devices. This task provides network engineers with a reliable and efficient method for managing device configurations, firmware updates, and other critical files. By leveraging the capabilities of the Nornir service, users can automate file transfers.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_file_copy/#nornir-file-copy-sample-usage","title":"Nornir File Copy Sample Usage","text":"","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_file_copy/#norfab-nornir-file-copy-shell-reference","title":"NORFAB Nornir File Copy Shell Reference","text":"<p>NorFab shell supports these command options for Nornir <code>file-copy</code> task:</p> <pre><code>nf#man tree nornir.file_copy\nroot\n\u2514\u2500\u2500 nornir:    Nornir service\n    \u2514\u2500\u2500 file-copy:    Copy files to/from devices\n        \u251c\u2500\u2500 timeout:    Job timeout\n        \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u251c\u2500\u2500 add-details:    Add task details to results, default 'False'\n        \u251c\u2500\u2500 num-workers:    RetryRunner number of threads for tasks execution\n        \u251c\u2500\u2500 num-connectors:    RetryRunner number of threads for device connections\n        \u251c\u2500\u2500 connect-retry:    RetryRunner number of connection attempts\n        \u251c\u2500\u2500 task-retry:    RetryRunner number of attempts to run task\n        \u251c\u2500\u2500 reconnect-on-fail:    RetryRunner perform reconnect to host on task failure\n        \u251c\u2500\u2500 connect-check:    RetryRunner test TCP connection before opening actual connection\n        \u251c\u2500\u2500 connect-timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n        \u251c\u2500\u2500 creds-retry:    RetryRunner list of connection credentials and parameters to retry\n        \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n        \u251c\u2500\u2500 tf-skip-failed:    Save results to file for failed tasks\n        \u251c\u2500\u2500 diff:    File group name to run the diff for\n        \u251c\u2500\u2500 diff-last:    File version number to diff, default is 1 (last)\n        \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n        \u251c\u2500\u2500 table:    Table format (brief, terse, extend) or parameters or True\n        \u251c\u2500\u2500 headers:    Table headers\n        \u251c\u2500\u2500 headers-exclude:    Table headers to exclude\n        \u251c\u2500\u2500 sortby:    Table header column to sort by\n        \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n        \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u251c\u2500\u2500 FM:    Filter hosts by platform\n        \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n        \u251c\u2500\u2500 FN:    Negate the match\n        \u251c\u2500\u2500 hosts:    Filter hosts to target\n        \u251c\u2500\u2500 *source-file:    Source file to copy\n        \u251c\u2500\u2500 plugin:    Connection plugin parameters\n        \u2502   \u2514\u2500\u2500 netmiko:    Use Netmiko plugin to copy files\n        \u2502       \u251c\u2500\u2500 destination-file:    Destination file to copy\n        \u2502       \u251c\u2500\u2500 file-system:    Destination file system\n        \u2502       \u251c\u2500\u2500 direction:    Direction of file copy, default 'put'\n        \u2502       \u251c\u2500\u2500 inline-transfer:    Use inline transfer, supported by Cisco IOS, default 'False'\n        \u2502       \u251c\u2500\u2500 overwrite-file:    Overwrite destination file if it exists, default 'False'\n        \u2502       \u251c\u2500\u2500 socket-timeout:    Socket timeout in seconds, default '10.0'\n        \u2502       \u2514\u2500\u2500 verify-file:    Verify destination file hash after copy, default 'True'\n        \u2514\u2500\u2500 dry-run:    Do not copy files, just show what would be done, default 'False'\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_file_copy/#python-api-reference","title":"Python API Reference","text":"<p>Task to transfer files to and from hosts using SCP.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>source_file</code> <code>str</code> <p>The path or URL of the source file to be copied in <code>nf://path/to/file</code> format</p> required <code>plugin</code> <code>str</code> <p>The plugin to use for file transfer. Supported plugins:</p> <ul> <li>netmiko - uses <code>netmiko_file_transfer</code> task plugin.</li> </ul> <code>'netmiko'</code> <code>to_dict</code> <code>bool</code> <p>Whether to return the result as a dictionary. Defaults to True.</p> <code>True</code> <code>add_details</code> <code>bool</code> <p>Whether to add detailed information to the result. Defaults to False.</p> <code>False</code> <code>dry_run</code> <code>bool</code> <p>If True, performs a dry run without making any changes. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the file transfer plugin.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>The result of the file copy operation.</p> <p>Raises:</p> Type Description <code>UnsupportedPluginError</code> <p>If the specified plugin is not supported.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef file_copy(\n    self,\n    job: Job,\n    source_file: str,\n    plugin: str = \"netmiko\",\n    to_dict: bool = True,\n    add_details: bool = False,\n    dry_run: bool = False,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Task to transfer files to and from hosts using SCP.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        source_file (str): The path or URL of the source file to be copied in\n            ``nf://path/to/file`` format\n        plugin (str, optional): The plugin to use for file transfer. Supported plugins:\n\n            - netmiko - uses `netmiko_file_transfer` task plugin.\n\n        to_dict (bool, optional): Whether to return the result as a dictionary. Defaults to True.\n        add_details (bool, optional): Whether to add detailed information to the result. Defaults to False.\n        dry_run (bool, optional): If True, performs a dry run without making any changes. Defaults to False.\n        **kwargs: Additional arguments to pass to the file transfer plugin.\n\n    Returns:\n        dict: The result of the file copy operation.\n\n    Raises:\n        UnsupportedPluginError: If the specified plugin is not supported.\n    \"\"\"\n    timeout = job.timeout * 0.9\n    ret = Result(task=f\"{self.name}:file_copy\", result={} if to_dict else [])\n\n    filtered_nornir, ret = self.filter_hosts_and_validate(kwargs, ret)\n    if ret.status == \"no_match\":\n        return ret\n\n    # download file from broker\n    if self.is_url(source_file):\n        source_file_local = self.fetch_file(\n            source_file, raise_on_fail=True, read=False\n        )\n\n    # decide on what send commands task plugin to use\n    if plugin == \"netmiko\":\n        task_plugin = netmiko_file_transfer\n        kwargs[\"source_file\"] = source_file_local\n        kwargs.setdefault(\"socket_timeout\", timeout / 5)\n        kwargs.setdefault(\"dest_file\", os.path.split(source_file_local)[-1])\n    else:\n        raise UnsupportedPluginError(f\"Plugin '{plugin}' not supported\")\n\n    nr = self._add_processors(filtered_nornir, kwargs, job)  # add processors\n\n    # run task\n    log.debug(\n        f\"{self.name} - running file copy with arguments '{kwargs}', is dry run - '{dry_run}'\"\n    )\n    if dry_run is True:\n        result = nr.run(task=nr_test, name=\"file_copy_dry_run\", **kwargs)\n    else:\n        with self.connections_lock:\n            result = nr.run(task=task_plugin, **kwargs)\n\n    ret.failed = result.failed  # failed is true if any of the hosts failed\n    ret.result = ResultSerializer(result, to_dict=to_dict, add_details=add_details)\n\n    self.watchdog.connections_update(nr, plugin)\n    self.watchdog.connections_clean()\n\n    return ret\n</code></pre>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_network/","title":"Nornir Service Network Task","text":"<p>task api name: <code>network</code></p> <p>The Nornir Service Network Task is a component of NorFab's Nornir service designed to facilitate various network-related operations. This task suite provides network professionals with essential tools for managing, troubleshooting, and monitoring network infrastructure. By leveraging the capabilities of the Nornir service, users can perform critical network functions such as ICMP echo requests (ping) and DNS resolution checks, ensuring the reliability and performance of their network devices and services.</p> <p>Key features of the Nornir Service Network Task include:</p> <ul> <li> <p>Network Ping: This task allows you to perform ICMP echo requests to verify the reachability of network devices. </p> </li> <li> <p>DNS Testing: This task enables you to perform DNS resolution checks to ensure that domain names are correctly mapped to their respective IP addresses. </p> </li> </ul> <p>The document also includes a reference for the NorFab shell commands related to the Nornir <code>network</code> task, detailing the available options and parameters. These commands provide granular control over the execution of network tasks, enabling users to tailor the behavior of the tasks to meet specific network management needs.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_network/#network-ping","title":"Network Ping","text":"<p>The Network Ping task in NorFab's Nornir service allows you to perform ICMP echo requests (pings) to verify the reachability of network devices. This task is essential for network troubleshooting and monitoring, as it helps you determine if a device is online and responsive. The ping task can be customized with various parameters such as timeout, number of retries, payload size and others. By using the ping task, you can quickly identify connectivity issues and ensure that your network devices are functioning correctly.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_network/#dns-testing","title":"DNS Testing","text":"<p>The DNS Testing task in NorFab's Nornir service enables you to perform DNS resolution checks to verify that domain names are correctly mapped to their respective IP addresses. This task is crucial for ensuring that your DNS infrastructure is working as expected and that your network services are accessible via their domain names. The DNS testing task can be configured with different parameters to control the behavior of the DNS queries, such as specifying the DNS server to use, query timeout, and the type of DNS record to query. By performing DNS tests, you can proactively identify and resolve DNS-related issues, ensuring seamless network operations.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_network/#norfab-nornir-network-shell-reference","title":"NORFAB Nornir Network Shell Reference","text":"<p>NorFab shell supports these command options for Nornir <code>network</code> task:</p> <pre><code>nf#man tree nornir.network\nroot\n\u2514\u2500\u2500 nornir:    Nornir service\n    \u2514\u2500\u2500 network:    Network utility functions - ping, dns etc.\n        \u251c\u2500\u2500 ping:    Ping devices\n        \u2502   \u251c\u2500\u2500 timeout:    Job timeout\n        \u2502   \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u2502   \u251c\u2500\u2500 add-details:    Add task details to results, default 'False'\n        \u2502   \u251c\u2500\u2500 num-workers:    RetryRunner number of threads for tasks execution\n        \u2502   \u251c\u2500\u2500 num-connectors:    RetryRunner number of threads for device connections\n        \u2502   \u251c\u2500\u2500 connect-retry:    RetryRunner number of connection attempts\n        \u2502   \u251c\u2500\u2500 task-retry:    RetryRunner number of attempts to run task\n        \u2502   \u251c\u2500\u2500 reconnect-on-fail:    RetryRunner perform reconnect to host on task failure\n        \u2502   \u251c\u2500\u2500 connect-check:    RetryRunner test TCP connection before opening actual connection\n        \u2502   \u251c\u2500\u2500 connect-timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n        \u2502   \u251c\u2500\u2500 creds-retry:    RetryRunner list of connection credentials and parameters to retry\n        \u2502   \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n        \u2502   \u251c\u2500\u2500 tf-skip-failed:    Save results to file for failed tasks\n        \u2502   \u251c\u2500\u2500 diff:    File group name to run the diff for\n        \u2502   \u251c\u2500\u2500 diff-last:    File version number to diff, default is 1 (last)\n        \u2502   \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n        \u2502   \u251c\u2500\u2500 table:    Table format (brief, terse, extend) or parameters or True\n        \u2502   \u251c\u2500\u2500 headers:    Table headers\n        \u2502   \u251c\u2500\u2500 headers-exclude:    Table headers to exclude\n        \u2502   \u251c\u2500\u2500 sortby:    Table header column to sort by\n        \u2502   \u251c\u2500\u2500 reverse:    Table reverse the sort by order\n        \u2502   \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n        \u2502   \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u2502   \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u2502   \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u2502   \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u2502   \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u2502   \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u2502   \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u2502   \u251c\u2500\u2500 FM:    Filter hosts by platform\n        \u2502   \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n        \u2502   \u251c\u2500\u2500 FN:    Negate the match\n        \u2502   \u251c\u2500\u2500 hosts:    Filter hosts to target\n        \u2502   \u251c\u2500\u2500 use-host-name:    Ping host's name instead of host's hostname\n        \u2502   \u251c\u2500\u2500 count:    Number of pings to run\n        \u2502   \u251c\u2500\u2500 ping-timeout:    Time in seconds before considering each non-arrived reply permanently lost\n        \u2502   \u251c\u2500\u2500 size:    Size of the entire packet to send\n        \u2502   \u251c\u2500\u2500 interval:    Interval to wait between pings\n        \u2502   \u251c\u2500\u2500 payload:    Payload content if size is not set\n        \u2502   \u251c\u2500\u2500 sweep-start:    If size is not set, initial size in a sweep of sizes\n        \u2502   \u251c\u2500\u2500 sweep-end:    If size is not set, final size in a sweep of sizes\n        \u2502   \u251c\u2500\u2500 df:    Don't Fragment flag value for IP Header\n        \u2502   \u251c\u2500\u2500 match:    Do payload matching between request and reply\n        \u2502   \u2514\u2500\u2500 source:    Source IP address\n        \u2514\u2500\u2500 dns:    Resolve DNS\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n            \u251c\u2500\u2500 add-details:    Add task details to results, default 'False'\n            \u251c\u2500\u2500 num-workers:    RetryRunner number of threads for tasks execution\n            \u251c\u2500\u2500 num-connectors:    RetryRunner number of threads for device connections\n            \u251c\u2500\u2500 connect-retry:    RetryRunner number of connection attempts\n            \u251c\u2500\u2500 task-retry:    RetryRunner number of attempts to run task\n            \u251c\u2500\u2500 reconnect-on-fail:    RetryRunner perform reconnect to host on task failure\n            \u251c\u2500\u2500 connect-check:    RetryRunner test TCP connection before opening actual connection\n            \u251c\u2500\u2500 connect-timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n            \u251c\u2500\u2500 creds-retry:    RetryRunner list of connection credentials and parameters to retry\n            \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n            \u251c\u2500\u2500 tf-skip-failed:    Save results to file for failed tasks\n            \u251c\u2500\u2500 diff:    File group name to run the diff for\n            \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n            \u251c\u2500\u2500 table:    Table format (brief, terse, extend) or parameters or True\n            \u251c\u2500\u2500 headers:    Table headers\n            \u251c\u2500\u2500 headers-exclude:    Table headers to exclude\n            \u251c\u2500\u2500 sortby:    Table header column to sort by\n            \u251c\u2500\u2500 reverse:    Table reverse the sort by order\n            \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n            \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n            \u251c\u2500\u2500 FH:    Filter hosts by hostname\n            \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n            \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n            \u251c\u2500\u2500 FG:    Filter hosts by group\n            \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n            \u251c\u2500\u2500 FL:    Filter hosts by names list\n            \u251c\u2500\u2500 FM:    Filter hosts by platform\n            \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n            \u251c\u2500\u2500 FN:    Negate the match\n            \u251c\u2500\u2500 hosts:    Filter hosts to target\n            \u251c\u2500\u2500 use-host-name:    Ping host's name instead of host's hostname\n            \u251c\u2500\u2500 servers:    List of DNS servers to use\n            \u251c\u2500\u2500 dns-timeout:    Time in seconds before considering request lost\n            \u251c\u2500\u2500 ipv4:    Resolve 'A' record\n            \u2514\u2500\u2500 ipv6:    Resolve 'AAAA' record\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_network/#python-api-reference","title":"Python API Reference","text":"<p>Task to call various network-related utility functions.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>fun</code> <code>str</code> <p>The name of the utility function to call.</p> required <code>kwargs</code> <code>dict</code> <p>Arguments to pass to the utility function.</p> <code>{}</code> <p>Available utility functions:</p> <ul> <li>resolve_dns Resolves hosts' hostname DNS, returning IP addresses using     <code>nornir_salt.plugins.tasks.network.resolve_dns</code> Nornir-Salt function.</li> <li>ping Executes ICMP ping to host using <code>nornir_salt.plugins.tasks.network.ping</code>     Nornir-Salt function.</li> </ul> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the results of the network utility function.</p> <p>Raises:</p> Type Description <code>UnsupportedPluginError</code> <p>If the specified utility function is not supported.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef network(self, job: Job, fun: str, **kwargs) -&gt; Result:\n    \"\"\"\n    Task to call various network-related utility functions.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        fun (str): The name of the utility function to call.\n        kwargs (dict): Arguments to pass to the utility function.\n\n    Available utility functions:\n\n    - **resolve_dns** Resolves hosts' hostname DNS, returning IP addresses using\n        `nornir_salt.plugins.tasks.network.resolve_dns` Nornir-Salt function.\n    - **ping** Executes ICMP ping to host using `nornir_salt.plugins.tasks.network.ping`\n        Nornir-Salt function.\n\n    Returns:\n        dict: A dictionary containing the results of the network utility function.\n\n    Raises:\n        UnsupportedPluginError: If the specified utility function is not supported.\n    \"\"\"\n    kwargs[\"call\"] = fun\n    return self.task(\n        job=job,\n        plugin=\"nornir_salt.plugins.tasks.network\",\n        **kwargs,\n    )\n</code></pre>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_parse/","title":"Nornir Service Parse Task","text":"<p>task api name: <code>parse</code></p> <p>The Nornir Service Parse Task is an integral part of NorFab's Nornir service, designed to facilitate the parsing and extraction of valuable information from network device outputs. This task provides network automation and developer engineers with powerful tools to transform raw command outputs into structured data, enabling more efficient network management and automation workflows.</p> <p>Key features of the Nornir Service Parse Task include:</p> <ul> <li> <p>TextFSM Parsing: This task allows you to use TextFSM templates to parse command outputs into structured data. TextFSM is a powerful text processing tool that uses templates to define how to extract data from unstructured text. By leveraging TextFSM, you can convert complex command outputs into easily readable and processable data formats, which can then be used for further analysis or automation tasks.</p> </li> <li> <p>TTP Parsing: The Template Text Parser (TTP) is a robust parsing tool supported by the Nornir Service Parse Task. TTP allows you to define templates for parsing text data, similar to TextFSM, but with additional flexibility and features. Using TTP, you can extract specific information from command outputs and transform it into structured data, making it easier to integrate with other systems and processes.</p> </li> <li> <p>NAPALM Getters: The Nornir Service Parse Task leverages NAPALM getters to retrieve and parse structured data directly from network devices. NAPALM getters are pre-defined methods that extract specific pieces of information from devices, such as interface details, routing tables, ARP tables, and more.</p> </li> </ul> <p>The Nornir Service Parse Task is essential for network automation and developer engineers who need to process and analyze large volumes of network data. By transforming raw command outputs into structured data, you can automate complex workflows, generate insightful reports, and ensure that your network devices are configured and operating correctly.</p> <p>This document also includes a reference for the NorFab shell commands related to the Nornir <code>parse</code> task, detailing the available options and parameters. These commands provide granular control over the parsing tasks.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_parse/#norfab-nornir-parse-shell-reference","title":"NORFAB Nornir Parse Shell Reference","text":"<p>NorFab shell supports these command options for Nornir <code>parse</code> task:</p> <pre><code>nf#man tree nornir.parse\nroot\n\u2514\u2500\u2500 nornir:    Nornir service\n    \u2514\u2500\u2500 parse:    Parse network devices output\n        \u251c\u2500\u2500 napalm:    Parse devices output using NAPALM getters\n        \u2502   \u251c\u2500\u2500 timeout:    Job timeout\n        \u2502   \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u2502   \u251c\u2500\u2500 add_details:    Add task details to results\n        \u2502   \u251c\u2500\u2500 run_num_workers:    RetryRunner number of threads for tasks execution\n        \u2502   \u251c\u2500\u2500 run_num_connectors:    RetryRunner number of threads for device connections\n        \u2502   \u251c\u2500\u2500 run_connect_retry:    RetryRunner number of connection attempts\n        \u2502   \u251c\u2500\u2500 run_task_retry:    RetryRunner number of attempts to run task\n        \u2502   \u251c\u2500\u2500 run_reconnect_on_fail:    RetryRunner perform reconnect to host on task failure\n        \u2502   \u251c\u2500\u2500 run_connect_check:    RetryRunner test TCP connection before opening actual connection\n        \u2502   \u251c\u2500\u2500 run_connect_timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n        \u2502   \u251c\u2500\u2500 run_creds_retry:    RetryRunner list of connection credentials and parameters to retry\n        \u2502   \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n        \u2502   \u251c\u2500\u2500 tf_skip_failed:    Save results to file for failed tasks\n        \u2502   \u251c\u2500\u2500 diff:    File group name to run the diff for\n        \u2502   \u251c\u2500\u2500 diff_last:    File version number to diff, default is 1 (last)\n        \u2502   \u251c\u2500\u2500 progress:    Emit execution progress\n        \u2502   \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n        \u2502   \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u2502   \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u2502   \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u2502   \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u2502   \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u2502   \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u2502   \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u2502   \u251c\u2500\u2500 FM:    Filter hosts by platform\n        \u2502   \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n        \u2502   \u251c\u2500\u2500 FN:    Negate the match\n        \u2502   \u251c\u2500\u2500 hosts:    Filter hosts to target\n        \u2502   \u2514\u2500\u2500 *getters:    Select NAPALM getters, default 'PydanticUndefined'\n        \u2514\u2500\u2500 ttp:    Parse devices output using TTP templates\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n            \u251c\u2500\u2500 add_details:    Add task details to results\n            \u251c\u2500\u2500 run_num_workers:    RetryRunner number of threads for tasks execution\n            \u251c\u2500\u2500 run_num_connectors:    RetryRunner number of threads for device connections\n            \u251c\u2500\u2500 run_connect_retry:    RetryRunner number of connection attempts\n            \u251c\u2500\u2500 run_task_retry:    RetryRunner number of attempts to run task\n            \u251c\u2500\u2500 run_connect_check:    RetryRunner test TCP connection before opening actual connection\n            \u251c\u2500\u2500 run_connect_timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n            \u251c\u2500\u2500 run_creds_retry:    RetryRunner list of connection credentials and parameters to retry\n            \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n            \u251c\u2500\u2500 tf_skip_failed:    Save results to file for failed tasks\n            \u251c\u2500\u2500 diff:    File group name to run the diff for\n            \u251c\u2500\u2500 diff_last:    File version number to diff, default is 1 (last)\n            \u251c\u2500\u2500 progress:    Emit execution progress\n            \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n            \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n            \u251c\u2500\u2500 FH:    Filter hosts by hostname\n            \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n            \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n            \u251c\u2500\u2500 FG:    Filter hosts by group\n            \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n            \u251c\u2500\u2500 FL:    Filter hosts by names list\n            \u251c\u2500\u2500 FM:    Filter hosts by platform\n            \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n            \u251c\u2500\u2500 FN:    Negate the match\n            \u251c\u2500\u2500 hosts:    Filter hosts to target\n            \u251c\u2500\u2500 *template:    TTP Template to parse commands output, default 'PydanticUndefined'\n            \u2514\u2500\u2500 commands:    Commands to collect form devices\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_parse/#python-api-reference","title":"Python API Reference","text":"<p>Parse network device output using specified plugin and options.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>plugin</code> <code>str</code> <p>The plugin to use for parsing. Options are:</p> <ul> <li>napalm - parse devices output using NAPALM getters</li> <li>ttp - use TTP Templates to parse devices output</li> <li>textfsm - use TextFSM templates to parse devices output</li> </ul> <code>'napalm'</code> <code>getters</code> <code>str</code> <p>The getters to use with the \"napalm\" plugin.</p> <code>'get_facts'</code> <code>template</code> <code>str</code> <p>The template to use with the \"ttp\" or \"textfsm\" plugin.</p> <code>None</code> <code>commands</code> <code>list</code> <p>The list of commands to run with the \"ttp\" or \"textfsm\" plugin.</p> <code>None</code> <code>to_dict</code> <code>bool</code> <p>Whether to convert the result to a dictionary.</p> <code>True</code> <code>add_details</code> <code>bool</code> <p>Whether to add details to the result.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the plugin.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>A Result object containing the parsed data.</p> <p>Raises:</p> Type Description <code>UnsupportedPluginError</code> <p>If the specified plugin is not supported.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef parse(\n    self,\n    job: Job,\n    plugin: str = \"napalm\",\n    getters: Union[str, list] = \"get_facts\",\n    template: str = None,\n    commands: Union[str, list] = None,\n    to_dict: bool = True,\n    add_details: bool = False,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Parse network device output using specified plugin and options.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        plugin (str): The plugin to use for parsing. Options are:\n\n            - napalm - parse devices output using NAPALM getters\n            - ttp - use TTP Templates to parse devices output\n            - textfsm - use TextFSM templates to parse devices output\n\n        getters (str): The getters to use with the \"napalm\" plugin.\n        template (str): The template to use with the \"ttp\" or \"textfsm\" plugin.\n        commands (list): The list of commands to run with the \"ttp\" or \"textfsm\" plugin.\n        to_dict (bool): Whether to convert the result to a dictionary.\n        add_details (bool): Whether to add details to the result.\n        **kwargs: Additional keyword arguments to pass to the plugin.\n\n    Returns:\n        Result: A Result object containing the parsed data.\n\n    Raises:\n        UnsupportedPluginError: If the specified plugin is not supported.\n    \"\"\"\n    filters = {k: kwargs.get(k) for k in list(kwargs.keys()) if k in FFun_functions}\n    ret = Result(task=f\"{self.name}:parse\", result={} if to_dict else [])\n\n    filtered_nornir, ret = self.filter_hosts_and_validate(kwargs, ret)\n    if ret.status == \"no_match\":\n        return ret\n\n    if plugin == \"napalm\":\n        nr = self._add_processors(filtered_nornir, kwargs, job)  # add processors\n        result = nr.run(task=napalm_get, getters=getters, **kwargs)\n        ret.result = ResultSerializer(\n            result, to_dict=to_dict, add_details=add_details\n        )\n        ret.failed = result.failed  # failed is true if any of the hosts failed\n    elif plugin == \"ttp\":\n        result = self.cli(\n            job=job,\n            commands=commands or [],\n            run_ttp=template,\n            **filters,\n            **kwargs,\n            to_dict=to_dict,\n            add_details=add_details,\n            plugin=\"netmiko\",\n        )\n        ret.result = result.result\n    elif plugin == \"textfsm\":\n        result = self.cli(\n            job=job,\n            commands=commands,\n            **filters,\n            **kwargs,\n            to_dict=to_dict,\n            add_details=add_details,\n            use_textfsm=True,\n            textfsm_template=template,\n            plugin=\"netmiko\",\n        )\n        ret.result = result.result\n    else:\n        raise UnsupportedPluginError(f\"Plugin '{plugin}' not supported\")\n\n    return ret\n</code></pre>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_runtime_inventory/","title":"Nornir Service Runtime Inventory Task","text":"<p>task api name: <code>runtime_inventory</code></p> <p>The Nornir Service <code>runtime_inventory</code> task designed to work with Nornir inventory content at a runtime. This task uses nornir-salt <code>InventoryFun</code> functions to create, read, update or delete hosts.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_runtime_inventory/#sample-usage","title":"Sample Usage","text":"<p>Sample NorFab client call to invoke inventory host creation:</p> <pre><code>result = nfclient.run_job(\n    \"nornir\",\n    \"runtime_inventory\",\n    workers=[\"nornir-worker-1\"],\n    kwargs={\n        \"action\": \"create_host\",\n        \"name\": \"foobar\"\n    },\n)\n</code></pre> <p>Supported actions are:</p> <ul> <li><code>create_host</code> or <code>create</code> - creates new host or replaces existing host object</li> <li><code>read_host</code> or <code>read</code> - read host inventory content</li> <li><code>update_host</code> or <code>update</code> - non recursively update host attributes if host exists in Nornir inventory, do not create host if it does not exist</li> <li><code>delete_host</code> or <code>delete</code> - deletes host object from Nornir Inventory</li> <li><code>load</code> - to simplify calling multiple functions</li> <li><code>read_inventory</code> - read inventory content for groups, default and hosts</li> <li><code>read_host_data</code> - to return host's data under provided path keys</li> <li><code>list_hosts</code> - return a list of inventory's host names</li> <li><code>list_hosts_platforms</code> - return a dictionary of hosts' platforms</li> <li><code>update_defaults</code> - non recursively update defaults attributes</li> <li><code>load</code> - load Nornir inventory from external sources</li> </ul>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_runtime_inventory/#create-host-example","title":"Create Host Example","text":"<pre><code>nf#nornir inventory create-host name foobar\n--------------------------------------------- Job Events -----------------------------------------------\n15-Feb-2025 11:12:38.908 d42e073070b94d408225af2a880d1d26 job started\n15-Feb-2025 11:12:38.939 INFO nornir-worker-5 running nornir.runtime_inventory  - Performing 'create_host' action\n15-Feb-2025 11:12:39.162 d42e073070b94d408225af2a880d1d26 job completed in 0.254 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\n{\n    \"nornir-worker-5\": {\n        \"foobar\": true\n    }\n}\nnf#\n</code></pre>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_runtime_inventory/#nornir-and-containerlab-services-integration","title":"Nornir and Containerlab Services Integration","text":"<p>Nornir Service supports loading hosts inventory from running Containerlab labs using <code>nornir_inventory_load_containerlab</code> task. This is useful to easily onboard Containerlab environments into Nornir and helps with automation. Internally Nornir service uses Containerlab Service to fetch running containers details and construct Nornir inventory.</p> <p>Example</p> CLIPython <pre><code>nf#nornir inventory load containerlab clab-workers containerlab-worker-1 workers nornir-worker-1 lab-name three-routers-lab\n--------------------------------------------- Job Events -----------------------------------------------\n05-May-2025 21:32:07.155 ed210f6c91ac40ada02149118eede2c9 job started\n05-May-2025 21:32:07.172 INFO nornir-worker-1 running nornir.nornir_inventory_load_containerlab  - Pulling Containerlab 'three-routers-lab' lab inventory from 'containerlab-worker-1' workers\n05-May-2025 21:32:07.393 INFO nornir-worker-1 running nornir.nornir_inventory_load_containerlab  - Pulled Containerlab 'three-routers-lab' lab inventory\n05-May-2025 21:32:07.399 INFO nornir-worker-1 running nornir.nornir_inventory_load_containerlab  - Merged Containerlab 'three-routers-lab' lab inventory with Nornir runtime inventory\n05-May-2025 21:32:07.916 ed210f6c91ac40ada02149118eede2c9 job completed in 0.76 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\nnornir-worker-1:\n    True\nnf#\n</code></pre> <p>This code is complete and can run as is</p> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"nornir\",\n        workers=[\"nornir-worker-1\"],\n        task=\"nornir_inventory_load_containerlab\",\n        kwargs={\n            \"lab_name\": \"three-routers-lab\",\n            \"clab_workers\": [\"containerlab-worker-1\"],\n            \"lab_name\": \"three-routers-lab\"\n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_runtime_inventory/#norfab-nornir-runtime-inventory-shell-reference","title":"NORFAB Nornir Runtime Inventory Shell Reference","text":"<p>NorFab shell supports these command options for Nornir <code>runtime_inventory</code> task:</p> <pre><code>nf#man tree nornir.inventory\nroot\n\u2514\u2500\u2500 nornir:    Nornir service\n    \u2514\u2500\u2500 inventory:    Work with Nornir inventory\n        \u251c\u2500\u2500 create-host:    Create new host\n        \u2502   \u251c\u2500\u2500 timeout:    Job timeout\n        \u2502   \u251c\u2500\u2500 workers:    Nornir workers to target, default 'any'\n        \u2502   \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n        \u2502   \u251c\u2500\u2500 *name:    Name of the host\n        \u2502   \u251c\u2500\u2500 username:    Host connections username\n        \u2502   \u251c\u2500\u2500 password:    Host connections password\n        \u2502   \u251c\u2500\u2500 platform:    Host platform recognized by connection plugin\n        \u2502   \u251c\u2500\u2500 hostname:    Hostname of the host to initiate connection with, IP address or FQDN\n        \u2502   \u251c\u2500\u2500 port:    TCP port to initiate connection with, default '22'\n        \u2502   \u251c\u2500\u2500 connection-options:    JSON string with connection options\n        \u2502   \u251c\u2500\u2500 groups:    List of groups to associate with this host\n        \u2502   \u251c\u2500\u2500 data:    JSON string with arbitrary host data\n        \u2502   \u2514\u2500\u2500 progress:    Display progress events, default 'True'\n        \u251c\u2500\u2500 update-host:    Update existing host details\n        \u2502   \u251c\u2500\u2500 timeout:    Job timeout\n        \u2502   \u251c\u2500\u2500 workers:    Nornir workers to target, default 'all'\n        \u2502   \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n        \u2502   \u251c\u2500\u2500 *name:    Name of the host\n        \u2502   \u251c\u2500\u2500 username:    Host connections username\n        \u2502   \u251c\u2500\u2500 password:    Host connections password\n        \u2502   \u251c\u2500\u2500 platform:    Host platform recognized by connection plugin\n        \u2502   \u251c\u2500\u2500 hostname:    Hostname of the host to initiate connection with, IP address or FQDN\n        \u2502   \u251c\u2500\u2500 port:    TCP port to initiate connection with, default '22'\n        \u2502   \u251c\u2500\u2500 connection-options:    JSON string with connection options\n        \u2502   \u251c\u2500\u2500 groups:    List of groups to associate with this host\n        \u2502   \u251c\u2500\u2500 groups-action:    Action to perform with groups, default 'append'\n        \u2502   \u251c\u2500\u2500 data:    JSON string with arbitrary host data\n        \u2502   \u2514\u2500\u2500 progress:    Display progress events, default 'True'\n        \u251c\u2500\u2500 delete-host:    Delete host from inventory\n        \u2502   \u251c\u2500\u2500 timeout:    Job timeout\n        \u2502   \u251c\u2500\u2500 workers:    Nornir workers to target, default 'all'\n        \u2502   \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n        \u2502   \u251c\u2500\u2500 *name:    Name of the host\n        \u2502   \u2514\u2500\u2500 progress:    Display progress events, default 'True'\n        \u251c\u2500\u2500 read-host-data:    Return host data at given dor-separated key path\n        \u2502   \u251c\u2500\u2500 timeout:    Job timeout\n        \u2502   \u251c\u2500\u2500 workers:    Nornir workers to target, default 'all'\n        \u2502   \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n        \u2502   \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n        \u2502   \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u2502   \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u2502   \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u2502   \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u2502   \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u2502   \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u2502   \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u2502   \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n        \u2502   \u251c\u2500\u2500 FN:    Negate the match\n        \u2502   \u251c\u2500\u2500 *keys:    Dot separated path within host data, examples: config.interfaces.Lo0\n        \u2502   \u2514\u2500\u2500 progress:    Display progress events, default 'True'\n        \u2514\u2500\u2500 load:    Load inventory from external source\n            \u2514\u2500\u2500 containerlab:    Load inventory from running Containerlab lab(s)\n                \u251c\u2500\u2500 timeout:    Job timeout\n                \u251c\u2500\u2500 *workers:    Nornir workers to load inventory into\n                \u251c\u2500\u2500 verbose-result:    Control output details, default 'False'\n                \u251c\u2500\u2500 clab-workers:    Containerlab workers to load inventory from\n                \u251c\u2500\u2500 progress:    Display progress events, default 'True'\n                \u251c\u2500\u2500 lab-name:    Name of Containerlab lab to load hosts' inventory\n                \u251c\u2500\u2500 groups:    List of Nornir groups to associate with hosts\n                \u251c\u2500\u2500 use-default-credentials:    Use Containerlab default credentials for all hosts\n                \u2514\u2500\u2500 dry-run:    Do not refresh Nornir, only return pulled inventory\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_runtime_inventory/#python-api-reference","title":"Python API Reference","text":"<p>Task to work with Nornir runtime (in-memory) inventory.</p> <p>Supported actions:</p> <ul> <li><code>create_host</code> or <code>create</code> - creates new host or replaces existing host object</li> <li><code>read_host</code> or <code>read</code> - read host inventory content</li> <li><code>update_host</code> or <code>update</code> - non recursively update host attributes if host exists     in Nornir inventory, do not create host if it does not exist</li> <li><code>delete_host</code> or <code>delete</code> - deletes host object from Nornir Inventory</li> <li><code>load</code> - to simplify calling multiple functions</li> <li><code>read_inventory</code> - read inventory content for groups, default and hosts</li> <li><code>read_host_data</code> - to return host's data under provided path keys</li> <li><code>list_hosts</code> - return a list of inventory's host names</li> <li><code>list_hosts_platforms</code> - return a dictionary of hosts' platforms</li> <li><code>update_defaults</code> - non recursively update defaults attributes</li> </ul> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>action</code> <code>str</code> <p>action to perform on inventory</p> required <code>kwargs</code> <code>Any</code> <p>arguments to use with the calling action</p> <code>{}</code> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef runtime_inventory(self, job: Job, action: str, **kwargs: Any) -&gt; Result:\n    \"\"\"\n    Task to work with Nornir runtime (in-memory) inventory.\n\n    Supported actions:\n\n    - `create_host` or `create` - creates new host or replaces existing host object\n    - `read_host` or `read` - read host inventory content\n    - `update_host` or `update` - non recursively update host attributes if host exists\n        in Nornir inventory, do not create host if it does not exist\n    - `delete_host` or `delete` - deletes host object from Nornir Inventory\n    - `load` - to simplify calling multiple functions\n    - `read_inventory` - read inventory content for groups, default and hosts\n    - `read_host_data` - to return host's data under provided path keys\n    - `list_hosts` - return a list of inventory's host names\n    - `list_hosts_platforms` - return a dictionary of hosts' platforms\n    - `update_defaults` - non recursively update defaults attributes\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        action: action to perform on inventory\n        kwargs: arguments to use with the calling action\n    \"\"\"\n    # clean up kwargs\n    _ = kwargs.pop(\"progress\", None)\n    job.event(f\"Performing '{action}' action\")\n    return Result(result=InventoryFun(self.nr, call=action, **kwargs))\n</code></pre>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_task/","title":"Nornir Service \"Task\" Task","text":"<p>task api name: <code>task</code></p> <p>The Nornir Service \"Task\" Task is a versatile component of NorFab's Nornir service, designed to execute any arbitrary Nornir task plugin function. This task provides network automation and developer engineers with the flexibility to run custom Nornir tasks, enabling them to tailor their network automation workflows to meet specific requirements.</p> <p>Key features of the Nornir Service \"Task\" Task include:</p> <ul> <li> <p>Custom Task Execution: The \"Task\" Task allows you to run custom Nornir task functions, which can be referenced using the OS path to the custom task Python file stored on broker or using dot notation to reference an import module. </p> </li> <li> <p>Integration with Nornir Plugins: The Nornir framework supports a wide range of community-built plugins, which can be called directly or leveraged to extend the functionality of your custom tasks. By integrating these plugins, you can enhance your automation capabilities and streamline complex network operations. Reference the Nornir Plugins page for a list of available plugins.</p> </li> <li> <p>Scalability and Reusability: Custom Nornir tasks can be designed to be scalable and reusable, allowing you to apply the same task logic across different network environments and scenarios. This promotes consistency and efficiency in your network automation workflows, reducing the need for repetitive coding and manual intervention.</p> </li> </ul>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_task/#nornir-tasks-sample-usage","title":"Nornir Tasks Sample Usage","text":"<p>Example of calling Nornir custom task function stored on NORFAB  broker under <code>nornir_tasks/echo.py</code> file path:</p> <pre><code>\u251c\u2500\u2500\u2500inventory.yaml\n\u2514\u2500\u2500\u2500nornir_tasks\n    \u2514\u2500\u2500\u2500echo.py\n</code></pre> <p>Task <code>echo.py</code> takes provided arguments and echoes them back in results:</p> <pre><code>from nornir.core.task import Result, Task\n\n\ndef task(task: Task, **kwargs) -&gt; Result:\n    task.name = \"echo\"\n    return Result(host=task.host, result=kwargs)\n</code></pre> <p>Example</p> CLIPython <pre><code>C:\\nf&gt;nfcli\nWelcome to NorFab Interactive Shell.\nnf#\nnf#nornir\nnf[nornir]#task\nnf[nornir-task]#plugin nf://nornir_tasks/echo.py arguments {\"foo\": \"bar\"} FC spine\nceos-spine-1:\n    echo:\n        foo: bar\nceos-spine-2:\n    echo:\n        foo: bar\nnf[nornir-task]#top\nnf#\n</code></pre> <p>Demo</p> <p></p> <p>Above runs <code>echo.py</code> custom Nornir task taking arguments <code>{\"foo\": \"bar\"}</code>  as an input and echoing them back. Task only executed for  Nornir hosts that contain <code>ceos-spine</code> in their hostname as  we use <code>FC</code> - \"Filter Contains\" Nornir hosts targeting  filter.</p> <p><code>inventory.yaml</code> should be located in same folder where we  start nfcli, unless <code>nfcli -i path_to_inventory.yaml</code> flag  used. Refer to Getting Started  section on how to construct  <code>inventory.yaml</code> file</p> <p>This code is complete and can run as is</p> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"nornir\",\n        task=\"task\",\n        kwargs={\n            \"plugin\": \"nf://nornir_tasks/echo.py\",\n            \"argument\": {\"foo\": \"bar\"},\n            \"FC\": \"ceos-spine\"    \n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre> <p>Once executed, above code should produce this output:</p> <pre><code>C:\\nf&gt;python nornir_task_docs.py\n{'nornir-worker-1': {'errors': [],\n                    'failed': False,\n                    'messages': [],\n                    'result': {'ceos-spine-1': {'echo': {'argument': {'foo': 'bar'}}},\n                                'ceos-spine-2': {'echo': {'argument': {'foo': 'bar'}}}},\n                    'task': 'nornir-worker-1:task'}}\n</code></pre> <p>Refer to Getting Started section on  how to construct  <code>inventory.yaml</code> file.    </p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_task/#use-community-module-task","title":"Use Community Module Task","text":"<p>It is possible to run any Nornir task plugin created by open  source community. For example, to use <code>netmiko_send_commands</code> from  <code>nornir_netmiko</code> module need to set plugin argument to  <code>nornir_netmiko.tasks.netmiko_send_commands</code> value and supply <code>arguments</code> option to provide further task parameters.</p> <p>Example</p> CLIPython <pre><code>C:\\nf&gt;nfcli\nWelcome to NorFab Interactive Shell.\nnf#\nnf#nornir\nnf[nornir]#task\nnf[nornir-task]#plugin \"nornir_netmiko.tasks.netmiko_send_command\" arguments {\"command_string\": \"show hostname\"} FC spine\nceos-spine-1:\n    netmiko_send_command:\n        Hostname: ceos-spine-1\n        FQDN:     ceos-spine-1\nceos-spine-2:\n    netmiko_send_command:\n        Hostname: ceos-spine-2\n        FQDN:     ceos-spine-2\nnf[nornir-task]#top\nnf#\n</code></pre> <p>Demo</p> <p></p> <p>Above runs <code>netmiko_send_command</code> Nornir task from <code>nornir_netmiko</code> module and collects <code>show hostname</code> command output from hosts that contain <code>ceos-spine</code> in their host name  since the use of targeting filter <code>FC</code> - \"Filter Contains\".</p> <p><code>inventory.yaml</code> should be located in same folder where we  start nfcli, unless <code>nfcli -i path_to_inventory.yaml</code> flag  used. Refer to Getting Started  section on how to construct  <code>inventory.yaml</code> file</p> <p>This code is complete and can run as is</p> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"nornir\",\n        task=\"task\",\n        kwargs={\n            \"plugin\": \"nornir_netmiko.tasks.netmiko_send_command\",\n            \"command_string\": \"show hostname\",\n            \"FC\": \"ceos-spine\"    \n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre> <p>Notice slight difference, python api does not make use of <code>arguments</code> option and need to supply task parameters as is  inside of <code>kwargs</code> dictionary.</p> <p>Once executed, above code should produce this output:</p> <pre><code>C:\\nf&gt;python nornir_task_module_docs.py\n{'nornir-worker-1': {'errors': [],\n                    'failed': False,\n                    'messages': [],\n                    'result': {'ceos-spine-1': {'netmiko_send_command': 'Hostname: '\n                                                                        'ceos-spine-1\\n'\n                                                                        'FQDN:     '\n                                                                        'ceos-spine-1'},\n                                'ceos-spine-2': {'netmiko_send_command': 'Hostname: '\n                                                                        'ceos-spine-2\\n'\n                                                                        'FQDN:     '\n                                                                        'ceos-spine-2'}}}\n</code></pre> <p>Refer to Getting Started section on  how to construct  <code>inventory.yaml</code> file.    </p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_task/#norfab-nornir-task-shell-reference","title":"NORFAB Nornir Task Shell Reference","text":"<p>NorFab shell supports these command options for Nornir <code>task</code> task:</p> <pre><code>nf#man tree nornir.task\nroot\n\u2514\u2500\u2500 nornir:    Nornir service\n    \u2514\u2500\u2500 task:    Run Nornir task\n        \u251c\u2500\u2500 timeout:    Job timeout\n        \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u251c\u2500\u2500 add_details:    Add task details to results\n        \u251c\u2500\u2500 run_num_workers:    RetryRunner number of threads for tasks execution\n        \u251c\u2500\u2500 run_num_connectors:    RetryRunner number of threads for device connections\n        \u251c\u2500\u2500 run_connect_retry:    RetryRunner number of connection attempts\n        \u251c\u2500\u2500 run_task_retry:    RetryRunner number of attempts to run task\n        \u251c\u2500\u2500 run_reconnect_on_fail:    RetryRunner perform reconnect to host on task failure\n        \u251c\u2500\u2500 run_connect_check:    RetryRunner test TCP connection before opening actual connection\n        \u251c\u2500\u2500 run_connect_timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n        \u251c\u2500\u2500 run_creds_retry:    RetryRunner list of connection credentials and parameters to retry\n        \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n        \u251c\u2500\u2500 tf_skip_failed:    Save results to file for failed tasks\n        \u251c\u2500\u2500 diff:    File group name to run the diff for\n        \u251c\u2500\u2500 diff_last:    File version number to diff, default is 1 (last)\n        \u251c\u2500\u2500 progress:    Emit execution progress\n        \u251c\u2500\u2500 table:    Table format (brief, terse, extend) or parameters or True\n        \u251c\u2500\u2500 headers:    Table headers\n        \u251c\u2500\u2500 headers_exclude:    Table headers to exclude\n        \u251c\u2500\u2500 sortby:    Table header column to sort by\n        \u251c\u2500\u2500 reverse:    Table reverse the sort by order\n        \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u251c\u2500\u2500 FM:    Filter hosts by platform\n        \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n        \u251c\u2500\u2500 FN:    Negate the match\n        \u251c\u2500\u2500 hosts:    Filter hosts to target\n        \u251c\u2500\u2500 *plugin:    Nornir task.plugin.name to import or nf://path/to/plugin/file.py\n        \u2514\u2500\u2500 arguments:    Plugin arguments JSON formatted string\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_task/#python-api-reference","title":"Python API Reference","text":"<p>Execute a Nornir task plugin.</p> <p>This method dynamically imports and executes a specified Nornir task plugin, using the provided arguments and keyword arguments. The <code>plugin</code> attribute can refer to a file to fetch from a file service, which must contain a function named <code>task</code> that accepts a Nornir task object as the first positional argument.</p> <p>Example of a custom task function file:</p> <pre><code># define connection name for RetryRunner to properly detect it\nCONNECTION_NAME = \"netmiko\"\n\n# create task function\ndef task(nornir_task_object, **kwargs):\n    pass\n</code></pre> Note <p>The <code>CONNECTION_NAME</code> must be defined within the custom task function file if RetryRunner is in use. Otherwise, the connection retry logic is skipped, and connections to all hosts are initiated simultaneously up to the number of <code>num_workers</code>.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>plugin</code> <code>str</code> <p>The path to the plugin function to import, or a NorFab URL to download a custom task or template URL that resolves to a file.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the specified task plugin.</p> <code>{}</code> Notes <ul> <li><code>add_details</code> (bool): If True, adds task execution details to the results.</li> <li><code>to_dict</code> (bool): If True, returns results as a dictionary. Defaults to True.</li> <li>Host filters: keys matching <code>FFun_functions</code> are treated as Nornir host filters.</li> </ul> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An instance of the Result class containing the task execution results.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified plugin file cannot be downloaded.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef task(self, job: Job, plugin: str, **kwargs: Any) -&gt; Result:\n    \"\"\"\n    Execute a Nornir task plugin.\n\n    This method dynamically imports and executes a specified Nornir task plugin,\n    using the provided arguments and keyword arguments. The `plugin` attribute\n    can refer to a file to fetch from a file service, which must contain a function\n    named `task` that accepts a Nornir task object as the first positional argument.\n\n    Example of a custom task function file:\n\n    ```python\n    # define connection name for RetryRunner to properly detect it\n    CONNECTION_NAME = \"netmiko\"\n\n    # create task function\n    def task(nornir_task_object, **kwargs):\n        pass\n    ```\n\n    Note:\n        The `CONNECTION_NAME` must be defined within the custom task function file if\n        RetryRunner is in use. Otherwise, the connection retry logic is skipped, and\n        connections to all hosts are initiated simultaneously up to the number of `num_workers`.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        plugin (str): The path to the plugin function to import, or a NorFab\n            URL to download a custom task or template URL that resolves to a file.\n        **kwargs (Any): Additional arguments to pass to the specified task plugin.\n\n    Notes:\n        - `add_details` (bool): If True, adds task execution details to the results.\n        - `to_dict` (bool): If True, returns results as a dictionary. Defaults to True.\n        - Host filters: keys matching `FFun_functions` are treated as Nornir host filters.\n\n    Returns:\n        Result: An instance of the Result class containing the task execution results.\n\n    Raises:\n        FileNotFoundError: If the specified plugin file cannot be downloaded.\n    \"\"\"\n    # extract attributes\n    add_details = kwargs.pop(\"add_details\", False)  # ResultSerializer\n    to_dict = kwargs.pop(\"to_dict\", True)  # ResultSerializer\n    ret = Result(task=f\"{self.name}:task\", result={} if to_dict else [])\n\n    filtered_nornir, no_match_result = self.filter_hosts_and_validate(kwargs, ret)\n    if ret.status == \"no_match\":\n        return ret\n\n    # download task from broker and load it\n    if plugin.startswith(\"nf://\"):\n        function_text = self.fetch_file(plugin)\n        if function_text is None:\n            raise FileNotFoundError(\n                f\"{self.name} - '{plugin}' task plugin download failed\"\n            )\n\n        # load task function running exec\n        globals_dict = {}\n        exec(function_text, globals_dict, globals_dict)\n        task_function = globals_dict[\"task\"]\n    # import task function\n    elif \".\" in plugin:\n        # below same as \"from nornir.plugins.tasks import task_fun as task_function\"\n        module_name, func_name = plugin.rsplit(\".\", 1)\n        module = __import__(module_name, fromlist=[func_name])  # nosec\n        task_function = getattr(module, func_name)\n    else:\n        raise RuntimeError(\n            f\"{self.name} - '{plugin}' task should either be a path \"\n            f\"to a file or a module import string\"\n        )\n\n    nr = self._add_processors(filtered_nornir, kwargs, job)  # add processors\n\n    # run task\n    log.debug(f\"{self.name} - running Nornir task '{plugin}', kwargs '{kwargs}'\")\n    with self.connections_lock:\n        result = nr.run(task=task_function, **kwargs)\n\n    ret.failed = result.failed  # failed is true if any of the hosts failed\n    ret.result = ResultSerializer(result, to_dict=to_dict, add_details=add_details)\n\n    self.watchdog.connections_clean()\n\n    return ret\n</code></pre>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_test/","title":"Nornir Service Test Task","text":"<p>task api name: <code>test</code></p> <p>The Nornir Service <code>test</code> task designed to facilitate the execution of network tests. This task provides network operations engineers and network automation developers with tools to validate network configurations, ensure compliance, and monitor network performance. By leveraging the capabilities of the Nornir service, users can automate testing process, identify issues proactively, and maintain a robust network infrastructure.</p> <p>Nornir service <code>test</code> task uses Nornir TestsProcessor to run the tests and support test suites definition in YAML format, where test suite YAML files can be stored on and sourced from broker.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_test/#nornir-test-sample-usage","title":"Nornir Test Sample Usage","text":"<p>Nornir service <code>test</code> task uses suites in YAML format to define tests, sample tests suite:</p> suite_3.txt<pre><code>- name: Check ceos version\n  task: \"show version\"\n  test: contains\n  pattern: \"4.30.0F\"\n- name: Check NTP status\n  test: ncontains\n  pattern: \"unsynchronised\"\n  task: \"show ntp status\"\n- name: Check Mgmt Interface Status\n  test: contains\n  pattern: \"is up, line protocol is up\"\n  task: \"show interface management0\" \n</code></pre> <p>File <code>suite_3.txt</code> stored on broker and downloaded by Nornir service prior to running tests, below is an example of how to run the tests suite.</p> <p>Example</p> CLIPython <pre><code>C:\\nf&gt;nfcli\nWelcome to NorFab Interactive Shell.\nnf#\nnf#nornir\nnf[nornir-test]#\nnf[nornir-test]#suite nf://nornir_test_suites/suite_3.txt FC spine,leaf\n--------------------------------------------- Job Events -----------------------------------------------\n07-Jan-2025 18:44:35 0c3309c54ee44397b055257a0d442e62 job started\n07-Jan-2025 18:44:35.207 nornir nornir-worker-1 ceos-spine-1, ceos-spine-2 task started - 'netmiko_send_commands'\n07-Jan-2025 18:44:35.211 nornir nornir-worker-2 ceos-leaf-1, ceos-leaf-2, ceos-leaf-3 task started - 'netmiko_send_commands'\n&lt;omitted for brevity&gt;\n07-Jan-2025 18:44:36 0c3309c54ee44397b055257a0d442e62 job completed in 1.391 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\n+----+--------------+-----------------------------+----------+-------------------+\n|    | host         | name                        | result   | exception         |\n+====+==============+=============================+==========+===================+\n|  0 | ceos-leaf-1  | Check ceos version          | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\n|  1 | ceos-leaf-1  | Check NTP status            | FAIL     | Pattern in output |\n+----+--------------+-----------------------------+----------+-------------------+\n|  2 | ceos-leaf-1  | Check Mgmt Interface Status | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\n|  3 | ceos-leaf-2  | Check ceos version          | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\n|  4 | ceos-leaf-2  | Check NTP status            | FAIL     | Pattern in output |\n+----+--------------+-----------------------------+----------+-------------------+\n|  5 | ceos-leaf-2  | Check Mgmt Interface Status | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\n|  6 | ceos-leaf-3  | Check ceos version          | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\n|  7 | ceos-leaf-3  | Check NTP status            | FAIL     | Pattern in output |\n+----+--------------+-----------------------------+----------+-------------------+\n|  8 | ceos-leaf-3  | Check Mgmt Interface Status | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\n|  9 | ceos-spine-1 | Check ceos version          | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\n| 10 | ceos-spine-1 | Check NTP status            | FAIL     | Pattern in output |\n+----+--------------+-----------------------------+----------+-------------------+\n| 12 | ceos-spine-2 | Check ceos version          | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\n| 12 | ceos-spine-2 | Check ceos version          | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\n| 12 | ceos-spine-2 | Check ceos version          | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\n| 12 | ceos-spine-2 | Check ceos version          | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\n| 12 | ceos-spine-2 | Check ceos version          | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\n| 13 | ceos-spine-2 | Check NTP status            | FAIL     | Pattern in output |\n+----+--------------+-----------------------------+----------+-------------------+\n| 12 | ceos-spine-2 | Check ceos version          | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\n| 12 | ceos-spine-2 | Check ceos version          | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\n| 13 | ceos-spine-2 | Check NTP status            | FAIL     | Pattern in output |\n+----+--------------+-----------------------------+----------+-------------------+\n| 14 | ceos-spine-2 | Check Mgmt Interface Status | PASS     |                   |\n+----+--------------+-----------------------------+----------+-------------------+\nnf[nornir-test]#\nnf[nornir-test]#top\nnf#\n</code></pre> <p>Demo</p> <p></p> <p>In this example:</p> <ul> <li><code>nfcli</code> command starts the NorFab Interactive Shell.</li> <li><code>nornir</code> command switches to the Nornir sub-shell.</li> <li><code>test</code> command switches to the <code>test</code> task sub-shell.</li> <li><code>suite</code> argument refers to a path for <code>suite_3.txt</code> file with a set of tests to run. </li> <li>Devices filtered using <code>FC</code> - \"Filter Contains\" Nornir hosts targeting filter to only run tests on devices that contain <code>spine</code> or <code>leaf</code> in their hostname.</li> </ul> <p><code>inventory.yaml</code> should be located in same folder where we start nfcli, unless <code>nfcli -i path_to_inventory.yaml</code> flag used. Refer to Getting Started section on how to construct  <code>inventory.yaml</code> file</p> <p>This code is complete and can run as is</p> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"nornir\",\n        task=\"test\",\n        kwargs={\n            \"suite\": \"nf://nornir_test_suites/suite_3.txt\",\n            \"FC\": \"spine,leaf\"          \n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre> <p>Refer to Getting Started section on how to construct  <code>inventory.yaml</code> file.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_test/#formatting-tests-output","title":"Formatting Tests Output","text":"<p>NorFab interactive shell allows you to format the results of network tests into text tables. This is particularly useful for presenting test results in a clear and organized manner, making it easier to analyze and interpret the data. The NorFab interactive shell supports the <code>table</code> command, which relies on the tabulate module to generate text tables. By outputting test results in table format, you can quickly identify issues and take appropriate action.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_test/#markdown-results-output-clientrun_job","title":"Markdown Results Output (client.run_job)","text":"<p>NorFab Python client can return Nornir <code>test</code> results as a Markdown report by passing <code>markdown=True</code> to <code>client.run_job(...)</code>.</p> <p>This is convenient when you want to:</p> <ul> <li>Save results into a <code>.md</code> file</li> <li>Post results into ticketing systems / chat tools</li> <li>Render results in a UI (for example, using a Markdown renderer such as Markwon)</li> </ul> <p>The content of the report depends on the <code>extensive</code> keyword in <code>kwargs</code>:</p> <ul> <li><code>extensive=False</code> (default) produces a summary table and debug section, without per-test details and command outputs.</li> <li><code>extensive=True</code> includes hierarchical per-host test details, device command outputs, devices inventory, and test suite definitions.</li> </ul> <p>Python: Markdown report (brief)</p> <p>Sample python script that produces brief markdown tests report:</p> <pre><code>from norfab.core.nfapi import NorFab\n\n\nif __name__ == \"__main__\":\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    report_md = client.run_job(\n        \"nornir\",\n        \"test\",\n        kwargs={\n            \"suite\": \"nf://nornir_test_suites/suite_1.txt\",\n            \"FC\": [\"spine\", \"leaf\"],\n        },\n        markdown=True,\n    )\n\n    print(report_md)\n\n    nf.destroy()\n</code></pre> <p> Sample output (extensive=False) <pre><code># Tests Execution Report\n\n## Summary\n\n\nHigh-level table with all test results.\n|Host|Test Name|Result|Exception|\n| :--- | :--- | :--- | :--- |\n|ceos-leaf-1|check NTP status|\u274c FAIL||\n|ceos-leaf-1|check ceos version|\u2705 PASS||\n|ceos-leaf-2|check NTP status|\u274c FAIL||\n|ceos-leaf-2|check ceos version|\u2705 PASS||\n|ceos-leaf-3|check NTP status|\u274c FAIL||\n|ceos-leaf-3|check ceos version|\u2705 PASS||\n|ceos-spine-1|check NTP status|\u274c FAIL||\n|ceos-spine-1|check ceos version|\u2705 PASS||\n|ceos-spine-2|check NTP status|\u274c FAIL||\n|ceos-spine-2|check ceos version|\u2705 PASS||\n\n## Tests Details\n\n\n\u274c No detailed results available. Set `extensive` to `True` in input kwargs arguments.\n\n\n## Device Outputs\n\n\n\u274c No hosts outputs available. Set `extensive` to `True` in input kwargs arguments.\n\n\n## Debug\n\n\nThis section contains detailed debugging information for troubleshooting and inspection. Includes input arguments and complete raw results data used to produce sections above.\n\n\u274c No hosts inventory available. Set `extensive` to `True` in input kwargs arguments.\n\n\n\n\u274c No hosts test suites available. Set `extensive` to `True` in input kwargs arguments.\n\n\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;Input Arguments (kwargs)&lt;/summary&gt;\n\n```json\n{\n  \"suite\": \"nf://nornir_test_suites/suite_1.txt\",\n  \"FC\": [\n    \"spine\",\n    \"leaf\"\n  ]\n}\n```\n\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;Complete Results (JSON)&lt;/summary&gt;\n\n```json\n{\n  \"status\": \"202\",\n  \"results\": {\n    \"nornir-worker-5\": {\n      \"result\": {},\n      \"failed\": false,\n      \"errors\": [],\n      \"task\": \"nornir-worker-5:test\",\n      \"messages\": [\n        \"nornir-worker-5 - nothing to do, no hosts matched by filters '{'FC': ['spine', 'leaf']}'\"\n      ],\n      \"juuid\": \"6dab68539bc3410d850a78b4fe3c4300\",\n      \"resources\": [],\n      \"status\": \"no_match\",\n      \"task_started\": \"Fri Jan  2 18:12:21 2026\",\n      \"task_completed\": \"Fri Jan  2 18:12:21 2026\",\n      \"service\": \"nornir\",\n      \"diff\": null,\n      \"dry_run\": false\n    },\n    \"nornir-worker-6\": {\n      \"result\": {},\n      \"failed\": false,\n      \"errors\": [],\n      \"task\": \"nornir-worker-6:test\",\n      \"messages\": [\n        \"nornir-worker-6 - nothing to do, no hosts matched by filters '{'FC': ['spine', 'leaf']}'\"\n      ],\n      \"juuid\": \"6dab68539bc3410d850a78b4fe3c4300\",\n      \"resources\": [],\n      \"status\": \"no_match\",\n      \"task_started\": \"Fri Jan  2 18:12:21 2026\",\n      \"task_completed\": \"Fri Jan  2 18:12:21 2026\",\n      \"service\": \"nornir\",\n      \"diff\": null,\n      \"dry_run\": false\n    },\n    \"nornir-worker-4\": {\n      \"result\": {},\n      \"failed\": false,\n      \"errors\": [],\n      \"task\": \"nornir-worker-4:test\",\n      \"messages\": [\n        \"nornir-worker-4 - nothing to do, no hosts matched by filters '{'FC': ['spine', 'leaf']}'\"\n      ],\n      \"juuid\": \"6dab68539bc3410d850a78b4fe3c4300\",\n      \"resources\": [],\n      \"status\": \"no_match\",\n      \"task_started\": \"Fri Jan  2 18:12:21 2026\",\n      \"task_completed\": \"Fri Jan  2 18:12:21 2026\",\n      \"service\": \"nornir\",\n      \"diff\": null,\n      \"dry_run\": false\n    },\n    \"nornir-worker-2\": {\n      \"result\": {\n        \"ceos-leaf-2\": {\n          \"check ceos version\": \"PASS\",\n          \"check NTP status\": \"FAIL\"\n        },\n        \"ceos-leaf-3\": {\n          \"check ceos version\": \"PASS\",\n          \"check NTP status\": \"FAIL\"\n        },\n        \"ceos-leaf-1\": {\n          \"check ceos version\": \"PASS\",\n          \"check NTP status\": \"FAIL\"\n        }\n      },\n      \"failed\": true,\n      \"errors\": [],\n      \"task\": \"nornir-worker-2:test\",\n      \"messages\": [],\n      \"juuid\": \"6dab68539bc3410d850a78b4fe3c4300\",\n      \"resources\": [],\n      \"status\": \"completed\",\n      \"task_started\": \"Fri Jan  2 18:12:21 2026\",\n      \"task_completed\": \"Fri Jan  2 18:12:22 2026\",\n      \"service\": \"nornir\",\n      \"diff\": null,\n      \"dry_run\": false\n    },\n    \"nornir-worker-1\": {\n      \"result\": {\n        \"ceos-spine-1\": {\n          \"check ceos version\": \"PASS\",\n          \"check NTP status\": \"FAIL\"\n        },\n        \"ceos-spine-2\": {\n          \"check ceos version\": \"PASS\",\n          \"check NTP status\": \"FAIL\"\n        }\n      },\n      \"failed\": true,\n      \"errors\": [],\n      \"task\": \"nornir-worker-1:test\",\n      \"messages\": [],\n      \"juuid\": \"6dab68539bc3410d850a78b4fe3c4300\",\n      \"resources\": [],\n      \"status\": \"completed\",\n      \"task_started\": \"Fri Jan  2 18:12:21 2026\",\n      \"task_completed\": \"Fri Jan  2 18:12:22 2026\",\n      \"service\": \"nornir\",\n      \"diff\": null,\n      \"dry_run\": false\n    }\n  },\n  \"errors\": [],\n  \"workers\": {\n    \"requested\": [\n      \"nornir-worker-5\",\n      \"nornir-worker-6\",\n      \"nornir-worker-4\",\n      \"nornir-worker-1\",\n      \"nornir-worker-2\"\n    ],\n    \"done\": \"{'nornir-worker-5', 'nornir-worker-6', 'nornir-worker-4', 'nornir-worker-1', 'nornir-worker-2'}\",\n    \"dispatched\": \"{'nornir-worker-5', 'nornir-worker-6', 'nornir-worker-4', 'nornir-worker-1', 'nornir-worker-2'}\",\n    \"pending\": \"set()\"\n  }\n}\n```\n\n&lt;/details&gt;\n</code></pre> <p>Python: Markdown report (extensive)</p> <p>This example produces detailed markdown report:</p> <pre><code>from norfab.core.nfapi import NorFab\n\n\nif __name__ == \"__main__\":\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    report_md = client.run_job(\n        \"nornir\",\n        \"test\",\n        kwargs={\n            \"suite\": \"nf://nornir_test_suites/suite_1.txt\",\n            \"FC\": [\"spine\", \"leaf\"],\n            \"extensive\": True,\n        },\n        markdown=True,\n    )\n\n    print(report_md)\n\n    nf.destroy()\n</code></pre> <p> Sample output (extensive=True) <pre><code># Tests Execution Report\n\n## Summary\n\n\nHigh-level table with all test results.\n|Host|Test Name|Result|Exception|\n| :--- | :--- | :--- | :--- |\n|ceos-leaf-1|check NTP status|\u274c FAIL|Pattern not in output|\n|ceos-leaf-1|check ceos version|\u2705 PASS||\n|ceos-leaf-2|check NTP status|\u274c FAIL|Pattern not in output|\n|ceos-leaf-2|check ceos version|\u2705 PASS||\n|ceos-leaf-3|check NTP status|\u274c FAIL|Pattern not in output|\n|ceos-leaf-3|check ceos version|\u2705 PASS||\n|ceos-spine-1|check NTP status|\u274c FAIL|Pattern not in output|\n|ceos-spine-1|check ceos version|\u2705 PASS||\n|ceos-spine-2|check NTP status|\u274c FAIL|Pattern not in output|\n|ceos-spine-2|check ceos version|\u2705 PASS||\n\n## Tests Details\n\n\nHierarchical expandable sections organized by device, then test name, containing complete test result details.\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;ceos-leaf-1 (2 tests, \u2705 1 passed, \u274c 1 failed)&lt;/summary&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;check NTP status \u274c FAIL&lt;/summary&gt;\n\n- **Result:** FAIL\n- **Criteria:** 1.1.1.1\n- **Exception:** Pattern not in output\n- **Task:** show ntp associations\n- **Test:** contains_lines\n- **Success:** False\n- **Failed:** True\n- **Changed:** False\n\n- **Comments:** N/A\n\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;check ceos version \u2705 PASS&lt;/summary&gt;\n\n- **Result:** PASS\n- **Criteria:** cEOS\n- **Exception:** None\n- **Task:** show version\n- **Test:** contains\n- **Success:** True\n- **Failed:** False\n- **Changed:** False\n\n- **Comments:** N/A\n\n&lt;/details&gt;\n\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;ceos-leaf-2 (2 tests, \u2705 1 passed, \u274c 1 failed)&lt;/summary&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;check NTP status \u274c FAIL&lt;/summary&gt;\n\n- **Result:** FAIL\n- **Criteria:** 1.1.1.1\n- **Exception:** Pattern not in output\n- **Task:** show ntp associations\n- **Test:** contains_lines\n- **Success:** False\n- **Failed:** True\n- **Changed:** False\n\n- **Comments:** N/A\n\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;check ceos version \u2705 PASS&lt;/summary&gt;\n\n- **Result:** PASS\n- **Criteria:** cEOS\n- **Exception:** None\n- **Task:** show version\n- **Test:** contains\n- **Success:** True\n- **Failed:** False\n- **Changed:** False\n\n- **Comments:** N/A\n\n&lt;/details&gt;\n\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;ceos-leaf-3 (2 tests, \u2705 1 passed, \u274c 1 failed)&lt;/summary&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;check NTP status \u274c FAIL&lt;/summary&gt;\n\n- **Result:** FAIL\n- **Criteria:** 1.1.1.1\n- **Exception:** Pattern not in output\n- **Task:** show ntp associations\n- **Test:** contains_lines\n- **Success:** False\n- **Failed:** True\n- **Changed:** False\n\n- **Comments:** N/A\n\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;check ceos version \u2705 PASS&lt;/summary&gt;\n\n- **Result:** PASS\n- **Criteria:** cEOS\n- **Exception:** None\n- **Task:** show version\n- **Test:** contains\n- **Success:** True\n- **Failed:** False\n- **Changed:** False\n\n- **Comments:** N/A\n\n&lt;/details&gt;\n\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;ceos-spine-1 (2 tests, \u2705 1 passed, \u274c 1 failed)&lt;/summary&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;check NTP status \u274c FAIL&lt;/summary&gt;\n\n- **Result:** FAIL\n- **Criteria:** 1.1.1.1\n- **Exception:** Pattern not in output\n- **Task:** show ntp associations\n- **Test:** contains_lines\n- **Success:** False\n- **Failed:** True\n- **Changed:** False\n\n- **Comments:** N/A\n\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;check ceos version \u2705 PASS&lt;/summary&gt;\n\n- **Result:** PASS\n- **Criteria:** cEOS\n- **Exception:** None\n- **Task:** show version\n- **Test:** contains\n- **Success:** True\n- **Failed:** False\n- **Changed:** False\n\n- **Comments:** N/A\n\n&lt;/details&gt;\n\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;ceos-spine-2 (2 tests, \u2705 1 passed, \u274c 1 failed)&lt;/summary&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;check NTP status \u274c FAIL&lt;/summary&gt;\n\n- **Result:** FAIL\n- **Criteria:** 1.1.1.1\n- **Exception:** Pattern not in output\n- **Task:** show ntp associations\n- **Test:** contains_lines\n- **Success:** False\n- **Failed:** True\n- **Changed:** False\n\n- **Comments:** N/A\n\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;check ceos version \u2705 PASS&lt;/summary&gt;\n\n- **Result:** PASS\n- **Criteria:** cEOS\n- **Exception:** None\n- **Task:** show version\n- **Test:** contains\n- **Success:** True\n- **Failed:** False\n- **Changed:** False\n\n- **Comments:** N/A\n\n&lt;/details&gt;\n\n&lt;/details&gt;\n\n\n## Device Outputs\n\n\nExpandable sections containing outputs collected during test execution for each host.\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;ceos-leaf-1 (2 commands)&lt;/summary&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;show version&lt;/summary&gt;\n\n```\nArista cEOSLab\nHardware version:\nSerial number: CA49F479A3A974B25CEC002E92F7450D\nHardware MAC address: 001c.7372.ebcd\nSystem MAC address: 001c.7372.ebcd\n\nSoftware image version: 4.30.0F-31408673.4300F (engineering build)\nArchitecture: x86_64\nInternal build version: 4.30.0F-31408673.4300F\nInternal build ID: a35f0dc7-2d65-4f2a-a010-279cf445fd8c\nImage format version: 1.0\nImage optimization: None\n\ncEOS tools version: (unknown)\nKernel version: 5.15.0-164-generic\n\nUptime: 1 hour and 32 minutes\nTotal memory: 32827152 kB\nFree memory: 15965824 kB\n\n```\n\n&lt;/details&gt;\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;show ntp associations&lt;/summary&gt;\n\n```\nNTP is disabled.\n     remote          refid      st t when  poll reach   delay   offset  jitter\n==============================================================================\n```\n\n&lt;/details&gt;\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;ceos-leaf-2 (2 commands)&lt;/summary&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;show version&lt;/summary&gt;\n\n```\nArista cEOSLab\nHardware version:\nSerial number: 16921D773C3C0A23581B1260734452FF\nHardware MAC address: 001c.7393.6e5d\nSystem MAC address: 001c.7393.6e5d\n\nSoftware image version: 4.30.0F-31408673.4300F (engineering build)\nArchitecture: x86_64\nInternal build version: 4.30.0F-31408673.4300F\nInternal build ID: a35f0dc7-2d65-4f2a-a010-279cf445fd8c\nImage format version: 1.0\nImage optimization: None\n\ncEOS tools version: (unknown)\nKernel version: 5.15.0-164-generic\n\nUptime: 1 hour and 32 minutes\nTotal memory: 32827152 kB\nFree memory: 15965824 kB\n\n```\n\n&lt;/details&gt;\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;show ntp associations&lt;/summary&gt;\n\n```\nNTP is disabled.\n     remote          refid      st t when  poll reach   delay   offset  jitter\n==============================================================================\n```\n\n&lt;/details&gt;\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;ceos-leaf-3 (2 commands)&lt;/summary&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;show version&lt;/summary&gt;\n\n```\nArista cEOSLab\nHardware version:\nSerial number: D03FE1DE81A401F1AAD67A4B15E096C8\nHardware MAC address: 001c.73f3.053c\nSystem MAC address: 001c.73f3.053c\n\nSoftware image version: 4.30.0F-31408673.4300F (engineering build)\nArchitecture: x86_64\nInternal build version: 4.30.0F-31408673.4300F\nInternal build ID: a35f0dc7-2d65-4f2a-a010-279cf445fd8c\nImage format version: 1.0\nImage optimization: None\n\ncEOS tools version: (unknown)\nKernel version: 5.15.0-164-generic\n\nUptime: 1 hour and 32 minutes\nTotal memory: 32827152 kB\nFree memory: 15965824 kB\n\n```\n\n&lt;/details&gt;\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;show ntp associations&lt;/summary&gt;\n\n```\nNTP is disabled.\n     remote          refid      st t when  poll reach   delay   offset  jitter\n==============================================================================\n```\n\n&lt;/details&gt;\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;ceos-spine-1 (2 commands)&lt;/summary&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;show version&lt;/summary&gt;\n\n```\nArista cEOSLab\nHardware version:\nSerial number: C4889628D19280228439023C4F0C3EE4\nHardware MAC address: 001c.73a9.7d04\nSystem MAC address: 001c.73a9.7d04\n\nSoftware image version: 4.30.0F-31408673.4300F (engineering build)\nArchitecture: x86_64\nInternal build version: 4.30.0F-31408673.4300F\nInternal build ID: a35f0dc7-2d65-4f2a-a010-279cf445fd8c\nImage format version: 1.0\nImage optimization: None\n\ncEOS tools version: (unknown)\nKernel version: 5.15.0-164-generic\n\nUptime: 1 hour and 32 minutes\nTotal memory: 32827152 kB\nFree memory: 15965824 kB\n\n```\n\n&lt;/details&gt;\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;show ntp associations&lt;/summary&gt;\n\n```\nNTP is disabled.\n     remote          refid      st t when  poll reach   delay   offset  jitter\n==============================================================================\n```\n\n&lt;/details&gt;\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;ceos-spine-2 (2 commands)&lt;/summary&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;show version&lt;/summary&gt;\n\n```\nArista cEOSLab\nHardware version:\nSerial number: F8B8101D77067B49C0437B3711AA1719\nHardware MAC address: 001c.735c.3067\nSystem MAC address: 001c.735c.3067\n\nSoftware image version: 4.30.0F-31408673.4300F (engineering build)\nArchitecture: x86_64\nInternal build version: 4.30.0F-31408673.4300F\nInternal build ID: a35f0dc7-2d65-4f2a-a010-279cf445fd8c\nImage format version: 1.0\nImage optimization: None\n\ncEOS tools version: (unknown)\nKernel version: 5.15.0-164-generic\n\nUptime: 1 hour and 32 minutes\nTotal memory: 32827152 kB\nFree memory: 15965824 kB\n\n```\n\n&lt;/details&gt;\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;show ntp associations&lt;/summary&gt;\n\n```\nNTP is disabled.\n     remote          refid      st t when  poll reach   delay   offset  jitter\n==============================================================================\n```\n\n&lt;/details&gt;\n&lt;/details&gt;\n\n\n## Debug\n\n\nThis section contains detailed debugging information for troubleshooting and inspection. Includes input arguments and complete raw results data used to produce sections above.\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;Devices Inventory&lt;/summary&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;ceos-leaf-1&lt;/summary&gt;\n\n```json\n{\n  \"name\": \"ceos-leaf-1\",\n  \"connection_options\": {\n    \"scrapli_netconf\": {\n      \"extras\": null,\n      \"hostname\": null,\n      \"port\": 8302,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    },\n    \"napalm\": {\n      \"extras\": {\n        \"optional_args\": {\n          \"transport\": \"https\",\n          \"port\": 4402\n        }\n      },\n      \"hostname\": null,\n      \"port\": null,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    },\n    \"ncclient\": {\n      \"extras\": null,\n      \"hostname\": null,\n      \"port\": 8302,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    }\n  },\n  \"groups\": [\n    \"eos_params\"\n  ],\n  \"data\": {},\n  \"hostname\": \"192.168.1.130\",\n  \"port\": 2202,\n  \"username\": \"admin\",\n  \"password\": \"admin\",\n  \"platform\": \"arista_eos\"\n}\n```\n\n&lt;/details&gt;\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;ceos-leaf-2&lt;/summary&gt;\n\n```json\n{\n  \"name\": \"ceos-leaf-2\",\n  \"connection_options\": {\n    \"scrapli_netconf\": {\n      \"extras\": null,\n      \"hostname\": null,\n      \"port\": 8303,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    },\n    \"napalm\": {\n      \"extras\": {\n        \"optional_args\": {\n          \"transport\": \"https\",\n          \"port\": 4403\n        }\n      },\n      \"hostname\": null,\n      \"port\": null,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    },\n    \"ncclient\": {\n      \"extras\": null,\n      \"hostname\": null,\n      \"port\": 8303,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    }\n  },\n  \"groups\": [\n    \"eos_params\"\n  ],\n  \"data\": {},\n  \"hostname\": \"192.168.1.130\",\n  \"port\": 2203,\n  \"username\": \"admin\",\n  \"password\": \"admin\",\n  \"platform\": \"arista_eos\"\n}\n```\n\n&lt;/details&gt;\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;ceos-leaf-3&lt;/summary&gt;\n\n```json\n{\n  \"name\": \"ceos-leaf-3\",\n  \"connection_options\": {\n    \"scrapli_netconf\": {\n      \"extras\": null,\n      \"hostname\": null,\n      \"port\": 8304,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    },\n    \"napalm\": {\n      \"extras\": {\n        \"optional_args\": {\n          \"transport\": \"https\",\n          \"port\": 4404\n        }\n      },\n      \"hostname\": null,\n      \"port\": null,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    },\n    \"ncclient\": {\n      \"extras\": null,\n      \"hostname\": null,\n      \"port\": 8304,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    }\n  },\n  \"groups\": [\n    \"eos_params\"\n  ],\n  \"data\": {},\n  \"hostname\": \"192.168.1.130\",\n  \"port\": 2204,\n  \"username\": \"admin\",\n  \"password\": \"admin\",\n  \"platform\": \"arista_eos\"\n}\n```\n\n&lt;/details&gt;\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;ceos-spine-1&lt;/summary&gt;\n\n```json\n{\n  \"name\": \"ceos-spine-1\",\n  \"connection_options\": {\n    \"scrapli_netconf\": {\n      \"extras\": null,\n      \"hostname\": null,\n      \"port\": 8300,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    },\n    \"napalm\": {\n      \"extras\": {\n        \"optional_args\": {\n          \"transport\": \"https\",\n          \"port\": 4400\n        }\n      },\n      \"hostname\": null,\n      \"port\": null,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    },\n    \"ncclient\": {\n      \"extras\": null,\n      \"hostname\": null,\n      \"port\": 8300,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    }\n  },\n  \"groups\": [\n    \"eos_params\"\n  ],\n  \"data\": {\n    \"interfaces\": [\n      \"loopback0\",\n      \"ethernet1\"\n    ]\n  },\n  \"hostname\": \"192.168.1.130\",\n  \"port\": 2200,\n  \"username\": \"admin\",\n  \"password\": \"admin\",\n  \"platform\": \"arista_eos\"\n}\n```\n\n&lt;/details&gt;\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;ceos-spine-2&lt;/summary&gt;\n\n```json\n{\n  \"name\": \"ceos-spine-2\",\n  \"connection_options\": {\n    \"scrapli_netconf\": {\n      \"extras\": null,\n      \"hostname\": null,\n      \"port\": 8301,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    },\n    \"napalm\": {\n      \"extras\": {\n        \"optional_args\": {\n          \"transport\": \"https\",\n          \"port\": 4401\n        }\n      },\n      \"hostname\": null,\n      \"port\": null,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    },\n    \"ncclient\": {\n      \"extras\": null,\n      \"hostname\": null,\n      \"port\": 8301,\n      \"username\": null,\n      \"password\": null,\n      \"platform\": null\n    }\n  },\n  \"groups\": [\n    \"eos_params\"\n  ],\n  \"data\": {\n    \"interfaces\": [\n      \"ethernet1\"\n    ]\n  },\n  \"hostname\": \"192.168.1.130\",\n  \"port\": 2201,\n  \"username\": \"admin\",\n  \"password\": \"admin\",\n  \"platform\": \"arista_eos\"\n}\n```\n\n&lt;/details&gt;\n&lt;/details&gt;\n\n\n\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;Test suites definitions for each host&lt;/summary&gt;\n\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;ceos-leaf-1 (2 tests)&lt;/summary&gt;\n\n```json\n[\n  {\n    \"task\": \"show version\",\n    \"test\": \"contains\",\n    \"pattern\": \"cEOS\",\n    \"name\": \"check ceos version\"\n  },\n  {\n    \"test\": \"contains_lines\",\n    \"pattern\": [\n      \"1.1.1.1\"\n    ],\n    \"task\": \"show ntp associations\",\n    \"name\": \"check NTP status\"\n  }\n]\n```\n\n&lt;/details&gt;\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;ceos-leaf-2 (2 tests)&lt;/summary&gt;\n\n```json\n[\n  {\n    \"task\": \"show version\",\n    \"test\": \"contains\",\n    \"pattern\": \"cEOS\",\n    \"name\": \"check ceos version\"\n  },\n  {\n    \"test\": \"contains_lines\",\n    \"pattern\": [\n      \"1.1.1.1\"\n    ],\n    \"task\": \"show ntp associations\",\n    \"name\": \"check NTP status\"\n  }\n]\n```\n\n&lt;/details&gt;\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;ceos-leaf-3 (2 tests)&lt;/summary&gt;\n\n```json\n[\n  {\n    \"task\": \"show version\",\n    \"test\": \"contains\",\n    \"pattern\": \"cEOS\",\n    \"name\": \"check ceos version\"\n  },\n  {\n    \"test\": \"contains_lines\",\n    \"pattern\": [\n      \"1.1.1.1\"\n    ],\n    \"task\": \"show ntp associations\",\n    \"name\": \"check NTP status\"\n  }\n]\n```\n\n&lt;/details&gt;\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;ceos-spine-1 (2 tests)&lt;/summary&gt;\n\n```json\n[\n  {\n    \"task\": \"show version\",\n    \"test\": \"contains\",\n    \"pattern\": \"cEOS\",\n    \"name\": \"check ceos version\"\n  },\n  {\n    \"test\": \"contains_lines\",\n    \"pattern\": [\n      \"1.1.1.1\"\n    ],\n    \"task\": \"show ntp associations\",\n    \"name\": \"check NTP status\"\n  }\n]\n```\n\n&lt;/details&gt;\n&lt;details style=\"margin-left:40px;\"&gt;\n&lt;summary&gt;ceos-spine-2 (2 tests)&lt;/summary&gt;\n\n```json\n[\n  {\n    \"task\": \"show version\",\n    \"test\": \"contains\",\n    \"pattern\": \"cEOS\",\n    \"name\": \"check ceos version\"\n  },\n  {\n    \"test\": \"contains_lines\",\n    \"pattern\": [\n      \"1.1.1.1\"\n    ],\n    \"task\": \"show ntp associations\",\n    \"name\": \"check NTP status\"\n  }\n]\n```\n\n&lt;/details&gt;\n\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;Input Arguments (kwargs)&lt;/summary&gt;\n\n```json\n{\n  \"suite\": \"nf://nornir_test_suites/suite_1.txt\",\n  \"FC\": [\n    \"spine\",\n    \"leaf\"\n  ],\n  \"extensive\": true\n}\n```\n\n&lt;/details&gt;\n\n&lt;details style=\"margin-left:20px;\"&gt;\n&lt;summary&gt;Complete Results (JSON)&lt;/summary&gt;\n\n```json\n{\n  \"status\": \"202\",\n  \"results\": {\n    \"nornir-worker-5\": {\n      \"result\": {\n        \"test_results\": [],\n        \"suite\": {},\n      },\n      \"failed\": false,\n      \"errors\": [],\n      \"task\": \"nornir-worker-5:test\",\n      \"messages\": [\n        \"nornir-worker-5 - nothing to do, no hosts matched by filters '{'FC': ['spine', 'leaf']}'\"\n      ],\n      \"juuid\": \"4f974374692749019e5cf23e842f5922\",\n      \"resources\": [],\n      \"status\": \"no_match\",\n      \"task_started\": \"Fri Jan  2 18:14:10 2026\",\n      \"task_completed\": \"Fri Jan  2 18:14:10 2026\",\n      \"service\": \"nornir\",\n      \"diff\": null,\n      \"dry_run\": false\n    },\n    \"nornir-worker-4\": {\n      \"result\": {\n        \"test_results\": [],\n        \"suite\": {},\n      },\n      \"failed\": false,\n      \"errors\": [],\n      \"task\": \"nornir-worker-4:test\",\n      \"messages\": [\n        \"nornir-worker-4 - nothing to do, no hosts matched by filters '{'FC': ['spine', 'leaf']}'\"\n      ],\n      \"juuid\": \"4f974374692749019e5cf23e842f5922\",\n      \"resources\": [],\n      \"status\": \"no_match\",\n      \"task_started\": \"Fri Jan  2 18:14:10 2026\",\n      \"task_completed\": \"Fri Jan  2 18:14:10 2026\",\n      \"service\": \"nornir\",\n      \"diff\": null,\n      \"dry_run\": false\n    },\n    \"nornir-worker-6\": {\n      \"result\": {\n        \"test_results\": [],\n        \"suite\": {},\n      },\n      \"failed\": false,\n      \"errors\": [],\n      \"task\": \"nornir-worker-6:test\",\n      \"messages\": [\n        \"nornir-worker-6 - nothing to do, no hosts matched by filters '{'FC': ['spine', 'leaf']}'\"\n      ],\n      \"juuid\": \"4f974374692749019e5cf23e842f5922\",\n      \"resources\": [],\n      \"status\": \"no_match\",\n      \"task_started\": \"Fri Jan  2 18:14:10 2026\",\n      \"task_completed\": \"Fri Jan  2 18:14:10 2026\",\n      \"service\": \"nornir\",\n      \"diff\": null,\n      \"dry_run\": false\n    },\n    \"nornir-worker-2\": {\n      \"result\": {\n        \"test_results\": [\n          {\n            \"result\": \"Arista cEOSLab\\nHardware version: \\nSerial number: 16921D773C3C0A23581B1260734452FF\\nHardware MAC address: 001c.7393.6e5d\\nSystem MAC address: 001c.7393.6e5d\\n\\nSoftware image version: 4.30.0F-31408673.4300F (engineering build)\\nArchitecture: x86_64\\nInternal build version: 4.30.0F-31408673.4300F\\nInternal build ID: a35f0dc7-2d65-4f2a-a010-279cf445fd8c\\nImage format version: 1.0\\nImage optimization: None\\n\\ncEOS tools version: (unknown)\\nKernel version: 5.15.0-164-generic\\n\\nUptime: 1 hour and 32 minutes\\nTotal memory: 32827152 kB\\nFree memory: 15965824 kB\\n\",\n            \"changed\": false,\n            \"diff\": \"\",\n            \"failed\": false,\n            \"exception\": null,\n            \"name\": \"show version\",\n            \"connection_retry\": 0,\n            \"task_retry\": 0,\n            \"host\": \"ceos-leaf-2\"\n          }\n        ],\n        \"suite\": {},\n      },\n      \"failed\": true,\n      \"errors\": [],\n      \"task\": \"nornir-worker-2:test\",\n      \"messages\": [],\n      \"juuid\": \"4f974374692749019e5cf23e842f5922\",\n      \"resources\": [],\n      \"status\": \"completed\",\n      \"task_started\": \"Fri Jan  2 18:14:10 2026\",\n      \"task_completed\": \"Fri Jan  2 18:14:11 2026\",\n      \"service\": \"nornir\",\n      \"diff\": null,\n      \"dry_run\": false\n    },\n    \"nornir-worker-1\": {\n      \"result\": {\n        \"test_results\": [],\n        \"suite\": {},\n      },\n      \"failed\": true,\n      \"errors\": [],\n      \"task\": \"nornir-worker-1:test\",\n      \"messages\": [],\n      \"juuid\": \"4f974374692749019e5cf23e842f5922\",\n      \"resources\": [],\n      \"status\": \"completed\",\n      \"task_started\": \"Fri Jan  2 18:14:10 2026\",\n      \"task_completed\": \"Fri Jan  2 18:14:11 2026\",\n      \"service\": \"nornir\",\n      \"diff\": null,\n      \"dry_run\": false\n    }\n  },\n  \"errors\": [],\n  \"workers\": {\n    \"requested\": [\n      \"nornir-worker-1\",\n      \"nornir-worker-4\",\n      \"nornir-worker-6\",\n      \"nornir-worker-2\",\n      \"nornir-worker-5\"\n    ],\n    \"done\": \"{'nornir-worker-1', 'nornir-worker-4', 'nornir-worker-6', 'nornir-worker-2', 'nornir-worker-5'}\",\n    \"dispatched\": \"{'nornir-worker-1', 'nornir-worker-4', 'nornir-worker-6', 'nornir-worker-2', 'nornir-worker-5'}\",\n    \"pending\": \"set()\"\n  }\n}\n```\n\n&lt;/details&gt;\n</code></pre>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_test/#using-jinja2-templates-to-generate-tests","title":"Using Jinja2 Templates to Generate Tests","text":"<p>Using Jinja2 Templates enables you to create dynamic test suites based on variables defined in your inventory or passed as job data. This approach allows you to tailor tests to specific devices or scenarios, ensuring that the tests are relevant and accurate. Jinja2 templates provide a powerful way to automate the creation of complex test cases, incorporating conditional logic, loops, and other advanced features to meet your testing requirements.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_test/#templating-tests-with-inline-job-data","title":"Templating Tests with Inline Job Data","text":"<p>Inline Job Data allows you to define test parameters directly within the <code>job_data</code> argument, making it easy to customize tests on the fly. This feature is particularly useful for scenarios where test parameters need to be adjusted frequently or based on specific conditions. By templating tests with inline job data, you can ensure that your tests are always up-to-date and aligned with the current network state.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_test/#using-dry-run","title":"Using Dry Run","text":"<p>The Using Dry Run feature allows you to generate the content of network test suites without actually performing any actions on the devices. This is useful for validation purposes, as it enables you to verify the correctness of your tests before running them. By using dry run, you can identify potential issues and make necessary adjustments, ensuring that your tests will execute successfully when run for real.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_test/#running-a-subset-of-tests","title":"Running a Subset of Tests","text":"<p>Running a Subset of Tests allows you to execute only a specific set of tests, rather than running the entire test suite. This is useful for targeted testing, such as validating changes in a particular part of the network configuration or focusing on specific devices features. By running a subset of tests, you can save time and resources, while still ensuring that critical aspects of the network are thoroughly tested.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_test/#returning-only-failed-tests","title":"Returning Only Failed Tests","text":"<p>Returning only failed tests enables you to filter the test results to show only the tests that have failed. This is particularly useful for quickly identifying and addressing issues, as it allows you to focus on the areas that require attention. By returning only failed tests, you can streamline the troubleshooting process and ensure that network problems are resolved efficiently.</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_test/#norfab-nornir-test-shell-reference","title":"NORFAB Nornir Test Shell Reference","text":"<p>The NORFAB Nornir Test Shell Reference provides a comprehensive set of command options for the Nornir <code>test</code> task. These commands allow you to control various aspects of the test execution, such as setting job timeouts, filtering devices, adding task details to results, and configuring retry mechanisms. By leveraging these command options, you can tailor the behavior of the tests to meet your specific network management needs, ensuring that your network remains reliable and performant.</p> <p>NorFab shell supports these command options for Nornir <code>test</code> task:</p> <pre><code>nf#man tree nornir.test\nroot\n\u2514\u2500\u2500 nornir:    Nornir service\n    \u2514\u2500\u2500 test:    Run network tests\n        \u251c\u2500\u2500 timeout:    Job timeout\n        \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u251c\u2500\u2500 add-details:    Add task details to results, default 'False'\n        \u251c\u2500\u2500 num-workers:    RetryRunner number of threads for tasks execution\n        \u251c\u2500\u2500 num-connectors:    RetryRunner number of threads for device connections\n        \u251c\u2500\u2500 connect-retry:    RetryRunner number of connection attempts\n        \u251c\u2500\u2500 task-retry:    RetryRunner number of attempts to run task\n        \u251c\u2500\u2500 reconnect-on-fail:    RetryRunner perform reconnect to host on task failure\n        \u251c\u2500\u2500 connect-check:    RetryRunner test TCP connection before opening actual connection\n        \u251c\u2500\u2500 connect-timeout:    RetryRunner timeout in seconds to wait for test TCP connection to establish\n        \u251c\u2500\u2500 creds-retry:    RetryRunner list of connection credentials and parameters to retry\n        \u251c\u2500\u2500 tf:    File group name to save task results to on worker file system\n        \u251c\u2500\u2500 tf-skip-failed:    Save results to file for failed tasks\n        \u251c\u2500\u2500 diff:    File group name to run the diff for\n        \u251c\u2500\u2500 diff-last:    File version number to diff, default is 1 (last)\n        \u251c\u2500\u2500 table:    Table format (brief, terse, extend) or parameters or True, default 'brief'\n        \u251c\u2500\u2500 headers:    Table headers\n        \u251c\u2500\u2500 headers-exclude:    Table headers to exclude\n        \u251c\u2500\u2500 sortby:    Table header column to sort by\n        \u251c\u2500\u2500 reverse:    Table reverse the sort by order\n        \u251c\u2500\u2500 FO:    Filter hosts using Filter Object\n        \u251c\u2500\u2500 FB:    Filter hosts by name using Glob Patterns\n        \u251c\u2500\u2500 FH:    Filter hosts by hostname\n        \u251c\u2500\u2500 FC:    Filter hosts containment of pattern in name\n        \u251c\u2500\u2500 FR:    Filter hosts by name using Regular Expressions\n        \u251c\u2500\u2500 FG:    Filter hosts by group\n        \u251c\u2500\u2500 FP:    Filter hosts by hostname using IP Prefix\n        \u251c\u2500\u2500 FL:    Filter hosts by names list\n        \u251c\u2500\u2500 FM:    Filter hosts by platform\n        \u251c\u2500\u2500 FX:    Filter hosts excluding them by name\n        \u251c\u2500\u2500 FN:    Negate the match\n        \u251c\u2500\u2500 hosts:    Filter hosts to target\n        \u251c\u2500\u2500 *suite:    Nornir suite nf://path/to/file.py\n        \u251c\u2500\u2500 dry-run:    Return produced per-host tests suite content without running tests\n        \u251c\u2500\u2500 subset:    Filter tests by name\n        \u251c\u2500\u2500 failed-only:    Return test results for failed tests only\n        \u251c\u2500\u2500 remove-tasks:    Include/Exclude tested task results\n        \u2514\u2500\u2500 job-data:    Path to YAML file with job data\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["nornir"]},{"location":"workers/nornir/services_nornir_service_tasks_test/#python-api-reference","title":"Python API Reference","text":"<p>Function to test networks using a suite of tests.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata</p> required <code>suite</code> <code>Union[list, str]</code> <p>URL Path to YAML file with tests or a list of test definitions or template URL that resolves to a file path.</p> required <code>subset</code> <code>str</code> <p>List or string with comma-separated non-case-sensitive glob patterns to filter tests by name. Ignored if dry_run is True.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, returns produced per-host tests suite content only.</p> <code>False</code> <code>remove_tasks</code> <code>bool</code> <p>If False, results will include other tasks output.</p> <code>True</code> <code>failed_only</code> <code>bool</code> <p>If True, returns test results for failed tests only.</p> <code>False</code> <code>return_tests_suite</code> <code>bool</code> <p>If True, returns rendered per-host tests suite content in addition to test results using a dictionary with <code>results</code> and <code>suite</code> keys.</p> <code>False</code> <code>job_data</code> <code>str</code> <p>URL to YAML file with data or dictionary/list of data to pass on to Jinja2 rendering context.</p> <code>None</code> <code>extensive</code> <code>bool</code> <p>return extensive results, equivalent to using these arguments:</p> <ul> <li>remove_tasks = False</li> <li>return_tests_suite = True</li> <li>add_details = True</li> <li>to_dict = False</li> </ul> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Any additional arguments to pass on to the Nornir service task.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Result</code> <p>A dictionary containing the test results. If <code>return_tests_suite</code> is True, the dictionary will contain both the test results and the rendered test suite.</p> Note <p>Result <code>failed</code> attribute is set to True if any of the tests failed for any of the hosts.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error in rendering the Jinja2 templates or loading the YAML.</p> Source code in <code>norfab\\workers\\nornir_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef test(\n    self,\n    job: Job,\n    suite: Union[list, str],\n    subset: str = None,\n    dry_run: bool = False,\n    remove_tasks: bool = True,\n    failed_only: bool = False,\n    return_tests_suite: bool = False,\n    job_data: Any = None,\n    extensive: bool = False,\n    **kwargs: Any,\n) -&gt; Result:\n    \"\"\"\n    Function to test networks using a suite of tests.\n\n    Args:\n        job: NorFab Job object containing relevant metadata\n        suite (Union[list, str]): URL Path to YAML file with tests or a list of test definitions\n            or template URL that resolves to a file path.\n        subset (str, optional): List or string with comma-separated non-case-sensitive glob\n            patterns to filter tests by name. Ignored if dry_run is True.\n        dry_run (bool, optional): If True, returns produced per-host tests suite content only.\n        remove_tasks (bool, optional): If False, results will include other tasks output.\n        failed_only (bool, optional): If True, returns test results for failed tests only.\n        return_tests_suite (bool, optional): If True, returns rendered per-host tests suite\n            content in addition to test results using a dictionary with ``results`` and ``suite`` keys.\n        job_data (str, optional): URL to YAML file with data or dictionary/list of data\n            to pass on to Jinja2 rendering context.\n        extensive (bool, optional): return extensive results, equivalent to using these arguments:\n\n            - remove_tasks = False\n            - return_tests_suite = True\n            - add_details = True\n            - to_dict = False\n\n        **kwargs: Any additional arguments to pass on to the Nornir service task.\n\n    Returns:\n        dict: A dictionary containing the test results. If `return_tests_suite` is True,\n            the dictionary will contain both the test results and the rendered test suite.\n\n    Note:\n        Result `failed` attribute is set to True if any of the tests failed for any of the hosts.\n\n    Raises:\n        RuntimeError: If there is an error in rendering the Jinja2 templates or loading the YAML.\n    \"\"\"\n    tests = {}  # dictionary to hold per-host test suites\n    # set extensive details flags\n    if extensive is True:\n        kwargs[\"add_details\"] = True\n        kwargs[\"to_dict\"] = False\n        remove_tasks = False\n        return_tests_suite = True\n    add_details = kwargs.get(\"add_details\", False)  # ResultSerializer\n    to_dict = kwargs.get(\"to_dict\", True)  # ResultSerializer\n    filters = {k: kwargs.get(k) for k in list(kwargs.keys()) if k in FFun_functions}\n    ret = Result(task=f\"{self.name}:test\", result={} if to_dict else [])\n    suites = {}  # dictionary to hold combined test suites\n\n    filtered_nornir, ret = self.filter_hosts_and_validate(kwargs, ret)\n    if ret.status == \"no_match\":\n        if return_tests_suite is True:\n            ret.result = {\"test_results\": [], \"suite\": {}}\n        return ret\n\n    # download job data\n    job_data = self.load_job_data(job_data)\n\n    # generate per-host test suites\n    for host_name, host in filtered_nornir.inventory.hosts.items():\n        # render suite using Jinja2\n        try:\n            rendered_suite = self.jinja2_render_templates(\n                templates=[suite],\n                context={\n                    \"host\": host,\n                    \"norfab\": self.client,\n                    \"job_data\": job_data,\n                    \"netbox\": self.add_jinja2_netbox(),\n                },\n                filters=self.add_jinja2_filters(),\n            )\n        except Exception as e:\n            msg = f\"{self.name} - '{suite}' Jinja2 rendering failed: '{type(e).__name__}:{e}'\"\n            raise RuntimeError(msg) from e\n        # load suit using YAML\n        try:\n            tests[host_name] = yaml.safe_load(rendered_suite) or []\n        except Exception as e:\n            msg = f\"{self.name} - '{suite}' YAML load failed: '{type(e).__name__}:{e}'\"\n            raise RuntimeError(msg) from e\n\n    # validate tests suite\n    try:\n        _ = modelTestsProcessorSuite(tests=tests)\n    except Exception as e:\n        msg = f\"{self.name} - '{suite}' suite validation failed: '{type(e).__name__}:{e}'\"\n        raise RuntimeError(msg) from e\n\n    # download pattern, schema and custom function files\n    for host_name in tests.keys():\n        for index, item in enumerate(tests[host_name]):\n            for k in [\"pattern\", \"schema\", \"function_file\"]:\n                if self.is_url(item.get(k)):\n                    item[k] = self.fetch_file(\n                        item[k], raise_on_fail=True, read=True\n                    )\n                    if k == \"function_file\":\n                        item[\"function_text\"] = item.pop(k)\n            tests[host_name][index] = item\n\n    # save per-host tests suite content before mutating it\n    if return_tests_suite is True:\n        return_suite = copy.deepcopy(tests)\n\n    log.debug(f\"{self.name} - running test '{suite}', is dry run - '{dry_run}'\")\n    # run dry run task\n    if dry_run is True:\n        result = filtered_nornir.run(\n            task=nr_test, name=\"tests_dry_run\", ret_data_per_host=tests\n        )\n        ret.result = ResultSerializer(\n            result, to_dict=to_dict, add_details=add_details\n        )\n    # combine per-host tests in suites based on task and arguments\n    # Why - to run tests using any nornir service tasks with various arguments\n    else:\n        for host_name, host_tests in tests.items():\n            for test in host_tests:\n                dhash = hashlib.md5()\n                test_args = test.pop(\"norfab\", {})\n                nrtask = test_args.get(\"nrtask\", \"cli\")\n                assert nrtask in [\n                    \"cli\",\n                    \"network\",\n                    \"cfg\",\n                    \"task\",\n                ], f\"{self.name} - unsupported NorFab Nornir Service task '{nrtask}'\"\n                test_json = json.dumps(test_args, sort_keys=True).encode()\n                dhash.update(test_json)\n                test_hash = dhash.hexdigest()\n                suites.setdefault(test_hash, {\"params\": test_args, \"tests\": {}})\n                suites[test_hash][\"tests\"].setdefault(host_name, [])\n                suites[test_hash][\"tests\"][host_name].append(test)\n        log.debug(\n            f\"{self.name} - combined per-host tests suites based on NorFab Nornir Service task and arguments:\\n{suites}\"\n        )\n        # run test suites collecting output from devices\n        for tests_suite in suites.values():\n            nrtask = tests_suite[\"params\"].pop(\"nrtask\", \"cli\")\n            function_kwargs = {\n                **tests_suite[\"params\"],\n                **kwargs,\n                **filters,\n                \"tests\": tests_suite[\"tests\"],\n                \"remove_tasks\": remove_tasks,\n                \"failed_only\": failed_only,\n                \"subset\": subset,\n            }\n            result = getattr(self, nrtask)(\n                job=job, **function_kwargs\n            )  # returns Result object\n            # save test results into overall results\n            if to_dict == True:\n                for host_name, host_res in result.result.items():\n                    ret.result.setdefault(host_name, {})\n                    ret.result[host_name].update(host_res)\n                    # set return result failed to true if any of the tests failed\n                    for test_res in host_res.values():\n                        if add_details:\n                            if test_res[\"result\"] != \"PASS\":\n                                ret.failed = True\n                        elif test_res != \"PASS\":\n                            ret.failed = True\n            else:\n                ret.result.extend(result.result)\n                # set return result failed to true if any of the tests failed\n                if any(r[\"result\"] != \"PASS\" for r in result.result):\n                    ret.failed = True\n\n    # check if need to return tests suite content\n    if return_tests_suite is True:\n        ret.result = {\"test_results\": ret.result, \"suite\": return_suite}\n\n    return ret\n</code></pre>","tags":["nornir"]},{"location":"workers/workflow/api_reference_workers_workflow_worker/","title":"Workflow Worker","text":""},{"location":"workers/workflow/api_reference_workers_workflow_worker/#norfab.workers.workflow_worker.WorkflowWorker","title":"<code>WorkflowWorker(inventory: Any, broker: str, worker_name: str, exit_event=None, init_done_event=None, log_level: str = 'WARNING', log_queue: object = None)</code>","text":"<p>               Bases: <code>NFPWorker</code></p> <p>WorkflowWorker class for managing and executing workflows.</p> <p>This class extends the NFPWorker class and provides methods to load inventory, retrieve version information, manage workflow results, and execute workflows.</p> <p>Attributes:</p> Name Type Description <code>init_done_event</code> <code>Event</code> <p>Event to signal the completion of initialization.</p> <code>workflow_worker_inventory</code> <code>dict</code> <p>Inventory loaded from the broker.</p> <p>Parameters:</p> Name Type Description Default <code>inventory</code> <code>Any</code> <p>The inventory object to be used by the worker.</p> required <code>broker</code> <code>str</code> <p>The broker address.</p> required <code>worker_name</code> <code>str</code> <p>The name of the worker.</p> required <code>exit_event</code> <code>Event</code> <p>Event to signal the worker to exit. Defaults to None.</p> <code>None</code> <code>init_done_event</code> <code>Event</code> <p>Event to signal that initialization is done. Defaults to None.</p> <code>None</code> <code>log_level</code> <code>str</code> <p>The logging level. Defaults to \"WARNING\".</p> <code>'WARNING'</code> <code>log_queue</code> <code>object</code> <p>The logging queue. Defaults to None.</p> <code>None</code> Source code in <code>norfab\\workers\\workflow_worker.py</code> <pre><code>def __init__(\n    self,\n    inventory: Any,\n    broker: str,\n    worker_name: str,\n    exit_event=None,\n    init_done_event=None,\n    log_level: str = \"WARNING\",\n    log_queue: object = None,\n):\n    super().__init__(\n        inventory, broker, SERVICE, worker_name, exit_event, log_level, log_queue\n    )\n    self.init_done_event = init_done_event\n\n    # get inventory from broker\n    self.workflow_worker_inventory = self.load_inventory()\n\n    self.init_done_event.set()\n    log.info(f\"{self.name} - Started\")\n</code></pre>"},{"location":"workers/workflow/api_reference_workers_workflow_worker/#norfab.workers.workflow_worker.WorkflowWorker.get_version","title":"<code>get_version() -&gt; Result</code>","text":"<p>Generate a report of the versions of specific Python packages and system information.</p> <p>This method collects the version information of several Python packages and system details, including the Python version, platform, and a specified language model.</p> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An object containing a dictionary with the package names as keys and their     respective version numbers as values. If a package is not found, its version     will be an empty string.</p> Source code in <code>norfab\\workers\\workflow_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_version(self) -&gt; Result:\n    \"\"\"\n    Generate a report of the versions of specific Python packages and system information.\n\n    This method collects the version information of several Python packages and system details,\n    including the Python version, platform, and a specified language model.\n\n    Returns:\n        Result: An object containing a dictionary with the package names as keys and their\n                respective version numbers as values. If a package is not found, its version\n                will be an empty string.\n    \"\"\"\n    libs = {\n        \"norfab\": \"\",\n        \"python\": sys.version.split(\" \")[0],\n        \"platform\": sys.platform,\n    }\n    # get version of packages installed\n    for pkg in libs.keys():\n        try:\n            libs[pkg] = importlib.metadata.version(pkg)\n        except importlib.metadata.PackageNotFoundError:\n            pass\n\n    return Result(result=libs)\n</code></pre>"},{"location":"workers/workflow/api_reference_workers_workflow_worker/#norfab.workers.workflow_worker.WorkflowWorker.get_inventory","title":"<code>get_inventory() -&gt; Result</code>","text":"<p>NorFab task to retrieve the workflow's worker inventory.</p> <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>An instance of the Result class containing the workflow's worker inventory.</p> Source code in <code>norfab\\workers\\workflow_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"GET\"]})\ndef get_inventory(self) -&gt; Result:\n    \"\"\"\n    NorFab task to retrieve the workflow's worker inventory.\n\n    Returns:\n        Result: An instance of the Result class containing the workflow's worker inventory.\n    \"\"\"\n    return Result(result=self.workflow_worker_inventory)\n</code></pre>"},{"location":"workers/workflow/api_reference_workers_workflow_worker/#norfab.workers.workflow_worker.WorkflowWorker.remove_no_match_results","title":"<code>remove_no_match_results(results: Dict) -&gt; Dict</code>","text":"<p>Remove results from the workflow results for workers that did not have any resources matched by given task.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Dict</code> <p>The workflow results.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>The workflow results with empty results removed.</p> Source code in <code>norfab\\workers\\workflow_worker.py</code> <pre><code>def remove_no_match_results(self, results: Dict) -&gt; Dict:\n    \"\"\"\n    Remove results from the workflow results for workers that did not\n    have any resources matched by given task.\n\n    Args:\n        results (Dict): The workflow results.\n\n    Returns:\n        Dict: The workflow results with empty results removed.\n    \"\"\"\n    ret = {}\n    for step, task_results in results.items():\n        ret[step] = {}\n        for worker_name, worker_result in task_results.items():\n            # check results for tasks that did not fail\n            if worker_result[\"failed\"] is False:\n                if worker_result[\"status\"] != \"no_match\":\n                    ret[step][worker_name] = worker_result\n            # add failed tasks regardless of status content\n            else:\n                ret[step][worker_name] = worker_result\n    return ret\n</code></pre>"},{"location":"workers/workflow/api_reference_workers_workflow_worker/#norfab.workers.workflow_worker.WorkflowWorker.skip_step_check","title":"<code>skip_step_check(results: dict, step: str, data: dict) -&gt; Tuple[bool, str]</code>","text":"<p>Determines whether a step should be skipped based on the provided conditions.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict</code> <p>The results of previous steps.</p> required <code>step</code> <code>str</code> <p>The name of the current step.</p> required <code>data</code> <code>dict</code> <p>A dictionary containing conditions for skipping the step. Possible keys are:</p> <ul> <li>\"run_if_fail_any\": List of step names. Skip if any of the workers have failed these steps.</li> <li>\"run_if_pass_any\": List of step names. Skip if any of the workers have passed these steps.</li> <li>\"run_if_fail_all\": List of step names. Skip if all of the workers have failed these steps.</li> <li>\"run_if_pass_all\": List of step names. Skip if all of the workers have passed these steps.</li> </ul> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[bool, str]</code> <p>tuple of boolean status and message, if status is true step should be skipped.</p> Source code in <code>norfab\\workers\\workflow_worker.py</code> <pre><code>def skip_step_check(self, results: dict, step: str, data: dict) -&gt; Tuple[bool, str]:\n    \"\"\"\n    Determines whether a step should be skipped based on the provided conditions.\n\n    Args:\n        results (dict): The results of previous steps.\n        step (str): The name of the current step.\n        data (dict): A dictionary containing conditions for skipping the step. Possible keys are:\n\n            - \"run_if_fail_any\": List of step names. Skip if any of the workers have failed these steps.\n            - \"run_if_pass_any\": List of step names. Skip if any of the workers have passed these steps.\n            - \"run_if_fail_all\": List of step names. Skip if all of the workers have failed these steps.\n            - \"run_if_pass_all\": List of step names. Skip if all of the workers have passed these steps.\n\n    Returns:\n        tuple: tuple of boolean status and message, if status is true step should be skipped.\n    \"\"\"\n    if data.get(\"run_if_fail_any\"):\n        # check if have results for all needed steps\n        for k in data[\"run_if_fail_any\"]:\n            if k not in results:\n                return (\n                    \"error\",\n                    f\"run_if_fail_any check failed for '{step}', '{k}' results not found\",\n                )\n        # check if any of the steps failed\n        for step_name in data[\"run_if_fail_any\"]:\n            for worker_name, worker_result in results[step_name].items():\n                if (\n                    worker_result[\"failed\"] is True\n                    and worker_result[\"status\"] != \"no_match\"\n                ):\n                    # do not skip this step since one of the steps failed\n                    return False, None\n        else:\n            return (\n                True,  # skip this step since none of the steps failed\n                f\"Skipping {step}, no workers failed any of run_if_fail_any steps: {', '.join(data['run_if_fail_any'])}\",\n            )\n    if data.get(\"run_if_pass_any\"):\n        # check if have results for all needed steps\n        for k in data[\"run_if_pass_any\"]:\n            if k not in results:\n                return (\n                    \"error\",\n                    f\"run_if_pass_any check failed for '{step}', '{k}' results not found\",\n                )\n        # check if any of the steps passed\n        for step_name in data[\"run_if_pass_any\"]:\n            for worker_name, worker_result in results[step_name].items():\n                if (\n                    worker_result[\"failed\"] is False\n                    and worker_result[\"status\"] != \"no_match\"\n                ):\n                    # do not skip this step since one of the steps passed\n                    return False, None\n        else:\n            return (\n                True,  # skip this step since none of the steps passed\n                f\"Skipping {step}, no workers passed any of run_if_pass_any steps: {', '.join(data['run_if_pass_any'])}\",\n            )\n    if data.get(\"run_if_fail_all\"):\n        # check if have results for all needed steps\n        for k in data[\"run_if_fail_all\"]:\n            if k not in results:\n                return (\n                    \"error\",\n                    f\"run_if_fail_all check failed for '{step}', '{k}' results not found\",\n                )\n        # check if all workers failed the step(s)\n        for step_name in data[\"run_if_fail_all\"]:\n            for worker_name, worker_result in results[step_name].items():\n                if (\n                    worker_result[\"failed\"] is False\n                    and worker_result[\"status\"] != \"no_match\"\n                ):\n                    # skip this step since one of the workers passed this step\n                    return (\n                        True,\n                        f\"Skipping {step}, worker {worker_name} not failed one of run_if_fail_all steps: {step_name}.\",\n                    )\n        else:\n            # do not skip this step since all steps failed\n            return False, None\n    if data.get(\"run_if_pass_all\"):\n        # check if have results for all needed steps\n        for k in data[\"run_if_pass_all\"]:\n            if k not in results:\n                return (\n                    \"error\",\n                    f\"run_if_pass_all check failed for '{step}', '{k}' results not found\",\n                )\n        # check if all workers passed the step(s)\n        for step_name in data[\"run_if_pass_all\"]:\n            for worker_name, worker_result in results[step_name].items():\n                if (\n                    worker_result[\"failed\"] is True\n                    and worker_result[\"status\"] != \"no_match\"\n                ):\n                    # skip this step since one of the workers failed this step\n                    return (\n                        True,\n                        f\"Skipping {step}, worker {worker_name} not passed one of run_if_pass_all steps: {step_name}.\",\n                    )\n        else:\n            # do not skip this step since all steps passed\n            return False, None\n\n    return False, None  # do not skip this step\n</code></pre>"},{"location":"workers/workflow/api_reference_workers_workflow_worker/#norfab.workers.workflow_worker.WorkflowWorker.stop_workflow_check","title":"<code>stop_workflow_check(result: dict, step: str, data: dict) -&gt; bool</code>","text":"<p>Determines whether to stop the workflow based on the result of a specific step and provided data.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>dict</code> <p>The results dictionary for given step.</p> required <code>step</code> <code>str</code> <p>The specific step to check within the result.</p> required <code>data</code> <code>dict</code> <p>A dictionary containing step data, including a flag to stop if a failure occurs.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the workflow should be stopped due to a failure in the specified step and the stop_on_failure flag is set; otherwise, False.</p> Source code in <code>norfab\\workers\\workflow_worker.py</code> <pre><code>def stop_workflow_check(self, result: dict, step: str, data: dict) -&gt; bool:\n    \"\"\"\n    Determines whether to stop the workflow based on the result of\n    a specific step and provided data.\n\n    Args:\n        result (dict): The results dictionary for given step.\n        step (str): The specific step to check within the result.\n        data (dict): A dictionary containing step data, including a flag\n            to stop if a failure occurs.\n\n    Returns:\n        bool: True if the workflow should be stopped due to a failure in\n            the specified step and the stop_on_failure flag is set; otherwise, False.\n    \"\"\"\n    if data.get(\"stop_on_failure\") is True:\n        for worker_name, worker_result in result.items():\n            if worker_result[\"failed\"] is True:\n                return True  # stop the workflow since a failure occurred\n    return False\n</code></pre>"},{"location":"workers/workflow/api_reference_workers_workflow_worker/#norfab.workers.workflow_worker.WorkflowWorker.run","title":"<code>run(job: Job, workflow: Union[str, Dict]) -&gt; Result</code>","text":"<p>Executes a workflow defined by a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata.</p> required <code>workflow</code> <code>Union[str, Dict]</code> <p>The workflow to execute. This can be a URL to a YAML file.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Result</code> <p>A dictionary containing the results of the workflow execution.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the workflow is not a valid URL or dictionary.</p> Source code in <code>norfab\\workers\\workflow_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef run(self, job: Job, workflow: Union[str, Dict]) -&gt; Result:\n    \"\"\"\n    Executes a workflow defined by a dictionary.\n\n    Args:\n        job (Job): NorFab Job object containing relevant metadata.\n        workflow (Union[str, Dict]): The workflow to execute. This can be a URL to a YAML file.\n\n    Returns:\n        Dict: A dictionary containing the results of the workflow execution.\n\n    Raises:\n        ValueError: If the workflow is not a valid URL or dictionary.\n    \"\"\"\n    ret = Result(task=f\"{self.name}:run\", result={})\n\n    # load workflow from URL\n    if self.is_url(workflow):\n        workflow_name = (\n            os.path.split(workflow)[-1].replace(\".yaml\", \"\").replace(\".yml\", \"\")\n        )\n        workflow = self.fetch_file(workflow)\n        workflow = yaml.safe_load(workflow)\n\n    # extract workflow parameters\n    workflow_name = workflow.pop(\"name\", \"workflow\")\n    workflow_description = workflow.pop(\"description\", \"\")\n    remove_no_match_results = workflow.pop(\"remove_no_match_results\", True)\n\n    job.event(f\"Starting workflow '{workflow_name}'\")\n    log.info(f\"Starting workflow '{workflow_name}': {workflow_description}\")\n\n    ret.result[workflow_name] = {}\n\n    # run each step in the workflow\n    for step, data in workflow.items():\n        # check if need to skip step based on run_if_x flags\n        skip_status, message = self.skip_step_check(\n            ret.result[workflow_name], step, data\n        )\n        if skip_status is True:\n            ret.result[workflow_name][step] = {\n                \"all-workers\": {\n                    \"failed\": False,\n                    \"result\": None,\n                    \"status\": \"skipped\",\n                    \"task\": data[\"task\"],\n                    \"errors\": [],\n                    \"messages\": [message],\n                    \"juuid\": None,\n                }\n            }\n            job.event(\n                f\"Skipping workflow step '{step}', one of run_if_x conditions not satisfied\"\n            )\n            continue\n        # stop workflow execution on error\n        elif skip_status == \"error\":\n            ret.result[workflow_name][step] = {\n                \"all-workers\": {\n                    \"failed\": True,\n                    \"result\": None,\n                    \"status\": \"error\",\n                    \"task\": data[\"task\"],\n                    \"errors\": [message],\n                    \"messages\": [],\n                    \"juuid\": None,\n                }\n            }\n            job.event(message)\n            log.error(message)\n            break\n\n        job.event(f\"Doing workflow step '{step}'\")\n\n        ret.result[workflow_name][step] = self.client.run_job(\n            service=data[\"service\"],\n            task=data[\"task\"],\n            workers=data.get(\"workers\", \"all\"),\n            kwargs=data.get(\"kwargs\", {}),\n            args=data.get(\"args\", []),\n            timeout=data.get(\"timeout\", 600),\n        )\n\n        # check if need to stop workflow based on stop_if_fail flag\n        if (\n            self.stop_workflow_check(ret.result[workflow_name][step], step, data)\n            is True\n        ):\n            job.event(\n                f\"Stopping workflow, step '{step}' failed and has stop_if_fail flag\"\n            )\n            break\n\n    if remove_no_match_results:\n        ret.result[workflow_name] = self.remove_no_match_results(\n            ret.result[workflow_name]\n        )\n\n    return ret\n</code></pre>"},{"location":"workers/workflow/services_workflow_service/","title":"Workflow Service","text":"<p>The Workflow Service allows users to define workflows as YAML files, where each workflow consists of multiple steps. Each step represents a task to be executed, with support for conditional execution based on the results of other steps. This makes it easy to automate complex processes while maintaining control over execution flow.</p> <p> </p> <p>How it works:</p> <ol> <li>Client submit request to run workflow</li> <li>Workflow worker executes workflow steps interacting with NorFab services</li> <li>Results of workflow execution returned to client</li> </ol> <p>Key Features</p> <ul> <li>YAML-Based DSL: Simple and human-readable workflow definitions.</li> <li>Conditional Execution: Dynamically control step execution using conditions like <code>run_if_fail_all</code> or <code>run_if_pass_any</code> etc..</li> <li>Error Handling: Stop workflows on critical failures using <code>stop_on_failure</code>.</li> <li>Integration: Seamlessly integrates with other Norfab services like Nornir and Netbox.</li> <li>Scalability: Supports workflows with multiple steps and dependencies.</li> </ul>","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service/#workflow-service-tasks","title":"Workflow Service Tasks","text":"<p>Workflow Service supports a number of tasks to interact with NorFab service.</p> Task Description Use Cases run Run Workflow Automate multi-step processes with conditional logic.","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service/#workflow-language-specification","title":"Workflow Language Specification","text":"<p>The workflow language defines a structure for creating and executing workflows in Norfab. A workflow consists of multiple steps, each representing a task to be executed. Steps can include conditions to determine whether they should run based on the results of other steps.</p> <p>Notes:</p> <ol> <li>Steps are executed in the order they are defined in the workflow file.</li> <li>Conditional execution fields (run_if_fail_all, run_if_pass_all etc.) allow for dynamic control of step execution based on the results of previous steps.</li> <li>The kwargs field is highly flexible and allows passing task-specific arguments to customize behavior.</li> <li>This specification provides a clear structure for defining workflows, enabling complex automation scenarios with conditional step execution. </li> </ol>","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service/#top-level-fields","title":"Top-Level Fields","text":"<ul> <li><code>name</code> (string, required): The name of the workflow.</li> <li><code>description</code> (string, optional): A description of the workflow's purpose.</li> <li><code>any steps</code> (dict, required): A dictionary of steps, where each key is the step name and the value defines the step's configuration.</li> </ul>","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service/#step-configuration","title":"Step Configuration","text":"<p>Each step is defined as a dictionary with the following fields.</p> <p>Common FIelds</p> <ul> <li><code>service</code> (string, required): The service to be used for the step (e.g., <code>nornir</code>).</li> <li><code>task</code> (string, required): The task to be executed by the service (e.g., <code>cli</code>, <code>task</code>).</li> <li><code>kwargs</code> (dict, optional): A dictionary of arguments to pass to the task. These arguments are task-specific.</li> </ul> <p>Step Conditionals</p> <ul> <li><code>run_if_fail_any</code> (list, optional): A list of step names. The current step will only run if any of the specified steps have failed.</li> <li><code>run_if_pass_any</code> (list, optional): A list of step names. The current step will only run if any of the specified steps have passed.</li> <li><code>run_if_fail_all</code> (list, optional): A list of step names. The current step will only run if all the specified steps have failed.</li> <li><code>run_if_pass_all</code> (list, optional): A list of step names. The current step will only run if all the specified steps have passed.</li> <li><code>stop_on_failure</code> (bool, optional): If set to <code>True</code>, the execution of the entire workflow will stop if this step fails.</li> </ul>","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service/#example-workflow","title":"Example Workflow","text":"<pre><code>name: test_workflow_run_if_fail_all\ndescription: Sample Workflow to show what can be done\n\nstep1_failed:\n  service: nornir\n  task: task\n  kwargs:\n    FC: spine\n    plugin: nornir_salt.plugins.tasks.nr_test\n    excpt: True # raise exception\n\nstep2_failed:\n  service: nornir\n  task: task\n  kwargs:\n    FC: leaf\n    plugin: nornir_salt.plugins.tasks.nr_test\n    excpt: True # raise exception\n\nstep3_passed:\n  service: nornir\n  task: task\n  kwargs:\n    FC: spine,leaf\n    plugin: nornir_salt.plugins.tasks.nr_test\n    ret_data: True\n\nstep4-should-run:\n  service: nornir\n  task: cli\n  kwargs:\n    FC: leaf\n    commands:\n      - show hostname\n      - show ntp status\n  run_if_fail_all: \n    - step1_failed\n    - step2_failed\n\nstep5-should-not-run:\n  service: nornir\n  task: cli\n  kwargs:\n    FC: leaf\n    commands:\n      - show hostname\n      - show ntp status\n  run_if_fail_all: \n    - step1_failed\n    - step2_failed\n    - step3_passed\n</code></pre>","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service/#conditional-execution","title":"Conditional Execution","text":"","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service/#run_if_fail_all","title":"<code>run_if_fail_all</code>","text":"<p>Description: Specifies a list of steps. The current step will only execute if all the listed steps have failed.</p> <p>Use Case: Ensures that the step only runs if all workers failed execution of specified steps.</p> <p>Example:</p> <pre><code>run_if_fail_all:\n  - step1\n  - step2\n</code></pre>","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service/#run_if_pass_all","title":"<code>run_if_pass_all</code>","text":"<p>Description: Specifies a list of steps. The current step will only execute if all the listed steps have passed.</p> <p>Use Case: Ensures that the step only runs if all workers successfully executed specified steps.</p> <p>Example:</p> <pre><code>run_if_fail_all:\n  - step1\n  - step2\n</code></pre>","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service/#run_if_pass_any","title":"<code>run_if_pass_any</code>","text":"<p>Description: Specifies a list of steps. The current step will only execute if any of the listed steps have passed.</p> <p>Use Case: Useful when subsequent steps should only run if any of the specified steps succeed.</p> <p>Example:</p> <pre><code>run_if_pass_any:\n  - step1\n  - step2\n</code></pre>","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service/#run_if_fail_any","title":"<code>run_if_fail_any</code>","text":"<p>Description: Specifies a list of steps. The current step will only execute if any of the listed steps have failed.</p> <p>Use Case: Useful when subsequent steps depend on the success of at least one worker in the specified steps.</p> <p>Example:</p> <pre><code>run_if_fail_any:\n  - step1\n  - step2\n</code></pre>","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service/#stop_on_failure","title":"<code>stop_on_failure</code>","text":"<p>Description: A flag that, when set to <code>True</code>, stops the execution of the entire workflow if the current step fails.</p> <p>Use Case: Useful for workflows where failure in a critical step should halt further execution to prevent cascading errors or unnecessary processing.</p> <p>Example:</p> <pre><code>stop_on_failure: True\n</code></pre>","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service/#workflow-service-shell-show-commands","title":"Workflow Service Shell Show Commands","text":"<p>Workflow service shell comes with this set of show commands to query various information:</p> <pre><code>nf#man tree show.workflow\nroot\n\u2514\u2500\u2500 show:    NorFab show commands\n    \u2514\u2500\u2500 workflow:    Show Workflow service\n        \u251c\u2500\u2500 inventory:    Show workflow workers inventory data\n        \u2502   \u251c\u2500\u2500 timeout:    Job timeout\n        \u2502   \u2514\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u2514\u2500\u2500 version:    Show workflow service workers version report\n            \u251c\u2500\u2500 timeout:    Job timeout\n            \u2514\u2500\u2500 workers:    Filter worker to target, default 'all'\nnf#\n</code></pre>","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service_inventory/","title":"Workflow Worker Inventory","text":"<p>Content of <code>inventory.yaml</code> need to be updated to include workflow worker details:</p> inventory.yaml<pre><code>broker: \n  endpoint: \"tcp://127.0.0.1:5555\" \n  shared_key: \"5z1:yW}]n?UXhGmz+5CeHN1&gt;:S9k!eCh6JyIhJqO\"\n\nworkers:\n  workflow-worker-1: \n    - workflow/workflow-worker-1.yaml\n\ntopology: \n  workers: \n    - workflow-worker-1\n</code></pre> <p>To obtain broker <code>shared_key</code> run this command on broker:</p> <pre><code>cd &lt;path/to/broker/inventory.yaml&gt;\nnfcli --show-broker-shared-key\n</code></pre> <p>Sample workflow worker inventory definition</p> workflowworkflow-worker-1.yaml<pre><code>service: workflow\n</code></pre>"},{"location":"workers/workflow/services_workflow_service_tasks_run/","title":"Workflow Service Run Task","text":"<p>task api name: <code>run</code></p> <p>Run workflow defined using YAML file.</p>","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service_tasks_run/#workflow-sample-usage","title":"Workflow Sample Usage","text":"<p>Workflow service <code>run</code> task uses YAML formatted files to execute workflow steps:</p> workflow-1.yaml<pre><code>name: workflow_1\ndescription: Sample workflow with two steps.\n\nstep1:\n  service: nornir\n  task: cli\n  kwargs:\n    FC: spine\n    commands:\n      - show version\n      - show ip int brief\n\nstep2:\n  service: nornir\n  task: cli\n  kwargs:\n    FC: leaf\n    commands:\n      - show hostname\n      - show ntp status\n</code></pre> <p>File <code>workflow-1.yaml</code> stored on broker and downloaded by Workflow service prior to running steps, below is an example of how to run the the workflow.</p> <p>Example</p> CLIPython <p><pre><code>C:\\nf&gt;nfcli\nWelcome to NorFab Interactive Shell.\nnf#\nnf#workflow run workflow nf://workflow/workflow-1.yaml\n--------------------------------------------- Job Events -----------------------------------------------\n05-Apr-2025 21:34:53.846 d1634ce3dc764f56ac00971950a033cc job started\n05-Apr-2025 21:34:53.883 INFO workflow-worker-1 running workflow.run  - Starting workflow 'workflow_1'\n05-Apr-2025 21:34:53.883 INFO workflow-worker-1 running workflow.run  - Doing workflow step 'step1'\n05-Apr-2025 21:34:55.008 INFO workflow-worker-1 running workflow.run  - Doing workflow step 'step2'\n05-Apr-2025 21:34:56.557 d1634ce3dc764f56ac00971950a033cc job completed in 2.711 seconds\n\n--------------------------------------------- Job Results --------------------------------------------\n\nworkflow-worker-1:\n    ----------\n    workflow_1:\n        ----------\n        step1:\n            ----------\n            nornir-worker-1:\n                ----------\n                task:\n                    nornir-worker-1:cli\n                failed:\n                    False\n                errors:\n                result:\n                    ----------\n                    ceos-spine-2:\n                        ----------\n                        show version:\n                            Arista cEOSLab\n                            Hardware version:\n                            Serial number: 8B7EBC67A4FA6C48F1D1BCC5438866A7\n                            Hardware MAC address: 001c.73ab.5167\n                            System MAC address: 001c.73ab.5167\n\n                            Software image version: 4.30.0F-31408673.4300F (engineering build)\n                            Architecture: x86_64\n                            Internal build version: 4.30.0F-31408673.4300F\n                            Internal build ID: a35f0dc7-2d65-4f2a-a010-279cf445fd8c\n                            Image format version: 1.0\n                            Image optimization: None\n\n                            cEOS tools version: (unknown)\n                            Kernel version: 5.15.0-136-generic\n\n                            Uptime: 1 hour and 17 minutes\n                            Total memory: 32827264 kB\n                            Free memory: 19525528 kB\n                        show ip int brief:\n                                                                                                              Address\n                            Interface         IP Address              Status       Protocol            MTU    Owner\n                            ----------------- ----------------------- ------------ -------------- ----------- -------\n                            Loopback0         unassigned              up           up                65535\n                            Loopback123       unassigned              up           up                65535\n                            Management0       172.100.100.11/24       up           up                 1500\n                    ceos-spine-1:\n                        ----------\n                        show version:\n                            Arista cEOSLab\n                            Hardware version:\n                            Serial number: 7B5E3CF8CB9A6DE53FB8411896DE476F\n                            Hardware MAC address: 001c.730a.5369\n                            System MAC address: 001c.730a.5369\n\n                            Software image version: 4.30.0F-31408673.4300F (engineering build)\n                            Architecture: x86_64\n                            Internal build version: 4.30.0F-31408673.4300F\n                            Internal build ID: a35f0dc7-2d65-4f2a-a010-279cf445fd8c\n                            Image format version: 1.0\n                            Image optimization: None\n\n                            cEOS tools version: (unknown)\n                            Kernel version: 5.15.0-136-generic\n\n                            Uptime: 1 hour and 17 minutes\n                            Total memory: 32827264 kB\n                            Free memory: 19525528 kB\n                        show ip int brief:\n                                                                                                              Address\n                            Interface         IP Address              Status       Protocol            MTU    Owner\n                            ----------------- ----------------------- ------------ -------------- ----------- -------\n                            Loopback0         unassigned              up           up                65535\n                            Loopback123       unassigned              up           up                65535\n                            Management0       172.100.100.10/24       up           up                 1500\n                messages:\n                juuid:\n                    0bef61db66cf46318735e02bdb0389c0\n                status:\n                    completed\n        step2:\n            ----------\n            nornir-worker-2:\n                ----------\n                task:\n                    nornir-worker-2:cli\n                failed:\n                    False\n                errors:\n                result:\n                    ----------\n                    ceos-leaf-2:\n                        ----------\n                        show hostname:\n                            Hostname: ceos-leaf-2\n                            FQDN:     ceos-leaf-2\n                            unsynchronised\n                            poll interval unknown\n                    ceos-leaf-1:\n                        ----------\n                        show hostname:\n                            Hostname: ceos-leaf-1\n                            FQDN:     ceos-leaf-1\n                        show ntp status:\n                            unsynchronised\n                            poll interval unknown\n                    ceos-leaf-3:\n                        ----------\n                        show hostname:\n                            Hostname: ceos-leaf-3\n                            FQDN:     ceos-leaf-3\n                        show ntp status:\n                            unsynchronised\n                            poll interval unknown\n                messages:\n                juuid:\n                    972287e3abc94e86acfff99b54940ef9\n                status:\n                    completed\nnf#\n</code></pre> In this example:</p> <ul> <li><code>nfcli</code> command starts the NorFab Interactive Shell.</li> <li><code>workflow run</code> command runs <code>workflow-1.yaml</code> workflow.</li> </ul> <p>This code is complete and can run as is</p> <pre><code>import pprint\n\nfrom norfab.core.nfapi import NorFab\n\nif __name__ == '__main__':\n    nf = NorFab(inventory=\"inventory.yaml\")\n    nf.start()\n\n    client = nf.make_client()\n\n    res = client.run_job(\n        service=\"workflow\",\n        task=\"run\",\n        kwargs={\n            \"workflow\": \"nf://workflow/workflow-1.yaml\",\n        }\n    )\n\n    pprint.pprint(res)\n\n    nf.destroy()\n</code></pre>","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service_tasks_run/#norfab-workflow-test-shell-reference","title":"NORFAB Workflow Test Shell Reference","text":"<p>NorFab shell supports these command options for workflow <code>run</code> task:</p> <pre><code>nf#man tree workflow\nroot\n\u2514\u2500\u2500 workflow:    Workflow service\n    \u2514\u2500\u2500 run:    Run workflows\n        \u251c\u2500\u2500 timeout:    Job timeout\n        \u251c\u2500\u2500 workers:    Filter worker to target, default 'all'\n        \u251c\u2500\u2500 workflow:    Workflow to run\n        \u2514\u2500\u2500 progress:    Display progress events, default 'True'\nnf#\n</code></pre> <p><code>*</code> - mandatory/required command argument</p>","tags":["workflow"]},{"location":"workers/workflow/services_workflow_service_tasks_run/#python-api-reference","title":"Python API Reference","text":"<p>Executes a workflow defined by a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>NorFab Job object containing relevant metadata.</p> required <code>workflow</code> <code>Union[str, Dict]</code> <p>The workflow to execute. This can be a URL to a YAML file.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Result</code> <p>A dictionary containing the results of the workflow execution.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the workflow is not a valid URL or dictionary.</p> Source code in <code>norfab\\workers\\workflow_worker.py</code> <pre><code>@Task(fastapi={\"methods\": [\"POST\"]})\ndef run(self, job: Job, workflow: Union[str, Dict]) -&gt; Result:\n    \"\"\"\n    Executes a workflow defined by a dictionary.\n\n    Args:\n        job (Job): NorFab Job object containing relevant metadata.\n        workflow (Union[str, Dict]): The workflow to execute. This can be a URL to a YAML file.\n\n    Returns:\n        Dict: A dictionary containing the results of the workflow execution.\n\n    Raises:\n        ValueError: If the workflow is not a valid URL or dictionary.\n    \"\"\"\n    ret = Result(task=f\"{self.name}:run\", result={})\n\n    # load workflow from URL\n    if self.is_url(workflow):\n        workflow_name = (\n            os.path.split(workflow)[-1].replace(\".yaml\", \"\").replace(\".yml\", \"\")\n        )\n        workflow = self.fetch_file(workflow)\n        workflow = yaml.safe_load(workflow)\n\n    # extract workflow parameters\n    workflow_name = workflow.pop(\"name\", \"workflow\")\n    workflow_description = workflow.pop(\"description\", \"\")\n    remove_no_match_results = workflow.pop(\"remove_no_match_results\", True)\n\n    job.event(f\"Starting workflow '{workflow_name}'\")\n    log.info(f\"Starting workflow '{workflow_name}': {workflow_description}\")\n\n    ret.result[workflow_name] = {}\n\n    # run each step in the workflow\n    for step, data in workflow.items():\n        # check if need to skip step based on run_if_x flags\n        skip_status, message = self.skip_step_check(\n            ret.result[workflow_name], step, data\n        )\n        if skip_status is True:\n            ret.result[workflow_name][step] = {\n                \"all-workers\": {\n                    \"failed\": False,\n                    \"result\": None,\n                    \"status\": \"skipped\",\n                    \"task\": data[\"task\"],\n                    \"errors\": [],\n                    \"messages\": [message],\n                    \"juuid\": None,\n                }\n            }\n            job.event(\n                f\"Skipping workflow step '{step}', one of run_if_x conditions not satisfied\"\n            )\n            continue\n        # stop workflow execution on error\n        elif skip_status == \"error\":\n            ret.result[workflow_name][step] = {\n                \"all-workers\": {\n                    \"failed\": True,\n                    \"result\": None,\n                    \"status\": \"error\",\n                    \"task\": data[\"task\"],\n                    \"errors\": [message],\n                    \"messages\": [],\n                    \"juuid\": None,\n                }\n            }\n            job.event(message)\n            log.error(message)\n            break\n\n        job.event(f\"Doing workflow step '{step}'\")\n\n        ret.result[workflow_name][step] = self.client.run_job(\n            service=data[\"service\"],\n            task=data[\"task\"],\n            workers=data.get(\"workers\", \"all\"),\n            kwargs=data.get(\"kwargs\", {}),\n            args=data.get(\"args\", []),\n            timeout=data.get(\"timeout\", 600),\n        )\n\n        # check if need to stop workflow based on stop_if_fail flag\n        if (\n            self.stop_workflow_check(ret.result[workflow_name][step], step, data)\n            is True\n        ):\n            job.event(\n                f\"Stopping workflow, step '{step}' failed and has stop_if_fail flag\"\n            )\n            break\n\n    if remove_no_match_results:\n        ret.result[workflow_name] = self.remove_no_match_results(\n            ret.result[workflow_name]\n        )\n\n    return ret\n</code></pre>","tags":["workflow"]}]}